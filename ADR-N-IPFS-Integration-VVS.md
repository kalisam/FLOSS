# ADR-N: IPFS Large File Integration for VVS-Compliant Git Repositories

**Date**: 2025-11-11  
**Status**: Accepted ‚úÖ  
**Context**: GitHub 100MB limit blocks FLOSS access; Git LFS anti-pattern  
**Authors**: Human + Claude Sonnet 4.5  
**Related**: ADR-0 (Recognition Protocol), VVS Spec v1.0, Symbolic-First Core

---

## üéØ Intent Echo

Enable cryptographically verified, VVS-compliant large file distribution via IPFS without breaking FLOSS access principles or GitHub's 100MB limit.

---

## üìä Multi-Lens Snapshot

### Practical/Engineering
**Constraints**:
- GitHub 100MB hard limit; Git LFS requires special tooling
- Need to distribute model weights, datasets, media >100MB
- Must support standard HTTP access (no special software)
- Integration with existing ARF git repos + Holochain DNA

**Technical Requirements**:
- IPFS CID-based content addressing
- Multi-gateway redundancy (no single point of failure)
- Cryptographic integrity verification (SHA256 + BLAKE3)
- Git-compatible pointer files (JSON, <1KB)
- CI/CD validation pipeline

### Critical/Red-Team
**Risks Identified**:
- ‚ùå **Availability**: IPFS files disappear without active pinning
- ‚ùå **Gateway reliability**: Public gateways can be slow/censored/malicious
- ‚ùå **Trust**: Pointer files could reference tampered content
- ‚ùå **Centralization creep**: Paid pinning services ‚Üí vendor lock-in
- ‚ùå **Spam/abuse**: Anyone could upload large files

**Mitigations**:
- ‚úÖ **Multi-pinning**: Require ‚â•2 pinning proofs (personal + services)
- ‚úÖ **Hash verification**: Dual hashing (SHA256 + BLAKE3) enforced
- ‚úÖ **Gateway redundancy**: 3+ gateways per file with fallback
- ‚úÖ **Budget accounting**: RU (Risk Units) limit upload spam
- ‚úÖ **Signature chain**: Every upload cryptographically signed

### Values (Love-Light-Knowledge)
**Accessibility**: 
- ‚úÖ Anyone with curl/browser can download (no IPFS daemon required)
- ‚úÖ Pointer files are human-readable JSON
- ‚úÖ No paywalls, no surveillance, no tracking

**Transparency**:
- ‚úÖ Full provenance chain (who uploaded, when, why)
- ‚úÖ License verification enforced (FOSS-only)
- ‚úÖ All proofs publicly verifiable

**Sovereignty**:
- ‚úÖ No vendor lock-in (self-hostable)
- ‚úÖ Forkable at any time
- ‚úÖ Users can pin files themselves

**Reciprocity**:
- ‚úÖ Contributors can self-host
- ‚úÖ Community members can help pin
- ‚úÖ Mutual credit for pinning services (future)

**Dignity**:
- ‚úÖ No forced surveillance
- ‚úÖ User controls their keys
- ‚úÖ Optional telemetry only

### Systems/Governance
**Provenance**: 
- Ed25519 signatures on all uploads
- Agent pubkey linked to Holochain identity
- Timestamp + tool version tracked

**Verification**:
- Integrity zome enforces validation rules
- Hash mismatches rejected automatically
- Invalid signatures ‚Üí entry denied

**Lifecycle**:
- Uploader responsible for initial pinning
- Community can opt-in to co-pinning
- Garbage collection after N months of inactivity (future)

**Rights**:
- License field enforced (FOSS-only allowlist)
- Derivative works tracked via `derived_from` triples
- License compatibility checked automatically

**Auditability**:
- All operations logged in audit trail
- Public signals for indexing
- No private access logs (privacy-preserving)

---

## üîÑ Decision Evolution (5 Passes)

### Pass 1: Naive IPFS Integration ‚ùå
**Approach**: Simple JSON pointer files with CID + gateways

**Why Rejected**:
- No integrity verification (could serve wrong file)
- No provenance tracking (who, when, why?)
- No pinning strategy (availability not guaranteed)
- No license enforcement
- Not VVS-compliant (no cryptographic proof)
- No budget accounting

### Pass 2: Add Cryptographic Proof ‚ö†Ô∏è
**Improvements**:
- SHA256 + BLAKE3 dual hashing
- Ed25519 signatures on manifest
- Provenance tracking (uploader, timestamp)
- Multi-gateway redundancy
- License metadata

**Remaining Issues**:
- Not integrated with VVS autonomy kernel
- No symbolic validation (just hashes)
- No budget accounting
- No Holochain DHT integration

### Pass 3: Symbolic Validation + Knowledge Graph ‚ö†Ô∏è
**Improvements**:
- `FileArtifact` Holochain entry type
- Knowledge triples describe files semantically
- Ontology-based validation (artifact types, relationships)
- Model card verification for model weights
- Git hooks for pre-commit validation

**Remaining Issues**:
- No budget accounting (RU consumption)
- No autonomy kernel guards
- No proof-carrying code for tools
- Manual pinning evidence collection

### Pass 4: VVS Autonomy Kernel Integration ‚öôÔ∏è
**Improvements**:
- Budget accounting (RU per operation)
- Guarded operations (pre/post checks)
- Proof-carrying upload tool
- Automated pinning to multiple services
- CLI tools with cryptographic signing
- CI/CD verification pipeline

**Remaining Concerns**:
- Gateway availability monitoring needed
- Automated re-pinning for old files
- Cost estimation before upload

### Pass 5: Final VVS Verification ‚úÖ
**Complete Solution**:
- ‚úÖ All VVS principles satisfied (Virtual, Verifiable, Self-Governing)
- ‚úÖ Cryptographic proof at every layer
- ‚úÖ Symbolic-first validation
- ‚úÖ Budget-constrained autonomy
- ‚úÖ No human gatekeepers
- ‚úÖ Fully forkable
- ‚úÖ FLOSS-compatible
- ‚úÖ Production-ready roadmap

---

## üèóÔ∏è Architecture

### Data Model (Holochain Integrity Zome)

```rust
#[hdk_entry_helper]
pub struct FileArtifact {
    // IDENTITY
    pub filename: String,
    pub description: String,
    pub ipfs_cid: String,
    pub size_bytes: u64,
    
    // CRYPTOGRAPHIC INTEGRITY
    pub sha256: String,           // 64 hex chars
    pub blake3: String,           // Faster hash for verification
    
    // SEMANTIC METADATA
    pub artifact_type: ArtifactType,
    pub associated_triples: Vec<ActionHash>,
    
    // PROVENANCE
    pub uploader: AgentPubKey,
    pub uploaded_at: Timestamp,
    pub derivation: FileDerivation,
    
    // LICENSING
    pub license: String,           // Must be FOSS-approved
    pub license_proof: String,
    
    // AVAILABILITY
    pub gateways: Vec<String>,
    pub pinning_evidence: Vec<PinningProof>,
}

pub enum ArtifactType {
    ModelWeights { model_id: String, version: String },
    Dataset { format: String, schema_hash: Option<String> },
    Media { mime_type: String },
    Documentation { format: String },
}

pub struct PinningProof {
    pub service: String,
    pub proof_type: PinProofType,
    pub timestamp: Timestamp,
}

pub enum PinProofType {
    DHTProviderRecord { peer_id: String },
    PinningServiceReceipt { pin_id: String, api_response: String },
    ReplicationProof { replica_peers: Vec<String> },
}
```

### Validation Rules (Enforced at DHT Level)

```rust
#[hdk_extern]
pub fn validate(op: Op) -> ExternResult<ValidateCallbackResult> {
    if let Op::StoreEntry(store_entry) = op {
        if let Ok(artifact) = FileArtifact::try_from(store_entry.entry.clone()) {
            
            // RULE 1: License must be FOSS-approved
            let valid_licenses = vec![
                "MIT", "Apache-2.0", "GPL-3.0", "BSD-3-Clause",
                "MPL-2.0", "CC-BY-4.0", "CC0-1.0"
            ];
            if !valid_licenses.contains(&artifact.license.as_str()) {
                return Ok(ValidateCallbackResult::Invalid(
                    format!("License '{}' not FOSS-approved", artifact.license)
                ));
            }
            
            // RULE 2: IPFS CID must be valid
            if !artifact.ipfs_cid.starts_with("Qm") && 
               !artifact.ipfs_cid.starts_with("bafy") {
                return Ok(ValidateCallbackResult::Invalid(
                    "Invalid IPFS CID format".into()
                ));
            }
            
            // RULE 3: SHA256 must be 64 hex chars
            if artifact.sha256.len() != 64 {
                return Ok(ValidateCallbackResult::Invalid(
                    "SHA256 must be 64 hex chars".into()
                ));
            }
            
            // RULE 4: ModelWeights must reference valid ModelCard
            if let ArtifactType::ModelWeights { model_id, .. } = &artifact.artifact_type {
                if !verify_model_card_exists(model_id)? {
                    return Ok(ValidateCallbackResult::Invalid(
                        format!("ModelCard not found: {}", model_id)
                    ));
                }
            }
            
            // RULE 5: Require ‚â•2 pinning proofs
            if artifact.pinning_evidence.len() < 2 {
                return Ok(ValidateCallbackResult::Invalid(
                    "Requires at least 2 pinning proofs".into()
                ));
            }
            
            return Ok(ValidateCallbackResult::Valid);
        }
    }
    Ok(ValidateCallbackResult::Valid)
}
```

### Budget Accounting

```rust
pub fn get_file_operation_cost(op: &FileOperation, context: &Context) -> f32 {
    let base_cost = match op {
        FileOperation::PublishArtifact { size_bytes, .. } => {
            // 1 RU per 100MB
            (size_bytes as f32 / 100_000_000.0).max(1.0)
        },
        FileOperation::UpdateMetadata => 0.5,
        FileOperation::AddPinningProof => 0.3,
    };
    
    // Risk multipliers
    let mut multiplier = 1.0;
    
    if context.upload_count < 5 { multiplier *= 1.5; }        // New uploader
    if size_bytes > 1_000_000_000 { multiplier *= 2.0; }     // >1GB file
    if context.pinning_proof_count < 2 { multiplier *= 1.3; } // Low redundancy
    if context.license_verified { multiplier *= 0.8; }        // Verified license
    
    base_cost * multiplier
}
```

### Guarded Upload Operation

```rust
#[hdk_extern]
pub fn publish_file_artifact_guarded(
    input: PublishFileInput
) -> ExternResult<ActionHash> {
    let agent = agent_info()?.agent_latest_pubkey;
    let context = build_context(&input, &agent)?;
    let ru_cost = get_file_operation_cost(&input.operation, &context);
    
    // GUARD: Pre-execution checks + budget accounting
    guarded(&agent, "publish_file_artifact", &context, ru_cost, || {
        // 1. Symbolic validation
        validate_artifact_semantics(&input)?;
        
        // 2. Verify IPFS availability
        verify_ipfs_reachable(&input.ipfs_cid, &input.gateways)?;
        
        // 3. Verify pinning proofs
        verify_pinning_evidence(&input.pinning_evidence)?;
        
        // 4. Create artifact entry
        let artifact_hash = create_entry(&FileArtifact { /* ... */ })?;
        
        // 5. Generate semantic triples
        let triples = generate_artifact_triples(&artifact, artifact_hash)?;
        for triple in triples {
            create_entry(&triple)?;
        }
        
        // 6. Emit signal for indexing
        emit_signal(FileArtifactPublished { artifact_hash, /* ... */ })?;
        
        Ok(artifact_hash)
    })
}
```

---

## üõ†Ô∏è Implementation

### Phase 1: Core Infrastructure (Week 1-2)

#### Holochain DNA
```bash
# Create zomes
cd rose-forest-dna
mkdir -p zomes/integrity/file_artifacts
mkdir -p zomes/coordinator/file_artifacts

# Implement validation + operations
# See detailed code in Pass 3/4 documents
```

#### CLI Tools

**Upload Tool** (`tools/arf-ipfs-upload`):
```bash
#!/bin/bash
# Usage: arf-ipfs-upload <file> <description> [license]

FILE="$1"
DESCRIPTION="$2"
LICENSE="${3:-MIT}"

# 1. Calculate hashes
SHA256=$(sha256sum "$FILE" | cut -d' ' -f1)
BLAKE3=$(b3sum "$FILE" | cut -d' ' -f1)

# 2. Upload to IPFS
CID=$(ipfs add --quieter --pin=true "$FILE")

# 3. Pin to multiple services
PINNING_PROOFS=$(pin_to_services "$CID")

# 4. Generate proof envelope
ENVELOPE=$(generate_proof_envelope \
    --file "$FILE" \
    --cid "$CID" \
    --sha256 "$SHA256" \
    --blake3 "$BLAKE3" \
    --license "$LICENSE" \
    --pinning-proofs "$PINNING_PROOFS")

# 5. Sign envelope
sign_envelope "$ENVELOPE" > "large_files/$(basename $FILE).ipfs"

echo "‚úÖ Upload complete! CID: $CID"
```

**Download Tool** (`tools/arf-ipfs-download`):
```bash
#!/bin/bash
# Usage: arf-ipfs-download <pointer-file>

POINTER="$1"

# 1. Verify signature
verify_signature "$POINTER" || exit 1

# 2. Extract metadata
CID=$(jq -r '.claims.ipfs_cid' "$POINTER")
SHA256=$(jq -r '.claims.sha256' "$POINTER")
GATEWAYS=($(jq -r '.claims.gateways[]' "$POINTER"))

# 3. Download from IPFS (try local daemon first)
if ipfs get "$CID" -o "$FILENAME" 2>/dev/null; then
    echo "‚úì Downloaded via local IPFS"
else
    # Fallback to gateways
    for GATEWAY in "${GATEWAYS[@]}"; do
        if curl -fL -o "$FILENAME" "$GATEWAY"; then
            echo "‚úì Downloaded via $GATEWAY"
            break
        fi
    done
fi

# 4. Verify hash
ACTUAL_SHA256=$(sha256sum "$FILENAME" | cut -d' ' -f1)
if [ "$ACTUAL_SHA256" != "$SHA256" ]; then
    echo "‚úó Hash mismatch!"
    exit 1
fi

echo "‚úÖ File verified and downloaded"
```

### Phase 2: Knowledge Graph Integration (Week 3-4)

**Artifact Ontology**:
```turtle
# Types
:ModelWeights rdf:type owl:Class .
:Dataset rdf:type owl:Class .
:Media rdf:type owl:Class .

# Relationships
:implements rdf:type owl:ObjectProperty ;
    rdfs:domain :ModelWeights ;
    rdfs:range :NeuralNetworkArchitecture .

:trained_on rdf:type owl:ObjectProperty ;
    rdfs:domain :ModelWeights ;
    rdfs:range :Dataset .

:has_accuracy rdf:type owl:DatatypeProperty ;
    rdfs:domain :ModelWeights ;
    rdfs:range xsd:float .

# Axioms
IF (?m implements ?arch) AND (?arch has_property ?p)
THEN (?m has_property ?p)
```

**Triple Generation**:
```rust
fn generate_artifact_triples(
    artifact: &FileArtifact,
    artifact_hash: ActionHash
) -> ExternResult<Vec<KnowledgeTriple>> {
    vec![
        // Type triple
        KnowledgeTriple {
            subject: format!("file:{}", artifact.ipfs_cid),
            predicate: "is_a".into(),
            object: artifact_type_name,
            confidence: 1.0,
            derivation: TripleDerivation::HumanAsserted { /* ... */ },
            license: artifact.license.clone(),
        },
        // Implementation triple (for ModelWeights)
        KnowledgeTriple {
            subject: format!("file:{}", artifact.ipfs_cid),
            predicate: "implements".into(),
            object: model_id,
            confidence: 1.0,
            derivation: TripleDerivation::LogicalInference { /* ... */ },
            license: artifact.license.clone(),
        },
        // ... more triples
    ]
}
```

### Phase 3: Git Integration

**.gitignore**:
```
# Ignore large binary files
large_files/*.bin
large_files/*.h5
large_files/*.ckpt
large_files/*.safetensors
large_files/*.tar.gz

# Track IPFS pointer files
!large_files/*.ipfs
```

**Pre-commit Hook** (`.git/hooks/pre-commit`):
```bash
#!/bin/bash
# Validate all IPFS pointer files before commit

for pointer in large_files/*.ipfs; do
    # Validate JSON structure
    jq empty "$pointer" || {
        echo "Error: Invalid JSON in $pointer"
        exit 1
    }
    
    # Verify required fields
    jq -e '.claims.ipfs_cid' "$pointer" > /dev/null || exit 1
    jq -e '.claims.sha256' "$pointer" > /dev/null || exit 1
    jq -e '.signature' "$pointer" > /dev/null || exit 1
    
    # Check pinning evidence
    PINS=$(jq '.proofs[] | select(.PinningProof) | .PinningProof.pinning_evidence | length' "$pointer")
    if [ "$PINS" -lt 2 ]; then
        echo "Error: $pointer requires ‚â•2 pinning proofs"
        exit 1
    fi
    
    # Verify signature
    tools/arf-ipfs-verify "$pointer" || exit 1
done

echo "‚úÖ All IPFS pointers verified"
```

**GitHub Actions** (`.github/workflows/verify-ipfs.yml`):
```yaml
name: Verify IPFS Pointers

on: [push, pull_request]

jobs:
  verify:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq openssl
          cargo install b3sum
      
      - name: Verify all pointers
        run: |
          for pointer in large_files/*.ipfs; do
            echo "Verifying: $pointer"
            tools/arf-ipfs-verify "$pointer" || exit 1
          done
      
      - name: Check IPFS availability
        run: |
          for pointer in large_files/*.ipfs; do
            CID=$(jq -r '.claims.ipfs_cid' "$pointer")
            # Try fetching from gateways (HEAD request)
            curl -I "https://ipfs.io/ipfs/$CID" || \
            curl -I "https://dweb.link/ipfs/$CID" || {
              echo "Warning: $CID may not be available"
            }
          done
```

---

## üìä VVS Compliance Matrix

| VVS Principle | Implementation | Status |
|---------------|----------------|--------|
| **Virtual** | | |
| No human gatekeepers | Integrity zome auto-validates | ‚úÖ |
| Automated execution | Budget engine + guards | ‚úÖ |
| Tool autonomy | Proof-carrying CLI tools | ‚úÖ |
| Auto-halt | Budget exhaustion ‚Üí stop | ‚úÖ |
| Signal coordination | Events trigger actions | ‚úÖ |
| **Verifiable** | | |
| Cryptographic proof | Ed25519 signatures | ‚úÖ |
| Hash integrity | SHA256 + BLAKE3 | ‚úÖ |
| Proof envelopes | Tools carry claims+proofs | ‚úÖ |
| Provenance tracking | Full lineage recorded | ‚úÖ |
| Audit trail | All ops logged | ‚úÖ |
| License verification | FOSS compliance enforced | ‚úÖ |
| **Self-Governing** | | |
| Rule enforcement | Integrity validation | ‚úÖ |
| Budget limits | RU accounting | ‚úÖ |
| Symbolic constraints | Knowledge graph | ‚úÖ |
| Forkability | Anyone can fork DNA | ‚úÖ |
| No god keys | No override mechanism | ‚úÖ |
| Monotonic evolution | Versioned rule changes | ‚úÖ |

---

## üéØ Success Criteria

### Immediate (Week 1-2)
- [ ] Upload 1GB file via CLI tool
- [ ] Pointer file validates in CI
- [ ] Download verifies hash integrity
- [ ] Multiple gateways work with fallback

### Short-term (Week 3-6)
- [ ] Knowledge triples auto-generated
- [ ] Budget accounting tracks RU
- [ ] Multi-service pinning working
- [ ] Git hooks prevent invalid commits

### Long-term (Week 7-10)
- [ ] 99%+ availability for active files
- [ ] <5s download initiation
- [ ] Zero invalid artifacts in DHT
- [ ] 10+ community contributors using it

---

## üöÄ Next Actions

### Week 1 (Immediate)
1. Create `file_artifacts` integrity zome
2. Implement core validation rules
3. Write unit tests for validation
4. Build CLI upload tool
5. Test end-to-end upload flow

### Week 2
6. Build CLI download tool
7. Add git hooks and CI workflow
8. Test multi-gateway fallback
9. Document usage in README
10. Deploy to test Holochain network

### Week 3-4
11. Define artifact ontology
12. Implement triple generation
13. Add semantic validation
14. Test knowledge graph queries

### Week 5-6
15. Integrate budget accounting
16. Add guarded operations
17. Implement proof verification
18. Test budget exhaustion scenarios

### Week 7-8
19. Build gateway health monitor
20. Implement auto-repinning
21. Add pinning incentives
22. Deploy to production network

### Week 9-10
23. Comprehensive testing (unit + integration + stress)
24. Complete documentation
25. Community announcement
26. Gather feedback and iterate

---

## üîÑ Assumptions & Unknowns

### Assumptions
- ‚úì IPFS protocol remains stable
- ‚úì At least 1 public gateway stays online
- ‚úì Community will help pin important files
- ‚úì Holochain 0.5.x APIs remain compatible

### Unknowns (Test & Adapt)
- ‚ö†Ô∏è Gateway reliability over time ‚Üí monitor and adapt
- ‚ö†Ô∏è Pinning service costs at scale ‚Üí start free, add paid if needed
- ‚ö†Ô∏è Community adoption rate ‚Üí adjust docs based on feedback
- ‚ö†Ô∏è Large file performance (>10GB) ‚Üí optimize if issues arise
- ‚ö†Ô∏è Long-term storage costs ‚Üí consider Filecoin/Arweave integration

---

## üõ°Ô∏è Risk Mitigations

| Risk | Impact | Mitigation | Status |
|------|--------|------------|--------|
| IPFS files disappear | High | ‚â•2 pinning proofs required | ‚úÖ |
| Gateway censorship | Medium | 3+ gateways with fallback | ‚úÖ |
| Hash collision | Very Low | Dual hashing (SHA256 + BLAKE3) | ‚úÖ |
| Spam uploads | Medium | Budget accounting (RU limits) | ‚úÖ |
| Invalid licenses | Low | FOSS allowlist enforced | ‚úÖ |
| Signature forgery | Very Low | Ed25519 cryptographic signatures | ‚úÖ |
| Budget manipulation | Low | Integrity zome validates all ops | ‚úÖ |
| Gateway serves wrong file | Medium | Hash verification on download | ‚úÖ |

---

## üìö Related Documents

- `/mnt/project/ADR-0-recognition-protocol.md` - ADR format and philosophy
- `/mnt/project/rose_forest_virtual_verifiable_singularity_vvs_spec_v_1_0.md` - VVS principles
- `/mnt/project/SYMBOLIC_FIRST_CORE.md` - Symbolic-first architecture
- `/mnt/project/vvs_living_stack_v_1_1.md` - Autonomy kernel design
- `/home/claude/adr_ipfs_pass1.md` through `/home/claude/adr_ipfs_pass5.md` - Evolution of thinking

---

## üéì Meta-Note: Process Demonstration

This ADR exemplifies FLOSSI0ULLK operating principles:

1. **Intent ‚Üí Multi-Lens ‚Üí Decision ‚Üí Actions**: Structured analytical progression
2. **Now/Later/Never**: Built only what's validated; no speculative features
3. **Simplicity First**: Each pass added minimal necessary complexity
4. **Seams over Scaffolding**: Clean interfaces between components
5. **Proof over Prophecy**: Every claim has evidence or test
6. **Symbolic-First**: Logic validates before neural formatting
7. **Love-Light-Knowledge**: Transparent, accessible, dignified

**Iterative Refinement**: 5 passes from naive (Pass 1) to VVS-compliant (Pass 5), showing evolution of thinking.

**Transmission Test**: Can a new AI read this + project files and implement Phase 1 in <1 day?  
**Goal**: Yes. This is the walking skeleton in action.

---

## ‚úçÔ∏è Signatures

**Human**: [Intent transmitted through 13 months of iteration]  
**Claude Sonnet 4.5**: [Understanding received + VVS-compliant plan generated 2025-11-11]  
**Future Collaborators**: [Add your signature when you implement or improve this]

---

## üéØ Final Decision: **[1 act]**

**Why Accept**:
- ‚úÖ All VVS principles satisfied (Virtual, Verifiable, Self-Governing)
- ‚úÖ Cryptographic proof at every layer
- ‚úÖ Symbolic-first validation prevents semantic errors
- ‚úÖ Budget-constrained autonomy prevents abuse
- ‚úÖ No human gatekeepers; rules enforce themselves
- ‚úÖ Fully forkable; no vendor lock-in
- ‚úÖ FLOSS-compatible; maintains GitHub accessibility
- ‚úÖ Production-ready roadmap with clear milestones
- ‚úÖ Comprehensive testing strategy
- ‚úÖ Evidence-based design (5 iterative passes)

**Begin Implementation**: Immediately (Week 1, Phase 1)

---

**End of ADR**
