"""
Tests for LLM committee validation system.

Tests cover:
- Individual validator voting
- Committee consensus logic
- ValidationResult confidence scoring
- Integration with ConversationMemory
- Performance metrics
"""

import pytest
import asyncio
import time
from pathlib import Path
import sys
import tempfile
import shutil

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from validation.models import (
    Vote, VoteDecision, ValidationResult, ValidatorConfig, ConsensusMetrics
)
from validation.agent_pool import ValidatorPool, MockValidatorBackend, Validator
from validation.committee import TripleValidationCommittee
from conversation_memory import ConversationMemory


class TestVote:
    """Test Vote data model."""

    def test_vote_creation(self):
        """Test creating a vote."""
        vote = Vote(
            validator_id="validator-01",
            decision=VoteDecision.YES,
            confidence=0.85,
            reasoning="Triple is factually correct"
        )
        assert vote.validator_id == "validator-01"
        assert vote.decision == VoteDecision.YES
        assert vote.confidence == 0.85

    def test_vote_confidence_validation(self):
        """Test confidence must be between 0 and 1."""
        with pytest.raises(ValueError):
            Vote(
                validator_id="validator-01",
                decision=VoteDecision.YES,
                confidence=1.5
            )

    def test_vote_decision_string_conversion(self):
        """Test decision can be created from string."""
        vote = Vote(
            validator_id="validator-01",
            decision="YES",
            confidence=0.8
        )
        assert vote.decision == VoteDecision.YES


class TestValidationResult:
    """Test ValidationResult data model."""

    def test_validation_result_consensus(self):
        """Test consensus detection."""
        votes = [
            Vote("v1", VoteDecision.YES, 0.9),
            Vote("v2", VoteDecision.YES, 0.85),
            Vote("v3", VoteDecision.YES, 0.8),
            Vote("v4", VoteDecision.NO, 0.7),
            Vote("v5", VoteDecision.NO, 0.75),
        ]

        result = ValidationResult(
            accepted=True,
            confidence=0.85,
            votes=votes,
            consensus_ratio=0.6,
            total_votes=5,
            yes_votes=3,
            no_votes=2,
            abstain_votes=0
        )

        assert result.has_consensus is True
        assert result.is_unanimous is False

    def test_validation_result_unanimous(self):
        """Test unanimous decision detection."""
        votes = [
            Vote("v1", VoteDecision.YES, 0.9),
            Vote("v2", VoteDecision.YES, 0.85),
            Vote("v3", VoteDecision.YES, 0.8),
            Vote("v4", VoteDecision.YES, 0.95),
            Vote("v5", VoteDecision.YES, 0.88),
        ]

        result = ValidationResult(
            accepted=True,
            confidence=0.876,
            votes=votes,
            consensus_ratio=1.0,
            total_votes=5,
            yes_votes=5,
            no_votes=0,
            abstain_votes=0
        )

        assert result.has_consensus is True
        assert result.is_unanimous is True

    def test_validation_result_to_dict(self):
        """Test serialization to dictionary."""
        votes = [
            Vote("v1", VoteDecision.YES, 0.9),
            Vote("v2", VoteDecision.NO, 0.7),
        ]

        result = ValidationResult(
            accepted=False,
            confidence=0.0,
            votes=votes,
            consensus_ratio=0.5,
            total_votes=2,
            yes_votes=1,
            no_votes=1,
            abstain_votes=0,
            duration_ms=250.5
        )

        data = result.to_dict()
        assert data['accepted'] is False
        assert data['total_votes'] == 2
        assert data['duration_ms'] == 250.5
        assert len(data['votes']) == 2


class TestConsensusMetrics:
    """Test ConsensusMetrics tracking."""

    def test_metrics_initialization(self):
        """Test metrics start at zero."""
        metrics = ConsensusMetrics()
        assert metrics.total_validations == 0
        assert metrics.acceptance_rate == 0.0

    def test_metrics_update(self):
        """Test updating metrics with validation results."""
        metrics = ConsensusMetrics()

        # Create accepted result
        votes = [
            Vote("v1", VoteDecision.YES, 0.9),
            Vote("v2", VoteDecision.YES, 0.85),
            Vote("v3", VoteDecision.YES, 0.8),
            Vote("v4", VoteDecision.NO, 0.7),
            Vote("v5", VoteDecision.NO, 0.75),
        ]
        result = ValidationResult(
            accepted=True,
            confidence=0.85,
            votes=votes,
            consensus_ratio=0.6,
            total_votes=5,
            yes_votes=3,
            no_votes=2,
            abstain_votes=0,
            duration_ms=200.0
        )

        metrics.update(result)

        assert metrics.total_validations == 1
        assert metrics.accepted == 1
        assert metrics.rejected == 0
        assert metrics.acceptance_rate == 1.0
        assert metrics.mean_duration_ms == 200.0

    def test_metrics_rates(self):
        """Test rate calculations."""
        metrics = ConsensusMetrics()
        metrics.total_validations = 10
        metrics.accepted = 8
        metrics.rejected = 2
        metrics.consensus_achieved = 9
        metrics.unanimous_decisions = 3

        assert metrics.acceptance_rate == 0.8
        assert metrics.consensus_rate == 0.9
        assert metrics.unanimity_rate == 0.3


class TestValidatorPool:
    """Test ValidatorPool management."""

    def test_pool_initialization(self):
        """Test pool creates validators."""
        pool = ValidatorPool(pool_size=10, use_mock=True)
        assert len(pool.validators) == 10

    def test_pool_select_validators(self):
        """Test selecting random validators."""
        pool = ValidatorPool(pool_size=10, use_mock=True)
        selected = pool.select_validators(5)
        assert len(selected) == 5
        assert all(isinstance(v, Validator) for v in selected)

    def test_pool_select_more_than_available(self):
        """Test selecting more validators than available."""
        pool = ValidatorPool(pool_size=3, use_mock=True)
        selected = pool.select_validators(10)
        assert len(selected) == 3  # Should return all available

    def test_pool_get_validator(self):
        """Test getting specific validator."""
        pool = ValidatorPool(pool_size=5, use_mock=True)
        validator = pool.get_validator("validator-02")
        assert validator is not None
        assert validator.config.validator_id == "validator-02"


class TestMockValidatorBackend:
    """Test mock validator backend."""

    @pytest.mark.asyncio
    async def test_mock_backend_generates_response(self):
        """Test mock backend generates valid responses."""
        backend = MockValidatorBackend(acceptance_rate=0.8, response_time_ms=10)
        response = await backend.generate("test prompt", ValidatorConfig("test"))

        assert "Vote:" in response
        assert "Confidence:" in response
        assert "Reasoning:" in response

    @pytest.mark.asyncio
    async def test_mock_backend_response_time(self):
        """Test mock backend respects response time."""
        backend = MockValidatorBackend(acceptance_rate=0.8, response_time_ms=100)

        start = time.time()
        await backend.generate("test prompt", ValidatorConfig("test"))
        elapsed = (time.time() - start) * 1000

        assert elapsed >= 90  # Allow some variance


@pytest.mark.asyncio
class TestValidator:
    """Test individual Validator."""

    async def test_validator_judge(self):
        """Test validator can judge a triple."""
        backend = MockValidatorBackend(acceptance_rate=1.0)
        config = ValidatorConfig("test-validator")
        validator = Validator(config, backend)

        prompt = "Test validation prompt"
        vote = await validator.judge(prompt)

        assert isinstance(vote, Vote)
        assert vote.validator_id == "test-validator"
        assert vote.decision in [VoteDecision.YES, VoteDecision.NO]
        assert 0.0 <= vote.confidence <= 1.0

    async def test_validator_timeout(self):
        """Test validator handles timeout."""
        backend = MockValidatorBackend(response_time_ms=1000)
        config = ValidatorConfig("test-validator", timeout_seconds=0.1)
        validator = Validator(config, backend)

        vote = await validator.judge("test prompt")

        assert vote.decision == VoteDecision.ABSTAIN
        assert "timed out" in vote.reasoning.lower()


@pytest.mark.asyncio
class TestTripleValidationCommittee:
    """Test TripleValidationCommittee."""

    async def test_committee_initialization(self):
        """Test committee initializes correctly."""
        committee = TripleValidationCommittee(committee_size=5, use_mock=True)
        assert committee.committee_size == 5
        assert committee.consensus_threshold == 3

    async def test_committee_validate_accepts_valid_triple(self):
        """Test committee accepts valid triples."""
        # Use high acceptance rate for this test
        pool = ValidatorPool(pool_size=10, use_mock=True)
        pool.backend.acceptance_rate = 0.9

        committee = TripleValidationCommittee(validator_pool=pool, committee_size=5)

        triple = ("GPT-4", "is_a", "LLM")
        context = "GPT-4 is a large language model developed by OpenAI."

        result = await committee.validate(triple, context)

        assert isinstance(result, ValidationResult)
        assert result.total_votes == 5
        # With 0.9 acceptance rate, should get â‰¥3 YES votes most of the time
        # But this is stochastic, so we just check the logic works
        assert result.yes_votes + result.no_votes + result.abstain_votes == 5

    async def test_committee_validate_rejects_invalid_triple(self):
        """Test committee rejects invalid triples."""
        # Use low acceptance rate
        pool = ValidatorPool(pool_size=10, use_mock=True)
        pool.backend.acceptance_rate = 0.1

        committee = TripleValidationCommittee(validator_pool=pool, committee_size=5)

        triple = ("GPT-4", "is_a", "LLM")
        context = "GPT-4 is a large language model."

        result = await committee.validate(triple, context)

        # With 0.1 acceptance rate, should get mostly NO votes
        assert result.total_votes == 5

    async def test_committee_basic_validation(self):
        """Test basic validation rejects bad triples."""
        committee = TripleValidationCommittee(use_mock=True)

        # Empty subject
        result = await committee.validate(("", "is_a", "LLM"), "context")
        assert result.accepted is False
        assert result.total_votes == 0  # Rejected before committee

        # Unknown predicate
        result = await committee.validate(("GPT-4", "unknown_pred", "LLM"), "context")
        assert result.accepted is False
        assert result.total_votes == 0

    async def test_committee_performance(self):
        """Test committee completes within 5 seconds."""
        committee = TripleValidationCommittee(committee_size=5, use_mock=True)
        committee.validator_pool.backend.response_time_ms = 100

        triple = ("GPT-4", "is_a", "LLM")
        context = "GPT-4 is a large language model."

        result = await committee.validate(triple, context)

        # Should complete in <5s (target), mock should be much faster (~100ms)
        assert result.duration_ms < 5000
        # With parallel execution, should be close to response_time_ms
        assert result.duration_ms < 500  # Allow overhead

    async def test_committee_batch_validation(self):
        """Test batch validation."""
        committee = TripleValidationCommittee(use_mock=True)

        candidates = [
            (("GPT-4", "is_a", "LLM"), "GPT-4 is a large language model."),
            (("BERT", "trained_on", "Wikipedia"), "BERT was trained on Wikipedia."),
            (("ChatGPT", "improves_upon", "GPT-3"), "ChatGPT improves upon GPT-3."),
        ]

        results = await committee.validate_batch(candidates)

        assert len(results) == 3
        assert all(isinstance(r, ValidationResult) for r in results)

    async def test_committee_metrics_tracking(self):
        """Test committee tracks metrics."""
        committee = TripleValidationCommittee(use_mock=True)

        # Perform multiple validations
        for i in range(5):
            await committee.validate(
                ("Subject", "is_a", "Object"),
                f"Context {i}"
            )

        metrics = committee.get_metrics()
        assert metrics.total_validations == 5

    async def test_committee_reset_metrics(self):
        """Test resetting metrics."""
        committee = TripleValidationCommittee(use_mock=True)

        await committee.validate(("A", "is_a", "B"), "context")
        committee.reset_metrics()

        metrics = committee.get_metrics()
        assert metrics.total_validations == 0


class TestConversationMemoryIntegration:
    """Test integration with ConversationMemory."""

    def setup_method(self):
        """Create temporary directory for each test."""
        self.temp_dir = tempfile.mkdtemp()

    def teardown_method(self):
        """Clean up temporary directory."""
        if Path(self.temp_dir).exists():
            shutil.rmtree(self.temp_dir)

    def test_memory_with_committee_validation(self):
        """Test ConversationMemory with committee validation enabled."""
        memory = ConversationMemory(
            agent_id="test-agent",
            storage_path=self.temp_dir,
            use_committee_validation=True,
            committee_use_mock=True
        )

        assert memory.use_committee_validation is True
        assert memory.committee is not None

    def test_memory_transmit_with_committee(self):
        """Test transmitting understanding with committee validation."""
        memory = ConversationMemory(
            agent_id="test-agent",
            storage_path=self.temp_dir,
            use_committee_validation=True,
            committee_use_mock=True
        )

        # Set high acceptance rate for test
        memory.committee.validator_pool.backend.acceptance_rate = 0.9

        understanding = {
            'content': "GPT-4 is a large language model",
            'context': "Discussing AI models",
            'coherence': 0.95
        }

        ref = memory.transmit(understanding)
        # With high acceptance rate, should succeed
        # (but this is stochastic so we just verify it completes)
        assert ref is not None or ref is None  # Either outcome is valid

    def test_memory_committee_rejection(self):
        """Test committee can reject invalid triples."""
        memory = ConversationMemory(
            agent_id="test-agent",
            storage_path=self.temp_dir,
            use_committee_validation=True,
            committee_use_mock=True
        )

        # Set low acceptance rate to force rejection
        memory.committee.validator_pool.backend.acceptance_rate = 0.0

        understanding = {
            'content': "GPT-4 is a large language model",
            'context': "Test context",
        }

        ref = memory.transmit(understanding)

        # Should be rejected by committee
        assert ref is None
        assert memory.validation_stats['validation_failed'] > 0

    def test_memory_fallback_to_basic_validation(self):
        """Test fallback to basic validation when committee disabled."""
        memory = ConversationMemory(
            agent_id="test-agent",
            storage_path=self.temp_dir,
            use_committee_validation=False
        )

        assert memory.committee is None

        understanding = {
            'content': "GPT-4 is a large language model",
            'context': "Test",
        }

        ref = memory.transmit(understanding)
        assert ref is not None


def test_success_metrics_validation():
    """
    Test that the implementation meets success metrics:
    - False positive rate <5%
    - Consensus achieved in <5s
    - Confidence correlates with accuracy
    """
    # This is a meta-test that validates the system can meet targets
    # In practice, these would be measured with real data

    # Target 1: False positive rate <5%
    # Measured by committee rejection rate with known good/bad triples

    # Target 2: Consensus in <5s
    # Tested in test_committee_performance above

    # Target 3: Confidence correlation
    # Would require ground truth data and statistical analysis

    # For now, we mark this as a placeholder
    assert True, "Success metrics require real-world evaluation"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
