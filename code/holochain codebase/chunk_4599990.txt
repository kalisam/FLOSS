    zome_name: ZomeName,
    fn_name: FunctionName,
    input: &I,
) -> ExternIO
where
    I: Serialize + std::fmt::Debug,
{
    let call_response = call_zome_fn_fallible(
        app_tx,
        cell_id,
        signing_keypair,
        cap_secret,
        zome_name,
        fn_name,
        input,
    )
    .await;
    match call_response {
        AppResponse::ZomeCalled(response) => *response,
        _ => panic!("zome call failed {call_response:?}"),
    }
}

pub async fn attach_app_interface(client: &WebsocketSender, port: Option<u16>) -> u16 {
    let request = AdminRequest::AttachAppInterface {
        port,
        allowed_origins: AllowedOrigins::Any,
        installed_app_id: None,
    };
    let response = client.request(request);
    let response = check_timeout(response, 3000).await.unwrap();
    match response {
        AdminResponse::AppInterfaceAttached { port } => port,
        _ => panic!("Attach app interface failed: {:?}", response),
    }
}

pub async fn retry_admin_interface(
    port: u16,
    mut attempts: usize,
    delay: Duration,
) -> WebsocketSender {
    loop {
        match websocket_client_by_port(port).await {
            Ok(c) => return c.0,
            Err(e) => {
                attempts -= 1;
                if attempts == 0 {
                    panic!("Failed to join admin interface");
                }
                warn!(
                    "Failed with {:?} to open admin interface, trying {} more times",
                    e, attempts
                );
                tokio::time::sleep(delay).await;
            }
        }
    }
}

pub async fn generate_agent_pub_key(
    client: &mut WebsocketSender,
    timeout: u64,
) -> WebsocketResult<AgentPubKey> {
    let request = AdminRequest::GenerateAgentPubKey;
    let response = client
        .request_timeout(request, Duration::from_millis(timeout))
        .await?;

    Ok(unwrap_to::unwrap_to!(response => AdminResponse::AgentPubKeyGenerated).clone())
}

/// Returns the hash of the DNA installed, after modifiers have been applied
pub async fn register_and_install_dna(
    client: &mut WebsocketSender,
    dna_path: PathBuf,
    properties: Option<YamlProperties>,
    role_name: RoleName,
    timeout: u64,
) -> WebsocketResult<CellId> {
    register_and_install_dna_named(
        client,
        dna_path,
        properties,
        role_name,
        "test".to_string(),
        timeout,
    )
    .await
}

/// Returns the hash of the DNA installed, after modifiers have been applied
#[allow(clippy::too_many_arguments)]
pub async fn register_and_install_dna_named(
    client: &mut WebsocketSender,
    dna_path: PathBuf,
    properties: Option<YamlProperties>,
    role_name: RoleName,
    name: String,
    timeout: u64,
) -> WebsocketResult<CellId> {
    let mods = DnaModifiersOpt {
        properties,
        ..Default::default()
    };

    let dna_bundle1 = DnaBundle::read_from_file(&dna_path).await.unwrap();
    let dna_bundle = DnaBundle::read_from_file(&dna_path).await.unwrap();
    let (dna, _) = dna_bundle1
        .into_dna_file(mods.clone().serialized().unwrap())
        .await
        .unwrap();
    let dna_hash = dna.dna_hash().clone();

    let roles = vec![AppRoleManifest {
        name: role_name,
        dna: AppRoleDnaManifest {
            location: Some(DnaLocation::Bundled(dna_path.clone())),
            modifiers: mods,
            installed_hash: None,
            clone_limit: 0,
        },
        provisioning: Some(CellProvisioning::Create { deferred: false }),
    }];

    let manifest = AppManifestCurrentBuilder::default()
        .name(name.clone())
        .description(None)
        .roles(roles)
        .build()
        .unwrap();

    let resources = vec![(dna_path.clone(), dna_bundle)];

    let bundle = AppBundle::new(manifest.clone().into(), resources, dna_path.clone())
        .await
        .unwrap()
        .encode()
        .expect("failed to encode AppBundle to bytes");

    let payload = InstallAppPayload {
        agent_key: None,
        source: AppBundleSource::Bytes(bundle),
        installed_app_id: Some(name),
        network_seed: None,
        roles_settings: Default::default(),
        ignore_genesis_failure: false,
        allow_throwaway_random_agent_key: true,
    };
    let request = AdminRequest::InstallApp(Box::new(payload));
    let response = client.request(request);
    let response = check_timeout_named("InstallApp", response, timeout).await?;
    if let AdminResponse::AppInstalled(app) = response {
        Ok(CellId::new(dna_hash, app.agent_pub_key))
    } else {
        panic!("InstallApp failed: {:?}", response);
    }
}

pub fn spawn_output(holochain: &mut Child) -> tokio::sync::oneshot::Receiver<u16> {
    let stdout = holochain.stdout.take().unwrap();
    let stderr = holochain.stderr.take().unwrap();
    let (tx, rx) = tokio::sync::oneshot::channel();
    // Wrap in an Option because it is used in a loop and cannot be cloned.
    let mut tx = Some(tx);
    tokio::task::spawn(async move {
        let mut reader = BufReader::new(stdout).lines();
        while let Ok(Some(line)) = reader.next_line().await {
            println!("holochain bin stdout: {}", &line);
            if let Some(port) = check_line_for_admin_port(&line) {
                if let Some(tx) = tx.take() {
                    let _ = tx.send(port);
                }
            }
        }
    });
    tokio::task::spawn(async move {
        let mut reader = BufReader::new(stderr).lines();
        while let Ok(Some(line)) = reader.next_line().await {
            eprintln!("holochain bin stderr: {}", &line);
        }
    });
    rx
}

fn check_line_for_admin_port(mut line: &str) -> Option<u16> {
    line = line.strip_prefix("###")?;
    line = line.strip_suffix("###")?;

    let port = line.strip_prefix("ADMIN_PORT:")?;
    port.parse::<u16>().ok()
}

pub async fn check_started(holochain: &mut Child) {
    let started = tokio::time::timeout(std::time::Duration::from_secs(1), holochain.wait()).await;
    if let Ok(status) = started {
        panic!("Holochain failed to start. status: {:?}", status);
    }
}

/// Create test config with test keystore and DPKI disabled
pub fn create_config(port: u16, data_root_path: DataRootPath) -> ConductorConfig {
    ConductorConfig {
        admin_interfaces: Some(vec![AdminInterfaceConfig {
            driver: InterfaceDriver::Websocket {
                port,
                allowed_origins: AllowedOrigins::Any,
            },
        }]),
        data_root_path: Some(data_root_path),
        keystore: KeystoreConfig::DangerTestKeystore,
        dpki: DpkiConfig::disabled(),
        network: KitsuneP2pConfig::mem(),
        ..Default::default()
    }
}

pub fn write_config(mut path: PathBuf, config: &ConductorConfig) -> PathBuf {
    path.push("conductor_config.yml");
    std::fs::write(path.clone(), serde_yaml::to_string(&config).unwrap()).unwrap();
    path
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(response)))]
pub async fn check_timeout<T>(
    response: impl Future<Output = WebsocketResult<T>>,
    timeout_ms: u64,
) -> WebsocketResult<T> {
    check_timeout_named("<unnamed>", response, timeout_ms).await
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(response)))]
async fn check_timeout_named<T>(
    name: &'static str,
    response: impl Future<Output = WebsocketResult<T>>,
    timeout_millis: u64,
) -> WebsocketResult<T> {
    match tokio::time::timeout(Duration::from_millis(timeout_millis), response).await {
        Ok(response) => response,
        Err(_) => Err(std::io::Error::other(format!(
            "{}: Timed out on request after {}",
            name, timeout_millis
        ))
        .into()),
    }
}

pub async fn dump_full_state(
    client: &mut WebsocketSender,
    cell_id: CellId,
    dht_ops_cursor: Option<u64>,
) -> WebsocketResult<FullStateDump> {
    let request = AdminRequest::DumpFullState {
        cell_id: Box::new(cell_id),
        dht_ops_cursor,
    };
    let response = client.request(request);
    let response = check_timeout(response, 3000).await?;

    match response {
        AdminResponse::FullStateDumped(state) => Ok(state),
        _ => Err(std::io::Error::other(format!("DumpFullState failed: {:?}", response)).into()),
    }
}



================================================
File: crates/holochain/tests/tests/websocket/mod.rs
================================================
use ::fixt::prelude::*;
use anyhow::Result;
use ed25519_dalek::ed25519::signature::SignerMut;
use ed25519_dalek::SigningKey;
use hdk::prelude::RemoteSignal;
use holochain::conductor::interface::websocket::MAX_CONNECTIONS;
use holochain::sweettest::SweetConductorBatch;
use holochain::sweettest::SweetConductorConfig;
use holochain::sweettest::SweetDnaFile;
use holochain::sweettest::{authenticate_app_ws_client, SweetConductor, WsPollRecv};
use holochain::{
    conductor::{
        api::{AdminRequest, AdminResponse, AppResponse},
        error::ConductorError,
        Conductor,
    },
    fixt::*,
};
use holochain_conductor_api::ExternalApiWireError;
use holochain_conductor_api::ZomeCallParamsSigned;
use std::net::{Ipv4Addr, Ipv6Addr, ToSocketAddrs};
use std::path::PathBuf;

use either::Either;
use holochain_conductor_api::{AdminInterfaceConfig, AppRequest, InterfaceDriver};
use holochain_types::websocket::AllowedOrigins;
use holochain_types::{
    prelude::*,
    test_utils::{fake_dna_zomes, write_fake_dna_file},
};
use holochain_wasm_test_utils::TestWasm;
use holochain_websocket::*;
use matches::assert_matches;
use rand::rngs::OsRng;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tempfile::TempDir;
use tracing::*;

use crate::tests::test_utils::*;

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
#[cfg_attr(target_os = "windows", ignore = "flaky")]
async fn call_admin() {
    holochain_trace::test_run();
    // NOTE: This is a full integration test that
    // actually runs the holochain binary

    let port = 0;

    let tmp_dir = TempDir::new().unwrap();
    let path = tmp_dir.path().to_path_buf();
    let environment_path = path.clone();
    let config = create_config(port, environment_path.into());
    let config_path = write_config(path, &config);

    let uuid = uuid::Uuid::new_v4();
    let dna = fake_dna_zomes(
        &uuid.to_string(),
        vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
    );

    let (_holochain, port) = start_holochain(config_path.clone()).await;
    let port = port.await.unwrap();

    let (mut client, rx) = websocket_client_by_port(port).await.unwrap();
    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    // Make properties
    let properties = holochain_zome_types::properties::YamlProperties::new(
        serde_yaml::from_str(
            r#"
test: "example"
how_many: 42
    "#,
        )
        .unwrap(),
    );

    let original_dna_hash = dna.dna_hash().clone();

    // Install Dna
    let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna.clone()).await.unwrap();

    let installed_cell_id = register_and_install_dna(
        &mut client,
        fake_dna_path,
        Some(properties.clone()),
        "role_name".into(),
        10000,
    )
    .await
    .unwrap();

    let installed_dna_hash = installed_cell_id.dna_hash().clone();

    assert_ne!(installed_dna_hash, original_dna_hash);

    // List Dnas
    let request = AdminRequest::ListDnas;
    let response = client.request(request);
    let response = check_timeout(response, 20_000).await.unwrap();

    assert_matches!(response, AdminResponse::DnasListed(a) if a.contains(&installed_dna_hash));
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
#[cfg_attr(target_os = "windows", ignore = "flaky")]
async fn zome_call_authentication() {
    // NOTE: This is a full integration test that
    // actually runs the holochain binary

    let TestCase {
        cell_id,
        _admin_rx,
        app_tx,
        app_rx,
        holochain: _holochain,
        zome_name,
        fn_name,
        cap_secret,
        mut signing_keypair,
        ..
    } = TestCase::new().await;

    let _app_rx = WsPollRecv::new::<AppResponse>(app_rx);

    // Authentication of zome call should fail with invalid signature.
    let (nonce, expires_at) = holochain_nonce::fresh_nonce(Timestamp::now()).unwrap();
    let mut zome_call_params = ZomeCallParams {
        provenance: cell_id.agent_pubkey().clone(),
        cell_id: cell_id.clone(),
        zome_name: zome_name.clone(),
        fn_name: fn_name.clone(),
        cap_secret: None,
        payload: ExternIO::encode(()).unwrap(),
        nonce,
        expires_at,
    };
    let bytes = encode(&zome_call_params).unwrap();
    let signature = fixt!(Signature);
    let request = AppRequest::CallZome(Box::new(ZomeCallParamsSigned::new(bytes, signature)));
    let response = app_tx.request(request);
    let response = check_timeout(response, 6000).await.unwrap();
    assert_matches!(
        response,
        AppResponse::Error(ExternalApiWireError::ZomeCallAuthenticationFailed(_))
    );

    // Authentication of zome call should fail if signature doesn't fit signed bytes.
    let (_, bytes_hash) = zome_call_params.serialize_and_hash().unwrap();
    let signature = signing_keypair.sign(&bytes_hash);
    // Change request now so that serialized bytes differ.
    zome_call_params.payload = ExternIO::encode("wrong").unwrap();
    let (bytes, _) = zome_call_params.serialize_and_hash().unwrap();
    let request = AppRequest::CallZome(Box::new(ZomeCallParamsSigned::new(
        bytes,
        Signature::from(signature.to_bytes()),
    )));
    let response = app_tx.request(request);
    let response = check_timeout(response, 6000).await.unwrap();
    assert_matches!(
        response,
        AppResponse::Error(ExternalApiWireError::ZomeCallAuthenticationFailed(_))
    );

    // Call zome should now authenticate successfully.
    tracing::info!("Calling zome");
    call_zome_fn(
        &app_tx,
        cell_id.clone(),
        &signing_keypair,
        cap_secret,
        zome_name.clone(),
        fn_name.clone(),
        &(),
    )
    .await;
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
#[cfg_attr(target_os = "windows", ignore = "flaky")]
async fn zome_call_with_conductor_restart() {
    // NOTE: This is a full integration test that
    // actually runs the holochain binary

    let TestCase {
        cell_id,
        admin_tx,
        _admin_rx,
        admin_port,
        app_tx,
        app_rx,
        holochain,
        config_path,
        zome_name,
        fn_name,
        cap_secret,
        signing_keypair,
    } = TestCase::new().await;

    let _app_rx = WsPollRecv::new::<AdminResponse>(app_rx);

    // Call zome works before conductor restart.
    tracing::info!("Calling zome");
    call_zome_fn(
        &app_tx,
        cell_id.clone(),
        &signing_keypair,
        cap_secret,
        zome_name.clone(),
        fn_name.clone(),
        &(),
    )
    .await;

    // Connect another admin websocket.
    let (_, mut receiver2) = websocket_client_by_port(admin_port).await.unwrap();

    // Ensure that the other client does not receive any messages, i.e. that
    // responses are not broadcast to all connected clients, only the one
    // that made the request.
    // Err means the timeout elapsed
    assert!(tokio::time::timeout(
        Duration::from_millis(500),
        receiver2.recv::<AdminResponse>(),
    )
    .await
    .is_err());

    // Shutdown holochain
    drop(holochain);
    drop(admin_tx);

    // Call zome after restart
    info!("Restarting conductor");
    let (_holochain, admin_port) = start_holochain(config_path).await;
    let admin_port = admin_port.await.unwrap();

    let (admin_tx, admin_rx) = websocket_client_by_port(admin_port).await.unwrap();
    let _admin_rx = WsPollRecv::new::<AdminResponse>(admin_rx);

    tokio::time::sleep(Duration::from_millis(1000)).await;

    let request = AdminRequest::ListAppInterfaces;
    let response = admin_tx.request(request);
    let response = check_timeout(response, 3000).await.unwrap();
    let app_port = match response {
        AdminResponse::AppInterfacesListed(ports) => ports.first().map(|i| i.port).unwrap(),
        _ => panic!("Unexpected response"),
    };

    let (app_tx, app_rx) = websocket_client_by_port(app_port).await.unwrap();
    let _app_rx = WsPollRecv::new::<AppResponse>(app_rx);
    authenticate_app_ws_client(app_tx.clone(), admin_port, "test".to_string()).await;

    // Call Zome again on the existing app interface port
    tracing::info!("Calling zome again");
    call_zome_fn(
        &app_tx,
        cell_id.clone(),
        &signing_keypair,
        cap_secret,
        zome_name.clone(),
        fn_name.clone(),
        &(),
    )
    .await;
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
#[cfg_attr(target_os = "macos", ignore = "flaky")]
#[cfg_attr(target_os = "windows", ignore = "flaky")]
async fn remote_signals() -> anyhow::Result<()> {
    use std::collections::HashSet;

    holochain_trace::test_run();
    const NUM_CONDUCTORS: usize = 2;

    let mut conductors = SweetConductorBatch::from_standard_config_rendezvous(NUM_CONDUCTORS).await;

    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::EmitSignal])
        .await
        .0;

    let apps = conductors.setup_app("app", &[dna_file]).await.unwrap();

    let all_agents: HashSet<_> = apps
        .cells_flattened()
        .into_iter()
        .map(|c| c.agent_pubkey().clone())
        .collect();

    assert_eq!(all_agents.len(), NUM_CONDUCTORS);

    let cells = apps.cells_flattened();

    let mut rxs = Vec::new();
    for h in conductors.iter() {
        rxs.push(h.subscribe_to_app_signals("app".to_string()))
    }

    let signal = fixt!(ExternIo);

    let _: () = conductors[0]
        .call(
            &cells[0].zome(TestWasm::EmitSignal),
            "signal_others",
            RemoteSignal {
                signal: signal.clone(),
                agents: all_agents.into_iter().collect(),
            },
        )
        .await;

    tokio::time::timeout(Duration::from_secs(60), async move {
        let signal = AppSignal::new(signal);
        for mut rx in rxs {
            let r = rx.recv().await;
            // Each handle should recv a signal
            match r {
                Ok(Signal::App { signal: r, .. }) => {
                    assert_eq!(r, signal);
                }
                oth => panic!("unexpected: {oth:?}"),
            }
        }
    })
    .await
    .unwrap();

    Ok(())
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
async fn emit_signals() {
    holochain_trace::test_run();
    // NOTE: This is a full integration test that
    // actually runs the holochain binary

    let admin_port = 0;

    let tmp_dir = TempDir::new().unwrap();
    let path = tmp_dir.path().to_path_buf();
    let environment_path = path.clone();
    let config = create_config(admin_port, environment_path.into());
    let config_path = write_config(path, &config);

    let (_holochain, admin_port) = start_holochain(config_path.clone()).await;
    let admin_port = admin_port.await.unwrap();

    let (mut admin_tx, admin_rx) = websocket_client_by_port(admin_port).await.unwrap();
    let _admin_rx = WsPollRecv::new::<AdminResponse>(admin_rx);

    let uuid = uuid::Uuid::new_v4();
    let dna = fake_dna_zomes(
        &uuid.to_string(),
        vec![(TestWasm::EmitSignal.into(), TestWasm::EmitSignal.into())],
    );
    let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna).await.unwrap();

    // Install Dna
    let cell_id = register_and_install_dna(&mut admin_tx, fake_dna_path, None, "".into(), 20_000)
        .await
        .unwrap();

    // Activate cells
    let request = AdminRequest::EnableApp {
        installed_app_id: "test".to_string(),
    };
    let response = admin_tx.request(request);
    let response = tokio::time::timeout(Duration::from_secs(3), response)
        .await
        .expect("Timeout waiting for response")
        .unwrap();
    assert_matches!(response, AdminResponse::AppEnabled { .. });

    // Generate signing key pair
    let mut rng = OsRng;
    let signing_keypair = ed25519_dalek::SigningKey::generate(&mut rng);
    let signing_key = AgentPubKey::from_raw_32(signing_keypair.verifying_key().as_bytes().to_vec());

    // Grant zome call capability for agent
    let zome_name = TestWasm::EmitSignal.coordinator_zome_name();
    let fn_name = FunctionName("emit".into());
    let cap_secret = grant_zome_call_capability(
        &mut admin_tx,
        &cell_id,
        zome_name.clone(),
        fn_name.clone(),
        signing_key,
    )
    .await
    .unwrap();

    // Attach App Interface
    let app_port = attach_app_interface(&admin_tx, None).await;

    ///////////////////////////////////////////////////////
    // Emit signals (the real test!)

    let (app_tx_1, mut app_rx_1) = websocket_client_by_port(app_port).await.unwrap();
    let (sig1_send, sig1_recv) = tokio::sync::oneshot::channel();
    let mut sig1_send = Some(sig1_send);
    let sig1_task = tokio::task::spawn(async move {
        loop {
            match app_rx_1.recv::<AppResponse>().await {
                Ok(ReceiveMessage::Signal(sig1)) => {
                    if let Some(sig1_send) = sig1_send.take() {
                        let _ = sig1_send.send(sig1);
                    }
                }
                oth => panic!("unexpected: {oth:?}"),
            }
        }
    });
    authenticate_app_ws_client(app_tx_1.clone(), admin_port, "test".to_string()).await;

    let (app_tx_2, mut app_rx_2) = websocket_client_by_port(app_port).await.unwrap();
    let (sig2_send, sig2_recv) = tokio::sync::oneshot::channel();
    let mut sig2_send = Some(sig2_send);
    let sig2_task = tokio::task::spawn(async move {
        loop {
            match app_rx_2.recv::<AppResponse>().await {
                Ok(ReceiveMessage::Signal(sig2)) => {
                    if let Some(sig2_send) = sig2_send.take() {
                        let _ = sig2_send.send(sig2);
                    }
                }
                oth => panic!("unexpected: {oth:?}"),
            }
        }
    });
    authenticate_app_ws_client(app_tx_2.clone(), admin_port, "test".to_string()).await;

    call_zome_fn(
        &app_tx_1,
        cell_id.clone(),
        &signing_keypair,
        cap_secret,
        zome_name.clone(),
        fn_name,
        &(),
    )
    .await;

    let sig1 = Signal::try_from_vec(sig1_recv.await.unwrap()).unwrap();
    let sig2 = Signal::try_from_vec(sig2_recv.await.unwrap()).unwrap();
    sig1_task.abort();
    sig2_task.abort();

    assert_eq!(
        Signal::App {
            cell_id,
            zome_name,
            signal: AppSignal::new(ExternIO::encode(()).unwrap()),
        },
        sig1,
    );
    assert_eq!(sig1, sig2);

    ///////////////////////////////////////////////////////
}

#[tokio::test(flavor = "multi_thread")]
async fn conductor_admin_interface_runs_from_config() -> Result<()> {
    holochain_trace::test_run();
    let tmp_dir = TempDir::new().unwrap();
    let environment_path = tmp_dir.path().to_path_buf();
    let config = create_config(0, environment_path.into());
    let conductor_handle = Conductor::builder()
        .config(config)
        .with_test_device_seed()
        .build()
        .await?;
    let (client, rx) = websocket_client(&conductor_handle).await?;
    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    let dna = fake_dna_zomes("", vec![(TestWasm::Foo.into(), TestWasm::Foo.into())]);
    let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna).await.unwrap();
    let register_payload = RegisterDnaPayload {
        modifiers: DnaModifiersOpt::none(),
        source: DnaSource::Path(fake_dna_path),
    };
    let request = AdminRequest::RegisterDna(Box::new(register_payload));
    let response = client.request(request).await.unwrap();
    assert_matches!(response, AdminResponse::DnaRegistered(_));

    conductor_handle.shutdown();

    Ok(())
}

#[tokio::test(flavor = "multi_thread")]
async fn list_app_interfaces_succeeds() -> Result<()> {
    holochain_trace::test_run();

    info!("creating config");
    let tmp_dir = TempDir::new().unwrap();
    let environment_path = tmp_dir.path().to_path_buf();
    let config = create_config(0, environment_path.into());
    let conductor_handle = Conductor::builder()
        .config(config)
        .with_test_device_seed()
        .build()
        .await?;
    let port = admin_port(&conductor_handle).await;
    info!("building conductor");
    let mut ws_config = WebsocketConfig::CLIENT_DEFAULT;
    ws_config.default_request_timeout = Duration::from_secs(1);
    let (client, rx): (WebsocketSender, WebsocketReceiver) = connect(
        Arc::new(ws_config),
        ConnectRequest::new(
            format!("localhost:{port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .unwrap(),
        ),
    )
    .await?;
    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    let request = AdminRequest::ListAppInterfaces;

    // Request the list of app interfaces that the conductor has attached
    let response: Result<Result<AdminResponse, _>, tokio::time::error::Elapsed> =
        tokio::time::timeout(Duration::from_secs(1), client.request(request)).await;

    // There should be no app interfaces listed
    assert_matches!(response, Ok(Ok(AdminResponse::AppInterfacesListed(interfaces))) if interfaces.is_empty());

    Ok(())
}

#[tokio::test(flavor = "multi_thread")]
async fn conductor_admin_interface_ends_with_shutdown() -> Result<()> {
    if let Err(e) = conductor_admin_interface_ends_with_shutdown_inner().await {
        panic!("{:#?}", e);
    }
    Ok(())
}

async fn conductor_admin_interface_ends_with_shutdown_inner() -> Result<()> {
    holochain_trace::test_run();

    info!("creating config");
    let tmp_dir = TempDir::new().unwrap();
    let environment_path = tmp_dir.path().to_path_buf();
    let config = create_config(0, environment_path.into());
    let conductor_handle = Conductor::builder()
        .config(config)
        .with_test_device_seed()
        .build()
        .await?;
    let port = admin_port(&conductor_handle).await;
    info!("building conductor");
    let mut ws_config = WebsocketConfig::CLIENT_DEFAULT;
    ws_config.default_request_timeout = Duration::from_secs(1);
    let (client, mut rx): (WebsocketSender, WebsocketReceiver) = holochain_websocket::connect(
        Arc::new(ws_config),
        ConnectRequest::new(
            format!("localhost:{port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .unwrap(),
        ),
    )
    .await?;

    info!("client connect");

    conductor_handle.shutdown();

    info!("shutdown");

    assert_matches!(
        conductor_handle.check_running(),
        Err(ConductorError::ShuttingDown)
    );

    assert!(tokio::time::timeout(
        std::time::Duration::from_secs(7),
        rx.recv::<AdminResponse>(),
    )
    .await
    .unwrap()
    .is_err());

    info!("About to make failing request");

    let dna = fake_dna_zomes("", vec![(TestWasm::Foo.into(), TestWasm::Foo.into())]);
    let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna).await.unwrap();
    let register_payload = RegisterDnaPayload {
        modifiers: DnaModifiersOpt::none(),
        source: DnaSource::Path(fake_dna_path),
    };
    let request = AdminRequest::RegisterDna(Box::new(register_payload));

    // send a request after the conductor has shutdown
    // let response: Result<Result<AdminResponse, _>, tokio::time::Elapsed> =
    //     tokio::time::timeout(Duration::from_secs(1), client.request(request)).await;
    let response: Result<Result<AdminResponse, _>, tokio::time::error::Elapsed> =
        tokio::time::timeout(Duration::from_secs(1), client.request(request)).await;

    // request should have encountered an error since the conductor shut down,
    // but should not have timed out (which would be an `Err(_)`)
    assert_matches!(response, Ok(Err(_)));

    Ok(())
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
async fn connection_limit_is_respected() {
    holochain_trace::test_run();

    let tmp_dir = TempDir::new().unwrap();
    let environment_path = tmp_dir.path().to_path_buf();
    let config = create_config(0, environment_path.into());
    let conductor_handle = Conductor::builder()
        .config(config)
        .with_test_device_seed()
        .build()
        .await
        .unwrap();
    let port = admin_port(&conductor_handle).await;

    let addr = format!("localhost:{port}")
        .to_socket_addrs()
        .unwrap()
        .next()
        .unwrap();
    let cfg = Arc::new(WebsocketConfig::CLIENT_DEFAULT);

    // Retain handles so that the test can control when to disconnect clients
    let mut handles = Vec::new();

    tracing::warn!("OPEN FIRST CONNECTION");
    // The first `MAX_CONNECTIONS` connections should succeed
    for count in 0..MAX_CONNECTIONS {
        let (sender, rx) = connect(cfg.clone(), addr).await.unwrap();
        let rx = WsPollRecv::new::<AdminResponse>(rx);
        let _: AdminResponse = sender
            .request(AdminRequest::ListDnas)
            .await
            .map_err(|e| Error::other(format!("Admin request should succeed because there are enough available connections: {count}: {e:?}")))
            .unwrap();
        handles.push((sender, rx));
    }

    // Try lots of failed connections to make sure the limit is respected
    for _ in 0..2 * MAX_CONNECTIONS {
        let (sender, rx) = connect(cfg.clone(), addr).await.unwrap();
        let _rx = WsPollRecv::new::<AdminResponse>(rx);

        // Getting a sender back isn't enough to know that the connection succeeded because the other side takes a moment to shutdown, try sending to be sure
        sender
            .request::<AdminRequest, AdminResponse>(AdminRequest::ListDnas)
            .await
            .expect_err("Should be no available connection slots");
    }

    // Disconnect all the clients
    handles.clear();

    // Wait up to 2 seconds for new connections to be permitted
    let now = Instant::now();
    while now.elapsed() < Duration::from_secs(2) {
        let (sender, rx) = connect(cfg.clone(), addr).await.unwrap();
        let rx = WsPollRecv::new::<AdminResponse>(rx);

        // Getting a sender back isn't enough to know that the connection succeeded because the other side takes a moment to shutdown, try sending to be sure
        if sender
            .request::<AdminRequest, AdminResponse>(AdminRequest::ListDnas)
            .await
            .is_ok()
        {
            handles.push((sender, rx));
            break;
        }
    }

    // Should now be possible to connect new clients
    while handles.len() < MAX_CONNECTIONS {
        let (sender, rx) = connect(cfg.clone(), addr).await.unwrap();
        let rx = WsPollRecv::new::<AdminResponse>(rx);
        let _: AdminResponse = sender
            .request(AdminRequest::ListDnas)
            .await
            .map_err(|e| Error::other(format!("Admin request should succeed because there are enough available connections: {e:?}")))
            .unwrap();
        handles.push((sender, rx));
    }

    conductor_handle.shutdown();
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
// TODO: duplicate/rewrite this to also test happ bundles in addition to dna
async fn concurrent_install_dna() {
    use futures::StreamExt;

    static NUM_DNA: u8 = 50;
    static NUM_CONCURRENT_INSTALLS: u8 = 10;
    static REQ_TIMEOUT_MS: u64 = 15000;

    holochain_trace::test_run();
    // NOTE: This is a full integration test that
    // actually runs the holochain binary

    let admin_port = 0;

    let tmp_dir = TempDir::new().unwrap();
    let path = tmp_dir.path().to_path_buf();
    let data_root_path = path.clone();
    let config = create_config(admin_port, data_root_path.into());
    let config_path = write_config(path, &config);

    let (_holochain, admin_port) = start_holochain(config_path.clone()).await;
    let admin_port = admin_port.await.unwrap();

    let (client, rx) = websocket_client_by_port(admin_port).await.unwrap();
    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    // let before = std::time::Instant::now();

    let install_tasks_stream = futures::stream::iter((0..NUM_DNA).map(|i| {
        let zomes = vec![(TestWasm::Foo.into(), TestWasm::Foo.into())];
        let mut client = client.clone();
        tokio::spawn(async move {
            let name = format!("fake_dna_{}", i);

            // Install Dna
            let dna = holochain_types::test_utils::fake_dna_zomes_named(
                &uuid::Uuid::new_v4().to_string(),
                &name,
                zomes.clone(),
            );
            let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna.clone()).await.unwrap();

            let _cell_id = register_and_install_dna_named(
                &mut client,
                fake_dna_path.clone(),
                None,
                name.clone(),
                name.clone(),
                REQ_TIMEOUT_MS,
            )
            .await;

            // println!(
            //     "[{}] installed app with cell id {} and name {}",
            //     i, _cell_id, name
            // );
        })
    }))
    .buffer_unordered(NUM_CONCURRENT_INSTALLS.into());

    let install_tasks = futures::StreamExt::collect::<Vec<_>>(install_tasks_stream);

    for r in install_tasks.await {
        r.unwrap();
    }

    // println!(
    //     "installed {} dna in {:?}",
    //     NUM_DNA,
    //     before.elapsed()
    // );
}

#[tokio::test(flavor = "multi_thread")]
async fn network_stats() {
    holochain_trace::test_run();

    let mut batch =
        SweetConductorBatch::from_config_rendezvous(2, SweetConductorConfig::rendezvous(true))
            .await;

    let dna_file = SweetDnaFile::unique_empty().await;

    let _ = batch.setup_app("app", &[dna_file]).await.unwrap();

    let (client, _rx) = batch
        .get(0)
        .unwrap()
        .admin_ws_client::<AdminResponse>()
        .await;

    const EXPECT: &str = "backendGoPion";

    let req = AdminRequest::DumpNetworkStats;
    let res: AdminResponse = client.request(req).await.unwrap();
    match res {
        AdminResponse::NetworkStatsDumped(json) => {
            println!("{json}");

            let parsed: serde_json::Value = serde_json::from_str(&json).unwrap();
            let backend = parsed.as_object().unwrap().get("backend").unwrap();
            assert_eq!(EXPECT, backend);
        }
        _ => panic!("unexpected"),
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn full_state_dump_cursor_works() {
    holochain_trace::test_run();

    let mut conductor = SweetConductor::from_standard_config().await;

    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::EmitSignal])
        .await
        .0;

    let app = conductor.setup_app("app", &[dna_file]).await.unwrap();

    let cell_id = app.into_cells()[0].cell_id().clone();

    let (mut client, _rx) = conductor.admin_ws_client::<AppResponse>().await;

    let full_state = dump_full_state(&mut client, cell_id.clone(), None)
        .await
        .unwrap();

    let integrated_ops_count = full_state.integration_dump.integrated.len();
    let validation_limbo_ops_count = full_state.integration_dump.validation_limbo.len();
    let integration_limbo_ops_count = full_state.integration_dump.integration_limbo.len();

    let all_dhts_ops_count =
        integrated_ops_count + validation_limbo_ops_count + integration_limbo_ops_count;
    assert_eq!(7, all_dhts_ops_count);

    // We are assuming we have at least one DhtOp in the Cell
    let full_state = dump_full_state(
        &mut client,
        cell_id,
        Some(full_state.integration_dump.dht_ops_cursor - 1),
    )
    .await
    .unwrap();

    let integrated_ops_count = full_state.integration_dump.integrated.len();
    let validation_limbo_ops_count = full_state.integration_dump.validation_limbo.len();
    let integration_limbo_ops_count = full_state.integration_dump.integration_limbo.len();

    let new_all_dht_ops_count =
        integrated_ops_count + validation_limbo_ops_count + integration_limbo_ops_count;

    assert_eq!(1, new_all_dht_ops_count);
}

#[tokio::test(flavor = "multi_thread")]
async fn admin_allowed_origins() {
    holochain_trace::test_run();

    let conductor = SweetConductor::from_standard_config().await;

    let ports = conductor
        .clone()
        .add_admin_interfaces(vec![AdminInterfaceConfig {
            driver: InterfaceDriver::Websocket {
                port: 0,
                allowed_origins: "http://localhost:3000".to_string().into(),
            },
        }])
        .await
        .unwrap();

    let port = *ports.first().unwrap();
    assert!(connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new(
            format!("localhost:{port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .unwrap()
        )
    )
    .await
    .is_err());

    let port = *ports.first().unwrap();
    let (client, rx) = connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new(
            format!("localhost:{port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .unwrap(),
        )
        .try_set_header("origin", "http://localhost:3000")
        .unwrap(),
    )
    .await
    .unwrap();

    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    let request = AdminRequest::ListAppInterfaces;
    let _: AdminResponse = client.request(request).await.unwrap();
}

#[tokio::test(flavor = "multi_thread")]
async fn holochain_websockets_listen_on_ipv4_and_ipv6() {
    holochain_trace::test_run();

    let conductor = SweetConductor::from_standard_config().await;

    let admin_port = conductor.get_arbitrary_admin_websocket_port().unwrap();

    //
    // Connect to the admin interface on ipv4 and ipv6 localhost
    //

    let (ipv4_admin_sender, rx) = connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new((Ipv4Addr::LOCALHOST, admin_port).into()),
    )
    .await
    .unwrap();
    let _rx4 = WsPollRecv::new::<AdminResponse>(rx);

    let response: AdminResponse = ipv4_admin_sender
        .request(AdminRequest::ListCellIds)
        .await
        .unwrap();
    match response {
        AdminResponse::CellIdsListed(_) => (),
        _ => panic!("unexpected response"),
    }

    let (ipv6_admin_sender, rx) = connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new((Ipv6Addr::LOCALHOST, admin_port).into()),
    )
    .await
    .unwrap();
    let _rx6 = WsPollRecv::new::<AdminResponse>(rx);

    let response: AdminResponse = ipv6_admin_sender
        .request(AdminRequest::ListCellIds)
        .await
        .unwrap();
    match response {
        AdminResponse::CellIdsListed(_) => (),
        _ => panic!("unexpected response"),
    }

    //
    // Do the same for an app interface
    //

    let app_port = conductor
        .clone()
        .add_app_interface(Either::Left(0), AllowedOrigins::Any, None)
        .await
        .unwrap();

    let (ipv4_app_sender, rx) = connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new((Ipv4Addr::LOCALHOST, app_port).into()),
    )
    .await
    .unwrap();
    let _rx4 = WsPollRecv::new::<AppResponse>(rx);
    authenticate_app_ws_client(ipv4_app_sender.clone(), admin_port, "".to_string()).await;

    let response: AppResponse = ipv4_app_sender.request(AppRequest::AppInfo).await.unwrap();
    match response {
        AppResponse::AppInfo(_) => (),
        _ => panic!("unexpected response"),
    }

    let (ipv6_app_sender, rx) = connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new((Ipv6Addr::LOCALHOST, app_port).into()),
    )
    .await
    .unwrap();
    let _rx6 = WsPollRecv::new::<AppResponse>(rx);
    authenticate_app_ws_client(ipv6_app_sender.clone(), admin_port, "".to_string()).await;

    let response: AppResponse = ipv6_app_sender.request(AppRequest::AppInfo).await.unwrap();
    match response {
        AppResponse::AppInfo(_) => (),
        _ => panic!("unexpected response"),
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn emit_signal_after_app_connection_closed() {
    holochain_trace::test_run();

    let mut conductor = SweetConductor::from_standard_config().await;

    // Install an app to emit signals from
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::EmitSignal])
        .await
        .0;
    let installed_app_id: InstalledAppId = "app".into();
    let app = conductor
        .setup_app(&installed_app_id, &[dna_file])
        .await
        .unwrap();
    let cells = app.into_cells();
    let cell = cells.first().unwrap();

    // Connect to the app interface
    let port = conductor
        .clone()
        .add_app_interface(Either::Left(0), AllowedOrigins::Any, None)
        .await
        .expect("Couldn't create app interface");
    let (tx, mut rx) = websocket_client_by_port(port).await.unwrap();

    authenticate_app_ws_client(
        tx.clone(),
        conductor
            .get_arbitrary_admin_websocket_port()
            .expect("No admin ports on this conductor"),
        installed_app_id.clone(),
    )
    .await;

    // Emit a signal
    let _: () = conductor
        .call(&cell.zome(TestWasm::EmitSignal), "emit", ())
        .await;

    // That should be received because the app interface is connected
    let received = rx.recv::<AppResponse>().await.unwrap();
    assert_matches!(received, ReceiveMessage::Signal(_));

    // Drop the app interface connection
    drop(tx);
    drop(rx);

    // Emit another signal
    let _: () = conductor
        .call(&cell.zome(TestWasm::EmitSignal), "emit", ())
        .await;

    // That should not be received because the app interface is disconnected
    // TODO assert that the tasks for this connection were shutdown and removed by this point.
    //      Can't currently do that with TaskMotel which I think is the right thing to query here.
}

#[tokio::test(flavor = "multi_thread")]
async fn filter_messages_that_do_not_deserialize() {
    holochain_trace::test_run();

    let mut conductor = SweetConductor::from_standard_config().await;

    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::EmitSignal])
        .await
        .0;

    conductor.setup_app("app", &[dna_file]).await.unwrap();

    let admin_port = conductor
        .get_arbitrary_admin_websocket_port()
        .expect("No admin port open on conductor");

    let mut config = WebsocketConfig::CLIENT_DEFAULT;
    config.default_request_timeout = Duration::from_secs(1);
    let config = Arc::new(config);

    let (admin_client, rx) = connect(
        config.clone(),
        ConnectRequest::new(
            format!("localhost:{admin_port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .ok_or_else(|| Error::other("Could not resolve localhost"))
                .unwrap(),
        ),
    )
    .await
    .unwrap();
    let _rx = WsPollRecv::new::<AdminResponse>(rx);

    // Try sending an app request to the admin interface
    admin_client
        .request::<_, AppResponse>(AppRequest::AppInfo)
        .await
        .unwrap_err();

    // Now the connection should still be usable
    for _ in 0..5 {
        let response: AdminResponse = admin_client.request(AdminRequest::ListDnas).await.unwrap();
        match response {
            AdminResponse::DnasListed(_) => (),
            r => panic!("unexpected response: {:?}", r),
        }
    }

    let app_port = attach_app_interface(&admin_client, None).await;

    let (app_client, app_rx) = connect(
        config,
        ConnectRequest::new(
            format!("localhost:{app_port}")
                .to_socket_addrs()
                .unwrap()
                .next()
                .ok_or_else(|| Error::other("Could not resolve localhost"))
                .unwrap(),
        ),
    )
    .await
    .unwrap();
    let _app_rx = WsPollRecv::new::<AppResponse>(app_rx);
    authenticate_app_ws_client(app_client.clone(), admin_port, "test".to_string()).await;

    // Try sending an admin request to the app interface
    app_client
        .request::<_, AdminResponse>(AdminRequest::ListDnas)
        .await
        .unwrap_err();

    // Now the connection should still be usable
    for _ in 0..5 {
        let response: AppResponse = app_client.request(AppRequest::AppInfo).await.unwrap();
        match response {
            AppResponse::AppInfo(_) => (),
            r => panic!("unexpected response: {:?}", r),
        }
    }
}

struct TestCase {
    cell_id: CellId,
    admin_tx: WebsocketSender,
    _admin_rx: WsPollRecv,
    app_tx: WebsocketSender,
    app_rx: WebsocketReceiver,
    admin_port: u16,
    holochain: SupervisedChild,
    config_path: PathBuf,
    zome_name: ZomeName,
    fn_name: FunctionName,
    cap_secret: CapSecret,
    signing_keypair: SigningKey,
}

impl TestCase {
    async fn new() -> Self {
        holochain_trace::test_run();
        let admin_port = 0;

        let tmp_dir = TempDir::new().unwrap();
        let path = tmp_dir.into_path();
        let environment_path = path.clone();
        let config = create_config(admin_port, environment_path.into());
        let config_path = write_config(path, &config);

        let (holochain, admin_port) = start_holochain(config_path.clone()).await;
        let admin_port = admin_port.await.unwrap();

        let (mut admin_tx, admin_rx) = websocket_client_by_port(admin_port).await.unwrap();
        let _admin_rx = WsPollRecv::new::<AdminResponse>(admin_rx);

        let uuid = uuid::Uuid::new_v4();
        let dna = fake_dna_zomes(
            &uuid.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );

        // Install Dna
        let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna.clone()).await.unwrap();
        let cell_id =
            register_and_install_dna(&mut admin_tx, fake_dna_path, None, "".into(), 10000)
                .await
                .unwrap();
        let installed_dna_hash = cell_id.dna_hash().clone();

        // List Dnas
        let request = AdminRequest::ListDnas;
        let response = admin_tx.request(request);
        let response = check_timeout(response, 15000).await.unwrap();

        assert_matches!(response, AdminResponse::DnasListed(a) if a.contains(&installed_dna_hash));

        // Activate cells
        let request = AdminRequest::EnableApp {
            installed_app_id: "test".to_string(),
        };
        let response = admin_tx.request(request);
        let response = check_timeout(response, 3000).await.unwrap();
        assert_matches!(response, AdminResponse::AppEnabled { .. });

        // Attach App Interface
        let app_port = attach_app_interface(&admin_tx, None).await;

        let (app_tx, app_rx) = websocket_client_by_port(app_port).await.unwrap();
        authenticate_app_ws_client(app_tx.clone(), admin_port, "test".to_string()).await;

        let zome_name = TestWasm::Foo.coordinator_zome_name();
        let fn_name = FunctionName("foo".into());

        // Generate signing key pair
        let mut rng = OsRng;
        let signing_keypair = ed25519_dalek::SigningKey::generate(&mut rng);
        let signing_key =
            AgentPubKey::from_raw_32(signing_keypair.verifying_key().as_bytes().to_vec());

        // Grant zome call capability for agent
        let cap_secret = grant_zome_call_capability(
            &mut admin_tx,
            &cell_id,
            zome_name.clone(),
            fn_name.clone(),
            signing_key,
        )
        .await
        .unwrap();

        TestCase {
            cell_id,
            admin_tx,
            _admin_rx,
            admin_port,
            app_tx,
            app_rx,
            holochain,
            config_path,
            zome_name,
            fn_name,
            cap_secret,
            signing_keypair,
        }
    }
}



================================================
File: crates/holochain_cascade/README.md
================================================
# holochain_cascade

## Cascade
### Retrieve vs Get
Get checks CRUD metadata before returning an the data
where as retrieve only checks that where the data was found
the appropriate validation has been run.



================================================
File: crates/holochain_cascade/Cargo.toml
================================================
[package]
name = "holochain_cascade"
version = "0.5.0-dev.21"
description = "Logic for cascading updates to Holochain state and network interaction"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_cascade"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
fixt = { version = "^0.5.0-dev.1", path = "../fixt" }
futures = "0.3"
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash", features = [
  "full",
] }
holochain_chc = { version = "^0.2.0-dev.21", path = "../holochain_chc" }
holochain_sqlite = { version = "^0.5.0-dev.19", path = "../holochain_sqlite" }
holochain_p2p = { version = "^0.5.0-dev.21", path = "../holochain_p2p" }
holochain_serialized_bytes = "=0.0.55"
holochain_state = { version = "^0.5.0-dev.21", path = "../holochain_state" }
holochain_types = { version = "^0.5.0-dev.21", path = "../holochain_types" }
holochain_trace = { version = "^0.5.0-dev.1", path = "../holochain_trace" }
holochain_util = { version = "^0.5.0-dev.1", path = "../holochain_util" }
holochain_zome_types = { version = "^0.5.0-dev.17", path = "../holochain_zome_types" }
holochain_nonce = { version = "^0.5.0-dev.2", path = "../holochain_nonce" }
kitsune_p2p = { version = "^0.5.0-dev.13", path = "../kitsune_p2p/kitsune_p2p" }
parking_lot = "0.12.1"
tokio = { version = "1.36.0", features = ["full"] }
thiserror = "1.0"
tracing = "0.1"
opentelemetry_api = { version = "=0.20.0", features = ["metrics"] }

kitsune2_api = "0.0.1-alpha.1"

async-trait = "0.1"
mockall = { version = "0.11.3", optional = true }

[dev-dependencies]
holochain_cascade = { path = ".", features = ["test_utils"] }

isotest = "0"
matches = "0.1"
pretty_assertions = "1.4"
test-case = "3.3"

[lints]
workspace = true

[features]
default = []

test_utils = [
  "mockall",
  "holochain_chc/test_utils",
  "holochain_p2p/test_utils",
  "holochain_types/test_utils",
  "holochain_state/test_utils",
]

instrument = []

sqlite-encrypted = [
  "holo_hash/sqlite-encrypted",
  "holochain_sqlite/sqlite-encrypted",
  "holochain_state/sqlite-encrypted",
  "holochain_types/sqlite-encrypted",
  "holochain_zome_types/sqlite-encrypted",
  "kitsune_p2p/sqlite-encrypted",
]
sqlite = [
  "holo_hash/sqlite",
  "holochain_sqlite/sqlite",
  "holochain_state/sqlite",
  "holochain_types/sqlite",
  "holochain_zome_types/sqlite",
  "kitsune_p2p/sqlite",
]

unstable-warrants = ["holochain_state/unstable-warrants"]



================================================
File: crates/holochain_cascade/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.21

## 0.5.0-dev.20

## 0.5.0-dev.19

## 0.5.0-dev.18

## 0.5.0-dev.17

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.28

## 0.4.0-dev.27

## 0.4.0-dev.26

## 0.4.0-dev.25

## 0.4.0-dev.24

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.47

## 0.3.0-beta-dev.46

## 0.3.0-beta-dev.45

## 0.3.0-beta-dev.44

## 0.3.0-beta-dev.43

## 0.3.0-beta-dev.42

## 0.3.0-beta-dev.41

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

- Change licensing from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.7

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.74

## 0.0.73

## 0.0.72

## 0.0.71

## 0.0.70

## 0.0.69

## 0.0.68

## 0.0.67

## 0.0.66

## 0.0.65

## 0.0.64

## 0.0.63

## 0.0.62

## 0.0.61

## 0.0.60

## 0.0.59

## 0.0.58

## 0.0.57

## 0.0.56

## 0.0.55

## 0.0.54

## 0.0.53

## 0.0.52

## 0.0.51

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

## 0.0.43

## 0.0.42

## 0.0.41

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

## 0.0.21

- Gets wont return private entries unless you are have committed a header for that entry. [\#1157](https://github.com/holochain/holochain/pull/1157)

## 0.0.20

## 0.0.19

- Fixes database queries that were running on the runtime thread instead of the background thread. Makes the connections wait for a permit before taking a database connection from the pool. [\#1145](https://github.com/holochain/holochain/pull/1145)

## 0.0.18

## 0.0.17

## 0.0.16

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

- Fix authority side get\_links query [\#1027](https://github.com/holochain/holochain/pull/1027).

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/holochain_cascade/src/agent_activity.rs
================================================
use super::*;
use holochain_p2p::actor::GetActivityOptions;

#[allow(clippy::result_large_err)]
pub(crate) fn merge_activities(
    agent: AgentPubKey,
    options: &GetActivityOptions,
    results: Vec<AgentActivityResponse>,
) -> CascadeResult<AgentActivityResponse> {
    if !options.include_rejected_activity
        && !options.include_valid_activity
        && !options.include_warrants
    {
        return Ok(merge_status_only(agent, results));
    }
    Ok(merge_activity_responses(agent, options, results))
}

fn merge_activity_responses(
    agent: AgentPubKey,
    options: &GetActivityOptions,
    results: Vec<AgentActivityResponse>,
) -> AgentActivityResponse {
    let mut status = ChainStatus::Empty;
    let mut valid = if options.include_valid_activity {
        if options.include_full_records {
            ChainItems::Full(Vec::new())
        } else {
            ChainItems::Hashes(Vec::new())
        }
    } else {
        ChainItems::NotRequested
    };
    let mut rejected = if options.include_rejected_activity {
        if options.include_full_records {
            ChainItems::Full(Vec::new())
        } else {
            ChainItems::Hashes(Vec::new())
        }
    } else {
        ChainItems::NotRequested
    };
    let mut warrants = Vec::new();
    let mut merged_highest_observed = None;
    for result in results {
        let AgentActivityResponse {
            agent: the_agent,
            highest_observed,
            valid_activity,
            rejected_activity,
            warrants: these_warrants,
            status: _,
        } = result;

        if the_agent != agent {
            continue;
        }

        warrants.extend(these_warrants);

        match (merged_highest_observed.take(), highest_observed) {
            (None, None) => {}
            (Some(h), None) | (None, Some(h)) => {
                merged_highest_observed = Some(h);
            }
            (Some(a), Some(b)) => {
                let c = if a.action_seq > b.action_seq { a } else { b };
                merged_highest_observed = Some(c);
            }
        }

        let (s, v, r) = if options.include_valid_activity && options.include_rejected_activity {
            match (valid, rejected, valid_activity, rejected_activity) {
                (
                    ChainItems::Full(mut v),
                    ChainItems::Full(mut r),
                    ChainItems::Full(valid),
                    ChainItems::Full(rejected),
                ) if options.include_full_records => {
                    v.extend(valid);
                    r.extend(rejected);
                    let (status, valid, rejected) =
                        compute_chain_status(v.into_iter(), r.into_iter());

                    (status, valid.to_chain_items(), rejected.to_chain_items())
                }
                (
                    ChainItems::Hashes(mut v),
                    ChainItems::Hashes(mut r),
                    ChainItems::Hashes(valid),
                    ChainItems::Hashes(rejected),
                ) => {
                    v.extend(valid);
                    r.extend(rejected);
                    let (status, valid, rejected) =
                        compute_chain_status(v.into_iter(), r.into_iter());

                    (status, valid.to_chain_items(), rejected.to_chain_items())
                }
                e => {
                    warn!("Invalid combination of chain items in merge_hashes: {e:?}");
                    (
                        ChainStatus::Empty,
                        ChainItems::NotRequested,
                        ChainItems::NotRequested,
                    )
                }
            }
        } else if options.include_valid_activity {
            match (valid, rejected, valid_activity, rejected_activity) {
                (ChainItems::Full(mut v), _, ChainItems::Full(valid), _)
                    if options.include_full_records =>
                {
                    v.extend(valid);
                    let (status, valid, rejected) =
                        compute_chain_status(v.into_iter(), Vec::with_capacity(0).into_iter());

                    (
                        status,
                        valid.to_chain_items(),
                        if rejected.is_empty() {
                            ChainItems::NotRequested
                        } else {
                            rejected.to_chain_items()
                        },
                    )
                }
                (ChainItems::Hashes(mut v), _, ChainItems::Hashes(valid), _) => {
                    v.extend(valid);
                    let (status, valid, rejected) =
                        compute_chain_status(v.into_iter(), Vec::with_capacity(0).into_iter());

                    (
                        status,
                        valid.to_chain_items(),
                        if rejected.is_empty() {
                            ChainItems::NotRequested
                        } else {
                            rejected.to_chain_items()
                        },
                    )
                }
                e => {
                    warn!("Invalid combination of chain items in merge_hashes: {e:?}");
                    (
                        ChainStatus::Empty,
                        ChainItems::NotRequested,
                        ChainItems::NotRequested,
                    )
                }
            }
        } else if options.include_rejected_activity {
            match (valid, rejected, valid_activity, rejected_activity) {
                (_, ChainItems::Full(mut r), _, ChainItems::Full(rejected))
                    if options.include_full_records =>
                {
                    r.extend(rejected);
                    let (status, valid, rejected) =
                        compute_chain_status(Vec::with_capacity(0).into_iter(), r.into_iter());

                    (
                        status,
                        if valid.is_empty() {
                            ChainItems::NotRequested
                        } else {
                            valid.to_chain_items()
                        },
                        rejected.to_chain_items(),
                    )
                }
                (_, ChainItems::Hashes(mut r), _, ChainItems::Hashes(rejected)) => {
                    r.extend(rejected);
                    let (status, valid, rejected) =
                        compute_chain_status(Vec::with_capacity(0).into_iter(), r.into_iter());

                    (
                        status,
                        if valid.is_empty() {
                            ChainItems::NotRequested
                        } else {
                            valid.to_chain_items()
                        },
                        rejected.to_chain_items(),
                    )
                }
                e => {
                    warn!("Invalid combination of chain items in merge_hashes: {e:?}");
                    (
                        ChainStatus::Empty,
                        ChainItems::NotRequested,
                        ChainItems::NotRequested,
                    )
                }
            }
        } else {
            (
                ChainStatus::Empty,
                ChainItems::NotRequested,
                ChainItems::NotRequested,
            )
        };

        valid = v;
        rejected = r;
        status = s;
    }

    AgentActivityResponse {
        status,
        agent,
        valid_activity: valid,
        rejected_activity: rejected,
        warrants,
        highest_observed: merged_highest_observed,
    }
}

pub(crate) fn compute_chain_status<T: ActionSequenceAndHash>(
    valid: impl Iterator<Item = T>,
    rejected: impl Iterator<Item = T>,
) -> (ChainStatus, Vec<T>, Vec<T>) {
    let mut valid: Vec<_> = valid.collect();
    let mut rejected: Vec<_> = rejected.collect();

    // Sort ascending.
    valid.sort_unstable_by_key(|a| a.action_seq());
    rejected.sort_unstable_by_key(|a| a.action_seq());

    let mut valid_out = Vec::with_capacity(valid.len());
    let mut status = None;

    for current in valid {
        if status.is_none() {
            let fork = valid_out.last().and_then(|v: &T| {
                if current.action_seq() == v.action_seq() {
                    Some(v)
                } else {
                    None
                }
            });

            if let Some(fork) = fork {
                status = Some(ChainStatus::Forked(ChainFork {
                    fork_seq: current.action_seq(),
                    first_action: current.address().clone(),
                    second_action: fork.address().clone(),
                }));
            }
        }

        valid_out.push(current);
    }

    // The chain status will have been set if we found a fork, otherwise decide the status from
    // the last valid and first rejected actions.
    let status = status.unwrap_or_else(|| match (valid_out.last(), rejected.first()) {
        (None, None) => ChainStatus::Empty,
        (Some(v), None) => ChainStatus::Valid(ChainHead {
            action_seq: v.action_seq(),
            hash: v.address().clone(),
        }),
        (None, Some(r)) => ChainStatus::Invalid(ChainHead {
            action_seq: r.action_seq(),
            hash: r.address().clone(),
        }),
        (Some(_), Some(r)) => ChainStatus::Invalid(ChainHead {
            action_seq: r.action_seq(),
            hash: r.address().clone(),
        }),
    });

    (status, valid_out, rejected)
}

fn merge_status_only(
    agent: AgentPubKey,
    results: Vec<AgentActivityResponse>,
) -> AgentActivityResponse {
    let mut merged_status = None;
    let mut merged_highest_observed = None;
    for result in results {
        let AgentActivityResponse {
            status,
            agent: the_agent,
            highest_observed,
            ..
        } = result;
        if the_agent != agent {
            continue;
        }
        match (merged_highest_observed.take(), highest_observed) {
            (None, None) => {}
            (Some(h), None) | (None, Some(h)) => {
                merged_highest_observed = Some(h);
            }
            (Some(a), Some(b)) => {
                let c = if a.action_seq > b.action_seq { a } else { b };
                merged_highest_observed = Some(c);
            }
        }
        match merged_status.take() {
            Some(last) => match (status, last) {
                (ChainStatus::Empty, ChainStatus::Empty) => {
                    merged_status = Some(ChainStatus::Empty);
                }
                (ChainStatus::Empty, ChainStatus::Valid(c))
                | (ChainStatus::Valid(c), ChainStatus::Empty) => {
                    merged_status = Some(ChainStatus::Valid(c));
                }
                (ChainStatus::Empty, ChainStatus::Forked(c))
                | (ChainStatus::Forked(c), ChainStatus::Empty) => {
                    merged_status = Some(ChainStatus::Forked(c));
                }
                (ChainStatus::Empty, ChainStatus::Invalid(c))
                | (ChainStatus::Invalid(c), ChainStatus::Empty) => {
                    merged_status = Some(ChainStatus::Invalid(c));
                }
                (ChainStatus::Valid(a), ChainStatus::Valid(b)) => {
                    let c = if a.action_seq > b.action_seq { a } else { b };
                    merged_status = Some(ChainStatus::Valid(c));
                }
                (ChainStatus::Valid(_), ChainStatus::Forked(c))
                | (ChainStatus::Forked(c), ChainStatus::Valid(_)) => {
                    // If the valid and forked chain heads are the same then they are in conflict here.
                    // TODO: BACKLOG: When we handle conflicts this should count as a conflict.
                    merged_status = Some(ChainStatus::Forked(c));
                }
                (ChainStatus::Invalid(c), ChainStatus::Valid(_))
                | (ChainStatus::Valid(_), ChainStatus::Invalid(c)) => {
                    // If the valid and invalid chain heads are the same then they are in conflict here.
                    // TODO: BACKLOG: When we handle conflicts this should count as a conflict.
                    merged_status = Some(ChainStatus::Invalid(c));
                }
                (ChainStatus::Forked(a), ChainStatus::Forked(b)) => {
                    let c = if a.fork_seq < b.fork_seq { a } else { b };
                    merged_status = Some(ChainStatus::Forked(c));
                }
                (ChainStatus::Invalid(a), ChainStatus::Invalid(b)) => {
                    let c = if a.action_seq < b.action_seq { a } else { b };
                    merged_status = Some(ChainStatus::Invalid(c));
                }
                (ChainStatus::Forked(a), ChainStatus::Invalid(b)) => {
                    if a.fork_seq < b.action_seq {
                        merged_status = Some(ChainStatus::Forked(a));
                    } else {
                        merged_status = Some(ChainStatus::Invalid(b));
                    };
                }
                (ChainStatus::Invalid(a), ChainStatus::Forked(b)) => {
                    if a.action_seq < b.fork_seq {
                        merged_status = Some(ChainStatus::Invalid(a));
                    } else {
                        merged_status = Some(ChainStatus::Forked(b));
                    };
                }
            },
            None => {
                merged_status = Some(status);
            }
        }
    }
    AgentActivityResponse {
        status: merged_status.unwrap_or(ChainStatus::Empty),
        agent,
        valid_activity: ChainItems::NotRequested,
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        highest_observed: merged_highest_observed,
    }
}



================================================
File: crates/holochain_cascade/src/authority.rs
================================================
//! Functions for the various authorities to handle queries

use self::get_agent_activity_query::must_get_agent_activity::must_get_agent_activity;
use self::get_entry_ops_query::GetEntryOpsQuery;
use self::get_links_ops_query::GetLinksOpsQuery;
use self::{
    get_agent_activity_query::deterministic::DeterministicGetAgentActivityQuery,
    get_record_query::GetRecordOpsQuery,
};
use super::error::CascadeResult;
use crate::authority::get_agent_activity_query::hashes::GetAgentActivityHashesQuery;
use crate::authority::get_agent_activity_query::records::GetAgentActivityRecordsQuery;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holochain_state::query::link::GetLinksQuery;
use holochain_state::query::CascadeTxnWrapper;
use holochain_state::query::{Query, Store};
use holochain_types::prelude::*;
use holochain_zome_types::agent_activity::DeterministicGetAgentActivityFilter;

#[cfg(test)]
mod test;

pub(crate) mod get_agent_activity_query;
pub(crate) mod get_entry_ops_query;
pub(crate) mod get_links_ops_query;
pub(crate) mod get_record_query;

/// Handler for get_entry query to an Entry authority
#[cfg_attr(feature = "instrument", tracing::instrument(skip(db)))]
pub async fn handle_get_entry(
    db: DbRead<DbKindDht>,
    hash: EntryHash,
    _options: holochain_p2p::event::GetOptions,
) -> CascadeResult<WireEntryOps> {
    let query = GetEntryOpsQuery::new(hash);
    let results = db
        .read_async(move |txn| query.run(CascadeTxnWrapper::from(txn)))
        .await?;
    Ok(results)
}

/// Handler for get_record query to a Record authority
#[cfg_attr(feature = "instrument", tracing::instrument(skip(env)))]
pub async fn handle_get_record(
    env: DbRead<DbKindDht>,
    hash: ActionHash,
    options: holochain_p2p::event::GetOptions,
) -> CascadeResult<WireRecordOps> {
    let query = GetRecordOpsQuery::new(hash, options);
    let results = env
        .read_async(move |txn| query.run(CascadeTxnWrapper::from(txn)))
        .await?;
    Ok(results)
}

/// Handler for get_agent_activity query to an Activity authority.
#[cfg_attr(feature = "instrument", tracing::instrument(skip(env)))]
pub async fn handle_get_agent_activity(
    env: DbRead<DbKindDht>,
    agent: AgentPubKey,
    query: ChainQueryFilter,
    options: holochain_p2p::event::GetActivityOptions,
) -> CascadeResult<AgentActivityResponse> {
    let results = env
        .read_async(move |txn| -> CascadeResult<AgentActivityResponse> {
            let txn = CascadeTxnWrapper::from(txn);

            let warrants =
                txn.get_warrants_for_basis(&AnyLinkableHash::from(agent.clone()), true)?;

            let mut activity_response = if options.include_full_records {
                // If the caller wanted records, prioritise giving those back.
                GetAgentActivityRecordsQuery::new(agent, query, options).run(txn)?
            } else {
                // Otherwise, just give back the hashes.
                GetAgentActivityHashesQuery::new(agent, query, options).run(txn)?
            };

            if !warrants.is_empty() {
                // TODO why did we retrieve warrants in the activity query if we're going to overwrite them here?
                activity_response.warrants =
                    warrants.into_iter().map(|w| w.into_warrant()).collect();
            }

            Ok(activity_response)
        })
        .await?;

    Ok(results)
}

/// Handler for must_get_agent_activity query to an Activity authority
#[cfg_attr(feature = "instrument", tracing::instrument(skip(env)))]
pub async fn handle_must_get_agent_activity(
    env: DbRead<DbKindDht>,
    author: AgentPubKey,
    filter: ChainFilter,
) -> CascadeResult<MustGetAgentActivityResponse> {
    Ok(must_get_agent_activity(env, author, filter).await?)
}

/// Handler for get_agent_activity_deterministic query to an Activity authority
#[cfg_attr(feature = "instrument", tracing::instrument(skip(env)))]
pub async fn handle_get_agent_activity_deterministic(
    env: DbRead<DbKindDht>,
    agent: AgentPubKey,
    filter: DeterministicGetAgentActivityFilter,
    options: holochain_p2p::event::GetActivityOptions,
) -> CascadeResult<DeterministicGetAgentActivityResponse> {
    let query = DeterministicGetAgentActivityQuery::new(agent, filter, options);
    let results = env
        .read_async(move |txn| query.run(CascadeTxnWrapper::from(txn)))
        .await?;
    Ok(results)
}

/// Handler for get_links query to a Record/Entry authority
#[cfg_attr(feature = "instrument", tracing::instrument(skip(env, _options)))]
pub async fn handle_get_links(
    env: DbRead<DbKindDht>,
    link_key: WireLinkKey,
    _options: holochain_p2p::event::GetLinksOptions,
) -> CascadeResult<WireLinkOps> {
    let query = GetLinksOpsQuery::new(link_key);
    let results = env
        .read_async(move |txn| query.run(CascadeTxnWrapper::from(txn)))
        .await?;
    Ok(results)
}

/// Handler for querying links
#[cfg_attr(feature = "instrument", tracing::instrument(skip(db)))]
pub async fn handle_get_links_query(
    db: DbRead<DbKindDht>,
    query: WireLinkQuery,
) -> CascadeResult<Vec<Link>> {
    let get_links_query = GetLinksQuery::new(
        query.base.clone(),
        query.link_type.clone(),
        query.tag_prefix.clone(),
        query.into(),
    );
    Ok(db
        .read_async(move |txn| get_links_query.run(CascadeTxnWrapper::from(txn)))
        .await?)
}



================================================
File: crates/holochain_cascade/src/error.rs
================================================
#![allow(missing_docs)]

use holo_hash::{ActionHash, AnyDhtHash};
use holochain_p2p::HolochainP2pError;
use holochain_serialized_bytes::SerializedBytesError;
use holochain_sqlite::error::DatabaseError;
use holochain_state::source_chain::SourceChainError;
use holochain_types::prelude::*;
use holochain_zome_types::action::conversions::WrongActionError;
// use holochain::conductor::CellError;
// use holochain::core::workflow::produce_dht_ops_workflow::dht_op_light::error::DhtOpConvertError;
use thiserror::Error;
use tokio::task::JoinError;

#[derive(Error, Debug)]
pub enum CascadeError {
    #[error(transparent)]
    DatabaseError(#[from] DatabaseError),

    #[error(transparent)]
    RecordGroupError(#[from] RecordGroupError),

    #[error(transparent)]
    ActionError(#[from] ActionError),

    #[error("Expected this Action to contain an Entry: {0}")]
    EntryMissing(ActionHash),

    #[error(transparent)]
    DhtOpError(#[from] DhtOpError),

    #[error("Got an invalid response from an authority for the request hash: {0:?}")]
    InvalidResponse(AnyDhtHash),

    #[error(transparent)]
    JoinError(#[from] JoinError),

    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),

    #[error(transparent)]
    NetworkError(#[from] HolochainP2pError),

    #[error(transparent)]
    SerializedBytesError(#[from] SerializedBytesError),

    #[error(transparent)]
    WrongActionError(#[from] WrongActionError),

    #[error("Cell is an authority for is missing or incorrect: {0}")]
    AuthorityDataError(#[from] AuthorityDataError),

    #[error(transparent)]
    QueryError(#[from] holochain_state::query::StateQueryError),

    #[error(transparent)]
    StateMutationError(#[from] holochain_state::mutations::StateMutationError),

    #[error(transparent)]
    SyncScratchError(#[from] holochain_state::scratch::SyncScratchError),
}

pub type CascadeResult<T> = Result<T, CascadeError>;

#[derive(Error, Debug)]
pub enum AuthorityDataError {
    // #[error(transparent)]
    // DhtOpConvertError(#[from] DhtOpConvertError),
    #[error(transparent)]
    WrongActionError(#[from] WrongActionError),
    #[error(transparent)]
    ActionError(#[from] ActionError),
    #[error("Missing record data: {0:?}")]
    MissingData(String),
    #[error("Missing metadata: {0:?}")]
    MissingMetadata(String),
}

impl AuthorityDataError {
    pub fn missing_data<T: std::fmt::Debug>(data: T) -> CascadeError {
        Self::MissingData(format!("Missing action {:?}", data)).into()
    }
    pub fn missing_data_entry<T: std::fmt::Debug>(data: T) -> CascadeError {
        Self::MissingData(format!("Missing entry for action {:?}", data)).into()
    }
    pub fn missing_metadata<T: std::fmt::Debug>(data: T) -> CascadeError {
        Self::MissingMetadata(format!("{:?}", data)).into()
    }
}



================================================
File: crates/holochain_cascade/src/lib.rs
================================================
//! The Cascade is a multi-tiered accessor for Holochain DHT data.
//!
//! Note that the docs for this crate are admittedly a bit *loose and imprecise*,
//! but they are not expected to be *incorrect*.
//!
//! It is named "the Cascade" because it performs "cascading" gets across multiple sources.
//! In general (but not in all cases), the flow is something like:
//! - First attempts to read the local storage
//! - If that fails, attempt to read data from the network cache
//! - If that fails, do a network request for the data, caching it if found
//!
//! ## Retrieve vs Get
//!
//! There are two words used in cascade functions: "get", and "retrieve".
//! They mean distinct things:
//!
//! - "get" ignores invalid data, and sometimes takes into account CRUD metadata
//!     before returning the data, so for instance, Deletes
//!     are allowed to annihilate Creates so that neither is returned. This is a more
//!     "refined" form of fetching data.
//! - "retrieve" only fetches the data if it exists, without regard to validation status.
//!     This is a more "raw" form of fetching data.
//!
#![warn(missing_docs)]

use error::CascadeResult;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holo_hash::AnyDhtHash;
use holo_hash::EntryHash;
use holochain_p2p::actor::GetActivityOptions;
use holochain_p2p::actor::GetLinksOptions;
use holochain_p2p::actor::GetOptions as NetworkGetOptions;
use holochain_p2p::GenericNetwork;
use holochain_state::host_fn_workspace::HostFnStores;
use holochain_state::host_fn_workspace::HostFnWorkspace;
use holochain_state::mutations::insert_action;
use holochain_state::mutations::insert_entry;
use holochain_state::mutations::insert_op_lite;
use holochain_state::mutations::set_validation_status;
use holochain_state::prelude::*;
use holochain_state::query::entry_details::GetEntryDetailsQuery;
use holochain_state::query::link::{GetLinksFilter, GetLinksQuery};
use holochain_state::query::link_details::GetLinkDetailsQuery;
use holochain_state::query::live_entry::GetLiveEntryQuery;
use holochain_state::query::live_record::GetLiveRecordQuery;
use holochain_state::query::record_details::GetRecordDetailsQuery;
use holochain_state::query::DbScratch;
use holochain_state::query::PrivateDataQuery;
use holochain_state::scratch::SyncScratch;
use metrics::create_cascade_duration_metric;
use metrics::CascadeDurationMetric;
use std::collections::HashMap;
use std::collections::HashSet;
use std::sync::Arc;
use std::time::Instant;
use tracing::*;

pub mod authority;
pub mod error;

mod agent_activity;
mod metrics;

#[cfg(feature = "test_utils")]
pub mod test_utils;

/// Get an item from an option
/// or return early from the function
macro_rules! some_or_return {
    ($n:expr) => {
        match $n {
            Some(n) => n,
            None => return Ok(()),
        }
    };
    ($n:expr, $ret:expr) => {
        match $n {
            Some(n) => n,
            None => return Ok($ret),
        }
    };
}

/// Marks whether data came from a local store or another node on the network
#[derive(Debug, Clone)]
pub enum CascadeSource {
    /// Data came from a local store
    Local,
    /// Data came from another node on the network
    Network,
}

/// The Cascade is a multi-tiered accessor for Holochain DHT data.
///
/// See the module-level docs for more info.
#[derive(Clone)]
pub struct CascadeImpl {
    authored: Option<DbRead<DbKindAuthored>>,
    dht: Option<DbRead<DbKindDht>>,
    cache: Option<DbWrite<DbKindCache>>,
    scratch: Option<SyncScratch>,
    network: Option<GenericNetwork>,
    private_data: Option<Arc<AgentPubKey>>,
    duration_metric: &'static CascadeDurationMetric,
}

impl CascadeImpl {
    /// Add the authored env to the cascade.
    pub fn with_authored(self, authored: DbRead<DbKindAuthored>) -> Self {
        Self {
            authored: Some(authored),
            ..self
        }
    }

    /// Add the ability to access private entries for this agent.
    pub fn with_private_data(self, author: Arc<AgentPubKey>) -> Self {
        Self {
            private_data: Some(author),
            ..self
        }
    }

    /// Add the dht env to the cascade.
    pub fn with_dht(self, dht: DbRead<DbKindDht>) -> Self {
        Self {
            dht: Some(dht),
            ..self
        }
    }

    /// Add the cache to the cascade.
    pub fn with_cache(self, cache: DbWrite<DbKindCache>) -> Self {
        Self {
            cache: Some(cache),
            ..self
        }
    }

    /// Add the cache to the cascade.
    pub fn with_scratch(self, scratch: SyncScratch) -> Self {
        Self {
            scratch: Some(scratch),
            ..self
        }
    }

    /// Add the network and cache to the cascade.
    pub fn with_network(
        self,
        network: GenericNetwork,
        cache_db: DbWrite<DbKindCache>,
    ) -> CascadeImpl {
        CascadeImpl {
            authored: self.authored,
            dht: self.dht,
            scratch: self.scratch,
            private_data: self.private_data,
            cache: Some(cache_db),
            network: Some(network),
            duration_metric: create_cascade_duration_metric(),
        }
    }

    /// Constructs an empty [Cascade].
    pub fn empty() -> Self {
        Self {
            authored: None,
            dht: None,
            network: None,
            cache: None,
            scratch: None,
            private_data: None,
            duration_metric: create_cascade_duration_metric(),
        }
    }

    /// Construct a [Cascade] with network access
    pub fn from_workspace_and_network<AuthorDb, DhtDb>(
        workspace: &HostFnWorkspace<AuthorDb, DhtDb>,
        network: GenericNetwork,
    ) -> CascadeImpl
    where
        AuthorDb: ReadAccess<DbKindAuthored>,
        DhtDb: ReadAccess<DbKindDht>,
    {
        let HostFnStores {
            authored,
            dht,
            cache,
            scratch,
        } = workspace.stores();
        let private_data = workspace.author();
        CascadeImpl {
            authored: Some(authored),
            dht: Some(dht),
            cache: Some(cache),
            private_data,
            scratch,
            network: Some(network),
            duration_metric: create_cascade_duration_metric(),
        }
    }

    /// Construct a [Cascade] with local-only access to the provided stores
    pub fn from_workspace_stores(stores: HostFnStores, author: Option<Arc<AgentPubKey>>) -> Self {
        let HostFnStores {
            authored,
            dht,
            cache,
            scratch,
        } = stores;
        Self {
            authored: Some(authored),
            dht: Some(dht),
            cache: Some(cache),
            scratch,
            network: None,
            private_data: author,
            duration_metric: create_cascade_duration_metric(),
        }
    }

    /// Getter
    pub fn cache(&self) -> Option<&DbWrite<DbKindCache>> {
        self.cache.as_ref()
    }
}

/// TODO
#[async_trait::async_trait]
#[cfg_attr(feature = "test_utils", mockall::automock)]
pub trait Cascade {
    /// Retrieve [`Entry`] either locally or from an authority.
    /// Data might not have been validated yet by the authority.
    async fn retrieve_entry(
        &self,
        hash: EntryHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(EntryHashed, CascadeSource)>>;

    /// Retrieve [`SignedActionHashed`] from either locally or from an authority.
    /// Data might not have been validated yet by the authority.
    async fn retrieve_action(
        &self,
        hash: ActionHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(SignedActionHashed, CascadeSource)>>;

    /// Retrieve data from either locally or from an authority.
    /// Data might not have been validated yet by the authority.
    async fn retrieve(
        &self,
        hash: AnyDhtHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(Record, CascadeSource)>>;
}

#[async_trait::async_trait]
impl Cascade for CascadeImpl {
    async fn retrieve_entry(
        &self,
        hash: EntryHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(EntryHashed, CascadeSource)>> {
        let private_data = self.private_data.clone();
        let result = self
            .find_map({
                let hash = hash.clone();
                move |store| {
                    Ok(store.get_public_or_authored_entry(
                        &hash,
                        private_data.as_ref().map(|a| a.as_ref()),
                    )?)
                }
            })
            .await?;
        if result.is_some() {
            return Ok(result.map(|e| (EntryHashed::from_content_sync(e), CascadeSource::Local)));
        }
        options.request_type = holochain_p2p::event::GetRequest::Pending;
        self.fetch_record(hash.clone().into(), options).await?;

        // Check if we have the data now after the network call.
        let private_data = self.private_data.clone();
        let result = self
            .find_map({
                let hash = hash.clone();
                move |store| {
                    Ok(store.get_public_or_authored_entry(
                        &hash,
                        private_data.as_ref().map(|a| a.as_ref()),
                    )?)
                }
            })
            .await?;
        Ok(result.map(|e| (EntryHashed::from_content_sync(e), CascadeSource::Network)))
    }

    async fn retrieve_action(
        &self,
        hash: ActionHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(SignedActionHashed, CascadeSource)>> {
        let result = self
            .find_map({
                let hash = hash.clone();
                move |store| Ok(store.get_action(&hash)?)
            })
            .await?;
        if result.is_some() {
            return Ok(result.map(|a| (a, CascadeSource::Local)));
        }
        options.request_type = holochain_p2p::event::GetRequest::Pending;
        self.fetch_record(hash.clone().into(), options).await?;

        // Check if we have the data now after the network call.
        let result = self
            .find_map(move |store| {
                Ok(store
                    .get_action(&hash)?
                    .map(|a| (a, CascadeSource::Network)))
            })
            .await?;
        Ok(result)
    }

    async fn retrieve(
        &self,
        hash: AnyDhtHash,
        mut options: NetworkGetOptions,
    ) -> CascadeResult<Option<(Record, CascadeSource)>> {
        let private_data = self.private_data.clone();
        let result = self
            .find_map({
                let hash = hash.clone();
                move |store| {
                    Ok(store.get_public_or_authored_record(
                        &hash,
                        private_data.as_ref().map(|a| a.as_ref()),
                    )?)
                }
            })
            .await?;
        if result.is_some() {
            return Ok(result.map(|r| (r, CascadeSource::Local)));
        }
        options.request_type = holochain_p2p::event::GetRequest::Pending;
        self.fetch_record(hash.clone(), options).await?;

        let private_data = self.private_data.clone();
        // Check if we have the data now after the network call.
        let result = self
            .find_map(move |store| {
                Ok(store.get_public_or_authored_record(
                    &hash,
                    private_data.as_ref().map(|a| a.as_ref()),
                )?)
            })
            .await?;
        Ok(result.map(|r| (r, CascadeSource::Network)))
    }
}

impl CascadeImpl {
    #[allow(clippy::result_large_err)] // TODO - investigate this lint
    fn insert_rendered_op(txn: &mut Txn<DbKindCache>, op: &RenderedOp) -> CascadeResult<()> {
        let RenderedOp {
            op_light,
            op_hash,
            action,
            validation_status,
        } = op;
        let op_order = OpOrder::new(op_light.get_type(), action.action().timestamp());
        let timestamp = action.action().timestamp();
        insert_action(txn, action)?;
        insert_op_lite(
            txn,
            op_light,
            op_hash,
            &op_order,
            &timestamp,
            todo_no_cache_transfer_data(),
        )?;
        if let Some(status) = validation_status {
            set_validation_status(txn, op_hash, *status)?;
        }
        // We set the integrated to for the cache so it can match the
        // same query as the vault. This can also be used for garbage collection.
        set_when_integrated(txn, op_hash, Timestamp::now())?;
        Ok(())
    }

    #[allow(clippy::result_large_err)] // TODO - investigate this lint
    fn insert_rendered_ops(txn: &mut Txn<DbKindCache>, ops: &RenderedOps) -> CascadeResult<()> {
        let RenderedOps {
            ops,
            entry,
            warrant,
        } = ops;

        if let Some(warrant) = warrant {
            let op = DhtOpHashed::from_content_sync(warrant.clone());
            insert_op_cache(txn, &op)?;
        }
        if let Some(entry) = entry {
            insert_entry(txn, entry.as_hash(), entry.as_content())?;
        }
        for op in ops {
            Self::insert_rendered_op(txn, op)?;
        }
        Ok(())
    }

    /// Insert a set of agent activity into the Cache.
    #[allow(clippy::result_large_err)] // TODO - investigate this lint
    fn insert_activity(
        txn: &mut Txn<DbKindCache>,
        ops: Vec<RegisterAgentActivity>,
    ) -> CascadeResult<()> {
        for op in ops {
            let RegisterAgentActivity {
                action:
                    SignedHashed {
                        hashed: HoloHashed { content, .. },
                        signature,
                    },
                ..
            } = op;
            let op =
                DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(signature, content));
            insert_op_cache(txn, &op)?;
            // We set the integrated to for the cache so it can match the
            // same query as the vault. This can also be used for garbage collection.
            set_when_integrated(txn, op.as_hash(), Timestamp::now())?;
        }
        Ok(())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn merge_ops_into_cache(&self, responses: Vec<WireOps>) -> CascadeResult<()> {
        let cache = some_or_return!(self.cache.as_ref());
        cache
            .write_async(|txn| {
                for response in responses {
                    let ops = response.render()?;
                    Self::insert_rendered_ops(txn, &ops)?;
                }
                CascadeResult::Ok(())
            })
            .await?;
        Ok(())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn merge_link_ops_into_cache(
        &self,
        responses: Vec<WireLinkOps>,
        key: WireLinkKey,
    ) -> CascadeResult<()> {
        let cache = some_or_return!(self.cache.as_ref());
        cache
            .write_async(move |txn| {
                for response in responses {
                    let ops = response.render(&key)?;
                    Self::insert_rendered_ops(txn, &ops)?;
                }
                CascadeResult::Ok(())
            })
            .await?;
        Ok(())
    }

    /// Add new activity to the Cache.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn add_activity_into_cache(
        &self,
        responses: Vec<MustGetAgentActivityResponse>,
    ) -> CascadeResult<MustGetAgentActivityResponse> {
        // Choose a response from all the responses.
        let response = if responses
            .iter()
            .zip(responses.iter().skip(1))
            .all(|(a, b)| a == b)
        {
            // All responses are the same so we can just use the first one.
            responses.into_iter().next()
        } else {
            tracing::info!(
                "Got different must_get_agent_activity responses from different authorities"
            );
            // TODO: Handle conflict.
            // For now try to find one that has got the activity
            responses
                .iter()
                .find(|a| matches!(a, MustGetAgentActivityResponse::Activity { .. }))
                .cloned()
        };

        let cache = some_or_return!(
            self.cache.as_ref(),
            response.unwrap_or(MustGetAgentActivityResponse::IncompleteChain)
        );

        // Commit the activity to the chain.
        match response {
            Some(MustGetAgentActivityResponse::Activity { activity, warrants }) => {
                // TODO: Avoid this clone by committing the ops as references to the db.
                cache
                    .write_async({
                        let activity = activity.clone();
                        let warrants = warrants.clone();
                        move |txn| {
                            Self::insert_activity(txn, activity)?;
                            for warrant in warrants {
                                let op = DhtOpHashed::from_content_sync(warrant);
                                insert_op_cache(txn, &op)?;
                            }

                            CascadeResult::Ok(())
                        }
                    })
                    .await?;
                Ok(MustGetAgentActivityResponse::Activity { activity, warrants })
            }
            Some(response) => Ok(response),
            // Got no responses so the chain is incomplete.
            None => Ok(MustGetAgentActivityResponse::IncompleteChain),
        }
    }

    /// Fetch a Record from the network, caching and returning the results
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    pub async fn fetch_record(
        &self,
        hash: AnyDhtHash,
        options: NetworkGetOptions,
    ) -> CascadeResult<()> {
        let network = some_or_return!(self.network.as_ref());
        let results = network
            .get(hash, options.clone())
            .instrument(debug_span!("fetch_record::network_get"))
            .await?;

        self.merge_ops_into_cache(results).await?;
        Ok(())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    async fn fetch_links(
        &self,
        link_key: WireLinkKey,
        options: GetLinksOptions,
    ) -> CascadeResult<()> {
        let network = some_or_return!(self.network.as_ref());
        let results = network.get_links(link_key.clone(), options).await?;

        self.merge_link_ops_into_cache(results, link_key.clone())
            .await?;
        Ok(())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    async fn fetch_agent_activity(
        &self,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: GetActivityOptions,
    ) -> CascadeResult<Vec<AgentActivityResponse>> {
        let network = some_or_return!(self.network.as_ref(), Vec::with_capacity(0));
        Ok(network.get_agent_activity(agent, query, options).await?)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    /// Fetch hash bounded agent activity from the network.
    async fn fetch_must_get_agent_activity(
        &self,
        author: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> CascadeResult<MustGetAgentActivityResponse> {
        let network = some_or_return!(
            self.network.as_ref(),
            MustGetAgentActivityResponse::IncompleteChain
        );
        let results = network.must_get_agent_activity(author, filter).await?;

        self.add_activity_into_cache(results).await
    }

    /// Get transactions for available databases.
    async fn get_txn_guards(&self) -> CascadeResult<Vec<PTxnGuard>> {
        let mut conns: Vec<_> = Vec::with_capacity(3);
        if let Some(cache) = &self.cache {
            conns.push(cache.get_read_txn().await?);
        }
        if let Some(dht) = &self.dht {
            conns.push(dht.get_read_txn().await?);
        }
        if let Some(authored) = &self.authored {
            conns.push(authored.get_read_txn().await?);
        }
        Ok(conns)
    }

    async fn cascading<Q>(&self, query: Q) -> CascadeResult<Q::Output>
    where
        Q: Query<Item = Judged<SignedActionHashed>> + Send + 'static,
        <Q as Query>::Output: Send + 'static,
    {
        let start = Instant::now();
        let mut txn_guards = self.get_txn_guards().await?;
        let scratch = self.scratch.clone();
        // TODO We may already be on a blocking thread here because this is accessible from a zome call. Ideally we'd have
        //      a way to check this situation and avoid spawning a new thread if we're already on an appropriate thread.
        let results = tokio::task::spawn_blocking(move || {
            let mut txns = Vec::with_capacity(txn_guards.len());
            for conn in &mut txn_guards {
                // TODO The transaction does not actually start here. We're asking for a deferred transaction which is the
                //      right thing to do, but SQLite won't launch that until we do a read operation. If we want a stricter
                //      'point in time' view across databases it might make sense to issue a lightweight read op to each txn here?
                let txn = conn.transaction()?;
                txns.push(txn);
            }
            let txns_ref: Vec<_> = txns.iter().collect();
            let results = match scratch {
                Some(scratch) => scratch
                    .apply_and_then(|scratch| query.run(DbScratch::new(&txns_ref, scratch)))?,
                None => query.run(Txns::from(&txns_ref[..]))?,
            };
            CascadeResult::Ok(results)
        })
        .await??;

        self.duration_metric
            .record(start.elapsed().as_secs_f64(), &[]);

        Ok(results)
    }

    /// Search through the stores and return the first non-none result.
    async fn find_map<F, T>(&self, mut f: F) -> CascadeResult<Option<T>>
    where
        T: Send + 'static,
        F: FnMut(&dyn Store) -> CascadeResult<Option<T>> + Send + Clone + 'static,
    {
        if let Some(cache) = self.cache.clone() {
            let r = cache
                .read_async({
                    let mut f = f.clone();
                    move |raw_txn| f(&CascadeTxnWrapper::from(raw_txn))
                })
                .await?;

            if r.is_some() {
                return Ok(r);
            }
        }

        if let Some(dht) = self.dht.clone() {
            let r = dht
                .read_async({
                    let mut f = f.clone();
                    move |raw_txn| f(&CascadeTxnWrapper::from(raw_txn))
                })
                .await?;

            if r.is_some() {
                return Ok(r);
            }
        }

        if let Some(authored) = self.authored.clone() {
            let r = authored
                .read_async({
                    let mut f = f.clone();
                    move |raw_txn| f(&CascadeTxnWrapper::from(raw_txn))
                })
                .await?;

            if r.is_some() {
                return Ok(r);
            }
        }

        if let Some(scratch) = &self.scratch {
            let r = scratch.apply_and_then(|scratch| f(scratch))?;

            if r.is_some() {
                return Ok(r);
            }
        }

        Ok(None)
    }

    /// Get Entry data along with all CRUD actions associated with it.
    ///
    /// Also returns Rejected actions, which may affect the interpreted validity status of this Entry.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    pub async fn get_entry_details(
        &self,
        entry_hash: EntryHash,
        options: GetOptions,
    ) -> CascadeResult<Option<EntryDetails>> {
        let query: GetEntryDetailsQuery = self.construct_query_with_data_access(entry_hash.clone());
        if let GetStrategy::Local = options.strategy {
            // Only return what is in the database.
            return self.cascading(query.clone()).await;
        }

        // If we are not in the process of authoring this hash or its
        // authority we need a network call.
        let authoring = self.am_i_authoring(&entry_hash.clone().into())?;
        let authority = self.am_i_an_authority(entry_hash.clone().into()).await?;
        if !(authoring || authority) {
            self.fetch_record(entry_hash.into(), options.into()).await?;
        }

        // Check if we have the data now after the network call.
        self.cascading(query).await
    }

    /// Get the specified Record along with all Updates and Deletes associated with it.
    ///
    /// Can return a Rejected Record.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    pub async fn get_record_details(
        &self,
        action_hash: ActionHash,
        options: GetOptions,
    ) -> CascadeResult<Option<RecordDetails>> {
        let query: GetRecordDetailsQuery =
            self.construct_query_with_data_access(action_hash.clone());

        // DESIGN: we can short circuit if we have any local deletes on an action.
        // Is this bad because we will not go back to the network until our
        // cache is cleared. Could someone create an attack based on this fact?

        if let GetStrategy::Local = options.strategy {
            // Only return what is in the database.
            return self.cascading(query.clone()).await;
        }

        // If we are not in the process of authoring this hash or its
        // authority we need a network call.
        let authoring = self.am_i_authoring(&action_hash.clone().into())?;
        let authority = self.am_i_an_authority(action_hash.clone().into()).await?;
        if !(authoring || authority) {
            self.fetch_record(action_hash.into(), options.into())
                .await?;
        }

        // Check if we have the data now after the network call.
        self.cascading(query).await
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    /// Returns the [Record] for this [ActionHash] if it is live
    /// by getting the latest available metadata from authorities
    /// combined with this agents authored data.
    /// _Note: Deleted actions are a tombstone set_
    pub async fn dht_get_action(
        &self,
        action_hash: ActionHash,
        options: GetOptions,
    ) -> CascadeResult<Option<Record>> {
        let query: GetLiveRecordQuery = self.construct_query_with_data_access(action_hash.clone());

        // DESIGN: we can short circuit if we have any local deletes on an action.
        // Is this bad because we will not go back to the network until our
        // cache is cleared. Could someone create an attack based on this fact?

        if let GetStrategy::Local = options.strategy {
            // Only return what is in the database.
            return self.cascading(query.clone()).await;
        }

        // If we are not in the process of authoring this hash or its
        // authority we need a network call.
        let authoring = self.am_i_authoring(&action_hash.clone().into())?;
        let authority = self.am_i_an_authority(action_hash.clone().into()).await?;
        if !(authoring || authority) {
            self.fetch_record(action_hash.into(), options.into())
                .await?;
        }

        // Check if we have the data now after the network call.
        self.cascading(query).await
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    /// Returns the oldest live [Record] for this [EntryHash] by getting the
    /// latest available metadata from authorities combined with this agents authored data.
    pub async fn dht_get_entry(
        &self,
        entry_hash: EntryHash,
        options: GetOptions,
    ) -> CascadeResult<Option<Record>> {
        let query: GetLiveEntryQuery = self.construct_query_with_data_access(entry_hash.clone());

        if let GetStrategy::Local = options.strategy {
            // Only return what is in the database.
            return self.cascading(query.clone()).await;
        }

        // If we are not in the process of authoring this hash or its
        // authority we need a network call.
        let authoring = self.am_i_authoring(&entry_hash.clone().into())?;
        let authority = self.am_i_an_authority(entry_hash.clone().into()).await?;
        if !(authoring || authority) {
            self.fetch_record(entry_hash.into(), options.into()).await?;
        }

        // Check if we have the data now after the network call.
        self.cascading(query).await
    }

    /// Perform a concurrent `get` on multiple hashes simultaneously, returning
    /// the resulting list of Records in the order that they come in
    /// (NOT the order in which they were requested!).
    pub async fn get_concurrent<I: IntoIterator<Item = AnyDhtHash>>(
        &self,
        hashes: I,
        options: GetOptions,
    ) -> CascadeResult<Vec<Option<Record>>> {
        use futures::stream::StreamExt;
        use futures::stream::TryStreamExt;
        let iter = hashes.into_iter().map({
            |hash| {
                let options = options.clone();
                let cascade = self.clone();
                async move { cascade.dht_get(hash, options).await }
            }
        });
        futures::stream::iter(iter)
            .buffer_unordered(10)
            .try_collect()
            .await
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    /// Updates the cache with the latest network authority data
    /// and returns what is in the cache.
    /// This gives you the latest possible picture of the current dht state.
    /// Data from your zome call is also added to the cache.
    pub async fn dht_get(
        &self,
        hash: AnyDhtHash,
        options: GetOptions,
    ) -> CascadeResult<Option<Record>> {
        match hash.into_primitive() {
            AnyDhtHashPrimitive::Entry(hash) => self.dht_get_entry(hash, options).await,
            AnyDhtHashPrimitive::Action(hash) => self.dht_get_action(hash, options).await,
        }
    }

    /// Get either [`EntryDetails`] or [`RecordDetails`], depending on the hash provided
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub async fn get_details(
        &self,
        hash: AnyDhtHash,
        options: GetOptions,
    ) -> CascadeResult<Option<Details>> {
        match hash.into_primitive() {
            AnyDhtHashPrimitive::Entry(hash) => Ok(self
                .get_entry_details(hash, options)
                .await?
                .map(Details::Entry)),
            AnyDhtHashPrimitive::Action(hash) => Ok(self
                .get_record_details(hash, options)
                .await?
                .map(Details::Record)),
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    /// Gets an links from the cas or cache depending on it's metadata
    // The default behavior is to skip deleted or replaced entries.
    pub async fn dht_get_links(
        &self,
        key: WireLinkKey,
        options: GetLinksOptions,
    ) -> CascadeResult<Vec<Link>> {
        // only fetch links from network if i am not an authority and
        // GetStrategy is Latest
        if let GetStrategy::Network = options.get_options.strategy {
            let authority = self.am_i_an_authority(key.base.clone()).await?;
            if !authority {
                self.fetch_links(key.clone(), options).await?;
            }
        }

        let query = GetLinksQuery::new(
            key.base,
            key.type_query,
            key.tag,
            GetLinksFilter {
                after: key.after,
                before: key.before,
                author: key.author,
            },
        );

        self.cascading(query).await
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, key, options)))]
    /// Return all CreateLink actions
    /// and DeleteLink actions ordered by time.
    pub async fn get_link_details(
        &self,
        key: WireLinkKey,
        options: GetLinksOptions,
    ) -> CascadeResult<Vec<(SignedActionHashed, Vec<SignedActionHashed>)>> {
        // only fetch link details from network if i am not an authority and
        // GetStrategy is Network
        if let GetStrategy::Network = options.get_options.strategy {
            let authority = self.am_i_an_authority(key.base.clone()).await?;
            if !authority {
                self.fetch_links(key.clone(), options).await?;
            }
        }
        let query = GetLinkDetailsQuery::new(key.base, key.type_query, key.tag);
        self.cascading(query).await
    }

    /// Count the number of links matching the `query`.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, query)))]
    pub async fn dht_count_links(&self, query: WireLinkQuery) -> CascadeResult<usize> {
        let mut links = HashSet::<ActionHash>::new();
        if !self.am_i_an_authority(query.base.clone()).await? {
            if let Some(network) = &self.network {
                links.extend(
                    network
                        .count_links(query.clone())
                        .await?
                        .create_link_actions(),
                );
            }
        }

        let get_links_query = GetLinksQuery::new(
            query.base.clone(),
            query.link_type.clone(),
            query.tag_prefix.clone(),
            query.into(),
        );

        links.extend(
            self.cascading(get_links_query)
                .await?
                .into_iter()
                .map(|l| l.create_link_hash),
        );

        Ok(links.len())
    }

    /// Request a hash bounded chain query.
    pub async fn must_get_agent_activity(
        &self,
        author: AgentPubKey,
        filter: ChainFilter,
    ) -> CascadeResult<MustGetAgentActivityResponse> {
        // Get the available databases.
        let mut txn_guards = self.get_txn_guards().await?;
        let scratch = self.scratch.clone();

        // For each store try to get the bounded activity.
        let results = tokio::task::spawn_blocking({
            let author = author.clone();
            let filter = filter.clone();
            move || {
                let mut results = Vec::with_capacity(txn_guards.len() + 1);
                for txn_guard in &mut txn_guards {
                    let txn = txn_guard.transaction()?;
                    let r = match &scratch {
                        Some(scratch) => {
                            scratch.apply_and_then(|scratch| {
                                authority::get_agent_activity_query::must_get_agent_activity::get_bounded_activity(&txn, Some(scratch), &author, filter.clone())
                            })?
                        }
                        None => authority::get_agent_activity_query::must_get_agent_activity::get_bounded_activity(&txn, None, &author, filter.clone())?
                    };
                    results.push(r);
                }
                CascadeResult::Ok(results)
            }
        })
            .await??;

        let merged_results = results.iter().fold(
            // It's sort of arbitrary what the initial value is as long as it's
            // not an activity response.
            BoundedMustGetAgentActivityResponse::EmptyRange,
            holochain_types::chain::merge_bounded_agent_activity_responses,
        );

        let result =
            authority::get_agent_activity_query::must_get_agent_activity::filter_then_check(
                merged_results,
            );

        // Short circuit if we have a result.
        if matches!(result, MustGetAgentActivityResponse::Activity { .. }) {
            return Ok(result);
        }

        // If we are the authority then don't go to the network.
        let i_am_authority = self.am_i_an_authority(author.clone().into()).await?;
        if i_am_authority {
            // If I am an authority and I didn't get a result before
            // this point then the chain is incomplete for this request.
            Ok(MustGetAgentActivityResponse::IncompleteChain)
        } else {
            Ok(self
                .fetch_must_get_agent_activity(author.clone(), filter)
                .await?)
        }
    }

    /// Get agent activity from agent activity authorities.
    ///
    /// Hashes are requested from the authority and cache for valid chains.
    ///
    /// Query:
    /// - [include_entries](ChainQueryFilter::include_entries) will also fetch the entries in parallel (requires include_full_records)
    /// - [sequence_range](ChainQueryFilter::sequence_range) will get all the activity in the exclusive range
    /// - [action_type](ChainQueryFilter::action_type) and [entry_type](ChainQueryFilter::entry_type) will filter the activity (requires include_full_actions)
    ///
    /// Options:
    /// - [include_valid_activity](GetActivityOptions::include_valid_activity) will include the valid chain hashes.
    /// - [include_rejected_activity](GetActivityOptions::include_rejected_activity) will include the invalid chain hashes.
    /// - [include_warrants](GetActivityOptions::include_warrants) will include the warrants for this agent.
    /// - [include_full_records](GetActivityOptions::include_full_records) will fetch the full records for each action matching the query.
    ///   This is only effective if [include_valid_activity](GetActivityOptions::include_valid_activity) or [include_rejected_activity](GetActivityOptions::include_rejected_activity) is true.
    ///   Even when this is set, entries will only be fetched if [include_entries](ChainQueryFilter::include_entries) is also true.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, agent, query, options))
    )]
    pub async fn get_agent_activity(
        &self,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: GetActivityOptions,
    ) -> CascadeResult<AgentActivityResponse> {
        let status_only = !(options.include_valid_activity || options.include_rejected_activity);

        // If we're an authority then we allow local queries. This means we consider ourselves an authority
        // for the agent in question. If the options specify network, for example because we are looking for
        // warrants we don't know about or for countersigning actions, then we will go to the network
        // regardless of authority status.
        let authority = self.am_i_an_authority(agent.clone().into()).await?;

        let merged_response = if authority && options.get_options.strategy == GetStrategy::Local {
            match self.dht.clone() {
                Some(vault) => {
                    authority::handle_get_agent_activity(
                        vault,
                        agent.clone(),
                        query.clone(),
                        (&options).into(),
                    )
                    .await?
                }
                None => {
                    info!("Unable to get agent activity because this cascade does not have DHT access");
                    agent_activity::merge_activities(
                        agent.clone(),
                        &options,
                        Vec::with_capacity(0),
                    )?
                }
            }
        } else {
            let results = self
                .fetch_agent_activity(agent.clone(), query.clone(), options.clone())
                .await?;
            let merged_response: AgentActivityResponse =
                agent_activity::merge_activities(agent.clone(), &options, results)?;
            merged_response
        };

        // If the response is empty we can finish.
        if let ChainStatus::Empty = &merged_response.status {
            return Ok(AgentActivityResponse::from_empty(merged_response));
        }

        // If the request is just for the status then return.
        if status_only {
            return Ok(AgentActivityResponse::status_only(merged_response));
        }

        let AgentActivityResponse {
            agent,
            mut valid_activity,
            mut rejected_activity,
            status,
            highest_observed,
            warrants,
        } = merged_response;

        // If records were requested then the activity authority might not have had all the entries.
        // That becomes more likely for new records as the number of agents on a network increases.
        // So we need to fill in the missing entries.
        if options.include_full_records && query.include_entries {
            tracing::debug!("Trying to fill missing entries for agent activity");
            valid_activity = self
                .fill_missing_chain_item_entries(valid_activity, options.get_options.clone())
                .await?;
            rejected_activity = self
                .fill_missing_chain_item_entries(rejected_activity, options.get_options)
                .await?;
        }

        let r = AgentActivityResponse {
            agent,
            valid_activity,
            rejected_activity,
            status,
            highest_observed,
            warrants,
        };

        Ok(r)
    }

    /// Looks through a [ChainItems] object and fills in any missing entry data.
    ///
    /// For any [RecordEntry::NotStored] entries, this function will attempt to fetch the entry data
    /// from either our cache when [GetOptions::local] is specified, or from the network when
    /// [GetOptions::network] is specified.
    ///
    /// Note that this will only take any action for [ChainItems::Full]. For other
    /// [ChainItems] variants, the function will just return its input.
    async fn fill_missing_chain_item_entries(
        &self,
        mut chain_items: ChainItems,
        get_options: GetOptions,
    ) -> CascadeResult<ChainItems> {
        let missing_entry_hashes = match &chain_items {
            ChainItems::Full(records) => records
                .iter()
                .filter_map(|r| match r.entry {
                    RecordEntry::NotStored => r.action().entry_hash().map(|h| h.clone().into()),
                    _ => None,
                })
                .collect(),
            _ => Vec::with_capacity(0),
        };

        if !missing_entry_hashes.is_empty() {
            trace!(
                "There are {} missing entries to fetch",
                missing_entry_hashes.len()
            );

            let maybe_provided_entry_records = self
                .get_concurrent(missing_entry_hashes, get_options)
                .await?;

            trace!("Got {:?} entries", maybe_provided_entry_records.len());

            let entry_lookup = maybe_provided_entry_records
                .iter()
                .filter_map(|r| match r {
                    Some(r) => r
                        .signed_action()
                        .action()
                        .entry_hash()
                        .map(|entry_hash| (entry_hash, &r.entry)),
                    None => None,
                })
                .collect::<HashMap<_, _>>();

            match &mut chain_items {
                ChainItems::Full(records) => {
                    for record in records.iter_mut() {
                        if let RecordEntry::NotStored = record.entry {
                            if let Some(entry_hash) = record.action().entry_hash() {
                                if let Some(entry) = entry_lookup.get(entry_hash) {
                                    record.entry = (*entry).clone();
                                }
                            }
                        }
                    }
                }
                _ => {
                    // Because of the match above, the valid activity should always be FullRecords
                    unreachable!()
                }
            }
        }

        Ok(chain_items)
    }

    #[allow(clippy::result_large_err)] // TODO - investigate this lint
    fn am_i_authoring(&self, hash: &AnyDhtHash) -> CascadeResult<bool> {
        let scratch = some_or_return!(self.scratch.as_ref(), false);
        Ok(scratch.apply_and_then(|scratch| scratch.contains_hash(hash))?)
    }

    async fn am_i_an_authority(&self, hash: OpBasis) -> CascadeResult<bool> {
        let network = some_or_return!(self.network.as_ref(), false);
        Ok(network.authority_for_hash(hash).await?)
    }

    /// Construct a query with private data access if this cascade has been
    /// constructed with private data access.
    fn construct_query_with_data_access<H, Q: PrivateDataQuery<Hash = H>>(&self, hash: H) -> Q {
        match self.private_data.clone() {
            Some(author) => Q::with_private_data_access(hash, author),
            None => Q::without_private_data_access(hash),
        }
    }
}

#[cfg(feature = "test_utils")]
impl MockCascade {
    /// Construct a mock which acts as if the given records were part of local storage
    pub fn with_records(records: Vec<Record>) -> Self {
        let mut cascade = Self::default();

        let map: HashMap<AnyDhtHash, Record> = records
            .into_iter()
            .flat_map(|r| {
                let mut items = vec![(r.action_address().clone().into(), r.clone())];
                if let Some(eh) = r.action().entry_hash() {
                    items.push((eh.clone().into(), r))
                }
                items
            })
            .collect();

        let map0 = Arc::new(parking_lot::Mutex::new(map));

        let map = map0.clone();
        cascade.expect_retrieve().returning(move |hash, _| {
            let m = map.lock();
            let result = m.get(&hash).map(|r| (r.clone(), CascadeSource::Local));
            Box::pin(async move { Ok(result) })
        });

        let map = map0.clone();
        cascade.expect_retrieve_action().returning(move |hash, _| {
            let m = map.lock();
            let result = m
                .get(&hash.into())
                .map(|r| (r.signed_action().clone(), CascadeSource::Local));
            Box::pin(async move { Ok(result) })
        });

        let map = map0;
        cascade.expect_retrieve_entry().returning(move |hash, _| {
            let m = map.lock();
            let result = m.get(&hash.into()).map(|r| {
                (
                    EntryHashed::from_content_sync(r.entry().as_option().unwrap().clone()),
                    CascadeSource::Local,
                )
            });
            Box::pin(async move { Ok(result) })
        });

        cascade
    }
}

#[tokio::test]
async fn test_mock_cascade_with_records() {
    use ::fixt::fixt;
    let records = vec![fixt!(Record), fixt!(Record), fixt!(Record)];
    let cascade = MockCascade::with_records(records.clone());
    let opts = NetworkGetOptions::default();
    let (r0, _) = cascade
        .retrieve(records[0].action_address().clone().into(), opts.clone())
        .await
        .unwrap()
        .unwrap();
    let (r1, _) = cascade
        .retrieve(records[1].action_address().clone().into(), opts.clone())
        .await
        .unwrap()
        .unwrap();
    let (r2, _) = cascade
        .retrieve(records[2].action_address().clone().into(), opts)
        .await
        .unwrap()
        .unwrap();
    assert_eq!(records, vec![r0, r1, r2]);
}



================================================
File: crates/holochain_cascade/src/metrics.rs
================================================
use opentelemetry_api::{global::meter_with_version, metrics::*};
use std::sync::OnceLock;

pub type CascadeDurationMetric = Histogram<f64>;

static DURATION_METRIC: OnceLock<CascadeDurationMetric> = OnceLock::new();

pub fn create_cascade_duration_metric() -> &'static CascadeDurationMetric {
    DURATION_METRIC.get_or_init(|| {
        meter_with_version(
            "hc.cascade",
            None::<&'static str>,
            None::<&'static str>,
            Some(vec![]),
        )
        .f64_histogram("hc.cascade.duration")
        .with_unit(Unit::new("s"))
        .with_description("The time taken to execute a cascade query")
        .init()
    })
}



================================================
File: crates/holochain_cascade/src/test_utils.rs
================================================
//! Test utils for holochain_cascade

use crate::authority;
use crate::authority::get_entry_ops_query::GetEntryOpsQuery;
use crate::authority::get_record_query::GetRecordOpsQuery;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holo_hash::AnyDhtHash;
use holo_hash::AnyDhtHashPrimitive;
use holo_hash::EntryHash;
use holochain_chc::ChcImpl;
use holochain_p2p::actor;
use holochain_p2p::event::CountersigningSessionNegotiationMessage;
use holochain_p2p::HolochainP2pDnaT;
use holochain_p2p::HolochainP2pError;
use holochain_sqlite::rusqlite::Transaction;
use holochain_state::prelude::*;
use holochain_types::test_utils::chain::chain_to_ops;
use holochain_types::test_utils::chain::entry_hash;
use holochain_types::test_utils::chain::TestChainItem;
use kitsune_p2p::agent_store::AgentInfoSigned;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::OpHashSized;
use kitsune_p2p::dht::Arq;
use std::collections::HashSet;
use std::sync::Arc;
use QueryFilter;
use Signature;
use ValidationStatus;

pub use activity_test_data::*;
pub use entry_test_data::*;
use holochain_p2p::actor::HolochainP2pResult;
use holochain_types::test_utils::ActionRefMut;
use holochain_types::validation_receipt::ValidationReceiptBundle;
pub use record_test_data::*;

mod activity_test_data;
mod entry_test_data;
mod record_test_data;

/// A network implementation which routes to the local databases,
/// and can declare itself an authority either for all ops, or for no ops.
#[derive(Clone)]
pub struct PassThroughNetwork {
    envs: Vec<DbRead<DbKindDht>>,
    authority: bool,
}

impl PassThroughNetwork {
    /// Declare that this node has full coverage
    pub fn authority_for_all(envs: Vec<DbRead<DbKindDht>>) -> Arc<Self> {
        Arc::new(Self {
            envs,
            authority: true,
        })
    }

    /// Declare that this node has zero coverage
    pub fn authority_for_nothing(envs: Vec<DbRead<DbKindDht>>) -> Arc<Self> {
        Arc::new(Self {
            envs,
            authority: false,
        })
    }
}

#[async_trait::async_trait]
impl HolochainP2pDnaT for PassThroughNetwork {
    async fn get(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetOptions,
    ) -> actor::HolochainP2pResult<Vec<WireOps>> {
        let mut out = Vec::new();
        match dht_hash.into_primitive() {
            AnyDhtHashPrimitive::Entry(hash) => {
                for db in &self.envs {
                    let r =
                        authority::handle_get_entry(db.clone(), hash.clone(), (&options).into())
                            .await
                            .map_err(|e| HolochainP2pError::Other(e.into()))?;
                    out.push(WireOps::Entry(r));
                }
            }
            AnyDhtHashPrimitive::Action(hash) => {
                for db in &self.envs {
                    let r =
                        authority::handle_get_record(db.clone(), hash.clone(), (&options).into())
                            .await
                            .map_err(|e| HolochainP2pError::Other(e.into()))?;
                    out.push(WireOps::Record(r));
                }
            }
        }
        Ok(out)
    }

    async fn get_meta(
        &self,
        _dht_hash: holo_hash::AnyDhtHash,
        _options: actor::GetMetaOptions,
    ) -> actor::HolochainP2pResult<Vec<MetadataSet>> {
        todo!()
    }

    async fn get_links(
        &self,
        link_key: WireLinkKey,
        options: actor::GetLinksOptions,
    ) -> actor::HolochainP2pResult<Vec<WireLinkOps>> {
        let mut out = Vec::new();
        for db in &self.envs {
            let r = authority::handle_get_links(db.clone(), link_key.clone(), (&options).into())
                .await
                .map_err(|e| HolochainP2pError::Other(e.into()))?;
            out.push(r);
        }
        Ok(out)
    }

    async fn count_links(
        &self,
        query: WireLinkQuery,
    ) -> actor::HolochainP2pResult<CountLinksResponse> {
        let mut out = HashSet::new();

        for db in &self.envs {
            let r = authority::handle_get_links_query(db.clone(), query.clone())
                .await
                .map_err(|e| HolochainP2pError::Other(e.into()))?;
            out.extend(r);
        }

        Ok(CountLinksResponse::new(
            out.into_iter()
                .map(|l| l.create_link_hash)
                .collect::<Vec<_>>(),
        ))
    }

    async fn get_agent_activity(
        &self,
        agent: AgentPubKey,
        query: QueryFilter,
        options: actor::GetActivityOptions,
    ) -> actor::HolochainP2pResult<Vec<AgentActivityResponse>> {
        let mut out = Vec::new();
        for db in &self.envs {
            let r = authority::handle_get_agent_activity(
                db.clone(),
                agent.clone(),
                query.clone(),
                (&options).into(),
            )
            .await
            .map_err(|e| HolochainP2pError::Other(e.into()))?;
            out.push(r);
        }
        Ok(out)
    }

    async fn must_get_agent_activity(
        &self,
        agent: AgentPubKey,
        filter: ChainFilter,
    ) -> actor::HolochainP2pResult<Vec<MustGetAgentActivityResponse>> {
        let mut out = Vec::new();
        for db in &self.envs {
            let r = authority::handle_must_get_agent_activity(
                db.clone(),
                agent.clone(),
                filter.clone(),
            )
            .await
            .map_err(|e| HolochainP2pError::Other(e.into()))?;
            out.push(r);
        }
        Ok(out)
    }

    async fn authority_for_hash(
        &self,
        _dht_hash: holo_hash::OpBasis,
    ) -> actor::HolochainP2pResult<bool> {
        Ok(self.authority)
    }

    fn dna_hash(&self) -> holo_hash::DnaHash {
        todo!()
    }

    async fn send_remote_signal(
        &self,
        _to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn publish(
        &self,
        _request_validation_receipt: bool,
        _countersigning_session: bool,
        _basis_hash: holo_hash::OpBasis,
        _source: AgentPubKey,
        _op_hash_list: Vec<OpHashSized>,
        _timeout_ms: Option<u64>,
        _reflect_ops: Option<Vec<crate::DhtOp>>,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn publish_countersign(
        &self,
        _flag: bool,
        _basis_hash: holo_hash::OpBasis,
        _op: crate::DhtOp,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn send_validation_receipts(
        &self,
        _to_agent: AgentPubKey,
        _receipts: ValidationReceiptBundle,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn countersigning_session_negotiation(
        &self,
        _agents: Vec<AgentPubKey>,
        _message: CountersigningSessionNegotiationMessage,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn new_integrated_data(&self) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn join(
        &self,
        _agent: AgentPubKey,
        _maybe_agent_info: Option<AgentInfoSigned>,
        _initial_arq: Option<Arq>,
    ) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn leave(&self, _agent: AgentPubKey) -> actor::HolochainP2pResult<()> {
        todo!()
    }

    async fn call_remote(
        &self,
        _to_agent: AgentPubKey,
        _zome_call_payload: ExternIO,
        _from_signature: Signature,
    ) -> actor::HolochainP2pResult<holochain_serialized_bytes::SerializedBytes> {
        todo!()
    }

    async fn storage_arcs(&self) -> HolochainP2pResult<Vec<kitsune2_api::DhtArc>> {
        todo!()
    }

    fn chc(&self) -> Option<ChcImpl> {
        None
    }
}

/// Insert ops directly into the database and mark integrated as valid
pub async fn fill_db<Db: DbKindT + DbKindOp>(db: &DbWrite<Db>, op: ChainOpHashed) {
    db.write_async(move |txn| -> DatabaseResult<()> {
        let hash = op.to_hash();
        insert_op_untyped(txn, &op.downcast()).unwrap();
        set_validation_status(txn, &hash, ValidationStatus::Valid).unwrap();
        set_when_integrated(txn, &hash, Timestamp::now()).unwrap();
        Ok(())
    })
    .await
    .unwrap();
}

/// Insert ops directly into the database and mark integrated as rejected
pub async fn fill_db_rejected<Db: DbKindT + DbKindOp>(db: &DbWrite<Db>, op: ChainOpHashed) {
    db.write_async(move |txn| -> DatabaseResult<()> {
        let hash = op.to_hash();
        insert_op_untyped(txn, &op.downcast()).unwrap();
        set_validation_status(txn, &hash, ValidationStatus::Rejected).unwrap();
        set_when_integrated(txn, &hash, Timestamp::now()).unwrap();
        Ok(())
    })
    .await
    .unwrap();
}

/// Insert ops directly into the database and mark valid and pending integration
pub async fn fill_db_pending<Db: DbKindT + DbKindOp>(db: &DbWrite<Db>, op: ChainOpHashed) {
    db.write_async(move |txn| -> DatabaseResult<()> {
        let hash = op.to_hash();
        insert_op_untyped(txn, &op.downcast()).unwrap();
        set_validation_status(txn, &hash, ValidationStatus::Valid).unwrap();
        Ok(())
    })
    .await
    .unwrap();
}

/// Insert ops into the authored database
pub async fn fill_db_as_author(db: &DbWrite<DbKindAuthored>, op: ChainOpHashed) {
    db.write_async(move |txn| -> DatabaseResult<()> {
        insert_op_untyped(txn, &op.downcast()).unwrap();
        Ok(())
    })
    .await
    .unwrap();
}

/// Utility for network simulation response to get entry.
pub fn handle_get_entry_txn(
    txn: &Transaction<'_>,
    hash: EntryHash,
    _options: holochain_p2p::event::GetOptions,
) -> WireEntryOps {
    let query = GetEntryOpsQuery::new(hash);
    query.run(CascadeTxnWrapper::from(txn)).unwrap()
}

/// Utility for network simulation response to get record.
pub fn handle_get_record_txn(
    txn: &Transaction<'_>,
    hash: ActionHash,
    options: holochain_p2p::event::GetOptions,
) -> WireRecordOps {
    let query = GetRecordOpsQuery::new(hash, options);
    query.run(CascadeTxnWrapper::from(txn)).unwrap()
}

/// Utility for network simulation response to get.
pub fn handle_get_txn(
    txn: &Transaction<'_>,
    hash: AnyDhtHash,
    options: holochain_p2p::event::GetOptions,
) -> WireOps {
    match hash.into_primitive() {
        AnyDhtHashPrimitive::Entry(hash) => {
            WireOps::Entry(handle_get_entry_txn(txn, hash, options))
        }
        AnyDhtHashPrimitive::Action(hash) => {
            WireOps::Record(handle_get_record_txn(txn, hash, options))
        }
    }
}

/// Commit the chain to a test in-memory database, returning a handle to that DB
pub fn commit_chain<Kind: DbKindT>(
    db_kind: Kind,
    chain: Vec<(AgentPubKey, Vec<TestChainItem>)>,
) -> DbWrite<Kind> {
    let data: Vec<_> = chain
        .into_iter()
        .map(|(a, c)| {
            chain_to_ops(c)
                .into_iter()
                .map(|mut op| {
                    *op.action.hashed.content.author_mut() = a.clone();
                    op
                })
                .collect::<Vec<_>>()
        })
        .collect();
    let db = test_in_mem_db(db_kind);

    db.test_write(move |txn| {
        for data in &data {
            for op in data {
                let op_lite = ChainOpLite::RegisterAgentActivity(
                    op.action.action_address().clone(),
                    op.action
                        .hashed
                        .entry_hash()
                        .cloned()
                        .unwrap_or_else(|| entry_hash(&[0]))
                        .into(),
                );

                let timestamp = Timestamp::now();
                let op_type = op_lite.get_type();
                let (_, hash) =
                    ChainOpUniqueForm::op_hash(op_type, op.action.hashed.content.clone()).unwrap();
                insert_action(txn, &op.action).unwrap();
                insert_op_lite(
                    txn,
                    &op_lite.into(),
                    &hash,
                    &OpOrder::new(op_type, timestamp),
                    &timestamp,
                    None,
                )
                .unwrap();
                set_validation_status(txn, &hash, ValidationStatus::Valid).unwrap();
                set_when_integrated(txn, &hash, Timestamp::now()).unwrap();
            }
        }
    });
    db
}

/// Add the items to the provided scratch
pub fn commit_scratch(scratch: SyncScratch, chain: Vec<(AgentPubKey, Vec<TestChainItem>)>) {
    let data = chain.into_iter().map(|(a, c)| {
        chain_to_ops(c)
            .into_iter()
            .map(|mut op| {
                *op.action.hashed.content.author_mut() = a.clone();
                op
            })
            .collect::<Vec<_>>()
    });

    scratch
        .apply(|scratch| {
            for data in data {
                for op in data {
                    scratch.add_action(op.action, Default::default());
                }
            }
        })
        .unwrap();
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query.rs
================================================
use crate::agent_activity::compute_chain_status;
use holo_hash::{ActionHash, AgentPubKey};
use holochain_p2p::event::GetActivityOptions;
use holochain_state::prelude::ActionSequenceAndHash;
use holochain_state::query::StateQueryResult;
use holochain_types::prelude::{
    ActionHashedContainer, AgentActivityResponse, ChainItems, ChainItemsSource,
};
use holochain_zome_types::prelude::{
    ChainFork, ChainHead, ChainQueryFilter, ChainStatus, HasValidationStatus, HighestObserved,
    Judged, ValidationStatus, Warrant,
};

pub mod deterministic;
pub mod hashes;
pub mod must_get_agent_activity;
pub mod records;

#[derive(Debug)]
pub struct State<T> {
    pub(super) valid: Vec<T>,
    pub(super) rejected: Vec<T>,
    pub(super) pending: Vec<T>,
    pub(super) warrants: Vec<Warrant>,
    pub(super) status: Option<ChainStatus>,
}

impl<T> Default for State<T> {
    fn default() -> Self {
        Self {
            valid: Vec::new(),
            rejected: Vec::new(),
            pending: Vec::new(),
            warrants: Vec::new(),
            status: None,
        }
    }
}

#[allow(clippy::large_enum_variant)]
pub enum Item<T> {
    Integrated(T),
    Pending(T),
    Warrant(Warrant),
}

fn fold<T: ActionHashedContainer>(
    mut state: State<T>,
    item: Judged<Item<T>>,
) -> StateQueryResult<State<T>> {
    let status = item.validation_status();
    match (status, item.data) {
        (Some(ValidationStatus::Valid), Item::Integrated(action)) => {
            let seq = action.action().action_seq();
            if state.status.is_none() {
                let fork = state.valid.last().and_then(|v| {
                    if seq == v.action().action_seq() {
                        Some(v)
                    } else {
                        None
                    }
                });
                if let Some(fork) = fork {
                    state.status = Some(ChainStatus::Forked(ChainFork {
                        fork_seq: seq,
                        first_action: action.action_hash().clone(),
                        second_action: fork.action_hash().clone(),
                    }));
                }
            }

            state.valid.push(action);
        }
        (Some(ValidationStatus::Rejected), Item::Integrated(action)) => {
            if state.status.is_none() {
                state.status = Some(ChainStatus::Invalid(ChainHead {
                    action_seq: action.action().action_seq(),
                    hash: action.action_hash().clone(),
                }));
            }

            state.rejected.push(action);
        }
        (_, Item::Pending(data)) => state.pending.push(data),
        (_, Item::Warrant(warrant)) => state.warrants.push(warrant),
        _ => (),
    }

    Ok(state)
}

/// Find the highest observed sequence number from multiple action lists.
///
/// THe function searches the valid, rejected, and pending action lists for the highest sequence
/// number. If there are multiple actions with the same sequence number, all of their hashes are
/// returned.
fn compute_highest_observed<T: ActionSequenceAndHash>(state: &State<T>) -> Option<HighestObserved> {
    let mut highest_observed = None;
    let mut hashes = Vec::new();
    let mut check_highest = |seq: u32, hash: &ActionHash| {
        if let Some(last) = highest_observed.as_mut() {
            match seq.cmp(last) {
                std::cmp::Ordering::Less => {}
                std::cmp::Ordering::Equal => hashes.push(hash.clone()),
                std::cmp::Ordering::Greater => {
                    hashes.clear();
                    hashes.push(hash.clone());
                    *last = seq;
                }
            }
        } else {
            highest_observed = Some(seq);
            hashes.push(hash.clone());
        }
    };
    if let Some(valid) = state.valid.last() {
        check_highest(valid.action_seq(), valid.address());
    }
    if let Some(rejected) = state.rejected.last() {
        check_highest(rejected.action_seq(), rejected.address());
    }
    if let Some(pending) = state.pending.last() {
        check_highest(pending.action_seq(), pending.address());
    }
    highest_observed.map(|action_seq| HighestObserved {
        action_seq,
        hash: hashes,
    })
}

fn render<T>(
    state: State<T>,
    agent: AgentPubKey,
    filter: &ChainQueryFilter,
    options: &GetActivityOptions,
) -> StateQueryResult<AgentActivityResponse>
where
    T: ActionHashedContainer + Clone,
    Vec<T>: ChainItemsSource,
{
    let highest_observed = compute_highest_observed(&state);

    let (status, valid, rejected) = compute_chain_status(
        state.valid.clone().into_iter(),
        state.rejected.clone().into_iter(),
    );

    let valid_activity = if options.include_valid_activity {
        let valid = filter.filter_actions(valid);
        valid.to_chain_items()
    } else {
        ChainItems::NotRequested
    };

    let rejected_activity = if options.include_rejected_activity {
        let rejected = filter.filter_actions(rejected);
        rejected.to_chain_items()
    } else {
        ChainItems::NotRequested
    };

    let warrants = if options.include_warrants {
        state.warrants
    } else {
        vec![]
    };

    Ok(AgentActivityResponse {
        agent,
        valid_activity,
        rejected_activity,
        warrants,
        status,
        highest_observed,
    })
}



================================================
File: crates/holochain_cascade/src/authority/get_entry_ops_query.rs
================================================
use std::sync::Arc;

use holo_hash::EntryHash;
use holochain_sqlite::rusqlite::named_params;
use holochain_sqlite::rusqlite::Row;
use holochain_state::prelude::*;
use holochain_state::query::StateQueryError;

#[derive(Debug, Clone)]
pub struct GetEntryOpsQuery(EntryHash);

impl GetEntryOpsQuery {
    pub fn new(hash: EntryHash) -> Self {
        Self(hash)
    }
}

pub struct Item {
    op_type: ChainOpType,
    action: SignedAction,
}

#[derive(Debug, Default)]
pub struct State {
    ops: WireEntryOps,
    entry_data: Option<(EntryHash, EntryType)>,
}

impl Query for GetEntryOpsQuery {
    type Item = Judged<Item>;
    type State = State;
    type Output = WireEntryOps;

    fn query(&self) -> String {
        "
        SELECT Action.blob AS action_blob, DhtOp.type AS dht_type,
        DhtOp.validation_status AS status
        FROM DhtOp
        JOIN Action On DhtOp.action_hash = Action.hash
        WHERE DhtOp.type IN (:store_entry, :delete, :update)
        AND
        DhtOp.basis_hash = :entry_hash
        AND
        DhtOp.when_integrated IS NOT NULL
        "
        .into()
    }

    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":store_entry": ChainOpType::StoreEntry,
            ":delete": ChainOpType::RegisterDeletedEntryAction,
            ":update": ChainOpType::RegisterUpdatedContent,
            ":entry_hash": self.0,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = |row: &Row| {
            let action =
                from_blob::<SignedAction>(row.get(row.as_ref().column_index("action_blob")?)?)?;
            let op_type = row.get(row.as_ref().column_index("dht_type")?)?;
            let validation_status = row.get(row.as_ref().column_index("status")?)?;
            Ok(Judged::raw(Item { op_type, action }, validation_status))
        };
        Arc::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(Default::default())
    }

    fn fold(&self, mut state: Self::State, dht_op: Self::Item) -> StateQueryResult<Self::State> {
        match &dht_op.data.op_type {
            ChainOpType::StoreEntry => {
                if dht_op
                    .data
                    .action
                    .entry_type()
                    .filter(|et| *et.visibility() == EntryVisibility::Public)
                    .is_some()
                {
                    let status = dht_op.validation_status();
                    if state.entry_data.is_none() {
                        state.entry_data = dht_op
                            .data
                            .action
                            .entry_data()
                            .map(|(h, t)| (h.clone(), t.clone()));
                    }
                    state
                        .ops
                        .creates
                        .push(Judged::raw(dht_op.data.action.try_into()?, status));
                }
            }
            ChainOpType::RegisterDeletedEntryAction => {
                let status = dht_op.validation_status();
                state
                    .ops
                    .deletes
                    .push(Judged::raw(dht_op.data.action.try_into()?, status));
            }
            ChainOpType::RegisterUpdatedContent => {
                let status = dht_op.validation_status();
                let action = dht_op.data.action;
                state.ops.updates.push(Judged::raw(
                    WireUpdateRelationship::try_from(action)?,
                    status,
                ));
            }
            _ => return Err(StateQueryError::UnexpectedOp(dht_op.data.op_type)),
        }
        Ok(state)
    }

    fn render<S>(&self, mut state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        if let Some((entry_hash, entry_type)) = state.entry_data {
            let entry = stores.get_entry(&entry_hash)?;
            state.ops.entry = entry.map(|entry| EntryData { entry, entry_type });
        }
        Ok(state.ops)
    }
}



================================================
File: crates/holochain_cascade/src/authority/get_links_ops_query.rs
================================================
use super::WireLinkKey;
use holo_hash::AnyLinkableHash;
use holochain_sqlite::rusqlite::named_params;
use holochain_sqlite::rusqlite::Row;
use holochain_state::prelude::*;
use holochain_state::query::StateQueryError;
use holochain_types::sql::ToSqlStatement;
use std::sync::Arc;

#[derive(Debug, Clone)]
pub struct GetLinksOpsQuery {
    base: Arc<AnyLinkableHash>,
    type_query: LinkTypeFilter,
    tag: Option<Arc<LinkTag>>,
}

impl GetLinksOpsQuery {
    pub fn new(key: WireLinkKey) -> Self {
        Self {
            base: Arc::new(key.base),
            type_query: key.type_query,
            tag: key.tag.map(Arc::new),
        }
    }
    pub fn tag_to_hex(tag: &LinkTag) -> String {
        holochain_util::hex::bytes_to_hex(&tag.0, true)
    }
}

pub struct Item {
    action: SignedAction,
    op_type: ChainOpType,
}

impl Query for GetLinksOpsQuery {
    type Item = Judged<Item>;
    type State = WireLinkOps;
    type Output = Self::State;

    fn query(&self) -> String {
        let create = "
            SELECT Action.blob AS action_blob, DhtOp.type AS dht_type,
            DhtOp.validation_status AS status
            FROM DhtOp
        ";
        let sub_create = "
            SELECT Action.hash FROM DhtOp
        ";
        let mut common_query = "
            JOIN Action On DhtOp.action_hash = Action.hash
            WHERE DhtOp.type = :create
            AND
            Action.base_hash = :base_hash
            AND
            DhtOp.when_integrated IS NOT NULL
        "
        .to_string();

        if let Some(tag) = &self.tag {
            let tag = Self::tag_to_hex(tag.as_ref());
            common_query = format!(
                "
                    {}
                    AND
                    HEX(Action.tag) LIKE '{}%'
                ",
                common_query, tag
            );
        }
        common_query = format!(
            "
            {}
            {}
            ",
            common_query,
            self.type_query.to_sql_statement(),
        );
        let create_query = format!("{}{}", create, common_query);
        let sub_create_query = format!("{}{}", sub_create, common_query);
        let delete_query = format!(
            "
            SELECT Action.blob AS action_blob, DhtOp.type AS dht_type,
            DhtOp.validation_status AS status
            FROM DhtOp
            JOIN Action On DhtOp.action_hash = Action.hash
            WHERE DhtOp.type = :delete
            AND
            DhtOp.when_integrated IS NOT NULL
            AND
            Action.create_link_hash IN ({})
            ",
            sub_create_query
        );
        format!("{} UNION ALL {}", create_query, delete_query)
    }

    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":create": ChainOpType::RegisterAddLink,
            ":delete": ChainOpType::RegisterRemoveLink,
            ":base_hash": self.base,
        };

        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = |row: &Row| {
            let action =
                from_blob::<SignedAction>(row.get(row.as_ref().column_index("action_blob")?)?)?;
            let op_type = row.get(row.as_ref().column_index("dht_type")?)?;
            let validation_status = row.get(row.as_ref().column_index("status")?)?;
            Ok(Judged::raw(Item { action, op_type }, validation_status))
        };
        Arc::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(WireLinkOps::new())
    }

    fn fold(&self, mut state: Self::State, dht_op: Self::Item) -> StateQueryResult<Self::State> {
        match &dht_op.data.op_type {
            ChainOpType::RegisterAddLink => {
                let validation_status = dht_op.validation_status();
                let item = dht_op.data.action;
                if let ((Action::CreateLink(action), signature), Some(validation_status)) =
                    (item.into(), validation_status)
                {
                    state.creates.push(WireCreateLink::condense(
                        action,
                        signature,
                        validation_status,
                    ));
                }
            }
            ChainOpType::RegisterRemoveLink => {
                let validation_status = dht_op.validation_status();
                let item = dht_op.data.action;
                if let ((Action::DeleteLink(action), signature), Some(validation_status)) =
                    (item.into(), validation_status)
                {
                    state.deletes.push(WireDeleteLink::condense(
                        action,
                        signature,
                        validation_status,
                    ));
                }
            }
            _ => return Err(StateQueryError::UnexpectedOp(dht_op.data.op_type)),
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        Ok(state)
    }
}



================================================
File: crates/holochain_cascade/src/authority/get_record_query.rs
================================================
use std::sync::Arc;

use holo_hash::ActionHash;
use holochain_p2p::event::GetOptions;
use holochain_sqlite::rusqlite::named_params;
use holochain_sqlite::rusqlite::Row;
use holochain_state::prelude::*;
use holochain_state::query::StateQueryError;

#[derive(Debug, Clone)]
pub struct GetRecordOpsQuery(ActionHash, GetOptions);

impl GetRecordOpsQuery {
    pub fn new(hash: ActionHash, request: GetOptions) -> Self {
        Self(hash, request)
    }
}

pub struct Item {
    op_type: ChainOpType,
    action: SignedAction,
}

impl Query for GetRecordOpsQuery {
    type Item = Judged<Item>;
    type State = WireRecordOps;
    type Output = Self::State;

    fn query(&self) -> String {
        let request_type = self.1.request_type.clone();
        let query = "
            SELECT Action.blob AS action_blob, DhtOp.type AS dht_type,
            DhtOp.validation_status AS status
            FROM DhtOp
            JOIN Action On DhtOp.action_hash = Action.hash
            WHERE DhtOp.type IN (:store_record, :delete, :update)
            AND
            DhtOp.basis_hash = :action_hash
        ";
        let is_integrated = "
            AND
            DhtOp.when_integrated IS NOT NULL
        ";
        match request_type {
            holochain_p2p::event::GetRequest::All
            | holochain_p2p::event::GetRequest::Content
            | holochain_p2p::event::GetRequest::Metadata => {
                format!("{}{}", query, is_integrated)
            }
            holochain_p2p::event::GetRequest::Pending => query.into(),
        }
    }

    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":store_record": ChainOpType::StoreRecord,
            ":delete": ChainOpType::RegisterDeletedBy,
            ":update": ChainOpType::RegisterUpdatedRecord,
            ":action_hash": self.0,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = |row: &Row| {
            let action =
                from_blob::<SignedAction>(row.get(row.as_ref().column_index("action_blob")?)?)?;
            let op_type = row.get(row.as_ref().column_index("dht_type")?)?;
            let validation_status = row.get(row.as_ref().column_index("status")?)?;
            Ok(Judged::raw(Item { op_type, action }, validation_status))
        };
        Arc::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(WireRecordOps::new())
    }

    fn fold(&self, mut state: Self::State, dht_op: Self::Item) -> StateQueryResult<Self::State> {
        match &dht_op.data.op_type {
            ChainOpType::StoreRecord => {
                if state.action.is_none() {
                    state.action = Some(dht_op.map(|d| d.action));
                }
            }
            ChainOpType::RegisterDeletedBy => {
                let status = dht_op.validation_status();
                state
                    .deletes
                    .push(Judged::raw(dht_op.data.action.try_into()?, status));
            }
            ChainOpType::RegisterUpdatedRecord => {
                let status = dht_op.validation_status();
                let action = dht_op.data.action;
                state.updates.push(Judged::raw(
                    WireUpdateRelationship::try_from(action)?,
                    status,
                ));
            }
            _ => return Err(StateQueryError::UnexpectedOp(dht_op.data.op_type)),
        }
        Ok(state)
    }

    fn render<S>(&self, mut state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        let entry_hash = state.action.as_ref().and_then(|wire_op| {
            wire_op
                .data
                .entry_data()
                .map(|(hash, et)| (hash, et.visibility()))
        });
        if let Some((entry_hash, EntryVisibility::Public)) = entry_hash {
            let entry = stores.get_entry(entry_hash)?;
            state.entry = entry;
        }
        Ok(state)
    }
}



================================================
File: crates/holochain_cascade/src/authority/test.rs
================================================
use super::*;
use crate::test_utils::*;
#[cfg(feature = "unstable-warrants")]
use holo_hash::fixt::ActionHashFixturator;
#[cfg(feature = "unstable-warrants")]
use holo_hash::fixt::AgentPubKeyFixturator;
use holochain_p2p::actor;
use holochain_p2p::event::GetRequest;
use holochain_state::prelude::test_dht_db;
#[cfg(feature = "unstable-warrants")]
use {crate::authority::handle_get_agent_activity, holochain_types::activity::ChainItems};

fn options() -> holochain_p2p::event::GetOptions {
    holochain_p2p::event::GetOptions {
        follow_redirects: false,
        all_live_actions_with_metadata: true,
        request_type: Default::default(),
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn get_entry() {
    holochain_trace::test_run();
    let db = test_dht_db();

    let td = EntryTestData::create();

    fill_db(&db.to_db(), td.store_entry_op.clone()).await;
    let options = options();

    let result = handle_get_entry(db.to_db().into(), td.hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireEntryOps {
        creates: vec![td.wire_create.clone()],
        deletes: vec![],
        updates: vec![],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.delete_entry_action_op.clone()).await;

    let result = handle_get_entry(db.to_db().into(), td.hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireEntryOps {
        creates: vec![td.wire_create.clone()],
        deletes: vec![td.wire_delete.clone()],
        updates: vec![],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.update_content_op.clone()).await;

    let result = handle_get_entry(db.to_db().into(), td.hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireEntryOps {
        creates: vec![td.wire_create.clone()],
        deletes: vec![td.wire_delete.clone()],
        updates: vec![td.wire_update.clone()],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);
}

#[tokio::test(flavor = "multi_thread")]
async fn get_record() {
    holochain_trace::test_run();
    let db = test_dht_db();

    let td = RecordTestData::create();

    fill_db(&db.to_db(), td.store_record_op.clone()).await;

    let options = options();

    let result = handle_get_record(db.to_db().into(), td.create_hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireRecordOps {
        action: Some(td.wire_create.clone()),
        deletes: vec![],
        updates: vec![],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.deleted_by_op.clone()).await;

    let result = handle_get_record(db.to_db().into(), td.create_hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireRecordOps {
        action: Some(td.wire_create.clone()),
        deletes: vec![td.wire_delete.clone()],
        updates: vec![],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.update_record_op.clone()).await;

    let result = handle_get_record(db.to_db().into(), td.create_hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireRecordOps {
        action: Some(td.wire_create.clone()),
        deletes: vec![td.wire_delete.clone()],
        updates: vec![td.wire_update.clone()],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.any_store_record_op.clone()).await;

    let result = handle_get_record(
        db.to_db().into(),
        td.any_action_hash.clone(),
        options.clone(),
    )
    .await
    .unwrap();
    let expected = WireRecordOps {
        action: Some(td.any_action.clone()),
        deletes: vec![],
        updates: vec![],
        entry: td.any_entry.clone(),
    };
    assert_eq!(result, expected);
}

#[tokio::test(flavor = "multi_thread")]
async fn retrieve_record() {
    holochain_trace::test_run();
    let db = test_dht_db();

    let td = RecordTestData::create();

    fill_db_pending(&db.to_db(), td.store_record_op.clone()).await;

    let mut options = options();
    options.request_type = GetRequest::Pending;

    let result = handle_get_record(db.to_db().into(), td.create_hash.clone(), options.clone())
        .await
        .unwrap();
    let expected = WireRecordOps {
        action: Some(td.wire_create.clone()),
        deletes: vec![],
        updates: vec![],
        entry: Some(td.entry.clone()),
    };
    assert_eq!(result, expected);
}

#[tokio::test(flavor = "multi_thread")]
async fn get_links() {
    holochain_trace::test_run();
    let db = test_dht_db();

    let td = EntryTestData::create();

    fill_db(&db.to_db(), td.store_entry_op.clone()).await;
    fill_db(&db.to_db(), td.create_link_op.clone()).await;
    let options = actor::GetLinksOptions::default();

    let result = handle_get_links(db.to_db().into(), td.link_key.clone(), (&options).into())
        .await
        .unwrap();
    let expected = WireLinkOps {
        creates: vec![td.wire_create_link.clone()],
        deletes: vec![],
    };
    assert_eq!(result, expected);

    fill_db(&db.to_db(), td.delete_link_op.clone()).await;

    let result = handle_get_links(
        db.to_db().into(),
        td.link_key_tag.clone(),
        (&options).into(),
    )
    .await
    .unwrap();
    let expected = WireLinkOps {
        creates: vec![td.wire_create_link_base.clone()],
        deletes: vec![td.wire_delete_link.clone()],
    };
    assert_eq!(result, expected);
}

#[cfg(feature = "unstable-warrants")]
#[tokio::test(flavor = "multi_thread")]
async fn get_agent_activity() {
    use ::fixt::fixt;
    use holochain_state::mutations::*;

    holochain_trace::test_run();
    let db = test_dht_db();

    let td = ActivityTestData::valid_chain_scenario(false);

    for hash_op in td.agent_activity_ops.iter().cloned() {
        fill_db(&db.to_db(), hash_op).await;
    }
    for hash_op in td.noise_agent_activity_ops.iter().cloned() {
        fill_db(&db.to_db(), hash_op).await;
    }

    let warrant_valid = Warrant::new_now(
        WarrantProof::ChainIntegrity(ChainIntegrityWarrant::InvalidChainOp {
            action_author: td.agent.clone(),
            action: (fixt!(ActionHash), fixt!(Signature)),
            validation_type: ValidationType::Sys,
        }),
        fixt!(AgentPubKey),
    );
    let warrant_invalid = Warrant::new_now(
        WarrantProof::ChainIntegrity(ChainIntegrityWarrant::InvalidChainOp {
            action_author: td.agent.clone(),
            action: (fixt!(ActionHash), fixt!(Signature)),
            validation_type: ValidationType::Sys,
        }),
        fixt!(AgentPubKey),
    );
    {
        let warrant_op_valid =
            WarrantOp::from(SignedWarrant::new(warrant_valid.clone(), fixt!(Signature)))
                .into_hashed();

        let warrant_op_invalid = WarrantOp::from(SignedWarrant::new(
            warrant_invalid.clone(),
            fixt!(Signature),
        ))
        .into_hashed();

        db.write_async(move |txn| {
            {
                let op: DhtOpHashed = warrant_op_valid.downcast();
                let hash = op.to_hash();
                insert_op_dht(txn, &op, None).unwrap();
                set_validation_status(txn, &hash, ValidationStatus::Valid).unwrap();
                set_when_integrated(txn, &hash, Timestamp::now()).unwrap();
            }
            {
                let op: DhtOpHashed = warrant_op_invalid.downcast();
                let hash = op.to_hash();
                insert_op_dht(txn, &op, None).unwrap();
                set_validation_status(txn, &hash, ValidationStatus::Rejected).unwrap();
                set_when_integrated(txn, &hash, Timestamp::now()).unwrap();
            }
            holochain_sqlite::error::DatabaseResult::Ok(())
        })
        .await
        .unwrap();
    }

    let options = actor::GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: false,
        ..Default::default()
    };

    let result = handle_get_agent_activity(
        db.to_db().into(),
        td.agent.clone(),
        QueryFilter::new(),
        (&options).into(),
    )
    .await
    .unwrap();
    let mut expected = AgentActivityResponse {
        agent: td.agent.clone(),
        valid_activity: td.valid_hashes.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![warrant_valid],
        status: ChainStatus::Valid(td.chain_head.clone()),
        highest_observed: Some(td.highest_observed.clone()),
    };
    pretty_assertions::assert_eq!(result, expected);

    expected.valid_activity = match expected.valid_activity.clone() {
        ChainItems::Hashes(v) => ChainItems::Hashes(v.into_iter().take(20).collect()),
        _ => unreachable!(),
    };

    let filter = QueryFilter::new().sequence_range(ChainQueryFilterRange::ActionSeqRange(0, 19));
    let result = handle_get_agent_activity(
        db.to_db().into(),
        td.agent.clone(),
        filter,
        (&options).into(),
    )
    .await
    .unwrap();

    pretty_assertions::assert_eq!(result, expected);
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query/deterministic.rs
================================================
//! Query for `deterministic_get_agent_activity`, designed for use in
//! validation callbacks.
//!
//! This is a deterministic version of `get_agent_activity`, designed such that
//! there can only be one possible valid result which satisfies the query
//! criteria, so if you get back a result, you can verify that it is correct
//! and safely use it in your own validation. If you don't get a value back,
//! you cannot proceed with validation.
//!
//! - The agent authority will fully validate Actions, so it's OK to pass the
//!   full actions to Wasm
//! - Must return a contiguous range of Actions so that the requestor can
//!   ensure that the data is valid (TODO we're skipping the actual validation
//!   on the requestor side for now).

use holo_hash::*;
use holochain_p2p::event::GetActivityOptions;
use holochain_sqlite::rusqlite::*;
use holochain_state::{
    prelude::*,
    query::{row_blob_and_hash_to_action, QueryData},
};
use std::{fmt::Debug, sync::Arc};

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct DeterministicGetAgentActivityQuery {
    agent: AgentPubKey,
    filter: DeterministicGetAgentActivityFilter,
    options: GetActivityOptions,
}

impl DeterministicGetAgentActivityQuery {
    pub fn new(
        agent: AgentPubKey,
        filter: DeterministicGetAgentActivityFilter,
        options: GetActivityOptions,
    ) -> Self {
        Self {
            agent,
            filter,
            options,
        }
    }
}

#[derive(Debug)]
pub struct DeterministicGetAgentActivityQueryState {
    chain: Vec<Judged<SignedAction>>,
    prev_action: Option<ActionHash>,
}

impl Query for DeterministicGetAgentActivityQuery {
    type Item = Judged<SignedActionHashed>;
    type State = DeterministicGetAgentActivityQueryState;
    type Output = DeterministicGetAgentActivityResponse;

    fn query(&self) -> String {
        "
            SELECT H.blob, H.hash, D.validation_status FROM Action AS H
            JOIN DhtOp as D
            ON D.action_hash = H.hash
            WHERE H.author = :author
            AND D.type = :op_type
            AND D.validation_status IS NOT NULL
            AND D.when_integrated IS NOT NULL
            AND (:hash_low IS NULL OR H.seq >= (SELECT seq FROM Action WHERE hash = :hash_low))
            AND H.seq <= (SELECT seq FROM Action WHERE hash = :hash_high)
            ORDER BY H.seq DESC
        "
        .to_string()
    }

    fn params(&self) -> Vec<holochain_state::query::Params> {
        (named_params! {
            ":author": self.agent,
            ":hash_low": self.filter.range.0,
            ":hash_high": self.filter.range.1,
            ":op_type": ChainOpType::RegisterAgentActivity,
        })
        .to_vec()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(DeterministicGetAgentActivityQueryState {
            chain: Vec::new(),
            prev_action: Some(self.filter.range.1.clone()),
        })
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        todo!()
    }

    fn fold(&self, mut state: Self::State, item: Self::Item) -> StateQueryResult<Self::State> {
        let (shh, status) = item.into();
        let SignedActionHashed {
            hashed:
                ActionHashed {
                    content: action,
                    hash,
                },
            signature,
        } = shh;
        let sh = SignedAction::new(action, signature);
        // By tracking the prev_action of the last action we added to the chain,
        // we can filter out branches. If we performed branch detection in this
        // query, it would not be deterministic.
        //
        // TODO: ensure that this still works with the scratch, and that we
        // never have to run this query including the Cache. That is, if we join
        // results from multiple Stores, the ordering of action_seq will be
        // discontinuous, and we will have to collect into a sorted list before
        // doing this fold.
        if Some(hash) == state.prev_action {
            state.prev_action = sh.action().prev_action().cloned();
            state.chain.push((sh, status).into());
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        Ok(DeterministicGetAgentActivityResponse::new(state.chain))
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_and_hash_to_action("blob", "hash");
        Arc::new(move |row| {
            let validation_status: ValidationStatus = row.get("validation_status")?;
            Ok(Judged::new(f(row)?, validation_status))
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::test_utils::fill_db;
    use ::fixt::prelude::*;
    use holo_hash::fixt::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn agent_activity_query() {
        holochain_trace::test_run();
        let test_db = test_dht_db();
        let db = test_db.to_db();
        let entry_type_1 = fixt!(EntryType);
        let agents = [fixt!(AgentPubKey), fixt!(AgentPubKey), fixt!(AgentPubKey)];
        let mut chains = vec![];

        #[allow(clippy::needless_range_loop)]
        for a in 0..3 {
            let mut chain: Vec<ActionHash> = Vec::new();
            for seq in 0..10 {
                let action: Action = if let Some(top) = chain.last() {
                    let mut action = fixt!(Create);
                    action.entry_type = entry_type_1.clone();
                    action.author = agents[a].clone();
                    action.prev_action = top.clone();
                    action.action_seq = seq;
                    let entry = Entry::App(fixt!(AppEntryBytes));
                    action.entry_hash = EntryHash::with_data_sync(&entry);
                    action.into()
                } else {
                    let mut action = fixt!(Dna);
                    action.author = agents[a].clone();
                    action.into()
                };
                chain.push(ActionHash::with_data_sync(&action));
                let op = ChainOp::RegisterAgentActivity(fixt!(Signature), action);
                let op = ChainOpHashed::from_content_sync(op);
                fill_db(&db, op).await;
            }
            chains.push(chain);
        }

        let filter_full = DeterministicGetAgentActivityFilter {
            range: (None, chains[2].last().unwrap().clone()),
            entry_type: Some(entry_type_1.clone()),
            action_type: None,
            include_entries: false,
        };

        let filter_partial = DeterministicGetAgentActivityFilter {
            range: (Some(chains[2][4].clone()), chains[2][8].clone()),
            entry_type: Some(entry_type_1),
            action_type: None,
            include_entries: false,
        };
        let options = GetActivityOptions::default();

        let results_full = crate::authority::handle_get_agent_activity_deterministic(
            db.clone().into(),
            agents[2].clone(),
            filter_full,
            options.clone(),
        )
        .await
        .unwrap();

        let results_partial = crate::authority::handle_get_agent_activity_deterministic(
            db.clone().into(),
            agents[2].clone(),
            filter_partial,
            options,
        )
        .await
        .unwrap();

        assert_eq!(results_full.chain.len(), 10);
        assert_eq!(results_partial.chain.len(), 5);
    }
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query/hashes.rs
================================================
use crate::authority::get_agent_activity_query::{fold, render, Item, State};
use holo_hash::{ActionHash, AgentPubKey, AnyLinkableHash, WarrantHash};
use holochain_p2p::dht::op::Timestamp;
use holochain_p2p::event::GetActivityOptions;
use holochain_sqlite::rusqlite::{named_params, Row};
use holochain_state::prelude::{from_blob, ActionHashed, Query, StateQueryResult, Store};
use holochain_state::query::QueryData;
use holochain_types::activity::AgentActivityResponse;
use holochain_types::dht_op::{ChainOpType, DhtOpType};
use holochain_types::prelude::WarrantOpType;
use holochain_zome_types::judged::Judged;
use holochain_zome_types::prelude::{
    ChainQueryFilter, SignedAction, SignedWarrant, ValidationStatus,
};
use std::sync::Arc;

#[derive(Debug, Clone)]
pub struct GetAgentActivityHashesQuery {
    pub(super) agent: AgentPubKey,
    pub(super) agent_basis: AnyLinkableHash,
    pub(super) filter: ChainQueryFilter,
    pub(super) options: GetActivityOptions,
}

impl GetAgentActivityHashesQuery {
    pub fn new(agent: AgentPubKey, filter: ChainQueryFilter, options: GetActivityOptions) -> Self {
        Self {
            agent_basis: agent.clone().into(),
            agent,
            filter,
            options,
        }
    }
}

impl Query for GetAgentActivityHashesQuery {
    type State = State<ActionHashed>;
    type Item = Judged<Item<ActionHashed>>;
    type Output = AgentActivityResponse;

    fn query(&self) -> String {
        "
            SELECT
            Action.hash,
            Action.blob AS action_blob,
            DhtOp.type AS dht_type,
            DhtOp.validation_status,
            DhtOp.when_integrated
            FROM Action
            JOIN DhtOp ON DhtOp.action_hash = Action.hash
            WHERE
            (
                -- is an action authored by this agent
                Action.author = :author
                AND DhtOp.type = :chain_op_type
            )
            OR
            (
                -- is an integrated, valid warrant
                DhtOp.basis_hash = :author_basis
                AND DhtOp.type = :warrant_op_type
                AND DhtOp.validation_status = :valid_status
                AND DhtOp.when_integrated IS NOT NULL
            )
            ORDER BY Action.seq ASC
        "
        .to_string()
    }

    fn params(&self) -> Vec<holochain_state::query::Params> {
        let params = named_params! {
            ":author": self.agent,
            ":author_basis": self.agent_basis,
            ":chain_op_type": ChainOpType::RegisterAgentActivity,
            ":warrant_op_type": WarrantOpType::ChainIntegrityWarrant,
            ":valid_status": ValidationStatus::Valid,
        };

        params.to_vec()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(Default::default())
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        unimplemented!("This query should not be used with the scratch")
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        Arc::new(move |row| {
            let op_type: DhtOpType = row.get("dht_type")?;
            let validation_status: Option<ValidationStatus> = row.get("validation_status")?;
            let integrated: Option<Timestamp> = row.get("when_integrated")?;

            match op_type {
                DhtOpType::Chain(_) => {
                    let hash: ActionHash = row.get("hash")?;
                    from_blob::<SignedAction>(row.get("action_blob")?).map(|action| {
                        let action = ActionHashed::with_pre_hashed(action.into_data(), hash);
                        let item = if integrated.is_some() {
                            Item::Integrated(action)
                        } else {
                            Item::Pending(action)
                        };
                        Judged::raw(item, validation_status)
                    })
                }
                DhtOpType::Warrant(_) => {
                    let _hash: WarrantHash = row.get("hash")?;
                    from_blob::<SignedWarrant>(row.get("action_blob")?).map(|warrant| {
                        let item = Item::Warrant(warrant.into_data());
                        Judged::raw(item, None)
                    })
                }
            }
        })
    }

    fn fold(&self, state: Self::State, data: Self::Item) -> StateQueryResult<Self::State> {
        fold(state, data)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        render(state, self.agent.clone(), &self.filter, &self.options)
    }
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query/must_get_agent_activity.rs
================================================
use std::ops::RangeInclusive;

use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holochain_sqlite::prelude::{DbKindDht, DbRead};
use holochain_sqlite::rusqlite::named_params;
use holochain_sqlite::rusqlite::OptionalExtension;
use holochain_sqlite::rusqlite::Transaction;
use holochain_sqlite::sql::sql_cell::must_get_agent_activity::*;
use holochain_state::prelude::*;

#[cfg(test)]
mod test;

/// Get the agent activity for a given agent and
/// hash bounded range of actions.
///
/// The full range must exist or this will return [`MustGetAgentActivityResponse::IncompleteChain`].
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn must_get_agent_activity(
    env: DbRead<DbKindDht>,
    author: AgentPubKey,
    filter: ChainFilter,
) -> StateQueryResult<MustGetAgentActivityResponse> {
    let result = env
        .read_async(move |txn| get_bounded_activity(txn, None, &author, filter))
        .await?;
    Ok(filter_then_check(result))
}

pub fn get_bounded_activity(
    txn: &Transaction,
    scratch: Option<&Scratch>,
    author: &AgentPubKey,
    filter: ChainFilter,
) -> StateQueryResult<BoundedMustGetAgentActivityResponse> {
    // Find the bounds of the range specified in the filter.
    let txn = CascadeTxnWrapper::from(txn);
    let warrants = txn.get_warrants_for_basis(&AnyLinkableHash::from(author.clone()), true)?;

    match find_bounds(&txn, scratch, author, filter)? {
        Sequences::Found(filter) => {
            // Get the full range of actions from the database.
            get_activity(&txn, scratch, author, filter.range()).map(|activity| {
                BoundedMustGetAgentActivityResponse::Activity {
                    activity,
                    filter,
                    warrants,
                }
            })
        }
        // One of the actions specified in the filter does not exist in the database.
        Sequences::ChainTopNotFound(a) => {
            Ok(BoundedMustGetAgentActivityResponse::ChainTopNotFound(a))
        }
        // The filter specifies a range that is empty.
        Sequences::EmptyRange => Ok(BoundedMustGetAgentActivityResponse::EmptyRange),
    }
}

/// Consume the chain filter (if present) from a bounded response to produce an
/// unbounded response.
pub fn filter_then_check(
    response: BoundedMustGetAgentActivityResponse,
) -> MustGetAgentActivityResponse {
    match response {
        BoundedMustGetAgentActivityResponse::Activity {
            activity,
            filter,
            warrants,
        } => {
            // Filter the activity from the database and check the invariants of the
            // filter still hold.
            filter.filter_then_check(activity, warrants)
        }
        BoundedMustGetAgentActivityResponse::IncompleteChain => {
            MustGetAgentActivityResponse::IncompleteChain
        }
        BoundedMustGetAgentActivityResponse::ChainTopNotFound(a) => {
            MustGetAgentActivityResponse::ChainTopNotFound(a)
        }
        BoundedMustGetAgentActivityResponse::EmptyRange => MustGetAgentActivityResponse::EmptyRange,
    }
}

/// Find the filters sequence bounds.
fn find_bounds(
    txn: &Transaction,
    scratch: Option<&Scratch>,
    author: &AgentPubKey,
    filter: ChainFilter,
) -> StateQueryResult<Sequences> {
    let mut statement = txn.prepare(ACTION_HASH_TO_SEQ)?;

    // Map an action hash to its sequence.
    let get_seq = move |hash: &ActionHash| {
        if let Some(scratch) = scratch {
            if let Some(action) = scratch.actions().find(|a| a.action_address() == hash) {
                return Ok(Some(action.action().action_seq()));
            }
        }
        let result = statement
                .query_row(named_params! {":hash": hash, ":author": author, ":activity": ChainOpType::RegisterAgentActivity}, |row| {
                    row.get(0)
                })
                .optional()?;
        Ok(result)
    };

    // For all the hashes in the filter, get their sequences.
    Sequences::find_sequences(filter, get_seq)
}

/// Get the agent activity for a given range of actions
/// from the database.
fn get_activity(
    txn: &Transaction,
    scratch: Option<&Scratch>,
    author: &AgentPubKey,
    range: &RangeInclusive<u32>,
) -> StateQueryResult<Vec<RegisterAgentActivity>> {
    let mut out = txn
        .prepare(MUST_GET_AGENT_ACTIVITY)?
        .query_and_then(
            named_params! {
                 ":author": author,
                 ":op_type": ChainOpType::RegisterAgentActivity,
                 ":lower_seq": range.start(),
                 ":upper_seq": range.end(),

            },
            |row| {
                let action: SignedAction = from_blob(row.get("blob")?)?;
                let (action, signature) = action.into();
                let hash: ActionHash = row.get("hash")?;
                let hashed = ActionHashed::with_pre_hashed(action, hash);
                let action = SignedActionHashed::with_presigned(hashed, signature);
                StateQueryResult::Ok(RegisterAgentActivity {
                    action,
                    // TODO: Implement getting the cached entries.
                    cached_entry: None,
                })
            },
        )?
        .collect::<Result<Vec<_>, _>>()?;
    if let Some(scratch) = scratch {
        let iter = scratch
            .actions()
            .filter(|a| {
                let action = a.action();
                action.author() == author
                    && action.action_seq() >= *range.start()
                    && action.action_seq() <= *range.end()
            })
            .map(|action| RegisterAgentActivity {
                action: action.clone(),
                // TODO: Implement getting the cached entries.
                cached_entry: None,
            });
        out.extend(iter);
    }
    Ok(out)
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query/records.rs
================================================
use crate::authority::get_agent_activity_query::{fold, render, Item, State};
use holo_hash::*;
use holochain_p2p::event::GetActivityOptions;
use holochain_sqlite::rusqlite::*;
use holochain_state::{prelude::*, query::QueryData};
use std::fmt::Debug;
use std::sync::Arc;

#[derive(Debug, Clone)]
pub struct GetAgentActivityRecordsQuery {
    pub(super) agent: AgentPubKey,
    pub(super) agent_basis: AnyLinkableHash,
    pub(super) filter: ChainQueryFilter,
    pub(super) options: GetActivityOptions,
}

impl GetAgentActivityRecordsQuery {
    pub fn new(agent: AgentPubKey, filter: ChainQueryFilter, options: GetActivityOptions) -> Self {
        Self {
            agent_basis: agent.clone().into(),
            agent,
            filter,
            options,
        }
    }
}

impl Query for GetAgentActivityRecordsQuery {
    type State = State<Record>;
    type Item = Judged<Item<Record>>;
    type Output = AgentActivityResponse;

    fn query(&self) -> String {
        "
            SELECT
            Action.hash,
            Action.blob AS action_blob,
            Action.private_entry,
            DhtOp.type AS dht_type,
            DhtOp.validation_status,
            DhtOp.when_integrated,
            Entry.blob AS entry_blob
            FROM Action
            JOIN DhtOp ON DhtOp.action_hash = Action.hash
            LEFT JOIN Entry ON Action.entry_hash = Entry.hash
            WHERE
            (
                -- is an action authored by this agent
                Action.author = :author
                AND DhtOp.type = :chain_op_type
            )
            OR
            (
                -- is an integrated, valid warrant
                DhtOp.basis_hash = :author_basis
                AND DhtOp.type = :warrant_op_type
                AND DhtOp.validation_status = :valid_status
                AND DhtOp.when_integrated IS NOT NULL
            )
            ORDER BY Action.seq ASC
        "
        .to_string()
    }

    fn params(&self) -> Vec<holochain_state::query::Params> {
        let params = named_params! {
            ":author": self.agent,
            ":author_basis": self.agent_basis,
            ":chain_op_type": ChainOpType::RegisterAgentActivity,
            ":warrant_op_type": WarrantOpType::ChainIntegrityWarrant,
            ":valid_status": ValidationStatus::Valid,
        };

        params.to_vec()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(Default::default())
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        unimplemented!("This query should not be used with the scratch")
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let include_entries = self.filter.include_entries;
        Arc::new(move |row| {
            let op_type: DhtOpType = row.get("dht_type")?;
            let validation_status: Option<ValidationStatus> = row.get("validation_status")?;
            let integrated: Option<Timestamp> = row.get("when_integrated")?;

            // Note that even if an entry is expected, it might not be present.
            // This code is intended to run on an agent authority which may not be an entry authority
            // for all the relevant entries. The caller will need to handle any missing entries that
            // they care about.
            let private_entry = row
                .get::<_, Option<bool>>("private_entry")?
                .unwrap_or(false);
            let maybe_entry = if !private_entry && include_entries {
                row.get::<_, Option<Vec<u8>>>("entry_blob")?
                    .map(from_blob)
                    .transpose()?
            } else {
                None
            };

            match op_type {
                DhtOpType::Chain(_) => {
                    let hash: ActionHash = row.get("hash")?;
                    from_blob::<SignedAction>(row.get("action_blob")?).map(|action| {
                        let action = SignedHashed {
                            hashed: ActionHashed::with_pre_hashed(action.action().clone(), hash),
                            signature: action.signature().clone(),
                        };
                        let record = Record::new(action, maybe_entry);
                        let item = if integrated.is_some() {
                            Item::Integrated(record)
                        } else {
                            Item::Pending(record)
                        };
                        Judged::raw(item, validation_status)
                    })
                }
                DhtOpType::Warrant(_) => {
                    let _hash: WarrantHash = row.get("hash")?;
                    from_blob::<SignedWarrant>(row.get("action_blob")?).map(|warrant| {
                        let item = Item::Warrant(warrant.into_data());
                        Judged::raw(item, None)
                    })
                }
            }
        })
    }

    fn fold(&self, state: Self::State, item: Self::Item) -> StateQueryResult<Self::State> {
        fold(state, item)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        render(state, self.agent.clone(), &self.filter, &self.options)
    }
}



================================================
File: crates/holochain_cascade/src/authority/get_agent_activity_query/must_get_agent_activity/test.rs
================================================
use std::sync::Arc;

use crate::test_utils::commit_chain;

use super::*;
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_sqlite::prelude::DbKindDht;
use holochain_types::test_utils::chain::*;
use isotest::Iso;
use test_case::test_case;

#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[8]))
    => agent_chain(&[(0, 0..9)]) ; "Extract full chain")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[8])).take(2)
    => agent_chain(&[(0, 7..9)]) ; "Take 2")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]),
    ChainFilter::new(action_hash(&[8])).until(action_hash(&[2]))
    => agent_chain(&[(0, 2..9)]) ; "Until 2")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]),
    ChainFilter::new(action_hash(&[8])).until(action_hash(&[2])).until(action_hash(&[4]))
    => agent_chain(&[(0, 4..9)]) ; "Until 2 Until 4")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]),
    ChainFilter::new(action_hash(&[8])).until(action_hash(&[2])).until(action_hash(&[4])).take(3)
    => agent_chain(&[(0, 6..9)]) ; "Until 2 Until 4 take 3")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]),
    ChainFilter::new(action_hash(&[8])).until(action_hash(&[2])).until(action_hash(&[4])).take(1)
    => agent_chain(&[(0, 8..9)]) ; "Until 2 Until 4 take 1")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]),
    ChainFilter::new(action_hash(&[8])).until(action_hash(&[8])).until(action_hash(&[4])).take(3)
    => agent_chain(&[(0, 8..9)]) ; "Until 8 Until 4 take 3")]
#[tokio::test(flavor = "multi_thread")]
/// Extracts the smallest range from the chain filter
/// and then returns all actions within that range
async fn returns_full_sequence_from_filter(
    chain: Vec<(AgentPubKey, Vec<TestChainItem>)>,
    agent: AgentPubKey,
    filter: ChainFilter,
) -> Vec<(AgentPubKey, Vec<TestChainItem>)> {
    let db = commit_chain(
        DbKindDht(Arc::new(DnaHash::from_raw_36(vec![0; 36]))),
        chain,
    );
    let data = must_get_agent_activity(db.clone().into(), agent.clone(), filter)
        .await
        .unwrap();
    let data = match data {
        MustGetAgentActivityResponse::Activity { activity, .. } => activity
            .into_iter()
            .map(
                |RegisterAgentActivity {
                     action: a,
                     cached_entry: _,
                 }| TestChainItem {
                    seq: a.hashed.action_seq(),
                    hash: TestChainHash::test(a.as_hash()),
                    prev: a.hashed.prev_action().map(TestChainHash::test),
                },
            )
            .collect(),
        d => unreachable!("{:?}", d),
    };
    vec![(agent, data)]
}

#[test_case(
    agent_chain(&[(0, 0..3), (0, 5..10)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[8]))
    => MustGetAgentActivityResponse::IncompleteChain ; "8 to genesis with 0 till 2 and 5 till 9")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[1]), ChainFilter::new(action_hash(&[8]))
    => MustGetAgentActivityResponse::ChainTopNotFound(action_hash(&[8])) ; "Different agent")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[15]))
    => MustGetAgentActivityResponse::ChainTopNotFound(action_hash(&[15])) ; "Starting chain_top not found")]
#[test_case(
    vec![(agent_hash(&[0]), forked_chain(&[0..6, 3..8]))], agent_hash(&[0]), ChainFilter::new(action_hash(&[7, 1])).take(7)
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 7 ; "Handles forks")]
#[test_case(
    agent_chain(&[(0, 0..5)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[4])).until(action_hash(&[2, 1]))
    => matches MustGetAgentActivityResponse::Activity { .. } ; "Until hash not found")]
#[test_case(
    vec![(agent_hash(&[0]), forked_chain(&[0..6, 3..8]))], agent_hash(&[0]),
    ChainFilter::new(action_hash(&[5, 0])).until(action_hash(&[4, 1]))
    => MustGetAgentActivityResponse::IncompleteChain ; "Unit hash on fork")]
#[test_case(
    agent_chain(&[(0, 0..10)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[8])).until(action_hash(&[9]))
    => matches MustGetAgentActivityResponse::Activity { .. }; "Until is higher then chain_top")]
#[test_case(
    agent_chain(&[(0, 0..2)]), agent_hash(&[0]), ChainFilter::new(action_hash(&[1])).take(0)
    => MustGetAgentActivityResponse::EmptyRange; "Take nothing produces an empty range")]
#[tokio::test(flavor = "multi_thread")]
/// Check the query returns the appropriate responses.
async fn test_responses(
    chain: Vec<(AgentPubKey, Vec<TestChainItem>)>,
    agent: AgentPubKey,
    filter: ChainFilter,
) -> MustGetAgentActivityResponse {
    let db = commit_chain(
        DbKindDht(Arc::new(DnaHash::from_raw_36(vec![0; 36]))),
        chain,
    );
    must_get_agent_activity(db.clone().into(), agent.clone(), filter)
        .await
        .unwrap()
}



================================================
File: crates/holochain_cascade/src/test_utils/activity_test_data.rs
================================================
use ::fixt::prelude::*;
use holo_hash::fixt::*;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holo_hash::EntryHash;
use holochain_types::activity::ChainItems;
use holochain_types::dht_op::ChainOp;
use holochain_types::dht_op::ChainOpHashed;
use holochain_types::prelude::NewEntryAction;
use holochain_zome_types::prelude::*;

/// A collection of fixtures used to create scenarios for testing the Cascade
#[derive(Debug, Clone)]
pub struct ActivityTestData {
    /// AgentActivity ops to expect being able to get
    pub agent_activity_ops: Vec<ChainOpHashed>,
    /// "Noise", to ensure that the query filter is doing its job
    pub noise_agent_activity_ops: Vec<ChainOpHashed>,
    /// StoreRecord ops to expect being able to get
    pub store_entry_ops: Vec<ChainOpHashed>,
    /// The author of the chain
    pub agent: AgentPubKey,
    /// The expected hash return values
    pub valid_hashes: ChainItems,
    /// The expected record return values
    pub valid_records: ChainItems,
    /// The head of the chain produced
    pub chain_head: ChainHead,
    /// Same as the chain_head
    pub highest_observed: HighestObserved,
}

impl ActivityTestData {
    /// Construct a set of test fixtures representing a valid source chain
    pub fn valid_chain_scenario(private_entries: bool) -> Self {
        // The agent we are querying.
        let agent = fixt!(AgentPubKey);

        // An entry that all actions can use to make things simpler.
        let entry = Entry::App(fixt!(AppEntryBytes));
