        let dynamic_arcs = self.config.tuning_params.gossip_dynamic_arcs;
        #[cfg(not(feature = "unstable-sharding"))]
        let dynamic_arcs = false;
        let internal_sender = self.i_s.clone();
        Ok(async move {
            let urls = vec![TxUrl::try_from(local_url)?];
            let mut peer_data = Vec::with_capacity(agent_list.len());
            for (agent, arq) in agent_list {
                let input = UpdateAgentInfoInput {
                    expires_after,
                    space: space.clone(),
                    agent,
                    bootstrap_net,
                    arq,
                    urls: &urls,
                    evt_sender: &evt_sender,
                    internal_sender: &internal_sender,
                    _mdns_handles: &mut mdns_handles,
                    bootstrap_service: &bootstrap_service,
                    dynamic_arcs,
                };
                peer_data.push(update_single_agent_info(input).await?);
            }
            internal_sender
                .publish_agent_info_signed(PutAgentInfoSignedEvt { peer_data })
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_update_single_agent_info(
        &mut self,
        agent: Arc<KitsuneAgent>,
    ) -> SpaceInternalHandlerResult<()> {
        let local_url = match self.ro_inner.get_local_url() {
            None => return Ok(async move { Ok(()) }.boxed().into()),
            Some(local_url) => local_url,
        };
        let space = self.space.clone();
        let bootstrap_net = self.ro_inner.bootstrap_net;
        let mut mdns_handles = self.mdns_handles.clone();
        let evt_sender = self.host_api.legacy.clone();
        let internal_sender = self.i_s.clone();
        let bootstrap_service = self.config.bootstrap_service.clone();
        let expires_after = self.config.tuning_params.agent_info_expires_after_ms as u64;
        #[cfg(feature = "unstable-sharding")]
        let dynamic_arcs = self.config.tuning_params.gossip_dynamic_arcs;
        #[cfg(not(feature = "unstable-sharding"))]
        let dynamic_arcs = false;
        let arc = self.get_agent_arq(&agent);

        Ok(async move {
            let urls = vec![TxUrl::try_from(local_url)?];
            let input = UpdateAgentInfoInput {
                expires_after,
                space: space.clone(),
                agent,
                bootstrap_net,
                arq: arc,
                urls: &urls,
                evt_sender: &evt_sender,
                internal_sender: &internal_sender,
                _mdns_handles: &mut mdns_handles,
                bootstrap_service: &bootstrap_service,
                dynamic_arcs,
            };
            let peer_data = vec![update_single_agent_info(input).await?];
            internal_sender
                .publish_agent_info_signed(PutAgentInfoSignedEvt { peer_data })
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_publish_agent_info_signed(
        &mut self,
        input: PutAgentInfoSignedEvt,
    ) -> SpaceInternalHandlerResult<()> {
        let timeout = self.config.tuning_params.implicit_timeout();
        let tasks: Result<Vec<_>, _> = input
            .peer_data
            .iter()
            .map(|agent_info| {
                self.handle_broadcast(
                    self.space.clone(),
                    Arc::new(KitsuneBasis::new(agent_info.agent.0.clone())),
                    timeout,
                    BroadcastData::AgentInfo(agent_info.clone()),
                )
            })
            .collect();
        let ep_hnd = self.ro_inner.ep_hnd.clone();
        Ok(async move {
            futures::future::join(
                ep_hnd.broadcast(
                    &wire::Wire::PeerUnsolicited(crate::wire::PeerUnsolicited {
                        peer_list: input.peer_data,
                    }),
                    timeout,
                ),
                futures::future::join_all(tasks?),
            )
            .await
            .0?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_get_all_local_joined_agent_infos(
        &mut self,
    ) -> SpaceInternalHandlerResult<Vec<AgentInfoSigned>> {
        let agent_infos: Vec<AgentInfoSigned> = self
            .local_joined_agents
            .values()
            .filter_map(|maybe_agent_info| maybe_agent_info.as_ref())
            .cloned()
            .collect();

        Ok(async move { Ok(agent_infos) }.boxed().into())
    }

    fn handle_is_agent_local(
        &mut self,
        agent: Arc<KitsuneAgent>,
    ) -> SpaceInternalHandlerResult<bool> {
        let res = self.local_joined_agents.contains_key(&agent);
        Ok(async move { Ok(res) }.boxed().into())
    }

    fn handle_update_agent_arc(
        &mut self,
        agent: Arc<KitsuneAgent>,
        arq: Arq,
    ) -> SpaceInternalHandlerResult<()> {
        self.agent_arqs.insert(agent, arq);
        self.update_metric_exchange_arcset();
        Ok(async move { Ok(()) }.boxed().into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn handle_incoming_delegate_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
        _to_agent: Arc<KitsuneAgent>,
        mod_idx: u32,
        mod_cnt: u32,
        data: BroadcastData,
    ) -> InternalHandlerResult<()> {
        // first, forward this incoming broadcast to all connected
        // local agents.
        let mut local_notify_events = Vec::new();
        let mut local_agent_info_events = Vec::new();
        let broadcast_source = match &data {
            BroadcastData::User(data) => {
                for agent in self.local_joined_agents.keys() {
                    if let Some(arc) = self.agent_arqs.get(agent) {
                        if arc.to_dht_arc_std().contains(basis.get_loc()) {
                            let fut = self.host_api.legacy.notify(
                                space.clone(),
                                agent.clone(),
                                data.clone(),
                            );
                            local_notify_events.push(async move {
                                if let Err(err) = fut.await {
                                    tracing::warn!(?err, "failed local broadcast");
                                }
                            });
                        }
                    }
                }

                None
            }
            BroadcastData::AgentInfo(agent_info) => {
                if self
                    .agent_arqs
                    .values()
                    .any(|arc| arc.to_dht_arc_std().contains(basis.get_loc()))
                {
                    let fut = self
                        .host_api
                        .legacy
                        .put_agent_info_signed(PutAgentInfoSignedEvt {
                            peer_data: vec![agent_info.clone()],
                        });
                    local_agent_info_events.push(async move {
                        if let Err(err) = fut.await {
                            tracing::warn!(?err, "failed local broadcast");
                        }
                    });
                }

                None
            }
            BroadcastData::Publish { source, .. } => {
                // Don't do anything here. This case is handled by the actor
                // invoking incoming_publish instead of
                // incoming_delegate_broadcast.
                Some(source.clone())
            }
        };

        // next, gather a list of agents covering this data to be
        // published to.
        let ro_inner = self.ro_inner.clone();
        let timeout = ro_inner.config.tuning_params.implicit_timeout();
        let fut =
            discover::search_remotes_covering_basis(ro_inner.clone(), basis.get_loc(), timeout);

        Ok(async move {
            futures::future::join_all(local_notify_events).await;
            futures::future::join_all(local_agent_info_events).await;

            let info_list = fut.await?;

            // for all agents in the gathered list, check the modulo params
            // i.e. if `agent.get_loc() % mod_cnt == mod_idx` we know we are
            // responsible for delegating the broadcast to that agent.
            let mut all = Vec::new();
            for info in info_list.into_iter().filter(|info| {
                info.agent.get_loc().as_u32() % mod_cnt == mod_idx
                    && Some(info.agent()) != broadcast_source
            }) {
                let ro_inner = ro_inner.clone();
                let space = space.clone();
                let data = data.clone();
                all.push(async move {
                    use discover::PeerDiscoverResult;

                    // attempt to establish a connection
                    let con_hnd = match discover::peer_connect(ro_inner, &info, timeout).await {
                        PeerDiscoverResult::OkShortcut => return,
                        PeerDiscoverResult::OkRemote { con_hnd, .. } => con_hnd,
                        PeerDiscoverResult::Err(err) => {
                            tracing::warn!(?err, "broadcast error");
                            return;
                        }
                    };

                    // generate our broadcast payload
                    let payload = wire::Wire::broadcast(space, info.agent.clone(), data);

                    // forward the data
                    if let Err(err) = con_hnd.notify(&payload, timeout).await {
                        tracing::warn!(?err, "broadcast error");
                    }
                })
            }

            futures::future::join_all(all).await;

            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_incoming_publish(
        &mut self,
        space: KSpace,
        to_agent: KAgent,
        source: KAgent,
        transfer_method: kitsune_p2p_fetch::TransferMethod,
        op_hash_list: OpHashList,
        context: kitsune_p2p_fetch::FetchContext,
        maybe_delegate: MaybeDelegate,
    ) -> InternalHandlerResult<()> {
        let ro_inner = self.ro_inner.clone();

        let just_hashes = op_hash_list.iter().map(|s| s.data()).collect();

        Ok(async move {
            let have_data_list = match ro_inner
                .host_api
                .check_op_data(space.clone(), just_hashes, Some(context))
                .await
                .map_err(KitsuneP2pError::other)
            {
                Err(err) => {
                    tracing::warn!(?err);
                    return Err(err);
                }
                Ok(res) => res,
            };

            for (op_hash, have_data) in op_hash_list.into_iter().zip(have_data_list) {
                if have_data {
                    if let Some((basis, mod_idx, mod_cnt)) = &maybe_delegate {
                        ro_inner
                            .i_s
                            .incoming_delegate_broadcast(
                                space.clone(),
                                basis.clone(),
                                to_agent.clone(),
                                *mod_idx,
                                *mod_cnt,
                                BroadcastData::Publish {
                                    source: source.clone(),
                                    transfer_method,
                                    op_hash_list: vec![op_hash],
                                    context,
                                },
                            )
                            .await?;
                    }
                    continue;
                } else {
                    // Add this hash to our fetch queue.
                    ro_inner.fetch_pool.push(FetchPoolPush {
                        key: FetchKey::Op(op_hash.data()),
                        space: space.clone(),
                        source: FetchSource::Agent(source.clone()),
                        size: op_hash.maybe_size(),
                        context: Some(context),
                        transfer_method: TransferMethod::Publish,
                    });

                    ro_inner.host_api.handle_op_hash_received(
                        &space,
                        &op_hash,
                        TransferMethod::Publish,
                    );

                    // Register a callback if maybe_delegate.is_some()
                    // to invoke the delegation on receipt of data.
                    if let Some((basis, mod_idx, mod_cnt)) = &maybe_delegate {
                        ro_inner.clone().publish_pending_delegate(
                            op_hash.data(),
                            PendingDelegate {
                                space: space.clone(),
                                basis: basis.clone(),
                                to_agent: to_agent.clone(),
                                mod_idx: *mod_idx,
                                mod_cnt: *mod_cnt,
                                data: BroadcastData::Publish {
                                    source: source.clone(),
                                    transfer_method,
                                    op_hash_list: vec![op_hash],
                                    context,
                                },
                            },
                        );
                    }
                }
            }

            Ok(())
        }
        .boxed()
        .into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn handle_notify(&mut self, to_agent: KAgent, data: wire::Wire) -> InternalHandlerResult<()> {
        let ro_inner = self.ro_inner.clone();
        let timeout = ro_inner.config.tuning_params.implicit_timeout();

        Ok(async move {
            match discover::search_and_discover_peer_connect(
                ro_inner.clone(),
                to_agent.clone(),
                timeout,
            )
            .await
            {
                discover::PeerDiscoverResult::OkShortcut => {
                    tracing::warn!("no reason to notify ourselves");
                }
                discover::PeerDiscoverResult::OkRemote { url: _, con_hnd } => {
                    if let Err(err) = con_hnd.notify(&data, timeout).await {
                        tracing::debug!(?err);
                    }
                }
                discover::PeerDiscoverResult::Err(err) => {
                    tracing::debug!(?err);
                }
            }

            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_resolve_publish_pending_delegates(
        &mut self,
        _space: KSpace,
        op_hash: KOpHash,
    ) -> InternalHandlerResult<()> {
        self.ro_inner.resolve_publish_pending_delegate(op_hash);

        unit_ok_fut()
    }

    fn handle_incoming_gossip(
        &mut self,
        _space: Arc<KitsuneSpace>,
        con: MetaNetCon,
        remote_url: String,
        data: Box<[u8]>,
        module_type: GossipModuleType,
    ) -> InternalHandlerResult<()> {
        match self.gossip_mod.get(&module_type) {
            Some(module) => module.incoming_gossip(con, remote_url, data)?,
            None => tracing::warn!(
                "Received gossip for {:?} but this gossip module isn't running",
                module_type
            ),
        }
        unit_ok_fut()
    }

    fn handle_incoming_metric_exchange(
        &mut self,
        _space: Arc<KitsuneSpace>,
        msgs: Vec<MetricExchangeMsg>,
    ) -> InternalHandlerResult<()> {
        self.ro_inner.metric_exchange.write().ingest_msgs(msgs);
        unit_ok_fut()
    }

    fn handle_new_con(&mut self, url: String, con: MetaNetCon) -> InternalHandlerResult<()> {
        self.ro_inner.metric_exchange.write().new_con(url, con);
        unit_ok_fut()
    }

    fn handle_del_con(&mut self, url: String) -> InternalHandlerResult<()> {
        self.ro_inner.metric_exchange.write().del_con(url);
        unit_ok_fut()
    }
}

struct UpdateAgentInfoInput<'borrow> {
    expires_after: u64,
    space: Arc<KitsuneSpace>,
    agent: Arc<KitsuneAgent>,
    bootstrap_net: BootstrapNet,
    arq: Arq,
    urls: &'borrow Vec<TxUrl>,
    evt_sender: &'borrow futures::channel::mpsc::Sender<KitsuneP2pEvent>,
    internal_sender: &'borrow ghost_actor::GhostSender<SpaceInternal>,
    _mdns_handles: &'borrow mut HashMap<Vec<u8>, Arc<AtomicBool>>,
    bootstrap_service: &'borrow Option<Url2>,
    dynamic_arcs: bool,
}

async fn update_arc_length(
    evt_sender: &futures::channel::mpsc::Sender<KitsuneP2pEvent>,
    space: Arc<KitsuneSpace>,
    arq: &mut Arq,
) -> KitsuneP2pResult<()> {
    let dim = SpaceDimension::standard();
    let arc = arq.to_dht_arc(dim);
    let view = evt_sender.query_peer_density(space.clone(), arc).await?;

    let cov_before = arq.coverage(dim) * 100.0;
    tracing::trace!("Updating arc for space {:?}:", space);

    #[cfg(feature = "test_utils")]
    tracing::trace!("Before: {:2.1}% |{}|", cov_before, arq.to_ascii(dim, 64));

    view.update_arq(arq);

    let cov_after = arq.coverage(dim) * 100.0;

    #[cfg(feature = "test_utils")]
    tracing::trace!("After:  {:2.1}% |{}|", cov_after, arq.to_ascii(dim, 64));

    tracing::trace!("Diff: {:-2.2}%", cov_after - cov_before);

    Ok(())
}

// TODO document me
async fn update_single_agent_info(
    input: UpdateAgentInfoInput<'_>,
) -> KitsuneP2pResult<AgentInfoSigned> {
    let UpdateAgentInfoInput {
        expires_after,
        space,
        agent,
        bootstrap_net,
        mut arq,
        urls,
        evt_sender,
        internal_sender,
        _mdns_handles,
        bootstrap_service,
        dynamic_arcs,
    } = input;

    if dynamic_arcs {
        update_arc_length(evt_sender, space.clone(), &mut arq).await?;
    }

    // Update the agents arc through the internal sender.
    internal_sender.update_agent_arc(agent.clone(), arq).await?;

    let signed_at_ms = kitsune_p2p_bootstrap_client::now_once(None, bootstrap_net).await?;
    let expires_at_ms = signed_at_ms + expires_after;

    let agent_info_signed = AgentInfoSigned::sign(
        space.clone(),
        agent.clone(),
        arq,
        urls.clone(),
        signed_at_ms,
        expires_at_ms,
        |d| {
            let data = Arc::new(d.to_vec());
            async {
                let sign_req = SignNetworkDataEvt {
                    space: space.clone(),
                    agent: agent.clone(),
                    data,
                };
                evt_sender
                    .sign_network_data(sign_req)
                    .await
                    .map(Arc::new)
                    .map_err(KitsuneError::other)
            }
        },
    )
    .await?;

    tracing::debug!(?agent_info_signed);

    // Write to the local peer store. The agent info will also be stored via a local broadcast,
    // except in the case of a zero arc where it gets filtered out, so we store it here too
    // to be sure it makes it into the local store.
    put_local_agent_info(evt_sender.clone(), agent_info_signed.clone()).await?;

    // Push to the network as well
    {
        // TODO: "mdns" stuff
    }

    // bootstrap stuff
    {
        kitsune_p2p_bootstrap_client::put(
            bootstrap_service.clone(),
            agent_info_signed.clone(),
            bootstrap_net,
        )
        .await?;
    }
    Ok(agent_info_signed)
}

use crate::spawn::actor::space::agent_info_update::AgentInfoUpdateTask;
use crate::spawn::actor::space::bootstrap_task::BootstrapTask;
use ghost_actor::dependencies::must_future::MustBoxFuture;

impl ghost_actor::GhostControlHandler for Space {
    fn handle_ghost_actor_shutdown(mut self) -> MustBoxFuture<'static, ()> {
        async move {
            // The line below was added when migrating to rust edition 2021, per
            // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
            let _ = &self;
            self.ro_inner.metric_exchange.write().shutdown();

            use futures::sink::SinkExt;
            // this is a curtesy, ok if fails
            let _ = self.host_api.legacy.close().await;
            for module in self.gossip_mod.values_mut() {
                module.close();
            }
        }
        .boxed()
        .into()
    }
}

impl ghost_actor::GhostHandler<KitsuneP2p> for Space {}

impl KitsuneP2pHandler for Space {
    fn handle_join(
        &mut self,
        space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<Arq>,
    ) -> KitsuneP2pHandlerResult<()> {
        tracing::debug!(?space, ?agent, ?initial_arq, "handle_join");

        match initial_arq {
            // This agent has been on the network before and has an arc stored on the Kitsune host.
            // Respect the current storage settings and apply this arc.
            Some(initial_arq) => {
                self.agent_arqs.insert(agent.clone(), initial_arq);
            }
            // This agent is new to the network and has no arc stored on the Kitsune host.
            // Get a default arc for the agent
            None => {
                self.agent_arqs
                    .insert(agent.clone(), self.default_arc_for_agent(agent.clone()));
            }
        }

        self.local_joined_agents
            .insert(agent.clone(), maybe_agent_info);
        for module in self.gossip_mod.values() {
            module.local_agent_join(agent.clone());
        }

        // TODO: make network join a purely local operation
        let fut = self.i_s.update_single_agent_info(agent);

        Ok(fut.boxed().into())
    }

    fn handle_leave(
        &mut self,
        _space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
    ) -> KitsuneP2pHandlerResult<()> {
        self.local_joined_agents.remove(&agent);
        self.agent_arqs.remove(&agent);
        self.update_metric_exchange_arcset();
        for module in self.gossip_mod.values() {
            module.local_agent_leave(agent.clone());
        }
        self.publish_leave_agent_info(agent)
    }

    fn handle_rpc_single(
        &mut self,
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
        timeout_ms: Option<u64>,
    ) -> KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.host_api.legacy.clone();

        let timeout_ms = match timeout_ms {
            None | Some(0) => self.config.tuning_params.default_rpc_single_timeout_ms as u64,
            _ => timeout_ms.unwrap(),
        };
        let timeout = KitsuneTimeout::from_millis(timeout_ms);

        let start = tokio::time::Instant::now();

        let discover_fut = discover::search_and_discover_peer_connect(
            self.ro_inner.clone(),
            to_agent.clone(),
            timeout,
        );

        let metrics = self.ro_inner.metrics.clone();

        Ok(async move {
            match discover_fut.await {
                discover::PeerDiscoverResult::OkShortcut => {
                    // reflect this request locally
                    evt_sender.call(space, to_agent, payload).await
                }
                discover::PeerDiscoverResult::OkRemote { con_hnd, .. } => {
                    let payload = wire::Wire::call(space.clone(), to_agent.clone(), payload.into());
                    let res = con_hnd.request(&payload, timeout).await?;
                    match res {
                        wire::Wire::Failure(wire::Failure { reason }) => {
                            metrics
                                .write()
                                .record_reachability_event(false, [&to_agent]);
                            metrics
                                .write()
                                .record_latency_micros(start.elapsed().as_micros(), [&to_agent]);
                            Err(reason.into())
                        }
                        wire::Wire::CallResp(wire::CallResp { data }) => {
                            metrics.write().record_reachability_event(true, [&to_agent]);
                            metrics
                                .write()
                                .record_latency_micros(start.elapsed().as_micros(), [&to_agent]);
                            Ok(data.into())
                        }
                        r => {
                            metrics
                                .write()
                                .record_reachability_event(false, [&to_agent]);
                            metrics
                                .write()
                                .record_latency_micros(start.elapsed().as_micros(), [&to_agent]);
                            Err(format!("invalid response: {:?}", r).into())
                        }
                    }
                }
                discover::PeerDiscoverResult::Err(e) => Err(e),
            }
        }
        .boxed()
        .into())
    }

    fn handle_rpc_multi(
        &mut self,
        input: actor::RpcMulti,
    ) -> KitsuneP2pHandlerResult<Vec<actor::RpcMultiResponse>> {
        let local_joined_agents = self.local_joined_agents.keys().cloned().collect();
        let fut =
            rpc_multi_logic::handle_rpc_multi(input, self.ro_inner.clone(), local_joined_agents);
        Ok(fut.boxed().into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn handle_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
        timeout: KitsuneTimeout,
        data: BroadcastData,
    ) -> KitsuneP2pHandlerResult<()> {
        // first, forward this data to all connected local agents.
        let mut local_notify_events = Vec::new();
        let mut local_agent_info_events = Vec::new();
        match &data {
            BroadcastData::User(data) => {
                for (agent_key, _agent_info) in self.local_joined_agents.iter() {
                    if let Some(arq) = self.agent_arqs.get(agent_key) {
                        if arq.to_dht_arc_std().contains(basis.get_loc()) {
                            let fut = self.host_api.legacy.notify(
                                space.clone(),
                                agent_key.clone(),
                                data.clone(),
                            );
                            local_notify_events.push(async move {
                                if let Err(err) = fut.await {
                                    tracing::warn!(?err, "failed local broadcast");
                                }
                            });
                        }
                    }
                }
            }
            BroadcastData::AgentInfo(agent_info) => {
                if self
                    .agent_arqs
                    .values()
                    .any(|arc| arc.to_dht_arc_std().contains(basis.get_loc()))
                {
                    let fut =
                        put_local_agent_info(self.host_api.legacy.clone(), agent_info.clone());
                    local_agent_info_events.push(async move {
                        if let Err(err) = fut.await {
                            tracing::warn!(?err, "failed local broadcast");
                        }
                    });
                }
            }
            BroadcastData::Publish { .. } => {
                // There is nothing to do here!
                // *We* are the node publishing
                // so we already have these hashes : )
            }
        }

        // then, find a list of agents in a potentially remote neighborhood
        // that should be responsible for holding the data.
        let ro_inner = self.ro_inner.clone();
        let discover_fut =
            discover::search_remotes_covering_basis(ro_inner.clone(), basis.get_loc(), timeout);
        Ok(async move {
            futures::future::join_all(local_notify_events).await;
            futures::future::join_all(local_agent_info_events).await;

            // NOTE
            // Holochain currently does most of its testing without any remote
            // nodes if we do this inline, it takes us to the 30 second timeout
            // on every one of those... so spawning for now, which means
            // we won't get notified if we are unable to publish to anyone.
            // Also, if conductor spams us with publishes, we could fill
            // the memory with publish tasks.
            let task_permit = ro_inner
                .parallel_notify_permit
                .clone()
                .acquire_owned()
                .await
                .ok();
            tokio::task::spawn(async move {
                let cover_nodes = discover_fut.await?;
                if cover_nodes.is_empty() {
                    return Err("failed to discover neighboring peers".into());
                }

                let mut all = Vec::new();

                // is there a better way to do this??
                //
                // since we're gathering the connections in one place,
                // if any of them take the full timeout, we won't have any
                // time to actually forward the message to them.
                //
                // and if a node is that slow anyways, maybe we don't want
                // to trust them to forward the message in any case...
                let half_timeout =
                    KitsuneTimeout::from_millis(timeout.time_remaining().as_millis() as u64 / 2);

                // attempt to open connections to the discovered remote nodes
                for info in cover_nodes {
                    let ro_inner = ro_inner.clone();
                    all.push(async move {
                        use discover::PeerDiscoverResult;
                        let con_hnd =
                            match discover::peer_connect(ro_inner, &info, half_timeout).await {
                                PeerDiscoverResult::OkShortcut => return None,
                                PeerDiscoverResult::OkRemote { con_hnd, .. } => con_hnd,
                                PeerDiscoverResult::Err(_) => return None,
                            };
                        Some((info.agent.clone(), con_hnd))
                    });
                }

                let con_list = futures::future::join_all(all)
                    .await
                    .into_iter()
                    .flatten()
                    .collect::<Vec<_>>();

                if con_list.is_empty() {
                    return Err("failed to connect to neighboring peers".into());
                }

                let mut all = Vec::new();

                // Determine the total number of nodes we'll be publishing to.
                // We'll make each remote responsible for a subset of delegate
                // broadcasting by having them apply the formula:
                // `agent.get_loc() % mod_cnt == mod_idx` -- if true,
                // they'll be responsible for forwarding the data to that node.
                let mod_cnt = con_list.len();
                for (mod_idx, (agent, con_hnd)) in con_list.into_iter().enumerate() {
                    // build our delegate message
                    let data = wire::Wire::delegate_broadcast(
                        space.clone(),
                        basis.clone(),
                        agent,
                        mod_idx as u32,
                        mod_cnt as u32,
                        data.clone(),
                    );

                    // notify the remote node
                    all.push(async move {
                        if let Err(err) = con_hnd.notify(&data, timeout).await {
                            tracing::warn!(?err, "delegate broadcast error");
                        }
                    });
                }

                futures::future::join_all(all).await;

                drop(task_permit);
                KitsuneP2pResult::Ok(())
            });

            Ok(())
        }
        .boxed()
        .into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn handle_targeted_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        agents: Vec<Arc<KitsuneAgent>>,
        timeout: KitsuneTimeout,
        payload: Vec<u8>,
        drop_at_limit: bool,
    ) -> KitsuneP2pHandlerResult<()> {
        let evt_sender = self.host_api.legacy.clone();
        let ro_inner = self.ro_inner.clone();
        Ok(async move {
            for agent in agents {
                let task_permit = if drop_at_limit {
                    match ro_inner.parallel_notify_permit.clone().try_acquire_owned() {
                        Ok(p) => Some(p),
                        Err(_) => {
                            tracing::debug!(
                                "Too many outstanding notifies, dropping notify to {:?}",
                                agent
                            );
                            continue;
                        }
                    }
                } else {
                    // limit spawns with semaphore.
                    ro_inner
                        .parallel_notify_permit
                        .clone()
                        .acquire_owned()
                        .await
                        .ok()
                };
                let space = space.clone();
                let payload = payload.clone();
                let evt_sender = evt_sender.clone();
                let ro_inner = ro_inner.clone();
                tokio::task::spawn(async move {
                    let discover_result = discover::search_and_discover_peer_connect(
                        ro_inner.clone(),
                        agent.clone(),
                        timeout,
                    )
                    .await;
                    match discover_result {
                        discover::PeerDiscoverResult::OkShortcut => {
                            // reflect this request locally
                            evt_sender
                                .notify(space, agent, payload)
                                .map(|r| {
                                    if let Err(e) = r {
                                        tracing::error!(
                                            "Failed to broadcast to local agent because: {:?}",
                                            e
                                        )
                                    }
                                })
                                .await;
                        }
                        discover::PeerDiscoverResult::OkRemote { con_hnd, .. } => {
                            let payload =
                                wire::Wire::broadcast(space, agent, BroadcastData::User(payload));
                            con_hnd
                                .notify(&payload, timeout)
                                .map(|r| {
                                    if let Err(e) = r {
                                        tracing::info!(
                                            "Failed to broadcast to remote agent because: {:?}",
                                            e
                                        )
                                    }
                                })
                                .await;
                        }
                        discover::PeerDiscoverResult::Err(e) => {
                            async move {
                                tracing::info!(
                                    "Failed to discover connection for {:?} because: {:?}",
                                    agent,
                                    e
                                );
                            }
                            .await
                        }
                    }

                    drop(task_permit);
                });
            }
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_new_integrated_data(&mut self, _: KSpace) -> InternalHandlerResult<()> {
        for module in self.gossip_mod.values() {
            module.new_integrated_data();
        }
        unit_ok_fut()
    }

    fn handle_authority_for_hash(
        &mut self,
        _space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
    ) -> KitsuneP2pHandlerResult<bool> {
        let loc = basis.get_loc();
        let r = self
            .agent_arqs
            .values()
            .any(|arq| arq.to_dht_arc_std().contains(loc));
        Ok(async move { Ok(r) }.boxed().into())
    }

    fn handle_dump_network_metrics(
        &mut self,
        _space: Option<Arc<KitsuneSpace>>,
    ) -> KitsuneP2pHandlerResult<serde_json::Value> {
        let space = self.ro_inner.space.clone();
        let metrics = self.ro_inner.metrics.read().dump();
        Ok(async move {
            Ok(serde_json::json!({
                "space": space.to_string(),
                "metrics": metrics,
            }))
        }
        .boxed()
        .into())
    }

    fn handle_dump_network_stats(&mut self) -> KitsuneP2pHandlerResult<serde_json::Value> {
        // call handled by parent actor and never delegated to spaces
        unreachable!()
    }

    fn handle_get_diagnostics(
        &mut self,
        _space: KSpace,
    ) -> KitsuneP2pHandlerResult<KitsuneDiagnostics> {
        let diagnostics = KitsuneDiagnostics {
            metrics: self.ro_inner.metrics.clone(),
            fetch_pool: self.ro_inner.fetch_pool.clone().into(),
        };
        Ok(async move { Ok(diagnostics) }.boxed().into())
    }

    fn handle_storage_arcs(
        &mut self,
        _space: KSpace,
    ) -> KitsuneP2pHandlerResult<Vec<kitsune2_api::DhtArc>> {
        let arcs = self
            .agent_arqs
            .values()
            .map(|arq| {
                let arc = arq.to_dht_arc_std();

                if arc.is_empty() {
                    kitsune2_api::DhtArc::Empty
                } else if arc.is_full() {
                    kitsune2_api::DhtArc::FULL
                } else {
                    let start = arc.start_loc().0 .0;
                    kitsune2_api::DhtArc::Arc(start, start.wrapping_add(arc.length() as u32))
                }
            })
            .collect();

        Ok(async move { Ok(arcs) }.boxed().into())
    }
}

pub(crate) struct PendingDelegate {
    pub(crate) space: KSpace,
    pub(crate) basis: KBasis,
    pub(crate) to_agent: KAgent,
    pub(crate) mod_idx: u32,
    pub(crate) mod_cnt: u32,
    pub(crate) data: BroadcastData,
}

pub(crate) struct SpaceReadOnlyInner {
    pub(crate) local_url: Arc<std::sync::Mutex<Option<String>>>,
    pub(crate) space: Arc<KitsuneSpace>,
    #[allow(dead_code)]
    pub(crate) i_s: ghost_actor::GhostSender<SpaceInternal>,
    pub(crate) host_api: HostApiLegacy,
    pub(crate) ep_hnd: MetaNet,
    #[allow(dead_code)]
    pub(crate) config: Arc<KitsuneP2pConfig>,
    pub(crate) bootstrap_net: BootstrapNet,
    pub(crate) parallel_notify_permit: Arc<tokio::sync::Semaphore>,
    pub(crate) metrics: MetricsSync,
    pub(crate) metric_exchange: MetricExchangeSync,
    pub(crate) publish_pending_delegates: parking_lot::Mutex<HashMap<KOpHash, PendingDelegate>>,
    #[allow(dead_code)]
    pub(crate) fetch_pool: FetchPool,
}

impl SpaceReadOnlyInner {
    pub(crate) fn get_local_url(&self) -> Option<String> {
        self.local_url.lock().unwrap().clone()
    }

    pub(crate) fn publish_pending_delegate(
        self: Arc<Self>,
        op_hash: KOpHash,
        pending_delegate: PendingDelegate,
    ) {
        {
            let this = self.clone();
            let op_hash = op_hash.clone();
            tokio::task::spawn(async move {
                tokio::time::sleep(
                    this.config
                        .tuning_params
                        .implicit_timeout()
                        .time_remaining(),
                )
                .await;

                this.publish_pending_delegates.lock().remove(&op_hash);
            });
        }

        self.publish_pending_delegates
            .lock()
            .insert(op_hash, pending_delegate);
    }

    pub(crate) fn resolve_publish_pending_delegate(&self, op_hash: KOpHash) {
        if let Some(PendingDelegate {
            space,
            basis,
            to_agent,
            mod_idx,
            mod_cnt,
            data,
        }) = self.publish_pending_delegates.lock().remove(&op_hash)
        {
            let i_s = self.i_s.clone();
            tokio::task::spawn(async move {
                let _ = i_s
                    .incoming_delegate_broadcast(space, basis, to_agent, mod_idx, mod_cnt, data)
                    .await;
            });
        }
    }
}

/// A Kitsune P2p Node can track multiple "spaces" -- Non-interacting namespaced
/// areas that share common transport infrastructure for communication.
pub(crate) struct Space {
    pub(crate) ro_inner: Arc<SpaceReadOnlyInner>,
    pub(crate) space: Arc<KitsuneSpace>,
    pub(crate) i_s: ghost_actor::GhostSender<SpaceInternal>,
    pub(crate) host_api: HostApiLegacy,
    pub(crate) local_joined_agents: HashMap<Arc<KitsuneAgent>, Option<AgentInfoSigned>>,
    pub(crate) agent_arqs: HashMap<Arc<KitsuneAgent>, Arq>,
    pub(crate) config: Arc<KitsuneP2pConfig>,
    mdns_handles: HashMap<Vec<u8>, Arc<AtomicBool>>,
    _mdns_listened_spaces: HashSet<String>,
    gossip_mod: HashMap<GossipModuleType, GossipModule>,
}

impl Space {
    /// space constructor
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        space: Arc<KitsuneSpace>,
        i_s: ghost_actor::GhostSender<SpaceInternal>,
        host_api: HostApiLegacy,
        ep_hnd: MetaNet,
        config: Arc<KitsuneP2pConfig>,
        bootstrap_net: BootstrapNet,
        bandwidth_throttles: BandwidthThrottles,
        parallel_notify_permit: Arc<tokio::sync::Semaphore>,
        fetch_pool: FetchPool,
        local_url: Arc<std::sync::Mutex<Option<String>>>,
    ) -> Self {
        let metrics = MetricsSync::default();

        {
            let space = space.clone();
            let metrics = metrics.clone();
            let host = host_api.clone();
            tokio::task::spawn(async move {
                loop {
                    tokio::time::sleep(std::time::Duration::from_millis(
                        HISTORICAL_METRIC_RECORD_FREQ_MS,
                    ))
                    .await;

                    let records = metrics.read().dump_historical();

                    let _ = host.record_metrics(space.clone(), records).await;
                }
            });
        }

        let metric_exchange = MetricExchangeSync::spawn(
            space.clone(),
            config.tuning_params.clone(),
            host_api.clone(),
            metrics.clone(),
        );

        let gossip_mod = if config.tuning_params.arc_clamping() == Some(ArqClamping::Empty) {
            // If arcs are clamped to zero, then completely disable gossip for this node.
            // Gossip will never resume
            tracing::info!("Gossip is disabled due to arc_clamping setting");
            HashMap::new()
        } else {
            config
                .tuning_params
                .gossip_strategy
                .split(',')
                .flat_map(|module| match module {
                    "sharded-gossip" => {
                        let mut gossips = vec![];
                        if !config.tuning_params.disable_recent_gossip {
                            gossips.push((
                                GossipModuleType::ShardedRecent,
                                crate::gossip::sharded_gossip::recent_factory(
                                    bandwidth_throttles.recent(),
                                ),
                            ));
                        }
                        if !config.tuning_params.disable_historical_gossip {
                            gossips.push((
                                GossipModuleType::ShardedHistorical,
                                crate::gossip::sharded_gossip::historical_factory(
                                    bandwidth_throttles.historical(),
                                ),
                            ));
                        }
                        gossips
                    }
                    "none" => vec![],
                    _ => {
                        panic!("unknown gossip strategy: {}", module);
                    }
                })
                .map(|(module, factory)| {
                    (
                        module,
                        factory.spawn_gossip_task(
                            config.clone(),
                            space.clone(),
                            ep_hnd.clone(),
                            host_api.clone(),
                            metrics.clone(),
                            fetch_pool.clone(),
                        ),
                    )
                })
                .collect()
        };

        AgentInfoUpdateTask::spawn(
            i_s.clone(),
            std::time::Duration::from_millis(
                config.tuning_params.gossip_agent_info_update_interval_ms as u64,
            ),
        );

        if config.bootstrap_service.is_some() {
            // spawn the periodic bootstrap pull
            BootstrapTask::spawn(
                i_s.clone(),
                host_api.legacy.clone(),
                space.clone(),
                // TODO: refactor to take just the Url. We know that nothing will
                //       happen if this is None.
                config.bootstrap_service.clone(),
                bootstrap_net,
                config
                    .tuning_params
                    .bootstrap_check_delay_backoff_multiplier,
                config.tuning_params.bootstrap_max_delay_s,
            );
        }

        let ro_inner = Arc::new(SpaceReadOnlyInner {
            local_url,
            space: space.clone(),
            i_s: i_s.clone(),
            host_api: host_api.clone(),
            ep_hnd,
            config: config.clone(),
            bootstrap_net,
            parallel_notify_permit,
            metrics,
            metric_exchange,
            publish_pending_delegates: parking_lot::Mutex::new(HashMap::new()),
            fetch_pool,
        });

        Self {
            ro_inner,
            space,
            i_s,
            host_api,
            local_joined_agents: HashMap::new(),
            agent_arqs: HashMap::new(),
            config,
            mdns_handles: HashMap::new(),
            _mdns_listened_spaces: HashSet::new(),
            gossip_mod,
        }
    }

    fn update_metric_exchange_arcset(&mut self) {
        let arc_set = self
            .agent_arqs
            .values()
            .map(|a| DhtArcSet::from_interval(DhtArcRange::from(a.to_dht_arc_std())))
            .fold(DhtArcSet::new_empty(), |a, i| a.union(&i));
        self.ro_inner.metric_exchange.write().update_arcset(arc_set);
    }

    fn publish_leave_agent_info(
        &mut self,
        agent: Arc<KitsuneAgent>,
    ) -> KitsuneP2pHandlerResult<()> {
        let space = self.space.clone();
        let bootstrap_net = self.ro_inner.bootstrap_net;
        let evt_sender = self.host_api.legacy.clone();
        let bootstrap_service = self.config.bootstrap_service.clone();
        let expires_after = self.config.tuning_params.agent_info_expires_after_ms as u64;
        let host = self.host_api.clone();

        Ok(async move {
            let signed_at_ms = kitsune_p2p_bootstrap_client::now_once(None, bootstrap_net).await?;
            let expires_at_ms = signed_at_ms + expires_after;
            let agent_info_signed = AgentInfoSigned::sign(
                space.clone(),
                agent.clone(),
                ArqSize::empty(), // no storage arc
                Vec::new(),       // no urls
                signed_at_ms,
                expires_at_ms,
                |d| {
                    let data = Arc::new(d.to_vec());
                    async {
                        let sign_req = SignNetworkDataEvt {
                            space: space.clone(),
                            agent: agent.clone(),
                            data,
                        };
                        evt_sender
                            .sign_network_data(sign_req)
                            .await
                            .map(Arc::new)
                            .map_err(KitsuneError::other)
                    }
                },
            )
            .await?;

            tracing::debug!(?agent_info_signed);

            // TODO: at some point, we should not remove agents who have left, but rather
            // there should be a flag indicating they have left. The removed agent may just
            // get re-gossiped to another local agent in the same space, defeating the purpose.
            host.remove_agent_info_signed(GetAgentInfoSignedEvt { space, agent })
                .await
                .map_err(KitsuneP2pError::other)?;

            // Push to the network as well

            {
                match kitsune_p2p_bootstrap_client::put(
                    bootstrap_service.clone(),
                    agent_info_signed,
                    bootstrap_net,
                )
                .await
                {
                    Ok(_) => {
                        tracing::debug!("Successfully publish agent info to the bootstrap service");
                    }
                    Err(err) => {
                        tracing::info!(
                            ?err,
                            "Failed to publish agent info to the bootstrap service while leaving"
                        );
                    }
                }
            }

            Ok(())
        }
        .boxed()
        .into())
    }

    /// Get the existing agent storage arc or create a new one.
    fn get_agent_arq(&self, agent: &Arc<KitsuneAgent>) -> Arq {
        // TODO: We are simply setting the initial arc to full.
        // In the future we may want to do something more intelligent.
        //
        // In the case an initial_arc is passed into the join request,
        // handle_join will initialize this agent_arcs map to that value.
        self.agent_arqs.get(agent).cloned().unwrap_or_else(|| {
            tracing::warn!("Using default arc for agent, this should be configured by the host or initialised on network join");
            self.default_arc_for_agent((*agent).clone())
        })
    }

    /// Get the default arc for an agent.
    ///
    /// The default arc depends on the Kitsune tuning parameters.
    /// - Either arc clamping is set to empty and the arc is empty, or
    /// - The arc clamping is set to full or not set, in which cases the arc is full.
    fn default_arc_for_agent(&self, agent: Arc<KitsuneAgent>) -> Arq {
        let dim = SpaceDimension::standard();
        match self.config.tuning_params.arc_clamping() {
            Some(ArqClamping::Empty) => Arq::new_empty(dim, agent.get_loc()),
            Some(ArqClamping::Full) | None => {
                let strat = self.config.tuning_params.to_arq_strat();
                Arq::new_full_max(dim, &strat, agent.get_loc())
            }
        }
    }
}

/// Function to add local agent info to the store.
/// There's nothing special about this method, it's just helpful to
/// semantically see the situations where local info is being added.
async fn put_local_agent_info(
    evt_sender: futures::channel::mpsc::Sender<KitsuneP2pEvent>,
    agent_info: AgentInfoSigned,
) -> KitsuneP2pResult<()> {
    evt_sender
        .put_agent_info_signed(PutAgentInfoSignedEvt {
            peer_data: vec![agent_info],
        })
        .await?;
    Ok(())
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/test_util.rs
================================================
// TODO: not sure how to fix these unused warnings, something to do with
// the weird module structure and features

#![allow(unused)]

mod internal_stub;
mod legacy_host_stub;
mod space_internal_stub;

pub use internal_stub::{InternalStub, InternalStubTest, InternalStubTestSender};
pub use legacy_host_stub::LegacyHostStub;
pub use space_internal_stub::SpaceInternalStub;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/discover/test_get_cached_remotes_near_basis.rs
================================================
use super::*;
use crate::test_util::data::mk_agent_info;

#[tokio::test(flavor = "multi_thread")]
async fn happy_path() {
    struct T;

    impl GetCachedRemotesNearBasisSpace for T {
        fn space(&self) -> Arc<KitsuneSpace> {
            Arc::new(KitsuneSpace::new(vec![0x11; 32]))
        }

        fn query_agents(
            &self,
            _query: QueryAgentsEvt,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>> {
            async move {
                Ok(vec![
                    mk_agent_info(1).await,
                    mk_agent_info(2).await,
                    mk_agent_info(3).await,
                ])
            }
            .boxed()
            .into()
        }

        fn is_agent_local(
            &self,
            _agent: Arc<KitsuneAgent>,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
            async move { Ok(false) }.boxed().into()
        }
    }

    assert_eq!(
        3,
        get_cached_remotes_near_basis(T, 0.into(), KitsuneTimeout::from_millis(100))
            .await
            .unwrap()
            .len()
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn is_agent_local_err() {
    struct T;

    impl GetCachedRemotesNearBasisSpace for T {
        fn space(&self) -> Arc<KitsuneSpace> {
            Arc::new(KitsuneSpace::new(vec![0x11; 32]))
        }

        fn query_agents(
            &self,
            _query: QueryAgentsEvt,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>> {
            async move { Ok(vec![mk_agent_info(1).await]) }
                .boxed()
                .into()
        }

        fn is_agent_local(
            &self,
            _agent: Arc<KitsuneAgent>,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
            async move { Err(KitsuneP2pError::other("yo")) }
                .boxed()
                .into()
        }
    }

    assert!(
        get_cached_remotes_near_basis(T, 0.into(), KitsuneTimeout::from_millis(100))
            .await
            .is_err()
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn removes_locals() {
    struct T;

    impl GetCachedRemotesNearBasisSpace for T {
        fn space(&self) -> Arc<KitsuneSpace> {
            Arc::new(KitsuneSpace::new(vec![0x11; 32]))
        }

        fn query_agents(
            &self,
            _query: QueryAgentsEvt,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>> {
            async move {
                let mut out = Vec::new();
                for i in 0..40 {
                    out.push(mk_agent_info(i).await);
                }
                Ok(out)
            }
            .boxed()
            .into()
        }

        fn is_agent_local(
            &self,
            agent: Arc<KitsuneAgent>,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
            async move { Ok(agent[0] >= 20) }.boxed().into()
        }
    }

    assert_eq!(
        20,
        get_cached_remotes_near_basis(T, 0.into(), KitsuneTimeout::from_millis(100))
            .await
            .unwrap()
            .len()
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn empty_is_err() {
    struct T;

    impl GetCachedRemotesNearBasisSpace for T {
        fn space(&self) -> Arc<KitsuneSpace> {
            Arc::new(KitsuneSpace::new(vec![0x11; 32]))
        }

        fn query_agents(
            &self,
            _query: QueryAgentsEvt,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>> {
            async move { Ok(vec![]) }.boxed().into()
        }

        fn is_agent_local(
            &self,
            _agent: Arc<KitsuneAgent>,
        ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
            async move { Ok(false) }.boxed().into()
        }
    }

    assert!(
        get_cached_remotes_near_basis(T, 0.into(), KitsuneTimeout::from_millis(100))
            .await
            .is_err()
    );
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/discover/test_search_and_discover_peer_connect.rs
================================================
use super::*;
use crate::test_util::data::mk_agent_info;

struct TestInner<L, A>
where
    L: Fn(Arc<KitsuneAgent>) -> KitsuneP2pResult<bool> + 'static + Send + Sync,
    A: Fn(
            Arc<KitsuneAgent>,
        ) -> Result<Option<AgentInfoSigned>, Box<dyn Send + Sync + std::error::Error>>
        + 'static
        + Send
        + Sync,
{
    l: L,
    a: A,
}

impl<L, A> SearchAndDiscoverPeerConnect for TestInner<L, A>
where
    L: Fn(Arc<KitsuneAgent>) -> KitsuneP2pResult<bool> + 'static + Send + Sync,
    A: Fn(
            Arc<KitsuneAgent>,
        ) -> Result<Option<AgentInfoSigned>, Box<dyn Send + Sync + std::error::Error>>
        + 'static
        + Send
        + Sync,
{
    fn is_agent_local(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
        let res = (self.l)(agent);
        async move { res }.boxed().into()
    }

    fn get_agent_info_signed(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'_, Result<Option<AgentInfoSigned>, Box<dyn Send + Sync + std::error::Error>>>
    {
        let res = (self.a)(agent);
        async move { res }.boxed().into()
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn is_local() {
    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Ok(true),
            a: |_a| unreachable!(),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldReturn(PeerDiscoverResult::OkShortcut)
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn have_info() {
    let agent_info = std::sync::Mutex::new(Some(mk_agent_info(6).await));

    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Ok(false),
            a: move |_a| Ok(agent_info.lock().unwrap().take()),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldPeerConnect(_)
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn ignore_local_err() {
    let agent_info = std::sync::Mutex::new(Some(mk_agent_info(6).await));

    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Err("err".into()),
            a: move |_a| Ok(agent_info.lock().unwrap().take()),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldPeerConnect(_)
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn should_search_peers() {
    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Ok(false),
            a: |_a| Ok(None),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldSearchPeers
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn timeout() {
    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Ok(false),
            a: |_a| Ok(None),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1),
    );

    tokio::time::sleep(std::time::Duration::from_millis(5)).await;

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldReturn(PeerDiscoverResult::Err(_))
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn ignore_peer_get_error() {
    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        TestInner {
            l: |_a| Ok(false),
            a: |_a| Err("err".into()),
        },
        1,
        1,
        Arc::new(KitsuneAgent::new(vec![6; 32])),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(
        logic.check_state().await,
        SearchAndDiscoverPeerConnectLogicResult::ShouldSearchPeers
    ));
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/discover/test_search_remotes_covering_basis.rs
================================================
use super::*;
use kitsune_p2p_types::dht::{arq::ArqSize, spacetime::SpaceDimension};
use kitsune_p2p_types::tx_utils::TxUrl;
use SearchRemotesCoveringBasisLogicResult::*;

async fn mk_agent_info(u: u8, covers: u32, offline: bool) -> AgentInfoSigned {
    let url_list = if offline {
        vec![]
    } else {
        vec![TxUrl::from_str_panicking("wss://test")]
    };

    AgentInfoSigned::sign(
        Arc::new(KitsuneSpace::new(vec![0x11; 32])),
        Arc::new(KitsuneAgent::new(vec![u; 32])),
        ArqSize::from_half_len(covers),
        url_list,
        0,
        0,
        |_| async move { Ok(Arc::new(KitsuneSignature(vec![0; 64]))) },
    )
    .await
    .unwrap()
}

#[tokio::test(flavor = "multi_thread")]
async fn happy_path() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1000),
    );

    assert!(matches!(logic.check_nodes(vec![]), ShouldWait));

    let near = mk_agent_info(1, SpaceDimension::standard().quantum, false).await;

    assert!(matches!(logic.check_nodes(vec![near]), QueryPeers(_)));

    let covers = mk_agent_info(2, u32::MAX, false).await;

    assert!(matches!(logic.check_nodes(vec![covers]), Success(_)));
}

#[tokio::test(flavor = "multi_thread")]
async fn timeout() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1),
    );

    tokio::time::sleep(std::time::Duration::from_millis(5)).await;

    assert!(matches!(logic.check_nodes(vec![]), Error(_)));
}

#[tokio::test(flavor = "multi_thread")]
async fn respect_max_covers() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1000),
    );

    let mut covers = Vec::new();
    for i in 0..5 {
        covers.push(mk_agent_info(i, u32::MAX, false).await);
    }

    assert!(matches!(
        logic.check_nodes(covers),
        Success(results) if results.len() == 2
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn respect_max_near() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1000),
    );

    let mut covers = Vec::new();
    for i in 0..5 {
        covers.push(mk_agent_info(i, SpaceDimension::standard().quantum, false).await);
    }

    assert!(matches!(
        logic.check_nodes(covers),
        QueryPeers(results) if results.len() == 2
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn ignore_offline_nodes() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1000),
    );

    let covers_offline = mk_agent_info(2, u32::MAX, true).await;

    assert!(matches!(
        logic.check_nodes(vec![covers_offline]),
        ShouldWait
    ));
}

#[tokio::test(flavor = "multi_thread")]
async fn ignore_zero_cover_nodes() {
    let mut logic = SearchRemotesCoveringBasisLogic::new(
        1,
        1,
        2,
        (u32::MAX / 4).into(),
        KitsuneTimeout::from_millis(1000),
    );

    let mut nodes = Vec::new();

    // this one just has a small arc
    nodes.push(mk_agent_info(2, SpaceDimension::standard().quantum, false).await);
    // this one is a full-on lurker
    nodes.push(mk_agent_info(2, 0, false).await);

    // don't bother querying lurkers
    assert!(matches!(
        logic.check_nodes(nodes),
        QueryPeers(results) if results.len() == 1
    ));
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/fetch/fetch_task.rs
================================================
use crate::spawn::actor::{Internal, InternalSender};
use crate::{HostApiLegacy, KitsuneP2pError};
use ghost_actor::{GhostError, GhostSender};
use kitsune_p2p_fetch::{FetchKey, FetchPool};
use kitsune_p2p_types::config::KitsuneP2pConfig;
use parking_lot::RwLock;
use std::sync::Arc;
use tracing::Instrument;

pub struct FetchTask {
    is_finished: bool,
}

impl FetchTask {
    pub fn spawn(
        _config: KitsuneP2pConfig,
        fetch_pool: FetchPool,
        host: HostApiLegacy,
        internal_sender: GhostSender<Internal>,
    ) -> Arc<RwLock<Self>> {
        let this = Arc::new(RwLock::new(FetchTask { is_finished: false }));

        tokio::spawn({
            let this = this.clone();
            async move {
                'task_loop: loop {
                    // Drop sources that aren't responding to fetch requests, and any items that have no remaining sources to fetch from.
                    fetch_pool.check_sources();

                    let list = fetch_pool.get_items_to_fetch();

                    for (key, space, source, context) in list {
                        let FetchKey::Op(op_hash) = &key;

                        if let Ok(mut res) = host
                            .check_op_data(space.clone(), vec![op_hash.clone()], context)
                            .await
                        {
                            if res.len() == 1 && res.remove(0) {
                                fetch_pool.remove(&key);
                                continue;
                            }
                        }

                        if let Err(err) = internal_sender.fetch(key, space, source).await {
                            match err {
                                KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                                    tracing::warn!("Fetch task is shutting down because the internal sender is closed");
                                    break 'task_loop;
                                }
                                // TODO are these so common that we can discard them? Should there be a metric to track this?
                                _ => tracing::debug!(?err),
                            }
                        }
                    }

                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                }

                tracing::info!("Fetch task is finishing");
                this.write().is_finished = true;
            }.in_current_span()
        });

        this
    }
}

#[cfg(test)]
mod tests {
    use super::FetchTask;
    use crate::spawn::actor::test_util::{InternalStub, InternalStubTest, InternalStubTestSender};
    use crate::spawn::actor::{Internal, KSpace};
    use crate::HostStub;
    use futures::FutureExt;
    use ghost_actor::actor_builder::GhostActorBuilder;
    use ghost_actor::{GhostControlSender, GhostSender};
    use kitsune_p2p_fetch::test_utils::{test_key_hash, test_req_op, test_source};
    use kitsune_p2p_fetch::FetchSource;
    use kitsune_p2p_fetch::{FetchKey, FetchPool};
    use kitsune_p2p_types::config::KitsuneP2pConfig;
    use kitsune_p2p_types::KOpHash;
    use parking_lot::{Mutex, RwLock};
    use std::collections::HashSet;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    use std::time::Duration;

    #[tokio::test(start_paused = true)]
    async fn fetch_single_op() {
        let (_task, fetch_pool, internal_sender_test, held_op_data, _) =
            setup(InternalStub::new()).await;

        fetch_pool.push(test_req_op(1, None, test_source(1)));
        wait_for_pool_n(&fetch_pool, 1).await;

        let fetched = wait_for_fetch_n(internal_sender_test.clone(), 1).await;

        // The item should get fetched
        assert_eq!(1, fetched.iter().flatten().count());

        // Simulate the requested item being sent back
        held_op_data.lock().insert(test_key_hash(1));

        // Move forwards by 5 minutes so that the item will be retried
        tokio::time::advance(Duration::from_secs(5 * 60)).await;

        // Then the item should be removed from the pool
        wait_for_pool_n(&fetch_pool, 0).await;

        internal_sender_test
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();
    }

    #[tokio::test(start_paused = true)]
    async fn fetch_task_shuts_down_if_internal_sender_closes() {
        let (task, fetch_pool, internal_sender_test, _held_op_data, _) =
            setup(InternalStub::new()).await;

        // Do enough testing to prove the loop is up and running
        fetch_pool.push(test_req_op(1, None, test_source(1)));
        wait_for_pool_n(&fetch_pool, 1).await;
        wait_for_fetch_n(internal_sender_test.clone(), 1).await;

        // Shut down ghost actor to close the sender
        internal_sender_test
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        // Move forwards by 5 minutes so that the item will be retried
        tokio::time::advance(Duration::from_secs(5 * 60)).await;

        tokio::time::timeout(Duration::from_secs(5), {
            let task = task.clone();
            async move {
                loop {
                    if task.read().is_finished {
                        return;
                    }

                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
        })
        .await
        .expect("Task should have shut down but is reporting that it is still running");

        assert!(task.read().is_finished);
    }

    // TODO the API supports batch queries, why not query in batch? We are pushing extra requests through a bottleneck
    #[tokio::test(start_paused = true)]
    async fn fetch_checks_op_status_one_by_one_to_host() {
        let (_task, fetch_pool, internal_sender_test, _held_op_data, check_op_data_call_count) =
            setup(InternalStub::new()).await;

        fetch_pool.push(test_req_op(1, None, test_source(1)));
        fetch_pool.push(test_req_op(2, None, test_source(2)));
        fetch_pool.push(test_req_op(3, None, test_source(3)));
        wait_for_pool_n(&fetch_pool, 3).await;

        let fetched = wait_for_fetch_n(internal_sender_test.clone(), 3).await;

        assert_eq!(3, fetched.iter().flatten().count());

        // This should be 1 if we passed all the ops to check at once.
        assert_eq!(3, check_op_data_call_count.load(Ordering::SeqCst));

        internal_sender_test
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();
    }

    async fn setup(
        task: InternalStub,
    ) -> (
        Arc<RwLock<FetchTask>>,
        FetchPool,
        GhostSender<InternalStubTest>,
        Arc<Mutex<HashSet<KOpHash>>>,
        Arc<AtomicUsize>,
    ) {
        let builder = GhostActorBuilder::new();

        let internal_sender = builder
            .channel_factory()
            .create_channel::<Internal>()
            .await
            .unwrap();

        let internal_test_sender = builder
            .channel_factory()
            .create_channel::<InternalStubTest>()
            .await
            .unwrap();

        tokio::spawn(builder.spawn(task));

        let fetch_pool = FetchPool::new_bitwise_or();

        let op_data = Arc::new(Mutex::new(HashSet::<KOpHash>::new()));
        let check_op_data_call_count = Arc::new(AtomicUsize::new(0));

        let (dummy_sender, _) = futures::channel::mpsc::channel(10);
        // if needed, use a real stub:
        // let (host_sender, host_receiver) = channel(10);
        // let host_receiver_stub = HostReceiverStub::start(host_receiver);

        // TODO this logic should just be common, and the HostStub can expose a hashset instead that
        //      tests can add to as required.
        let host_stub = HostStub::with_check_op_data({
            let op_data = op_data.clone();
            let check_op_data_call_count = check_op_data_call_count.clone();
            Box::new(move |_space, op_hashes, _ctx| {
                check_op_data_call_count.fetch_add(1, Ordering::SeqCst);
                let op_data = op_data.lock();

                let held_hashes = op_hashes
                    .into_iter()
                    .map(|hash| op_data.contains(&hash))
                    .collect();

                async move { Ok(held_hashes) }.boxed().into()
            })
        })
        .legacy(dummy_sender);

        let task = FetchTask::spawn(
            KitsuneP2pConfig::mem(),
            fetch_pool.clone(),
            host_stub,
            internal_sender,
        );

        (
            task,
            fetch_pool,
            internal_test_sender,
            op_data,
            check_op_data_call_count,
        )
    }

    async fn wait_for_pool_n(fetch_pool: &FetchPool, n: usize) {
        tokio::time::timeout(Duration::from_secs(1), async move {
            loop {
                if fetch_pool.len() == n {
                    return;
                }

                tokio::time::sleep(Duration::from_millis(1)).await;
            }
        })
        .await
        .unwrap_or_else(|_| {
            panic!(
                "Timeout while waiting for fetch pool to contain {} items, has {}",
                n,
                fetch_pool.len()
            )
        })
    }

    async fn wait_for_fetch_n(
        internal_sender_test: GhostSender<InternalStubTest>,
        n: usize,
    ) -> Vec<Vec<(FetchKey, KSpace, FetchSource)>> {
        tokio::time::timeout(Duration::from_secs(1), async move {
            let mut all_calls = vec![];

            loop {
                let calls = internal_sender_test.drain_fetch_calls().await.unwrap();

                all_calls.push(calls);
                if all_calls.iter().flatten().count() == n {
                    return all_calls;
                }
            }
        })
        .await
        .unwrap()
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/fetch/response_config.rs
================================================
use crate::spawn::meta_net::MetaNetCon;
use crate::wire;
use kitsune_p2p_types::config::KitsuneP2pTuningParams;
use kitsune_p2p_types::{dht, KOpData, KSpace};

#[derive(Clone)]
pub struct FetchResponseConfig(KitsuneP2pTuningParams);

impl FetchResponseConfig {
    pub fn new(params: KitsuneP2pTuningParams) -> Self {
        FetchResponseConfig(params)
    }
}

impl kitsune_p2p_fetch::FetchResponseConfig for FetchResponseConfig {
    type User = (
        MetaNetCon,
        String,
        Option<(dht::prelude::RegionCoords, bool)>,
    );

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn respond(
        &self,
        space: KSpace,
        user: Self::User,
        completion_guard: kitsune_p2p_fetch::FetchResponseGuard,
        op: KOpData,
    ) {
        let timeout = self.0.implicit_timeout();
        tokio::task::spawn(async move {
            let _completion_guard = completion_guard;

            // MAYBE: open a new connection if the con was closed??
            let (con, _url, region) = user;

            let item = wire::PushOpItem {
                op_data: op,
                region,
            };
            tracing::debug!("push_op_data: {:?}", item);
            let payload = wire::Wire::push_op_data(vec![(space, vec![item])]);

            if let Err(err) = con.notify(&payload, timeout).await {
                tracing::warn!(?err, "error responding to op fetch");
            }
        });
    }
}

#[cfg(test)]
mod tests {
    use crate::spawn::actor::fetch::FetchResponseConfig as RealFetchResponseConfig;
    use crate::spawn::meta_net::{MetaNetCon, MetaNetConTest};
    use kitsune_p2p_fetch::test_utils::test_space;
    use kitsune_p2p_fetch::{FetchResponseConfig, FetchResponseGuard};
    use kitsune_p2p_types::bin_types::KitsuneOpData;
    use kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams;
    use kitsune_p2p_types::dependencies::lair_keystore_api::dependencies::parking_lot::lock_api::RwLock;
    use std::sync::Arc;
    use std::time::Duration;

    #[tokio::test(flavor = "multi_thread")]
    async fn responds_by_doing_notify() {
        let config = RealFetchResponseConfig::new(Arc::new(KitsuneP2pTuningParams::default()));

        let (s, _r) = tokio::sync::oneshot::channel();

        let connection_state = Arc::new(RwLock::new(MetaNetConTest::default()));
        config.respond(
            test_space(1),
            (
                MetaNetCon::Test {
                    state: connection_state.clone(),
                },
                "".to_string(),
                None,
            ),
            FetchResponseGuard::new(s),
            Arc::new(KitsuneOpData(vec![1, 2, 3])),
        );

        tokio::time::timeout(Duration::from_millis(100), {
            let connection_state = connection_state.clone();
            async move {
                while connection_state.read().notify_call_count < 1 {
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
        })
        .await
        .unwrap();

        assert_eq!(1, connection_state.read().notify_call_count);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handles_error_during_notify() {
        let config = RealFetchResponseConfig::new(Arc::new(KitsuneP2pTuningParams::default()));

        let (s, _r) = tokio::sync::oneshot::channel();

        let connection_state = Arc::new(RwLock::new(MetaNetConTest::default()));
        connection_state.write().notify_succeed = false;

        config.respond(
            test_space(1),
            (
                MetaNetCon::Test {
                    state: connection_state.clone(),
                },
                "".to_string(),
                None,
            ),
            FetchResponseGuard::new(s),
            Arc::new(KitsuneOpData(vec![1, 2, 3])),
        );

        tokio::time::timeout(Duration::from_millis(100), {
            let connection_state = connection_state.clone();
            async move {
                while connection_state.read().notify_call_count < 1 {
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
        })
        .await
        .unwrap();

        assert_eq!(1, connection_state.read().notify_call_count);
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/meta_net/tests.rs
================================================
#![allow(clippy::field_reassign_with_default)] // just easier to read/wriet
use self::{
    spawn::actor::{InternalHandler, InternalHandlerResult},
    test_util::start_signal_srv,
};

use super::*;

use kitsune_p2p_fetch::OpHashSized;
use kitsune_p2p_timestamp::Timestamp;
use must_future::MustBoxFuture;
use std::sync::{atomic, Arc};

use kitsune_p2p_types::{
    bin_types::KitsuneSpace,
    dependencies::lair_keystore_api,
    dht::{
        arq::ArqSet,
        region::{Region, RegionCoords},
        region_set::RegionSetLtcs,
        spacetime::Topology,
    },
    dht_arc::DhtArcSet,
    metrics::MetricRecord,
    KOpData, KOpHash,
};

use crate::spawn::actor::FetchContext;
use crate::spawn::actor::FetchKey;
use crate::spawn::actor::FetchSource;
use crate::spawn::actor::MaybeDelegate;
use crate::spawn::actor::MetricExchangeMsg;
use crate::spawn::actor::OpHashList;
use crate::spawn::BroadcastData;

use crate::event::GetAgentInfoSignedEvt;
use crate::event::*;

use crate::spawn::Internal;

macro_rules! write_test_struct {
    ($(
        $ity:ty {
            $(
                fn $fna:ident (
                    $fself:ty,
                    $(
                        $fpna:ident: $fpty:ty,
                    )*
                ) -> $fret1:ty, $fret2:ty $fdef:block
            )*
        }
    )*) => {
        pub struct Test {
            recv: Arc<dyn Fn(MetaNetEvt) + 'static + Send + Sync>,
            $($(
                $fna: Arc<dyn Fn(
                    $(
                        $fpty,
                    )*
                ) -> $fret2 + 'static + Send + Sync>,
            )*)*
        }

        impl std::fmt::Debug for Test {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                f.debug_struct("Test").finish()
            }
        }

        impl Default for Test {
            fn default() -> Self {
                Self {
                    recv: Arc::new(|_evt: MetaNetEvt| {}),
                    $($(
                        $fna: Arc::new(|$(
                            $fpna: $fpty,
                        )*| $fdef),
                    )*)*
                }
            }
        }

        $(
            impl $ity for RunningTest {
                $(
                    fn $fna(self: $fself, $(
                        $fpna: $fpty,
                    )*) -> $fret1 {
                        (self.0.$fna)($(
                            $fpna,
                        )*).into()
                    }
                )*
            }
        )*
    };
}

type HostRes<T> = Result<T, Box<dyn Send + Sync + std::error::Error>>;
type HostRet<T> = std::pin::Pin<Box<dyn std::future::Future<Output = HostRes<T>> + 'static + Send>>;

write_test_struct! {
    KitsuneHost {
        fn block(&Self, _input: kitsune_p2p_block::Block,) -> KitsuneHostResult<()>, HostRet<()> {
            Box::pin(async move {
                Ok(())
            })
        }
        fn unblock(&Self, _input: kitsune_p2p_block::Block,) -> KitsuneHostResult<()>, HostRet<()> {
            Box::pin(async move {
                Ok(())
            })
        }
        fn is_blocked(&Self, _input: kitsune_p2p_block::BlockTargetId, _timestamp: Timestamp,) -> KitsuneHostResult<bool>, HostRet<bool> {
            Box::pin(async move {
                Ok(false)
            })
        }
        fn get_agent_info_signed(
            &Self,
            _input: GetAgentInfoSignedEvt,
        ) -> KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>>, HostRet<Option<crate::types::agent_store::AgentInfoSigned>> {
            Box::pin(async move {
                Ok(None)
            })
        }
        fn remove_agent_info_signed(&Self, _input: GetAgentInfoSignedEvt,) -> KitsuneHostResult<bool>, HostRet<bool> {
            Box::pin(async move {
                Ok(false)
            })
        }
        fn peer_extrapolated_coverage(
            &Self,
            _space: Arc<KitsuneSpace>,
            _dht_arc_set: DhtArcSet,
        ) -> KitsuneHostResult<Vec<f64>>, HostRet<Vec<f64>> {
            Box::pin(async move {
                Ok(vec![])
            })
        }
        fn query_region_set(
            &Self,
            _space: Arc<KitsuneSpace>,
            _arq_set: ArqSet,
        ) -> KitsuneHostResult<RegionSetLtcs>, HostRet<RegionSetLtcs> {
            Box::pin(async move {
                Ok(RegionSetLtcs::empty())
            })
        }
        fn query_size_limited_regions(
            &Self,
            _space: Arc<KitsuneSpace>,
            _size_limit: u32,
            _regions: Vec<Region>,
        ) -> KitsuneHostResult<Vec<Region>>, HostRet<Vec<Region>> {
            Box::pin(async move {
                Ok(vec![])
            })
        }
        fn query_op_hashes_by_region(
            &Self,
            _space: Arc<KitsuneSpace>,
            _region: RegionCoords,
        ) -> KitsuneHostResult<Vec<OpHashSized>>, HostRet<Vec<OpHashSized>> {
            Box::pin(async move {
                Ok(vec![])
            })
        }
        fn record_metrics(
            &Self,
            _space: Arc<KitsuneSpace>,
            _records: Vec<MetricRecord>,
        ) -> KitsuneHostResult<()>, HostRet<()> {
            Box::pin(async move {
                Ok(())
            })
        }
        fn get_topology(&Self, _space: Arc<KitsuneSpace>,) -> KitsuneHostResult<Topology>, HostRet<Topology> {
            Box::pin(async move {
                Ok(Topology::unit_zero())
            })
        }
        fn op_hash(&Self, _op_data: KOpData,) -> KitsuneHostResult<KOpHash>, HostRet<KOpHash> {
            Box::pin(async move {
                Ok(Arc::new(KitsuneOpHash::new(vec![0; 36])))
            })
        }
        fn check_op_data(
            &Self,
            space: Arc<KitsuneSpace>,
            op_hash_list: Vec<KOpHash>,
            _context: Option<kitsune_p2p_fetch::FetchContext>,
        ) -> KitsuneHostResult<Vec<bool>>, HostRet<Vec<bool>> {
            let _space = space;
            Box::pin(async move {
                Ok(op_hash_list.into_iter().map(|_| false).collect())
            })
        }
        fn lair_tag(&Self,) -> Option<Arc<str>>, Option<Arc<str>> {
            None
        }
        fn lair_client(&Self,) -> Option<lair_keystore_api::LairClient>, Option<lair_keystore_api::LairClient> {
            None
        }
    }
    InternalHandler {
        fn handle_new_address(
            &mut Self,
            _local_url: String,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_register_space_event_handler(
            &mut Self,
            _recv: futures::channel::mpsc::Receiver<KitsuneP2pEvent>,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_incoming_delegate_broadcast(
            &mut Self,
            _space: Arc<KitsuneSpace>,
            _basis: Arc<KitsuneBasis>,
            _to_agent: Arc<KitsuneAgent>,
            _mod_idx: u32,
            _mod_cnt: u32,
            _data: BroadcastData,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_incoming_publish(
            &mut Self,
            _space: KSpace,
            _to_agent: KAgent,
            _source: KAgent,
            _transfer_method: kitsune_p2p_fetch::TransferMethod,
            _op_hash_list: OpHashList,
            _context: kitsune_p2p_fetch::FetchContext,
            _maybe_delegate: MaybeDelegate,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_resolve_publish_pending_delegates(
            &mut Self,
            _space: KSpace,
            _op_hash: KOpHash,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_incoming_gossip(
            &mut Self,
            _space: Arc<KitsuneSpace>,
            _con: MetaNetCon,
            _remote_url: String,
            _data: Box<[u8]>,
            _module_type: GossipModuleType,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_incoming_metric_exchange(
            &mut Self,
            _space: Arc<KitsuneSpace>,
            _msgs: Vec<MetricExchangeMsg>,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_new_con(&mut Self, _url: String, _con: MetaNetCon,) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_del_con(&mut Self, _url: String,) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_fetch(
            &mut Self,
            _key: FetchKey,
            _space: KSpace,
            _source: FetchSource,
        ) -> InternalHandlerResult<()>, InternalHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_get_all_local_joined_agent_infos(
            &mut Self,
        ) -> InternalHandlerResult<Vec<AgentInfoSigned>>, InternalHandlerResult<Vec<AgentInfoSigned>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(vec![])
            }).into())
        }
    }
    KitsuneP2pEventHandler {
        fn handle_put_agent_info_signed(&mut Self, input: PutAgentInfoSignedEvt,) -> KitsuneP2pEventHandlerResult<Vec<kitsune_p2p_types::bootstrap::AgentInfoPut>>, KitsuneP2pEventHandlerResult<Vec<kitsune_p2p_types::bootstrap::AgentInfoPut>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(vec![])
            }).into())
        }
        fn handle_query_agents(&mut Self, input: QueryAgentsEvt,) -> KitsuneP2pEventHandlerResult<Vec<crate::types::agent_store::AgentInfoSigned>>, KitsuneP2pEventHandlerResult<Vec<crate::types::agent_store::AgentInfoSigned>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(vec![])
            }).into())
        }
        fn handle_query_peer_density(&mut Self, space: KSpace, dht_arc: kitsune_p2p_types::dht_arc::DhtArc,) -> KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView>, KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(kitsune_p2p_types::dht::PeerViewQ::new(
                    Topology::unit_zero(),
                    crate::dht::ArqStrat::default(),
                    vec![],
                ).into())
            }).into())
        }
        fn handle_call(&mut Self, space: KSpace, to_agent: KAgent, payload: Payload,) -> KitsuneP2pEventHandlerResult<Vec<u8>>, KitsuneP2pEventHandlerResult<Vec<u8>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(vec![])
            }).into())
        }
        fn handle_notify(&mut Self, space: KSpace, to_agent: KAgent, payload: Payload,) -> KitsuneP2pEventHandlerResult<()>, KitsuneP2pEventHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_receive_ops(
            &mut Self,
            space: KSpace,
            ops: Vec<KOp>,
            context: Option<FetchContext>,
        ) -> KitsuneP2pEventHandlerResult<()>, KitsuneP2pEventHandlerResult<()> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(())
            }).into())
        }
        fn handle_query_op_hashes(&mut Self, input: QueryOpHashesEvt,) -> KitsuneP2pEventHandlerResult<Option<(Vec<KOpHash>, TimeWindowInclusive)>>, KitsuneP2pEventHandlerResult<Option<(Vec<KOpHash>, TimeWindowInclusive)>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(None)
            }).into())
        }
        fn handle_fetch_op_data(&mut Self, input: FetchOpDataEvt,) -> KitsuneP2pEventHandlerResult<Vec<(KOpHash, KOp)>>, KitsuneP2pEventHandlerResult<Vec<(KOpHash, KOp)>> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(vec![])
            }).into())
        }
        fn handle_sign_network_data(&mut Self, input: SignNetworkDataEvt,) -> KitsuneP2pEventHandlerResult<super::KitsuneSignature>, KitsuneP2pEventHandlerResult<super::KitsuneSignature> {
            Ok(futures::future::FutureExt::boxed(async move {
                Ok(super::KitsuneSignature(vec![0; 64]))
            }).into())
        }
    }
}

#[derive(Debug, Clone)]
pub struct RunningTest(pub Arc<Test>);

impl ghost_actor::GhostControlHandler for RunningTest {}
impl ghost_actor::GhostHandler<Internal> for RunningTest {}
impl ghost_actor::GhostHandler<KitsuneP2pEvent> for RunningTest {}

impl RunningTest {
    fn spawn_receiver(&self, mut recv: MetaNetEvtRecv) {
        let inner = self.0.clone();
        tokio::task::spawn(async move {
            while let Some(evt) = recv.next().await {
                (inner.recv)(evt);
            }
        });
    }
}

impl Test {
    async fn spawn(
        self,
    ) -> (
        RunningTest,
        ghost_actor::GhostSender<Internal>,
        futures::channel::mpsc::Sender<KitsuneP2pEvent>,
    ) {
        let (send, recv) = futures::channel::mpsc::channel(10);

        let test = RunningTest(Arc::new(self));
        let builder = ghost_actor::actor_builder::GhostActorBuilder::new();
        let i_s = builder.channel_factory().create_channel().await.unwrap();
        builder
            .channel_factory()
            .attach_receiver(recv)
            .await
            .unwrap();
        tokio::task::spawn(builder.spawn(test.clone()));
        (test, i_s, send)
    }
}

struct Setup2Nodes {
    tuning_params: KitsuneP2pTuningParams,
    _sig_hnd: sbd_server::SbdServer,
    pub addr1: String,
    pub send1: MetaNet,
    pub addr2: String,
    pub send2: MetaNet,
}

impl Setup2Nodes {
    pub async fn new(test: Test) -> Self {
        Self::new_with_user_data(
            test,
            PreflightUserData::default(),
            PreflightUserData::default(),
        )
        .await
    }

    pub async fn new_with_user_data(
        test: Test,
        user_data_a: PreflightUserData,
        user_data_b: PreflightUserData,
    ) -> Self {
        let mut tuning_params = config::tuning_params_struct::KitsuneP2pTuningParams::default();
        tuning_params.tx5_timeout_s = 4;
        let tuning_params = Arc::new(tuning_params);

        let (sig_addr, _sig_hnd) = start_signal_srv().await;
        let (test, i_s, evt_sender) = test.spawn().await;

        tracing::warn!("-- test -- init node 1");

        let (send1, recv1, addr1) = MetaNet::new_tx5(
            tuning_params.clone(),
            HostApiLegacy {
                api: Arc::new(test.clone()),
                legacy: evt_sender.clone(),
            },
            i_s.clone(),
            format!("ws://{sig_addr}"),
            "{}".to_string(),
            user_data_a,
        )
        .await
        .unwrap();

        let addr1 = addr1.unwrap();

        test.spawn_receiver(recv1);

        tracing::warn!("-- test -- init node 2");

        let (send2, recv2, addr2) = MetaNet::new_tx5(
            tuning_params.clone(),
            HostApiLegacy {
                api: Arc::new(test.clone()),
                legacy: evt_sender.clone(),
            },
            i_s.clone(),
            format!("ws://{sig_addr}"),
            "{}".to_string(),
            user_data_b,
        )
        .await
        .unwrap();

        let addr2 = addr2.unwrap();

        test.spawn_receiver(recv2);

        Self {
            tuning_params,
            _sig_hnd,
            addr1,
            send1,
            addr2,
            send2,
        }
    }

    pub async fn shutdown(self) {
        let Self { send1, send2, .. } = self;
        send1.close(0, "").await;
        send2.close(0, "").await;
    }
}

/// notify helper
struct Notify(Arc<tokio::sync::Notify>);

impl Notify {
    pub fn notify(&self) {
        self.0.notify_waiters();
    }
}

/// notify helper
type NotifyWait = std::pin::Pin<Box<dyn std::future::Future<Output = ()> + 'static + Send>>;

/// notify helper
fn notify_pair() -> (Notify, NotifyWait) {
    let n = Arc::new(tokio::sync::Notify::new());
    let n2 = n.clone();
    let w = tokio::task::spawn(async move {
        n2.notified().await;
    });
    let w = Box::pin(async move {
        let _ = w.await;
    });
    (Notify(n), w)
}

#[tokio::test(flavor = "multi_thread")]
async fn basic_connected() {
    let (recv_not, recv_wait) = notify_pair();

    let mut test = Test::default();

    test.recv = Arc::new(move |evt| {
        if let MetaNetEvt::Connected { .. } = evt {
            recv_not.notify();
        }
    });

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    con.notify(
        &wire::Wire::failure("Hello World!".into()),
        nodes.tuning_params.implicit_timeout(),
    )
    .await
    .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(10), recv_wait)
        .await
        .unwrap();

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn close_and_disconnected() {
    let (recvc_not, recvc_wait) = notify_pair();
    let (recvd_not, recvd_wait) = notify_pair();

    let mut test = Test::default();

    test.recv = Arc::new(move |evt| {
        if matches!(evt, MetaNetEvt::Connected { .. }) {
            recvc_not.notify();
        } else if matches!(evt, MetaNetEvt::Disconnected { .. }) {
            recvd_not.notify();
        }
    });

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    con.notify(
        &wire::Wire::failure("Hello World!".into()),
        nodes.tuning_params.implicit_timeout(),
    )
    .await
    .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(10), async {
        recvc_wait.await;

        con.close(0, "").await;

        recvd_wait.await;
    })
    .await
    .unwrap();

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn basic_notify() {
    let (recv_not, recv_wait) = notify_pair();

    let mut test = Test::default();

    test.recv = Arc::new(move |evt| {
        if let MetaNetEvt::Notify { data, .. } = evt {
            assert!(matches!(
                data,
                wire::Wire::Failure(wire::Failure {
                    reason,
                }) if reason == "Hello World!",
            ));
            recv_not.notify();
        }
    });

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    con.notify(
        &wire::Wire::failure("Hello World!".into()),
        nodes.tuning_params.implicit_timeout(),
    )
    .await
    .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(10), recv_wait)
        .await
        .unwrap();

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn basic_broadcast() {
    holochain_trace::test_run();

    let recv_done = Arc::new(atomic::AtomicUsize::new(0));

    let mut test = Test::default();

    {
        let recv_done = recv_done.clone();
        test.recv = Arc::new(move |evt| {
            if let MetaNetEvt::Notify { data, .. } = evt {
                assert!(matches!(
                    data,
                    wire::Wire::Failure(wire::Failure {
                        reason,
                    }) if reason == "Hello World!",
                ));
                recv_done.fetch_add(1, atomic::Ordering::SeqCst);
            }
        });
    }

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    con.notify(
        &wire::Wire::failure("Hello World!".into()),
        nodes.tuning_params.implicit_timeout(),
    )
    .await
    .unwrap();

    nodes
        .send1
        .broadcast(
            &wire::Wire::failure("Hello World!".into()),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(10), async {
        // broadcast requires an open connection, so wait for two
        // notifies... the one from the initial notify to open the con
        // and the second from the actual broadcast
        loop {
            tokio::time::sleep(std::time::Duration::from_millis(10)).await;

            if recv_done.load(atomic::Ordering::SeqCst) == 2 {
                break;
            }
        }
    })
    .await
    .unwrap();

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn basic_request() {
    let mut test = Test::default();

    test.recv = Arc::new(move |evt| {
        if let MetaNetEvt::Request { data, respond, .. } = evt {
            assert!(matches!(
                data,
                wire::Wire::Failure(wire::Failure {
                    reason,
                }) if reason == "hello",
            ));
            tokio::task::spawn(respond(wire::Wire::failure("world".into())));
        }
    });

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    let resp = con
        .request(
            &wire::Wire::failure("hello".into()),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .unwrap();

    assert!(matches!(
        resp,
        wire::Wire::Failure(wire::Failure {
            reason,
        }) if reason == "world",
    ));

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn preflight() {
    let (recv_not, recv_wait) = notify_pair();

    let mut test = Test::default();

    {
        let agent_info = crate::test_util::data::mk_agent_info(1).await;
        test.handle_get_all_local_joined_agent_infos = Arc::new(move || {
            let agent_info = agent_info.clone();
            Ok(futures::future::FutureExt::boxed(async move { Ok(vec![agent_info]) }).into())
        });
        test.handle_put_agent_info_signed = Arc::new(move |_| {
            recv_not.notify();
            Ok(futures::future::FutureExt::boxed(async move { Ok(vec![]) }).into())
        });
    }

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    con.notify(
        &wire::Wire::failure("Hello World!".into()),
        nodes.tuning_params.implicit_timeout(),
    )
    .await
    .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(10), recv_wait)
        .await
        .unwrap();

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn preflight_user_data_mismatch() {
    let (recv_not, recv_wait) = notify_pair();

    let mut test = Test::default();

    {
        let agent_info = crate::test_util::data::mk_agent_info(1).await;
        test.handle_get_all_local_joined_agent_infos = Arc::new(move || {
            let agent_info = agent_info.clone();
            Ok(futures::future::FutureExt::boxed(async move { Ok(vec![agent_info]) }).into())
        });
        test.handle_put_agent_info_signed = Arc::new(move |_| {
            recv_not.notify();
            Ok(futures::future::FutureExt::boxed(async move { Ok(vec![]) }).into())
        });
    }

    let ud1 = PreflightUserData {
        bytes: vec![1, 2, 3],
        comparator: Box::new(|_, r| {
            println!("want 1, 2, 3, got {r:?}");
            (r == [1, 2, 3])
                .then_some(())
                .ok_or("preflight mismatch".into())
        }),
    };
    let ud2 = PreflightUserData {
        bytes: vec![9, 8, 7, 6, 5],
        comparator: Box::new(|_, r| {
            println!("want 9, 8, 7, 6, 5, got {r:?}");
            (r == [9, 8, 7, 6, 5])
                .then_some(())
                .ok_or("preflight mismatch".into())
        }),
    };

    let nodes = Setup2Nodes::new_with_user_data(test, ud1, ud2).await;

    println!("get con");
    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    println!("notify");
    // This should error out because preflight failed due to user data mismatch
    if con
        .notify(
            &wire::Wire::failure("Hello World!".into()),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .is_ok()
    {
        println!("WARN! notify was OKAY");
        // ...but if it *doesn't* error, the request should at least timeout because
        // preflight user data doesn't match
        //
        // FIXME: this may indicate a bug in tx5. We expect that the notify should
        //        always fail, but it doesn't.
        tokio::time::timeout(std::time::Duration::from_millis(500), recv_wait)
            .await
            .unwrap_err();
    }

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn notify_unauthorized() {
    let mut test = Test::default();
    test.is_blocked = Arc::new(|_, _| Box::pin(async move { Ok(true) }));

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    // note - notifies are fire-and-forget... so we get an okay here...
    assert!(con
        .notify(
            &wire::Wire::call(
                Arc::new(KitsuneSpace::new(vec![1; 36])),
                Arc::new(KitsuneAgent::new(vec![2; 36])),
                WireData(vec![]),
            ),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .is_ok());

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn notify_unauthorized_err() {
    let mut test = Test::default();
    test.is_blocked = Arc::new(|_, _| Box::pin(async move { Err("test".into()) }));

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    // note - notifies are fire-and-forget... so we get an okay here...
    assert!(con
        .notify(
            &wire::Wire::call(
                Arc::new(KitsuneSpace::new(vec![1; 36])),
                Arc::new(KitsuneAgent::new(vec![2; 36])),
                WireData(vec![]),
            ),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .is_ok());

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn request_unauthorized() {
    let mut test = Test::default();
    test.is_blocked = Arc::new(|_, _| Box::pin(async move { Ok(true) }));

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    assert!(con
        .request(
            &wire::Wire::call(
                Arc::new(KitsuneSpace::new(vec![1; 36])),
                Arc::new(KitsuneAgent::new(vec![2; 36])),
                WireData(vec![]),
            ),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .is_err());

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn request_unauthorized_err() {
    let mut test = Test::default();
    test.is_blocked = Arc::new(|_, _| Box::pin(async move { Err("test".into()) }));

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    assert!(con
        .request(
            &wire::Wire::call(
                Arc::new(KitsuneSpace::new(vec![1; 36])),
                Arc::new(KitsuneAgent::new(vec![2; 36])),
                WireData(vec![]),
            ),
            nodes.tuning_params.implicit_timeout(),
        )
        .await
        .is_err());

    nodes.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn request_timeout() {
    let mut test = Test::default();

    test.recv = Arc::new(move |evt| {
        if let MetaNetEvt::Request { data, respond, .. } = evt {
            assert!(matches!(
                data,
                wire::Wire::Failure(wire::Failure {
                    reason,
                }) if reason == "hello",
            ));
            // just never respond
            std::mem::forget(respond);
        }
    });

    let nodes = Setup2Nodes::new(test).await;

    let con = nodes
        .send1
        .get_connection(nodes.addr2.clone(), nodes.tuning_params.implicit_timeout())
        .await
        .unwrap();

    let resp = con
        .request(
            &wire::Wire::failure("hello".into()),
            KitsuneTimeout::from_millis(100),
        )
        .await;

    assert!(matches!(
        resp,
        Err(e) if format!("{e:?}").contains("timeout"),
    ));

    nodes.shutdown().await;
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/space/agent_info_update.rs
================================================
use crate::spawn::actor::space::{SpaceInternal, SpaceInternalSender};
use ghost_actor::{GhostControlSender, GhostSender};
use parking_lot::RwLock;
use std::sync::Arc;
use std::time::Duration;
use tracing::{error, info};

pub(super) struct AgentInfoUpdateTask {
    pub is_finished: bool,
}

impl AgentInfoUpdateTask {
    pub(super) fn spawn(
        internal_sender: GhostSender<SpaceInternal>,
        interval: Duration,
    ) -> Arc<RwLock<Self>> {
        let this = Arc::new(RwLock::new(AgentInfoUpdateTask { is_finished: false }));

        let task_this = this.clone();
        tokio::spawn(async move {
            loop {
                tokio::time::sleep(interval).await;
                if let Err(e) = internal_sender.update_agent_info().await {
                    if !internal_sender.ghost_actor_is_active() {
                        // Assume this task has been orphaned when the space was dropped and exit.
                        info!("AgentInfoUpdateTask will stop because the ghost actor it uses to communicate is closing");
                        break;
                    } else {
                        error!(failed_to_update_agent_info_for_space = ?e);
                    }
                }
            }

            info!("AgentInfoUpdateTask finished");
            task_this.write().is_finished = true;
        });

        this
    }
}

#[cfg(test)]
mod tests {
    use super::AgentInfoUpdateTask;
    use crate::spawn::actor::space::test_util::SpaceInternalStub;
    use crate::spawn::actor::space::SpaceInternal;
    use futures::FutureExt;
    use ghost_actor::actor_builder::GhostActorBuilder;
    use ghost_actor::{GhostControlSender, GhostError, GhostHandler, GhostSender};
    use parking_lot::RwLock;
    use std::sync::Arc;
    use std::time::{Duration, Instant};

    #[tokio::test(flavor = "multi_thread")]
    async fn update_agent_info() {
        let (test_sender, _) = setup(SpaceInternalStub::new()).await;

        tokio::time::timeout(Duration::from_millis(200), async {
            loop {
                tokio::time::sleep(Duration::from_millis(1)).await;
                if test_sender.get_called_count().await.unwrap() >= 3 {
                    break;
                }
            }
        })
        .await
        .expect("Timed out before seeing 3 task runs");

        let called_count = test_sender.get_called_count().await.unwrap();
        assert!(
            called_count >= 3,
            "Task should have run at least 3 times but was {}",
            called_count
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn task_shuts_down_cleanly() {
        let (test_sender, task) = setup(SpaceInternalStub::new()).await;
        test_sender.ghost_actor_shutdown().await.unwrap();

        let max_wait = Instant::now();
        while !task.read().is_finished && max_wait.elapsed() < Duration::from_millis(100) {
            tokio::time::sleep(Duration::from_millis(1)).await;
        }

        assert!(
            task.read().is_finished,
            "Task should have been marked finished after the ghost actor shut down"
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn task_stays_alive_when_update_call_errors() {
        let mut space_internal_impl = SpaceInternalStub::new();
        space_internal_impl.respond_with_error = true;
        let (test_sender, _) = setup(space_internal_impl).await;

        tokio::time::timeout(Duration::from_millis(300), async {
            loop {
                tokio::time::sleep(Duration::from_millis(1)).await;
                if test_sender.get_errored_count().await.unwrap() >= 3 {
                    break;
                }
            }
        })
        .await
        .expect("Timed out before seeing 3 errors");

        let errored_count = test_sender.get_errored_count().await.unwrap();
        assert!(
            errored_count >= 3,
            "Task should have run at least 3 times but was {}",
            errored_count
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    async fn setup(
        task: SpaceInternalStub,
    ) -> (GhostSender<TestChan>, Arc<RwLock<AgentInfoUpdateTask>>) {
        let builder = GhostActorBuilder::new();

        let internal_sender = builder
            .channel_factory()
            .create_channel::<SpaceInternal>()
            .await
            .unwrap();

        let test_sender = builder
            .channel_factory()
            .create_channel::<TestChan>()
            .await
            .unwrap();

        tokio::spawn(builder.spawn(task));

        let task = AgentInfoUpdateTask::spawn(internal_sender, Duration::from_millis(1));

        (test_sender, task)
    }

    ghost_actor::ghost_chan! {
        pub chan TestChan<GhostError> {
            fn get_called_count() -> usize;
            fn get_errored_count() -> usize;
        }
    }

    impl GhostHandler<TestChan> for SpaceInternalStub {}
    impl TestChanHandler for SpaceInternalStub {
        fn handle_get_called_count(&mut self) -> TestChanHandlerResult<usize> {
            let called_count = self.called_count;
            Ok(async move { Ok(called_count) }.boxed().into())
        }

        fn handle_get_errored_count(&mut self) -> TestChanHandlerResult<usize> {
            let errored_count = self.errored_count;
            Ok(async move { Ok(errored_count) }.boxed().into())
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/space/bootstrap_task.rs
================================================
use crate::event::{KitsuneP2pEvent, KitsuneP2pEventSender, PutAgentInfoSignedEvt};
use crate::spawn::actor::space::{SpaceInternal, SpaceInternalSender};
use crate::{KitsuneP2pError, KitsuneP2pResult, KitsuneSpace};
use futures::channel::mpsc::Sender;
use futures::future::BoxFuture;
use futures::FutureExt;
use ghost_actor::{GhostControlSender, GhostError, GhostSender};
use kitsune_p2p_bootstrap_client::BootstrapNet;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::bootstrap::RandomQuery;
use parking_lot::RwLock;
use std::sync::Arc;
use std::time::Duration;
use url2::Url2;

const MAX_AGENTS_PER_QUERY: u32 = 8;

pub(super) struct BootstrapTask {
    is_finished: bool,
    current_delay: Duration,
    max_delay: Duration,
}

// Trait for the bootstrap query to allow mocking in tests
trait BootstrapService: Send {
    fn random(&self, query: RandomQuery) -> BoxFuture<KitsuneP2pResult<Vec<AgentInfoSigned>>>;
}

struct DefaultBootstrapService {
    url: Option<Url2>,
    net: BootstrapNet,
}

impl BootstrapService for DefaultBootstrapService {
    fn random(&self, query: RandomQuery) -> BoxFuture<KitsuneP2pResult<Vec<AgentInfoSigned>>> {
        async move {
            Ok(kitsune_p2p_bootstrap_client::random(self.url.clone(), query, self.net).await?)
        }.boxed()
    }
}

impl BootstrapTask {
    pub(super) fn spawn(
        internal_sender: GhostSender<SpaceInternal>,
        host_sender: Sender<KitsuneP2pEvent>,
        space: Arc<KitsuneSpace>,
        bootstrap_service: Option<Url2>,
        bootstrap_net: BootstrapNet,
        bootstrap_check_delay_backoff_multiplier: u32,
        mut bootstrap_max_delay_s: u32,
    ) -> Arc<RwLock<Self>> {
        if bootstrap_max_delay_s < 60 {
            bootstrap_max_delay_s = 60;
        }

        let this = Arc::new(RwLock::new(BootstrapTask {
            is_finished: false,
            current_delay: Duration::from_secs(1),
            max_delay: Duration::from_secs(bootstrap_max_delay_s as u64),
        }));

        let bootstrap_query = DefaultBootstrapService {
            url: bootstrap_service,
            net: bootstrap_net,
        };

        BootstrapTask::spawn_inner(
            this,
            internal_sender,
            host_sender,
            space,
            Box::new(bootstrap_query),
            bootstrap_check_delay_backoff_multiplier,
        )
    }

    fn spawn_inner(
        this: Arc<RwLock<Self>>,
        internal_sender: GhostSender<SpaceInternal>,
        host_sender: Sender<KitsuneP2pEvent>,
        space: Arc<KitsuneSpace>,
        bootstrap_query: Box<impl BootstrapService + Sync + 'static>,
        bootstrap_check_delay_backoff_multiplier: u32,
    ) -> Arc<RwLock<Self>> {
        let task_this = this.clone();
        tokio::spawn(async move {
            let backoff_multiplier = if bootstrap_check_delay_backoff_multiplier < 2 {
                tracing::warn!(
                    "Using default bootstrap backoff multiplier 2 because configured value is too low - {}",
                    bootstrap_check_delay_backoff_multiplier
                );
                2
            } else {
                bootstrap_check_delay_backoff_multiplier
            };

            let max_delay = task_this.read().max_delay;

            loop {
                if !internal_sender.ghost_actor_is_active() {
                    break;
                }

                let current_delay = task_this.read().current_delay;
                tokio::time::sleep(current_delay).await;
                if current_delay <= max_delay {
                    // Backoff but don't exceed the configured max delay
                    task_this.write().current_delay =
                        std::cmp::min(current_delay * backoff_multiplier, max_delay);
                }

                match bootstrap_query
                    .random(RandomQuery {
                        space: space.clone(),
                        limit: MAX_AGENTS_PER_QUERY.into(),
                    })
                    .await
                {
                    Err(e) => {
                        tracing::error!(msg = "Failed to get peers from bootstrap", ?e);
                    }
                    Ok(list) => {
                        if list.len() > MAX_AGENTS_PER_QUERY as usize {
                            tracing::warn!("Expected no more than {} agents from the bootstrap server but got {}", MAX_AGENTS_PER_QUERY, list.len());
                            continue;
                        }

                        if !internal_sender.ghost_actor_is_active() {
                            break;
                        }
                        let mut peer_data = Vec::with_capacity(list.len());
                        for item in list {
                            match internal_sender.is_agent_local(item.agent.clone()).await {
                                Err(err) => tracing::error!(?err),
                                Ok(is_local) => {
                                    if !is_local {
                                        // we got a result - let's add it to our store for the future
                                        peer_data.push(item);
                                    }
                                }
                            }
                        }

                        if let Err(err) = host_sender
                            .put_agent_info_signed(PutAgentInfoSignedEvt { peer_data })
                            .await
                        {
                            match err {
                                KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                                    tracing::error!(?err, "Bootstrap task cannot communicate with the host, shutting down");
                                    break;
                                }
                                _ => {
                                    tracing::error!(?err, "error storing bootstrap agent_info");
                                }
                            }
                        }
                    }
                }
            }

            tracing::warn!(?space, "bootstrap fetch loop ending for space");
            task_this.write().is_finished = true;
        });

        this
    }
}

#[cfg(test)]
mod tests {
    use crate::event::PutAgentInfoSignedEvt;
    use crate::spawn::actor::space::bootstrap_task::{BootstrapService, BootstrapTask};
    use crate::spawn::actor::space::{
        KAgent, KBasis, KSpace, MaybeDelegate, OpHashList, Payload, SpaceInternal,
        SpaceInternalHandler, SpaceInternalHandlerResult, VecMXM, WireConHnd,
    };
    use crate::spawn::actor::test_util::LegacyHostStub;
    use crate::spawn::actor::MetaNetCon;
    use crate::types::actor::BroadcastData;
    use crate::wire::Wire;
    use crate::KitsuneP2pResult;
    use crate::{GossipModuleType, KitsuneP2pError};
    use ::fixt::prelude::*;
    use futures::channel::mpsc::channel;
    use futures::future::BoxFuture;
    use futures::{FutureExt, SinkExt, StreamExt};
    use ghost_actor::actor_builder::GhostActorBuilder;
    use ghost_actor::{GhostControlHandler, GhostControlSender, GhostHandler, GhostSender};
    use kitsune_p2p_bin_data::fixt::*;
    use kitsune_p2p_bootstrap_client::prelude::BootstrapClientError;
    use kitsune_p2p_fetch::FetchContext;
    use kitsune_p2p_types::agent_info::AgentInfoSigned;
    use kitsune_p2p_types::bootstrap::RandomQuery;
    use kitsune_p2p_types::dht::Arq;
    use kitsune_p2p_types::fixt::AgentInfoSignedFixturator;
    use kitsune_p2p_types::KOpHash;
    use parking_lot::RwLock;
    use std::collections::HashSet;
    use std::sync::atomic::{AtomicU32, AtomicUsize, Ordering};
    use std::sync::Arc;
    use std::time::{Duration, Instant};

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_relays_agent_info_from_boostrap_server_to_host() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, mut host_stub, _) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            2,
            false,
        )
        .await;

        let evt = host_stub.next_event(Duration::from_secs(5)).await;

        assert_eq!(1, evt.peer_data.len());

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_shuts_down_cleanly() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, _, task) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            2,
            false,
        )
        .await;

        test_sender.ghost_actor_shutdown().await.unwrap();

        tokio::time::timeout(Duration::from_secs(5), {
            let shutdown_wait_task = task.clone();
            async move {
                while !shutdown_wait_task.read().is_finished {
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
        })
        .await
        .ok();

        assert!(
            task.read().is_finished,
            "Task should have been marked finished after the ghost actor shut down"
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_handles_bootstrap_query_errors() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, mut host_stub, _) =
            setup(DummySpaceInternalImpl::new(HashSet::new()), agents, 2, true).await;

        let receives = Arc::new(AtomicUsize::new(0));
        tokio::time::timeout(Duration::from_secs(30), {
            let task_receives = receives.clone();
            async move {
                for _ in 0..3 {
                    host_stub.next_event(Duration::from_secs(5)).await;
                    task_receives.fetch_add(1, Ordering::SeqCst);
                }
            }
        })
        .await
        .unwrap();

        assert_eq!(
            3,
            receives.load(Ordering::SeqCst),
            "Expected 3 calls to have succeeded"
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_query_delay_increases_exponentially() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, mut host_stub, task) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            3,
            false,
        )
        .await;

        let start_time = Instant::now();
        let (mut sender, receiver) = channel(3);
        tokio::time::timeout(Duration::from_secs(30), {
            async move {
                for _ in 0..3 {
                    host_stub.next_event(Duration::from_secs(5)).await;
                    let send = task.read().current_delay;
                    sender.send(send).await.unwrap();
                }
            }
        })
        .await
        .unwrap();

        let durations = receiver.map(|d| d.as_millis()).collect::<Vec<u128>>().await;

        assert_eq!(
            vec![3, 9, 10],
            durations,
            "Expected durations to increase exponentially"
        );
        assert!(
            // It's 15 not 22 because the task actually slept for 1 + 3 + 9 milliseconds and we are reading the delay
            // after it has been updated.
            start_time.elapsed() >= Duration::from_millis(15),
            "Bootstrap task should have slept for at least as long as the delay values we saw"
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_prevents_sleep_disable_via_multiplier() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, mut host_stub, task) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            // Set to 0 to try and force skipping the sleep
            0,
            false,
        )
        .await;

        let (mut sender, receiver) = channel(3);
        tokio::time::timeout(Duration::from_secs(30), {
            async move {
                for _ in 0..3 {
                    host_stub.next_event(Duration::from_secs(5)).await;
                    let send = task.read().current_delay;
                    sender.send(send).await.unwrap();
                }
            }
        })
        .await
        .unwrap();

        let durations = receiver.map(|d| d.as_millis()).collect::<Vec<u128>>().await;

        // Reading the current delay above can be subject to timing. We might read the value after more than one increase
        // This test just cares that the increase happened at some point and wasn't disabled by setting the multiplier to 0.
        assert!(
            durations[durations.len() - 1] > durations[0],
            "Expected delay to increase"
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_query_delay_increase_respects_max() {
        let agents = vec![fixt!(AgentInfoSigned)];
        // Set a high delay multiplier so that a multiplication with no max check would move the delay to 1s from 1ms,
        // instead of the actual max at 10ms.
        let (test_sender, mut host_stub, task) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            1000,
            false,
        )
        .await;

        let (mut sender, receiver) = channel(3);
        tokio::time::timeout(Duration::from_secs(30), {
            async move {
                for _ in 0..3 {
                    host_stub.next_event(Duration::from_secs(5)).await;
                    let send = task.read().current_delay;
                    sender.send(send).await.unwrap();
                }
            }
        })
        .await
        .unwrap();

        let durations = receiver.map(|d| d.as_millis()).collect::<Vec<u128>>().await;

        assert_eq!(
            vec![10, 10, 10],
            durations,
            "Expected durations to increase exponentially"
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_query_agent_limit_is_checked() {
        // The code expects a max of 8 agents, send more
        let agents = std::iter::repeat_with(|| fixt!(AgentInfoSigned))
            .take(30)
            .collect::<Vec<_>>();
        let (test_sender, mut host_stub, _) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            2,
            false,
        )
        .await;

        let r = host_stub.try_next_event(Duration::from_secs(1)).await;

        // The error has to be an 'elapsed' error so that means nothing was sent to the host.
        assert!(r.is_err());

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_local_agents_in_response_are_filtered() {
        // The code expects a max of 8 agents, send more
        let agents = std::iter::repeat_with(|| fixt!(AgentInfoSigned))
            .take(8)
            .collect::<Vec<_>>();

        let local_agents = agents.iter().take(3).cloned().collect::<HashSet<_>>();

        let (test_sender, mut host_stub, _) =
            setup(DummySpaceInternalImpl::new(local_agents), agents, 2, false).await;

        let evt = host_stub.next_event(Duration::from_secs(5)).await;

        assert_eq!(5, evt.peer_data.len());

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_handles_errors_sending_to_host() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (test_sender, mut host_stub, _) =
            setup(DummySpaceInternalImpl::new(HashSet::new()), agents, 2, true).await;

        // Ask the host to respond with an error on each call, then wait a while and clear the flag
        host_stub.respond_with_error.store(true, Ordering::SeqCst);
        tokio::time::sleep(Duration::from_millis(10)).await;
        host_stub.respond_with_error.store(false, Ordering::SeqCst);

        // Now expect to receive from the task as usual
        let receives = Arc::new(AtomicUsize::new(0));
        tokio::time::timeout(Duration::from_secs(30), {
            let task_receives = receives.clone();
            async move {
                for _ in 0..3 {
                    host_stub.next_event(Duration::from_secs(5)).await;
                    task_receives.fetch_add(1, Ordering::SeqCst);
                }
            }
        })
        .await
        .unwrap();

        assert_eq!(
            3,
            receives.load(Ordering::SeqCst),
            "Expected 3 calls to have succeeded"
        );

        test_sender.ghost_actor_shutdown_immediate().await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn bootstrap_task_shuts_down_if_host_closes() {
        let agents = vec![fixt!(AgentInfoSigned)];
        let (_, host_stub, task) = setup(
            DummySpaceInternalImpl::new(HashSet::new()),
            agents,
            2,
            false,
        )
        .await;

        // Shuts down the stub task which will cause the host receiver to drop
        host_stub.abort();

        tokio::time::timeout(Duration::from_secs(5), {
            let shutdown_wait_task = task.clone();
            async move {
                while !shutdown_wait_task.read().is_finished {
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
        })
        .await
        .ok();

        assert!(
            task.read().is_finished,
            "Task should have been marked finished after the host closed"
        );
    }

    async fn setup(
        task: DummySpaceInternalImpl,
        agents: Vec<AgentInfoSigned>,
        delay_multiplier: u32,
        bootstrap_every_other_call_fails: bool,
    ) -> (
        GhostSender<SpaceInternal>,
        LegacyHostStub,
        Arc<RwLock<BootstrapTask>>,
    ) {
        let builder = GhostActorBuilder::new();

        let internal_sender = builder
            .channel_factory()
            .create_channel::<SpaceInternal>()
            .await
            .unwrap();

        let (host_sender, host_receiver) = channel(10);

        tokio::spawn(builder.spawn(task));

        let task_config = BootstrapTask {
            is_finished: false,
            current_delay: Duration::from_millis(1),
            max_delay: Duration::from_millis(10),
        };

        let space = fixt!(KitsuneSpace);
        let task = BootstrapTask::spawn_inner(
            Arc::new(RwLock::new(task_config)),
            internal_sender.clone(),
            host_sender,
            Arc::new(space),
            Box::new(TestBootstrapService::new(
                agents,
                bootstrap_every_other_call_fails,
            )),
            delay_multiplier,
        );

        let host_stub = LegacyHostStub::start(host_receiver);

        (internal_sender, host_stub, task)
    }

    struct DummySpaceInternalImpl {
        local_agents: HashSet<KAgent>,
    }

    impl DummySpaceInternalImpl {
        fn new(local_agents: HashSet<AgentInfoSigned>) -> Self {
            DummySpaceInternalImpl {
                local_agents: local_agents.into_iter().map(|a| a.agent.clone()).collect(),
            }
        }
    }

    impl GhostControlHandler for DummySpaceInternalImpl {}
    impl GhostHandler<SpaceInternal> for DummySpaceInternalImpl {}
    impl SpaceInternalHandler for DummySpaceInternalImpl {
        fn handle_new_address(&mut self, _: String) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_list_online_agents_for_basis_hash(
            &mut self,
            _space: KSpace,
            _from_agent: KAgent,
            _basis: KBasis,
        ) -> SpaceInternalHandlerResult<HashSet<KAgent>> {
            unreachable!()
        }

        fn handle_update_agent_info(&mut self) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_update_single_agent_info(
            &mut self,
            _agent: KAgent,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_publish_agent_info_signed(
            &mut self,
            _input: PutAgentInfoSignedEvt,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_get_all_local_joined_agent_infos(
            &mut self,
        ) -> SpaceInternalHandlerResult<Vec<AgentInfoSigned>> {
            unreachable!()
        }

        fn handle_is_agent_local(&mut self, agent: KAgent) -> SpaceInternalHandlerResult<bool> {
            let is_local = self.local_agents.contains(&agent);

            Ok(async move { Ok(is_local) }.boxed().into())
        }

        fn handle_update_agent_arc(
            &mut self,
            _agent: KAgent,
            _arq: Arq,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_incoming_delegate_broadcast(
            &mut self,
            _space: KSpace,
            _basis: KBasis,
            _to_agent: KAgent,
            _mod_idx: u32,
            _mod_cnt: u32,
            _data: BroadcastData,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_incoming_publish(
            &mut self,
            _space: KSpace,
            _to_agent: KAgent,
            _source: KAgent,
            _transfer_method: kitsune_p2p_fetch::TransferMethod,
            _op_hash_list: OpHashList,
            _context: FetchContext,
            _maybe_delegate: MaybeDelegate,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_notify(
            &mut self,
            _to_agent: KAgent,
            _data: Wire,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_resolve_publish_pending_delegates(
            &mut self,
            _space: KSpace,
            _op_hash: KOpHash,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_incoming_gossip(
            &mut self,
            _space: KSpace,
            _con: MetaNetCon,
            _remote_url: String,
            _data: Payload,
            _module_type: GossipModuleType,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_incoming_metric_exchange(
            &mut self,
            _space: KSpace,
            _msgs: VecMXM,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_new_con(
            &mut self,
            _url: String,
            _con: WireConHnd,
        ) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }

        fn handle_del_con(&mut self, _url: String) -> SpaceInternalHandlerResult<()> {
            unreachable!()
        }
    }

    struct TestBootstrapService {
        every_other_call_fails: bool,
        call_count: AtomicU32,
        agents: Vec<AgentInfoSigned>,
    }

    impl TestBootstrapService {
        fn new(agents: Vec<AgentInfoSigned>, every_other_call_fails: bool) -> Self {
            TestBootstrapService {
                agents,
                call_count: AtomicU32::new(0),
                every_other_call_fails,
            }
        }
    }

    impl BootstrapService for TestBootstrapService {
        fn random(&self, _query: RandomQuery) -> BoxFuture<KitsuneP2pResult<Vec<AgentInfoSigned>>> {
            let calls = self.call_count.fetch_add(1, Ordering::SeqCst);

            if self.every_other_call_fails && calls % 2 == 1 {
                return async move {
                    Err(KitsuneP2pError::Bootstrap(BootstrapClientError::Bootstrap(
                        "TestBootstrapService error".to_string().into_boxed_str(),
                    )))
                }
                .boxed();
            }

            async move { Ok(self.agents.clone()) }.boxed()
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/space/metric_exchange.rs
================================================
use super::*;
use crate::wire::MetricExchangeMsg;
use kitsune_p2p_types::config::KitsuneP2pTuningParams;
use kitsune_p2p_types::dht_arc::DhtArcSet;
use tokio::time::{Duration, Instant};

struct ShouldTrigger {
    last_sync: Option<Instant>,
    freq: Duration,
}

impl ShouldTrigger {
    pub fn new(freq: Duration) -> Self {
        Self {
            last_sync: None,
            freq,
        }
    }

    pub fn should_trigger(&mut self) -> bool {
        let now = Instant::now();
        if self
            .last_sync
            .map(|s| now.saturating_duration_since(s) > self.freq)
            .unwrap_or(true)
        {
            self.last_sync = Some(now);
            true
        } else {
            false
        }
    }
}

// update once per minute
const EXTRAP_COV_CHECK_FREQ: Duration = Duration::from_millis(1000 * 60);

// exchange once per minute
const METRIC_EXCHANGE_FREQ: Duration = Duration::from_millis(1000 * 60);

struct RemoteRef {
    con: MetaNetCon,
    last_sync: ShouldTrigger,
}

pub(crate) struct MetricExchange {
    space: Arc<KitsuneSpace>,
    tuning_params: KitsuneP2pTuningParams,
    shutdown: bool,
    extrap_cov: f32,
    #[allow(dead_code)]
    metrics: MetricsSync,
    remote_refs: HashMap<String, RemoteRef>,
    arc_set: DhtArcSet,
}

impl MetricExchange {
    pub fn spawn(
        space: Arc<KitsuneSpace>,
        tuning_params: KitsuneP2pTuningParams,
        metrics: MetricsSync,
    ) -> Self {
        Self {
            space,
            tuning_params,
            shutdown: false,
            extrap_cov: 0.0,
            metrics,
            remote_refs: HashMap::new(),
            arc_set: DhtArcSet::new_full(),
        }
    }

    pub fn shutdown(&mut self) {
        self.shutdown = true;
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub fn tick(&mut self) {
        for (_, r) in self.remote_refs.iter_mut() {
            if r.last_sync.should_trigger() {
                let space = self.space.clone();
                let timeout = self.tuning_params.implicit_timeout();
                let con = r.con.clone();
                let extrap_cov = self.extrap_cov;
                tokio::task::spawn(async move {
                    let payload = wire::Wire::metric_exchange(
                        space,
                        vec![MetricExchangeMsg::V1UniBlast {
                            extrap_cov_f32_le: extrap_cov.to_le_bytes().to_vec().into(),
                        }],
                    );
                    let _ = con.notify(&payload, timeout).await;
                });
            }
        }
    }

    pub fn update_arcset(&mut self, arc_set: DhtArcSet) {
        self.arc_set = arc_set;
    }

    pub fn new_con(&mut self, url: String, con: MetaNetCon) {
        use std::collections::hash_map::Entry::*;

        match self.remote_refs.entry(url) {
            Vacant(e) => {
                e.insert(RemoteRef {
                    con,
                    last_sync: ShouldTrigger::new(METRIC_EXCHANGE_FREQ),
                });
            }
            Occupied(mut e) => {
                if e.get().con != con {
                    let e = e.get_mut();
                    e.con = con;
                }
            }
        }
    }

    pub fn del_con(&mut self, url: String) {
        self.remote_refs.remove(&url);
    }

    pub fn ingest_msgs(&mut self, msgs: Vec<MetricExchangeMsg>) {
        for msg in msgs {
            match msg {
                MetricExchangeMsg::V1UniBlast { extrap_cov_f32_le } => {
                    if extrap_cov_f32_le.len() != 4 {
                        continue;
                    }
                    let mut tmp = [0; 4];
                    tmp.copy_from_slice(&extrap_cov_f32_le[0..4]);
                    let extrap_cov = f32::from_le_bytes(tmp);
                    self.metrics.write().record_extrap_cov_event(extrap_cov);
                }
                MetricExchangeMsg::UnknownMessage => (),
            }
        }
    }
}

#[derive(Clone)]
pub(crate) struct MetricExchangeSync(Arc<parking_lot::RwLock<MetricExchange>>);

impl std::ops::Deref for MetricExchangeSync {
    type Target = parking_lot::RwLock<MetricExchange>;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl MetricExchangeSync {
    pub fn spawn(
        space: Arc<KitsuneSpace>,
        tuning_params: KitsuneP2pTuningParams,
        host: HostApiLegacy,
        metrics: MetricsSync,
    ) -> Self {
        let out = Self(Arc::new(parking_lot::RwLock::new(MetricExchange::spawn(
            space.clone(),
            tuning_params,
            metrics.clone(),
        ))));

        {
            let mx = out.clone();
            tokio::task::spawn(async move {
                let mut last_extrap_cov = ShouldTrigger::new(EXTRAP_COV_CHECK_FREQ);

                loop {
                    tokio::time::sleep(std::time::Duration::from_millis(100)).await;

                    if last_extrap_cov.should_trigger() {
                        let arc_set = mx.read().arc_set.clone();
                        if let Ok(res) = host
                            .api
                            .peer_extrapolated_coverage(space.clone(), arc_set)
                            .await
                        {
                            // MAYBE: ignore outliers?
                            let count = res.len() as f64;
                            let res = res.into_iter().fold(0.0, |a, x| a + x) / count;
                            mx.write().extrap_cov = res as f32;
                            metrics.write().record_extrap_cov_event(res as f32);
                        }
                    }

                    let mut lock = mx.write();
                    if lock.shutdown {
                        return;
                    }
                    lock.tick();
                }
            });
        }

        out
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/space/rpc_multi_logic.rs
================================================
use super::*;
use futures::future::BoxFuture;

pub(crate) async fn handle_rpc_multi(
    input: actor::RpcMulti,
    ro_inner: Arc<SpaceReadOnlyInner>,
    local_joined_agents: HashSet<Arc<KitsuneAgent>>,
) -> KitsuneP2pResult<Vec<actor::RpcMultiResponse>> {
    handle_rpc_multi_as_single(input, ro_inner, local_joined_agents).await
}

pub(crate) async fn handle_rpc_multi_as_single(
    input: actor::RpcMulti,
    ro_inner: Arc<SpaceReadOnlyInner>,
    local_joined_agents: HashSet<Arc<KitsuneAgent>>,
) -> KitsuneP2pResult<Vec<actor::RpcMultiResponse>> {
    let RpcMulti {
        space,
        basis,
        payload,
        max_timeout,
        ..
    } = input;

    let ro_inner = &ro_inner;
    let space = &space;
    let payload = &payload;

    let make_req = move |con_hnd: MetaNetCon,
                         agent: Arc<KitsuneAgent>|
          -> BoxFuture<'_, KitsuneP2pResult<Vec<actor::RpcMultiResponse>>> {
        async move {
            let msg = wire::Wire::call(space.clone(), agent.clone(), payload.clone().into());

            let start = tokio::time::Instant::now();

            let res = con_hnd.request(&msg, max_timeout).await;

            match res {
                Ok(wire::Wire::CallResp(c)) => {
                    ro_inner
                        .metrics
                        .write()
                        .record_reachability_event(true, [&agent]);
                    ro_inner
                        .metrics
                        .write()
                        .record_latency_micros(start.elapsed().as_micros(), [&agent]);
                    Ok(vec![RpcMultiResponse {
                        agent: agent.clone(),
                        response: c.data.into(),
                    }])
                }
                oth => {
                    ro_inner
                        .metrics
                        .write()
                        .record_reachability_event(false, [&agent]);
                    ro_inner
                        .metrics
                        .write()
                        .record_latency_micros(start.elapsed().as_micros(), [&agent]);
                    tracing::warn!(?oth, "unexpected remote call result");
                    Err(format!("rpc_multi request failed: {:?}", oth).into())
                }
            }
        }
        .boxed()
    };

    max_timeout
        .mix("rpc_multi", async move {
            let mut errs = vec![];
            for _ in 0..2 {
                let mut infos = None;

                if let Ok(i) = discover::get_cached_remotes_near_basis(
                    ro_inner.clone(),
                    basis.get_loc(),
                    max_timeout,
                )
                .await
                {
                    if !i.is_empty() {
                        infos = Some(i);
                    }
                }

                if infos.is_none() {
                    tokio::time::sleep(std::time::Duration::from_millis(100)).await;

                    if let Ok(i) = discover::get_cached_remotes_near_basis(
                        ro_inner.clone(),
                        basis.get_loc(),
                        max_timeout,
                    )
                    .await
                    {
                        if !i.is_empty() {
                            infos = Some(i);
                        }
                    }
                }

                if let Some(mut infos) = infos {
                    rand::seq::SliceRandom::shuffle(infos.as_mut_slice(), &mut rand::thread_rng());

                    for info in infos {
                        use discover::PeerDiscoverResult;

                        let con_hnd = match discover::peer_connect(
                            ro_inner.clone(),
                            &info,
                            max_timeout,
                        )
                        .await
                        {
                            PeerDiscoverResult::OkShortcut => {
                                tracing::warn!("remote peer is local");
                                continue;
                            }
                            PeerDiscoverResult::Err(err) => {
                                tracing::warn!(?err, "peer discovery error");
                                errs.push(err);
                                continue;
                            }
                            PeerDiscoverResult::OkRemote { con_hnd, .. } => con_hnd,
                        };

                        match make_req(con_hnd, info.agent.clone()).await {
                            Ok(res) => return Ok(res),
                            Err(err) => {
                                tracing::warn!(?err, "remote call error");
                                errs.push(err);
                                continue;
                            }
                        }
                    }
                }
            }

            let num_local_agents = local_joined_agents.len();

            // fall back to self-get
            for agent in local_joined_agents {
                match ro_inner
                    .host_api
                    .legacy
                    .call(space.clone(), agent.clone(), payload.clone())
                    .await
                {
                    Ok(response) => {
                        return Ok(vec![RpcMultiResponse { agent, response }]);
                    }
                    Err(err) => {
                        tracing::warn!(?err, "local call error");
                        errs.push(err);
                        continue;
                    }
                }
            }

            // finally, return an error
            let error_msg = format!(
                "rpc_multi failed to get results. Local agents: {}, Errors: {:?}",
                num_local_agents, errs
            );
            tracing::error!("{}", error_msg);
            Err(error_msg.into())
        })
        .await
        .map_err(|err| err.into())
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/test_util/internal_stub.rs
================================================
#![allow(clippy::type_complexity)]

use crate::actor::BroadcastData;
use crate::actor::*;
use crate::spawn::actor::{
    EvtRcv, InternalHandler, InternalHandlerResult, KSpace, MaybeDelegate, OpHashList, VecMXM,
};
use crate::spawn::meta_net::MetaNetCon;
use crate::spawn::Internal;
use crate::{GossipModuleType, KitsuneP2pError};
use futures::FutureExt;
use ghost_actor::GhostError;
use ghost_actor::{GhostControlHandler, GhostHandler};
use kitsune_p2p_fetch::{FetchContext, FetchKey, FetchSource};
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::KOpHash;
use parking_lot::RwLock;
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::Arc;

#[derive(Clone)]
#[allow(clippy::type_complexity)]
pub struct InternalStub {
    fetch_calls: Vec<(FetchKey, KSpace, FetchSource)>,
    pub incoming_publish_calls: Arc<
        RwLock<
            Vec<(
                KSpace,
                crate::spawn::actor::KAgent,
                crate::spawn::actor::KAgent,
                kitsune_p2p_fetch::TransferMethod,
                OpHashList,
                FetchContext,
                MaybeDelegate,
            )>,
        >,
    >,
    pub incoming_delegate_broadcast_calls: Arc<
        RwLock<
            Vec<(
                crate::spawn::actor::KSpace,
                crate::spawn::actor::KBasis,
                crate::spawn::actor::KAgent,
                u32,
                u32,
                BroadcastData,
            )>,
        >,
    >,
    pub incoming_gossip_calls: Arc<
        RwLock<
            Vec<(
                crate::spawn::actor::KSpace,
                MetaNetCon,
                String,
                crate::spawn::actor::Payload,
                GossipModuleType,
            )>,
        >,
    >,
    pub connections: Arc<RwLock<HashMap<String, MetaNetCon>>>,
    pub respond_with_error_count: Arc<AtomicUsize>,
    pub respond_with_error: Arc<AtomicBool>,
}

impl InternalStub {
    pub fn new() -> Self {
        InternalStub {
            fetch_calls: vec![],
            incoming_publish_calls: Arc::new(RwLock::new(vec![])),
            incoming_delegate_broadcast_calls: Arc::new(RwLock::new(vec![])),
            incoming_gossip_calls: Arc::new(RwLock::new(vec![])),
            connections: Arc::new(parking_lot::RwLock::new(HashMap::new())),
            respond_with_error_count: Arc::new(AtomicUsize::new(0)),
            respond_with_error: Arc::new(AtomicBool::new(false)),
        }
    }

    fn maybe_error(&mut self) -> Result<(), KitsuneP2pError> {
        if let Ok(true) = self.respond_with_error.compare_exchange(
            true,
            false,
            Ordering::SeqCst,
            Ordering::SeqCst,
        ) {
            self.respond_with_error_count.fetch_add(1, Ordering::SeqCst);
            return Err("InternalStub error".into());
        }

        Ok(())
    }
}

impl GhostControlHandler for InternalStub {}
impl GhostHandler<Internal> for InternalStub {}
impl InternalHandler for InternalStub {
    fn handle_new_address(&mut self, _: String) -> InternalHandlerResult<()> {
        todo!()
    }

    fn handle_register_space_event_handler(&mut self, _recv: EvtRcv) -> InternalHandlerResult<()> {
        todo!()
    }

    fn handle_incoming_delegate_broadcast(
        &mut self,
        space: crate::spawn::actor::KSpace,
        basis: crate::spawn::actor::KBasis,
        to_agent: crate::spawn::actor::KAgent,
        mod_idx: u32,
        mod_cnt: u32,
        data: BroadcastData,
    ) -> InternalHandlerResult<()> {
        if let Err(e) = self.maybe_error() {
            return Ok(async move { Err(e) }.boxed().into());
        }

        self.incoming_delegate_broadcast_calls
            .write()
            .push((space, basis, to_agent, mod_idx, mod_cnt, data));

        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_incoming_publish(
        &mut self,
        space: crate::spawn::actor::KSpace,
        to_agent: crate::spawn::actor::KAgent,
        source: crate::spawn::actor::KAgent,
        transfer_method: kitsune_p2p_fetch::TransferMethod,
        op_hash_list: OpHashList,
        context: FetchContext,
        maybe_delegate: MaybeDelegate,
    ) -> InternalHandlerResult<()> {
        if let Err(e) = self.maybe_error() {
            return Ok(async move { Err(e) }.boxed().into());
        }

        self.incoming_publish_calls.write().push((
            space,
            to_agent,
            source,
            transfer_method,
            op_hash_list,
            context,
            maybe_delegate,
        ));

        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_resolve_publish_pending_delegates(
        &mut self,
        _space: crate::spawn::actor::KSpace,
        _op_hash: KOpHash,
    ) -> InternalHandlerResult<()> {
        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_incoming_gossip(
        &mut self,
        space: crate::spawn::actor::KSpace,
        con: MetaNetCon,
        remote_url: String,
        data: crate::spawn::actor::Payload,
        module_type: GossipModuleType,
    ) -> InternalHandlerResult<()> {
        if let Err(e) = self.maybe_error() {
            return Ok(async move { Err(e) }.boxed().into());
        }

        self.incoming_gossip_calls
            .write()
            .push((space, con, remote_url, data, module_type));

        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_incoming_metric_exchange(
        &mut self,
        _space: crate::spawn::actor::KSpace,
        _msgs: VecMXM,
    ) -> InternalHandlerResult<()> {
        todo!()
    }

    fn handle_new_con(&mut self, url: String, con: MetaNetCon) -> InternalHandlerResult<()> {
        self.connections.write().insert(url, con);

        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_del_con(&mut self, url: String) -> InternalHandlerResult<()> {
        self.connections.write().remove(&url);

        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_fetch(
        &mut self,
        key: FetchKey,
        space: crate::spawn::actor::KSpace,
        source: FetchSource,
    ) -> InternalHandlerResult<()> {
        self.fetch_calls.push((key, space, source));
        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_get_all_local_joined_agent_infos(
        &mut self,
    ) -> InternalHandlerResult<Vec<AgentInfoSigned>> {
        todo!()
    }
}

ghost_actor::ghost_chan! {
    pub chan InternalStubTest<GhostError> {
        fn drain_fetch_calls() -> Vec<(FetchKey, crate::spawn::actor::KSpace, FetchSource)>;
    }
}

impl GhostHandler<InternalStubTest> for InternalStub {}
impl InternalStubTestHandler for InternalStub {
    fn handle_drain_fetch_calls(
        &mut self,
    ) -> InternalStubTestHandlerResult<Vec<(FetchKey, KSpace, FetchSource)>> {
        let calls = self.fetch_calls.drain(..).collect();
        Ok(async move { Ok(calls) }.boxed().into())
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/test_util/legacy_host_stub.rs
================================================
#![allow(clippy::type_complexity)]
use crate::event::{
    FetchOpDataEvtQuery, KitsuneP2pEvent, KitsuneP2pEventHandlerResult, PutAgentInfoSignedEvt,
};
use crate::spawn::actor::{KAgent, KSpace};
use crate::test_util::data::mk_agent_info;
use crate::types::event::Payload;
use crate::{KOp, KitsuneP2pError};
use futures::channel::mpsc::{channel, Receiver};
use futures::{FutureExt, SinkExt, StreamExt};
use ghost_actor::GhostRespond;
use kitsune_p2p_types::bin_types::KitsuneOpData;
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::task::AbortHandle;
use tokio::time::error::Elapsed;

pub struct LegacyHostStub {
    pub respond_with_error: Arc<AtomicBool>,
    pub respond_with_error_count: Arc<AtomicUsize>,

    pub put_agent_info_signed_calls: Arc<parking_lot::RwLock<Vec<PutAgentInfoSignedEvt>>>,
    pub notify_calls: Arc<parking_lot::RwLock<Vec<(KSpace, KAgent, Payload)>>>,
    pub receive_ops_calls: Arc<parking_lot::RwLock<Vec<OpsCalls>>>,

    put_events: Receiver<PutAgentInfoSignedEvt>,
    abort_handle: AbortHandle,
}

pub type OpsCalls = (KSpace, Vec<KOp>, Option<kitsune_p2p_fetch::FetchContext>);

impl LegacyHostStub {
    pub fn start(mut host_receiver: Receiver<KitsuneP2pEvent>) -> Self {
        let (mut sender, receiver) = channel(10);

        let put_agent_info_signed_calls = Arc::new(parking_lot::RwLock::new(Vec::new()));
        let notify_calls = Arc::new(parking_lot::RwLock::new(Vec::new()));
        let receive_ops_calls = Arc::new(parking_lot::RwLock::new(Vec::new()));

        let respond_with_error = Arc::new(AtomicBool::new(false));
        let respond_with_error_count = Arc::new(AtomicUsize::new(0));

        let handle = tokio::spawn({
            let task_respond_with_error = respond_with_error.clone();
            let task_respond_with_error_count = respond_with_error_count.clone();

            let task_put_agent_info_signed_calls = put_agent_info_signed_calls.clone();
            let task_notify_calls = notify_calls.clone();
            let task_receive_ops_calls = receive_ops_calls.clone();

            async move {
                while let Some(evt) = host_receiver.next().await {
                    match evt {
                        KitsuneP2pEvent::PutAgentInfoSigned { input, respond, .. } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            task_put_agent_info_signed_calls.write().push(input.clone());
                            sender.send(input).await.unwrap();

                            respond
                                .unwrap()
                                .respond(Ok(async move { Ok(vec![]) }.boxed().into()));
                        }
                        KitsuneP2pEvent::Call {
                            payload, respond, ..
                        } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            // An echo response, no need for anything fancy here
                            respond
                                .unwrap()
                                .respond(Ok(async move { Ok(payload.to_vec()) }.boxed().into()));
                        }
                        KitsuneP2pEvent::QueryAgents { input, respond, .. } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            let len = input.limit.unwrap();

                            respond.unwrap().respond(Ok(async move {
                                let mut agents = vec![];
                                for i in 0..len {
                                    agents.push(mk_agent_info(i as u8).await);
                                }

                                Ok(agents)
                            }
                            .boxed()
                            .into()))
                        }
                        KitsuneP2pEvent::QueryPeerDensity { .. } => {}
                        KitsuneP2pEvent::Notify {
                            space,
                            to_agent,
                            payload,
                            respond,
                            ..
                        } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            task_notify_calls.write().push((space, to_agent, payload));

                            respond
                                .unwrap()
                                .respond(Ok(async move { Ok(()) }.boxed().into()))
                        }
                        KitsuneP2pEvent::FetchOpData { input, respond, .. } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            match input.query {
                                FetchOpDataEvtQuery::Hashes { op_hash_list, .. } => {
                                    let response = op_hash_list
                                        .into_iter()
                                        // TODO why are we responding with hashes when they are part of the input? It's an atomic
                                        //      operation in the sense that you get everything or an error so there is no matching to be done.
                                        .map(|h| (h, KitsuneOpData::new(vec![1, 2, 3])))
                                        .collect();

                                    respond
                                        .unwrap()
                                        .respond(Ok(async move { Ok(response) }.boxed().into()))
                                }
                                _ => {
                                    respond.unwrap().respond(Ok(async move {
                                        Err(KitsuneP2pError::other("a test error"))
                                    }
                                    .boxed()
                                    .into()));
                                }
                            }
                        }
                        KitsuneP2pEvent::ReceiveOps {
                            space,
                            ops,
                            context,
                            respond,
                            ..
                        } => {
                            let respond = maybe_respond_error(
                                task_respond_with_error.clone(),
                                task_respond_with_error_count.clone(),
                                respond,
                            );
                            if respond.is_none() {
                                continue;
                            }

                            task_receive_ops_calls.write().push((space, ops, context));

                            respond
                                .unwrap()
                                .respond(Ok(async move { Ok(()) }.boxed().into()))
                        }
                        _ => panic!("Unexpected event - {:?}", evt),
                    }
                }
            }
        });

        LegacyHostStub {
            respond_with_error,
            respond_with_error_count,
            put_agent_info_signed_calls,
            notify_calls,
            receive_ops_calls,
            put_events: receiver,
            abort_handle: handle.abort_handle(),
        }
    }

    pub async fn next_event(&mut self, timeout: Duration) -> PutAgentInfoSignedEvt {
        tokio::time::timeout(timeout, self.put_events.next())
            .await
            .unwrap()
            .unwrap()
    }

    pub async fn try_next_event(
        &mut self,
        timeout: Duration,
    ) -> Result<Option<PutAgentInfoSignedEvt>, Elapsed> {
        tokio::time::timeout(timeout, self.put_events.next()).await
    }

    pub fn abort(&self) {
        self.abort_handle.abort();
    }
}

fn maybe_respond_error<T>(
    task_respond_with_error: Arc<AtomicBool>,
    count: Arc<AtomicUsize>,
    respond: GhostRespond<KitsuneP2pEventHandlerResult<T>>,
) -> Option<GhostRespond<KitsuneP2pEventHandlerResult<T>>> {
    if let Ok(true) =
        task_respond_with_error.compare_exchange(true, false, Ordering::SeqCst, Ordering::SeqCst)
    {
        count.fetch_add(1, Ordering::SeqCst);
        respond.respond(Ok(
            async move { Err(KitsuneP2pError::other("a test error")) }
                .boxed()
                .into(),
        ));
        None
    } else {
        Some(respond)
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/test_util/space_internal_stub.rs
================================================
use crate::actor::BroadcastData;
use crate::event::PutAgentInfoSignedEvt;
use crate::spawn::actor::space::WireConHnd;
use crate::spawn::actor::space::{SpaceInternal, SpaceInternalHandler, SpaceInternalHandlerResult};
use crate::spawn::actor::MaybeDelegate;
use crate::spawn::actor::OpHashList;
use crate::spawn::actor::Payload;
use crate::spawn::actor::VecMXM;
use crate::spawn::meta_net::MetaNetCon;
use crate::wire::Wire;
use crate::{GossipModuleType, KitsuneP2pError};
use futures::FutureExt;
use ghost_actor::{GhostControlHandler, GhostHandler};
use kitsune_p2p_fetch::FetchContext;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::dht::Arq;
use kitsune_p2p_types::{KAgent, KBasis, KOpHash, KSpace};
use std::collections::HashSet;

pub struct SpaceInternalStub {
    pub(crate) called_count: usize,
    pub(crate) errored_count: usize,
    pub(crate) respond_with_error: bool,
}

impl SpaceInternalStub {
    pub fn new() -> Self {
        SpaceInternalStub {
            called_count: 0,
            errored_count: 0,
            respond_with_error: false,
        }
    }
}

impl GhostControlHandler for SpaceInternalStub {}
impl GhostHandler<SpaceInternal> for SpaceInternalStub {}
impl SpaceInternalHandler for SpaceInternalStub {
    fn handle_new_address(&mut self, _: String) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_list_online_agents_for_basis_hash(
        &mut self,
        _space: KSpace,
        _from_agent: KAgent,
        _basis: KBasis,
    ) -> SpaceInternalHandlerResult<HashSet<KAgent>> {
        unreachable!()
    }

    fn handle_update_agent_info(&mut self) -> SpaceInternalHandlerResult<()> {
        if self.respond_with_error {
            self.errored_count += 1;

            Ok(
                async move { Err(KitsuneP2pError::other("SpaceInternalStub error")) }
                    .boxed()
                    .into(),
            )
        } else {
            self.called_count += 1;

            Ok(async move { Ok(()) }.boxed().into())
        }
    }

    fn handle_update_single_agent_info(
        &mut self,
        _agent: KAgent,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_publish_agent_info_signed(
        &mut self,
        _input: PutAgentInfoSignedEvt,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_get_all_local_joined_agent_infos(
        &mut self,
    ) -> SpaceInternalHandlerResult<Vec<AgentInfoSigned>> {
        unreachable!()
    }

    fn handle_is_agent_local(&mut self, _agent: KAgent) -> SpaceInternalHandlerResult<bool> {
        unreachable!()
    }

    fn handle_update_agent_arc(
        &mut self,
        _agent: KAgent,
        _arq: Arq,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_incoming_delegate_broadcast(
        &mut self,
        _space: KSpace,
        _basis: KBasis,
        _to_agent: KAgent,
        _mod_idx: u32,
        _mod_cnt: u32,
        _data: BroadcastData,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_incoming_publish(
        &mut self,
        _space: KSpace,
        _to_agent: KAgent,
        _source: KAgent,
        _transfer_method: kitsune_p2p_fetch::TransferMethod,
        _op_hash_list: OpHashList,
        _context: FetchContext,
        _maybe_delegate: MaybeDelegate,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_notify(&mut self, _to_agent: KAgent, _data: Wire) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_resolve_publish_pending_delegates(
        &mut self,
        _space: KSpace,
        _op_hash: KOpHash,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_incoming_gossip(
        &mut self,
        _space: KSpace,
        _con: MetaNetCon,
        _remote_url: String,
        _data: Payload,
        _module_type: GossipModuleType,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_incoming_metric_exchange(
        &mut self,
        _space: KSpace,
        _msgs: VecMXM,
    ) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_new_con(&mut self, _url: String, _con: WireConHnd) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }

    fn handle_del_con(&mut self, _url: String) -> SpaceInternalHandlerResult<()> {
        unreachable!()
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util/data.rs
================================================
use crate::{KitsuneAgent, KitsuneBinType, KitsuneSignature, KitsuneSpace};
use kitsune_p2p_types::{agent_info::AgentInfoSigned, dht::arq::ArqSize};
use std::sync::Arc;

pub async fn mk_agent_info(u: u8) -> AgentInfoSigned {
    AgentInfoSigned::sign(
        Arc::new(KitsuneSpace::new(vec![0x11; 32])),
        Arc::new(KitsuneAgent::new(vec![u; 32])),
        ArqSize::empty(),
        vec![],
        0,
        0,
        |_| async move { Ok(Arc::new(KitsuneSignature(vec![0; 64]))) },
    )
    .await
    .unwrap()
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util/harness_actor.rs
================================================
use super::*;
use kitsune_p2p_types::config::TransportConfig;

type KAgent = Arc<KitsuneAgent>;
type KInfo = Arc<AgentInfoSigned>;

ghost_actor::ghost_chan! {
    /// The api for the test harness controller
    pub chan HarnessControlApi<KitsuneP2pError> {
        /// Create a new random space id
        /// + join all existing harness agents to it
        /// + all new harness agents will also join it
        fn add_space() -> Arc<KitsuneSpace>;

        /*
        /// Create a new agent configured to proxy for others.
        fn add_proxy_agent(nick: String) -> (
            Arc<KitsuneAgent>,
            ghost_actor::GhostSender<KitsuneP2p>,
        );

        /// Create a new directly addressable agent that will
        /// reject any proxy requests.
        fn add_direct_agent(nick: String) -> (
            Arc<KitsuneAgent>,
            ghost_actor::GhostSender<KitsuneP2p>,
        );

        /// Create a new directly addressable agent that will
        /// reject any proxy requests.
        fn add_publish_only_agent(nick: String) -> (
            Arc<KitsuneAgent>,
            ghost_actor::GhostSender<KitsuneP2p>,
        );

        /// Create a new agent that will connect via proxy.
        fn add_nat_agent(nick: String, proxy_url: url2::Url2) -> (
            Arc<KitsuneAgent>,
            ghost_actor::GhostSender<KitsuneP2p>,
        );
        */

        /// Magically exchange peer data between peers in harness
        fn magic_peer_info_exchange() -> ();

        /// Inject data for one specific agent to gossip to others
        fn inject_gossip_data(agent: KAgent, data: String) -> Arc<KitsuneOpHash>;

        /// Inject agent info for one agent.
        fn inject_peer_info(agent: KAgent, info: KInfo) -> ();

        /// Dump all local gossip data from a specific agent
        fn dump_local_gossip_data(agent: KAgent) -> HashMap<Arc<KitsuneOpHash>, String>;

        /// Dump all local peer data from a specific agent
        fn dump_local_peer_data(agent: KAgent) -> HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>;
    }
}

/// construct a test suite around a mem transport
pub async fn spawn_test_harness_mem() -> Result<
    (
        ghost_actor::GhostSender<HarnessControlApi>,
        HarnessEventChannel,
    ),
    KitsuneP2pError,
> {
    spawn_test_harness(TransportConfig::Mem {}).await
}

/*
/// construct a test suite around a quic transport
pub async fn spawn_test_harness_quic() -> Result<
    (
        ghost_actor::GhostSender<HarnessControlApi>,
        HarnessEventChannel,
    ),
    KitsuneP2pError,
> {
    spawn_test_harness(TransportConfig::Quic {
        bind_to: Some(url2::url2!("kitsune-quic://0.0.0.0:0")),
        override_host: None,
        override_port: None,
    })
    .await
}
*/

/// construct a test suite around a sub transport config concept
pub async fn spawn_test_harness(
    sub_config: TransportConfig,
) -> Result<
    (
        ghost_actor::GhostSender<HarnessControlApi>,
        HarnessEventChannel,
    ),
    KitsuneP2pError,
> {
    let harness_chan = HarnessEventChannel::new("");

    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let controller = builder
        .channel_factory()
        .create_channel::<HarnessControlApi>()
        .await?;

    let i_s = builder
        .channel_factory()
        .create_channel::<HarnessInner>()
        .await?;

    tokio::task::spawn(builder.spawn(HarnessActor::new(i_s, harness_chan.clone(), sub_config)));

    Ok((controller, harness_chan))
}

type KP2p = ghost_actor::GhostSender<KitsuneP2p>;
type KCtl = ghost_actor::GhostSender<HarnessAgentControl>;

ghost_actor::ghost_chan! {
    /// The api for the test harness controller
    chan HarnessInner<KitsuneP2pError> {
        fn finish_agent(
            agent: KAgent,
            p2p: KP2p,
            ctrl: KCtl,
        ) -> ();
    }
}

struct HarnessActor {
    i_s: ghost_actor::GhostSender<HarnessInner>,
    harness_chan: HarnessEventChannel,
    sub_config: TransportConfig,
    space_list: Vec<Arc<KitsuneSpace>>,
    agents: HashMap<
        Arc<KitsuneAgent>,
        (
            ghost_actor::GhostSender<KitsuneP2p>,
            ghost_actor::GhostSender<HarnessAgentControl>,
        ),
    >,
}

impl HarnessActor {
    pub fn new(
        i_s: ghost_actor::GhostSender<HarnessInner>,
        harness_chan: HarnessEventChannel,
        sub_config: TransportConfig,
    ) -> Self {
        Self {
            i_s,
            harness_chan,
            sub_config,
            space_list: Vec::new(),
            agents: HashMap::new(),
        }
    }
}

impl ghost_actor::GhostControlHandler for HarnessActor {
    fn handle_ghost_actor_shutdown(
        self,
    ) -> ghost_actor::dependencies::must_future::MustBoxFuture<'static, ()> {
        use ghost_actor::GhostControlSender;
        async move {
            // The line below was added when migrating to rust edition 2021, per
            // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
            let _ = &self;
            self.harness_chan.close();
            for (_, (p2p, ctrl)) in self.agents.iter() {
                let _ = p2p.ghost_actor_shutdown().await;
                let _ = ctrl.ghost_actor_shutdown().await;
            }
        }
        .boxed()
        .into()
    }
}

impl ghost_actor::GhostHandler<HarnessInner> for HarnessActor {}

impl HarnessInnerHandler for HarnessActor {
    fn handle_finish_agent(
        &mut self,
        agent: Arc<KitsuneAgent>,
        p2p: ghost_actor::GhostSender<KitsuneP2p>,
        ctrl: ghost_actor::GhostSender<HarnessAgentControl>,
    ) -> HarnessInnerHandlerResult<()> {
        self.agents.insert(agent.clone(), (p2p.clone(), ctrl));

        let harness_chan = self.harness_chan.clone();
        let space_list = self.space_list.clone();
        Ok(async move {
            for space in space_list {
                p2p.join(space.clone(), agent.clone(), None, None).await?;

                harness_chan.publish(HarnessEventType::Join {
                    agent: (&agent).into(),
                    space: space.into(),
                });
            }
            Ok(())
        }
        .boxed()
        .into())
    }
}

impl ghost_actor::GhostHandler<HarnessControlApi> for HarnessActor {}

impl HarnessControlApiHandler for HarnessActor {
    fn handle_add_space(&mut self) -> HarnessControlApiHandlerResult<Arc<KitsuneSpace>> {
        let space: Arc<KitsuneSpace> = TestVal::test_val();
        self.space_list.push(space.clone());
        let mut all = Vec::new();

        for (agent, (p2p, _)) in self.agents.iter() {
            all.push(p2p.join(space.clone(), agent.clone(), None, None));
        }
        Ok(async move {
            futures::future::try_join_all(all).await?;
            Ok(space)
        }
        .boxed()
        .into())
    }

    /*
    fn handle_add_proxy_agent(
        &mut self,
        nick: String,
    ) -> HarnessControlApiHandlerResult<(Arc<KitsuneAgent>, ghost_actor::GhostSender<KitsuneP2p>)>
    {
        let mut proxy_agent_config = KitsuneP2pConfig::empty();
        proxy_agent_config
            .transport_pool
            .push(TransportConfig::Proxy {
                sub_transport: Box::new(self.sub_config.clone()),
                proxy_config: ProxyConfig::LocalProxyServer {
                    proxy_accept_config: Some(ProxyAcceptConfig::AcceptAll),
                },
            });

        let sub_harness = self.harness_chan.sub_clone(nick);
        let i_s = self.i_s.clone();
        Ok(async move {
            let (agent, p2p, ctrl) = spawn_test_agent(sub_harness, proxy_agent_config).await?;

            i_s.finish_agent(agent.clone(), p2p.clone(), ctrl).await?;

            Ok((agent, p2p))
        }
        .boxed()
        .into())
    }

    fn handle_add_direct_agent(
        &mut self,
        nick: String,
    ) -> HarnessControlApiHandlerResult<(Arc<KitsuneAgent>, ghost_actor::GhostSender<KitsuneP2p>)>
    {
        let mut direct_agent_config = KitsuneP2pConfig::empty();
        direct_agent_config
            .transport_pool
            .push(TransportConfig::Proxy {
                sub_transport: Box::new(self.sub_config.clone()),
                proxy_config: ProxyConfig::LocalProxyServer {
                    proxy_accept_config: Some(ProxyAcceptConfig::RejectAll),
                },
            });

        let sub_harness = self.harness_chan.sub_clone(nick);
        let i_s = self.i_s.clone();
        Ok(async move {
            let (agent, p2p, ctrl) = spawn_test_agent(sub_harness, direct_agent_config).await?;

            i_s.finish_agent(agent.clone(), p2p.clone(), ctrl).await?;

            Ok((agent, p2p))
        }
        .boxed()
        .into())
    }

    fn handle_add_publish_only_agent(
        &mut self,
        nick: String,
    ) -> HarnessControlApiHandlerResult<(Arc<KitsuneAgent>, ghost_actor::GhostSender<KitsuneP2p>)>
    {
        let mut tp =
            kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
        tp.gossip_strategy = "none".to_string();
        let tp = Arc::new(tp);
        let mut direct_agent_config = KitsuneP2pConfig {
            tuning_params: tp,
            ..Default::default()
        };
        direct_agent_config
            .transport_pool
            .push(TransportConfig::Proxy {
                sub_transport: Box::new(self.sub_config.clone()),
                proxy_config: ProxyConfig::LocalProxyServer {
                    proxy_accept_config: Some(ProxyAcceptConfig::RejectAll),
                },
            });

        let sub_harness = self.harness_chan.sub_clone(nick);
        let i_s = self.i_s.clone();
        Ok(async move {
            let (agent, p2p, ctrl) = spawn_test_agent(sub_harness, direct_agent_config).await?;

            i_s.finish_agent(agent.clone(), p2p.clone(), ctrl).await?;

            Ok((agent, p2p))
        }
        .boxed()
        .into())
    }

    fn handle_add_nat_agent(
        &mut self,
        nick: String,
        proxy_url: url2::Url2,
    ) -> HarnessControlApiHandlerResult<(Arc<KitsuneAgent>, ghost_actor::GhostSender<KitsuneP2p>)>
    {
        let mut nat_agent_config = KitsuneP2pConfig::empty();
        nat_agent_config
            .transport_pool
            .push(TransportConfig::Proxy {
                sub_transport: Box::new(self.sub_config.clone()),
                proxy_config: ProxyConfig::RemoteProxyClient { proxy_url },
            });

        let sub_harness = self.harness_chan.sub_clone(nick);
        let i_s = self.i_s.clone();
        Ok(async move {
            let (agent, p2p, ctrl) = spawn_test_agent(sub_harness, nat_agent_config).await?;

            i_s.finish_agent(agent.clone(), p2p.clone(), ctrl).await?;

            Ok((agent, p2p))
        }
        .boxed()
        .into())
    }
    */

    fn handle_magic_peer_info_exchange(&mut self) -> HarnessControlApiHandlerResult<()> {
        let ctrls = self
            .agents
            .values()
            .map(|(_, ctrl)| ctrl.clone())
            .collect::<Vec<_>>();

        Ok(async move {
            let infos = ctrls.iter().map(|c| c.dump_agent_info());
            let infos = futures::future::try_join_all(infos).await?;
            let infos = infos.into_iter().fold(HashMap::new(), |acc, x| {
                x.into_iter().fold(acc, |mut acc, x| {
                    acc.insert(x.agent.clone(), x);
                    acc
                })
            });
            let infos = ctrls.iter().map(|c| c.inject_agent_info(infos.clone()));
            futures::future::try_join_all(infos).await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_inject_gossip_data(
        &mut self,
        agent: Arc<KitsuneAgent>,
        data: String,
    ) -> HarnessControlApiHandlerResult<Arc<KitsuneOpHash>> {
        let (_, ctrl) = self
            .agents
            .get(&agent)
            .ok_or_else(|| KitsuneP2pError::from("invalid agent"))?;
        let fut = ctrl.inject_gossip_data(data);
        Ok(fut.boxed().into())
    }

    fn handle_inject_peer_info(
        &mut self,
        agent: KAgent,
        info: KInfo,
    ) -> HarnessControlApiHandlerResult<()> {
        let (_, ctrl) = self
            .agents
            .get(&agent)
            .ok_or_else(|| KitsuneP2pError::from("invalid agent"))?;
        let map = maplit::hashmap! {
            info.agent.clone() => info
        };
        let fut = ctrl.inject_agent_info(map);
        Ok(fut.boxed().into())
    }

    fn handle_dump_local_gossip_data(
        &mut self,
        agent: Arc<KitsuneAgent>,
    ) -> HarnessControlApiHandlerResult<HashMap<Arc<KitsuneOpHash>, String>> {
        let (_, ctrl) = self
            .agents
            .get(&agent)
            .ok_or_else(|| KitsuneP2pError::from("invalid agent"))?;
        let fut = ctrl.dump_local_gossip_data();
        Ok(fut.boxed().into())
    }

    fn handle_dump_local_peer_data(
        &mut self,
        agent: Arc<KitsuneAgent>,
    ) -> HarnessControlApiHandlerResult<HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>> {
        let (_, ctrl) = self
            .agents
            .get(&agent)
            .ok_or_else(|| KitsuneP2pError::from("invalid agent"))?;
        let fut = ctrl.dump_local_peer_data();
        Ok(fut.boxed().into())
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util/harness_agent.rs
================================================
use self::meta_net::PreflightUserData;
use crate::dht::prelude::ArqSet;

use super::*;
use kitsune_p2p_types::config::KitsuneP2pConfig;

type KAgent = Arc<KitsuneAgent>;
type KAgentMap = HashMap<KAgent, Arc<AgentInfoSigned>>;

ghost_actor::ghost_chan! {
    /// controller for test harness agent actor
    pub(crate) chan HarnessAgentControl<KitsuneP2pError> {
        /// dump agent info from peer_store
        fn dump_agent_info() -> Vec<Arc<AgentInfoSigned>>;

        /// inject a bunch of agent info
        fn inject_agent_info(info: KAgentMap) -> ();

        /// inject data to be gradually gossiped
        fn inject_gossip_data(data: String) -> Arc<KitsuneOpHash>;

        /// dump all local gossip data from this agent
        fn dump_local_gossip_data() -> HashMap<Arc<KitsuneOpHash>, String>;

        /// dump all local peer data from this agent
        fn dump_local_peer_data() -> HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>;
    }
}

#[derive(Debug)]
pub struct HarnessHost;

impl HarnessHost {
    pub fn new() -> Arc<Self> {
        Arc::new(Self)
    }
}

impl FetchPoolConfig for HarnessHost {
    fn merge_fetch_contexts(&self, _a: u32, _b: u32) -> u32 {
        unimplemented!()
    }
}

impl KitsuneHostDefaultError for HarnessHost {
    const NAME: &'static str = "HarnessHost";

    fn peer_extrapolated_coverage(
        &self,
        _space: Arc<KitsuneSpace>,
        _dht_arc_set: DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>> {
        box_fut(Ok(vec![]))
    }

    fn query_region_set(
        &self,
        _space: Arc<KitsuneSpace>,
        _arq_set: ArqSet,
    ) -> KitsuneHostResult<RegionSetLtcs> {
        box_fut(Ok(RegionSetLtcs::empty()))
    }
}

pub(crate) async fn spawn_test_agent(
    harness_chan: HarnessEventChannel,
    config: KitsuneP2pConfig,
) -> Result<
    (
        Arc<KitsuneAgent>,
        ghost_actor::GhostSender<KitsuneP2p>,
        ghost_actor::GhostSender<HarnessAgentControl>,
    ),
    KitsuneP2pError,
> {
    let topology = Topology::standard_epoch_full();
    let host = HarnessHost::new();
    let (p2p, evt) = spawn_kitsune_p2p(
        config,
        kitsune_p2p_types::tls::TlsConfig::new_ephemeral()
            .await
            .unwrap(),
        host,
        PreflightUserData::default(),
    )
    .await?;

    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let channel_factory = builder.channel_factory().clone();

    channel_factory.attach_receiver(evt).await?;

    let control = channel_factory
        .create_channel::<HarnessAgentControl>()
        .await?;

    let harness = AgentHarness::new(harness_chan, topology).await?;
    let agent = harness.agent.clone();
    tokio::task::spawn(builder.spawn(harness));

    Ok((agent, p2p, control))
}

use kitsune_p2p_fetch::FetchPoolConfig;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::bootstrap::AgentInfoPut;
use kitsune_p2p_types::box_fut;
use kitsune_p2p_types::dependencies::lair_keystore_api::dependencies::sodoken;
use kitsune_p2p_types::dht::prelude::RegionSetLtcs;
use kitsune_p2p_types::dht::spacetime::{SpaceOffset, Topology};
use kitsune_p2p_types::dht::{ArqStrat, PeerStrat};
use kitsune_p2p_types::dht_arc::DhtArcSet;

struct AgentHarness {
    agent: Arc<KitsuneAgent>,
    priv_key: sodoken::BufReadSized<{ sodoken::sign::SECRETKEYBYTES }>,
    harness_chan: HarnessEventChannel,
    agent_store: HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>,
    gossip_store: HashMap<Arc<KitsuneOpHash>, String>,
    topology: Topology,
    strat: ArqStrat,
}

impl AgentHarness {
    pub async fn new(
        harness_chan: HarnessEventChannel,
        topology: Topology,
    ) -> Result<Self, KitsuneP2pError> {
        let pub_key = sodoken::BufWriteSized::new_no_lock();
        let priv_key = sodoken::BufWriteSized::new_no_lock();
        sodoken::sign::keypair(pub_key.clone(), priv_key.clone())
            .await
            .map_err(KitsuneP2pError::other)?;

        let pub_key = pub_key.read_lock().to_vec();
        let agent: Arc<KitsuneAgent> = Arc::new(KitsuneAgent::new(pub_key));
        Ok(Self {
            agent,
            priv_key: priv_key.to_read_sized(),
            harness_chan,
            agent_store: HashMap::new(),
            gossip_store: HashMap::new(),
            topology,
            strat: ArqStrat::default(),
        })
    }
}

impl ghost_actor::GhostControlHandler for AgentHarness {}

impl ghost_actor::GhostHandler<HarnessAgentControl> for AgentHarness {}

impl HarnessAgentControlHandler for AgentHarness {
    fn handle_dump_agent_info(
        &mut self,
    ) -> HarnessAgentControlHandlerResult<Vec<Arc<AgentInfoSigned>>> {
        let all = self.agent_store.values().cloned().collect();
        Ok(async move { Ok(all) }.boxed().into())
    }

    fn handle_inject_agent_info(
        &mut self,
        info: HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>,
    ) -> HarnessAgentControlHandlerResult<()> {
        self.agent_store.extend(info);
        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_inject_gossip_data(
        &mut self,
        data: String,
    ) -> HarnessAgentControlHandlerResult<Arc<KitsuneOpHash>> {
        let op_hash: Arc<KitsuneOpHash> = hash_op_data(data.as_bytes());
        self.gossip_store.insert(op_hash.clone(), data);
        Ok(async move { Ok(op_hash) }.boxed().into())
    }

    fn handle_dump_local_gossip_data(
        &mut self,
    ) -> HarnessAgentControlHandlerResult<HashMap<Arc<KitsuneOpHash>, String>> {
        let out = self.gossip_store.clone();
        Ok(async move { Ok(out) }.boxed().into())
    }

    fn handle_dump_local_peer_data(
        &mut self,
    ) -> HarnessAgentControlHandlerResult<HashMap<Arc<KitsuneAgent>, Arc<AgentInfoSigned>>> {
        let out = self.agent_store.clone();
        Ok(async move { Ok(out) }.boxed().into())
    }
}

impl ghost_actor::GhostHandler<KitsuneP2pEvent> for AgentHarness {}

impl KitsuneP2pEventHandler for AgentHarness {
    fn handle_put_agent_info_signed(
        &mut self,
        input: PutAgentInfoSignedEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<AgentInfoPut>> {
        for info in input.peer_data {
            let info = Arc::new(info);
            self.agent_store.insert(info.agent.clone(), info.clone());
            self.harness_chan.publish(HarnessEventType::StoreAgentInfo {
                agent: (&info.agent).into(),
                agent_info: info,
            });
        }
        Ok(async move { Ok(vec![]) }.boxed().into())
    }

    fn handle_query_agents(
        &mut self,
        QueryAgentsEvt {
            space: _,
            agents,
            window,
            arq_set: arc_set,
            near_basis: _,
            limit,
        }: QueryAgentsEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<crate::types::agent_store::AgentInfoSigned>> {
        let arq_set = arc_set.unwrap_or_else(ArqSet::<SpaceOffset>::full_std);
        let window = window.unwrap_or_else(full_time_window);
        // TODO - sort by near_basis if set
        let out = self
            .agent_store
            .iter()
            .filter(|(a, _)| {
                agents
                    .as_ref()
                    .map(|agents| agents.contains(*a))
                    .unwrap_or(true)
            })
            .filter(|(_, i)| arq_set.to_dht_arc_set_std().contains(i.agent.get_loc()))
            .filter(|(_, i)| window.contains(&Timestamp::from_micros(i.signed_at_ms as i64 * 1000)))
            .take(limit.unwrap_or(u32::MAX) as usize)
            .map(|(_, i)| (**i).clone())
            .collect();
        Ok(async move { Ok(out) }.boxed().into())
    }

    fn handle_query_peer_density(
        &mut self,
        _space: Arc<KitsuneSpace>,
        _arq: kitsune_p2p_types::dht_arc::DhtArc,
    ) -> KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView> {
        let strat = PeerStrat::from(self.strat.clone());
        let arcs: Vec<_> = self.agent_store.values().map(|v| v.storage_arq).collect();

        // contains is already checked in the iterator
        let view = strat.view(self.topology.clone(), arcs.as_slice());

        Ok(async move { Ok(view) }.boxed().into())
    }

    fn handle_call(
        &mut self,
        space: Arc<super::KitsuneSpace>,
        to_agent: Arc<super::KitsuneAgent>,
        payload: Vec<u8>,
    ) -> KitsuneP2pEventHandlerResult<Vec<u8>> {
        let data = String::from_utf8_lossy(&payload);
        self.harness_chan.publish(HarnessEventType::Call {
            space: space.into(),
            to_agent: to_agent.into(),
            payload: data.to_string(),
        });
        let data = format!("echo: {}", data);
        let data = data.into_bytes();
        Ok(async move { Ok(data) }.boxed().into())
    }

    fn handle_notify(
        &mut self,
        space: Arc<super::KitsuneSpace>,
        to_agent: Arc<super::KitsuneAgent>,
        payload: Vec<u8>,
    ) -> KitsuneP2pEventHandlerResult<()> {
        let data = String::from_utf8_lossy(&payload);
        self.harness_chan.publish(HarnessEventType::Notify {
            space: space.into(),
            to_agent: to_agent.into(),
            payload: data.to_string(),
        });
        Ok(async move { Ok(()) }.boxed().into())
    }

    fn handle_receive_ops(
        &mut self,
        _space: Arc<super::KitsuneSpace>,
        ops: Vec<KOp>,
        _context: Option<kitsune_p2p_fetch::FetchContext>,
    ) -> KitsuneP2pEventHandlerResult<()> {
        for op_data in ops {
            // TODO: check that we're handling string data uniformly in both directions
            let op_data = String::from_utf8_lossy(&op_data.0).to_string();
            let op_hash = hash_op_data(op_data.as_bytes());
            self.harness_chan.publish(HarnessEventType::Gossip {
                op_hash: (&op_hash).into(),
                op_data: op_data.clone(),
            });
            self.gossip_store.insert(op_hash, op_data);
        }
        Ok(async move { Ok(()) }.boxed().into())
