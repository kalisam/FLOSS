                // past a remove because that actually counts as covered.
                connected = last_remove
                    .as_ref()
                    .map(|l| (Wrapping(arm.pos) - Wrapping(*l)).0 <= 1)
                    .unwrap_or(false);

                // Add this id to the stack.
                stack.insert(arm.id);
            }
            Side::Right => {
                // Set the last removed.
                this_remove = Some(arm.pos);

                // Remove this id.
                stack.remove(&arm.id);
            }
        }
        // Get the current stack height.
        let len = stack.len();

        // If we have started and the length has dropped then set a
        // lower redundancy.
        let mut r = if len < r && started {
            // Only record removes that actually change the r level.
            last_remove = this_remove;
            len
        } else {
            r
        };

        // If this was actually a connected insert then undo the last remove.
        if connected {
            r += 1
        }
        (stack, r, started, last_remove)
    };

    // Run through the list once to find the stack remaining at the end of the run.
    let (stack, _, started, last_removed) = peers
        .iter()
        .fold((HashSet::new(), usize::MAX, false, None), stack_fold);

    // Now use that as the starting stack for the "real" run.
    let (_, r, _, _) = peers
        .iter()
        .fold((stack, usize::MAX, started, last_removed), stack_fold);

    // Our redundancy is whatever partial + any full redundancy
    r as u32 + full_r
}



================================================
File: crates/kitsune_p2p/dht_arc/src/dht_arc_set.rs
================================================
use gcollections::ops::*;
use interval::{interval_set::*, IntervalSet};
use std::{borrow::Borrow, collections::VecDeque};

use crate::{is_full, DhtArcRange, DhtLocation};

// For u32, IntervalSet excludes MAX from its set of valid values due to its
// need to be able to express the width of an interval using a u32.
// This min and max are set accordingly.
pub(crate) const MIN: u32 = u32::MIN;
pub(crate) const MAX: u32 = u32::MAX - 1;

#[derive(Clone, PartialEq, Eq)]
pub enum DhtArcSet {
    /// Full coverage.
    /// This needs a special representation because the underlying IntervalSet
    /// implementation excludes `u32::MAX` from its set of valid bounds
    Full,
    /// Any coverage other than full, including empty
    Partial(IntervalSet<DhtLocation>),
}

impl std::hash::Hash for DhtArcSet {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        match self {
            Self::Full => {
                state.write_u8(0);
            }
            Self::Partial(p) => {
                state.write_u8(1);
                for loc in p {
                    loc.lower().hash(state);
                    loc.upper().hash(state);
                }
            }
        }
    }
}

impl std::fmt::Debug for DhtArcSet {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Full => f.write_fmt(format_args!("DhtArcSet(Full)",)),
            Self::Partial(intervals) => f.write_fmt(format_args!(
                "DhtArcSet({:#?})",
                intervals.iter().collect::<Vec<_>>()
            )),
        }
    }
}

impl DhtArcSet {
    pub fn new_empty() -> Self {
        Self::Partial(vec![].to_interval_set())
    }

    pub fn new_full() -> Self {
        Self::Full
    }

    pub fn normalized(self) -> Self {
        let make_full = if let Self::Partial(intervals) = &self {
            intervals
                .iter()
                .any(|i| is_full(i.lower().into(), i.upper().into()))
        } else {
            false
        };

        if make_full {
            Self::Full
        } else {
            self
        }
    }

    pub fn from_bounds(start: DhtLocation, end: DhtLocation) -> Self {
        if is_full(start.into(), end.into()) {
            Self::new_full()
        } else {
            let start = start.as_u32().min(MAX).into();
            let end = end.as_u32().min(MAX).into();
            Self::Partial(
                if start <= end {
                    vec![(start, end)]
                } else {
                    vec![(MIN.into(), end), (start, MAX.into())]
                }
                .to_interval_set(),
            )
        }
    }

    pub fn from_interval<A: Borrow<DhtArcRange>>(arc: A) -> Self {
        match arc.borrow() {
            DhtArcRange::Full => Self::new_full(),
            DhtArcRange::Empty => Self::new_empty(),
            DhtArcRange::Bounded(start, end) => Self::from_bounds(*start, *end),
        }
    }

    pub fn intervals(&self) -> Vec<DhtArcRange> {
        match self {
            Self::Full => vec![DhtArcRange::Full],
            Self::Partial(intervals) => {
                let mut intervals: VecDeque<(DhtLocation, DhtLocation)> =
                    intervals.iter().map(|i| (i.lower(), i.upper())).collect();
                let wrapping = match (intervals.front(), intervals.back()) {
                    (Some(first), Some(last)) => {
                        // if there is an interval at the very beginning and one
                        // at the very end, let's interpret it as a single
                        // wrapping interval.
                        //
                        // NB: this checks for values greater than the MAX,
                        // because MAX is not u32::MAX. We don't expect values
                        // greater than MAX, but it's no harm if we do see one.
                        if first.0.as_u32() == MIN && last.1.as_u32() >= MAX {
                            Some((last.0, first.1))
                        } else {
                            None
                        }
                    }
                    _ => None,
                };
                // Condense the two bookend intervals into single wrapping interval
                if let Some(wrapping) = wrapping {
                    intervals.pop_front();
                    intervals.pop_back();
                    intervals.push_back(wrapping);
                }
                intervals
                    .into_iter()
                    .map(|(lo, hi)| DhtArcRange::from_bounds(lo, hi))
                    .collect()
            }
        }
    }

    pub fn is_empty(&self) -> bool {
        match self {
            Self::Full => false,
            Self::Partial(intervals) => intervals.is_empty(),
        }
    }

    pub fn contains(&self, t: DhtLocation) -> bool {
        self.overlap(&DhtArcSet::from(vec![(t, t)]))
    }

    /// Cheap check if the two sets have a non-null intersection
    pub fn overlap(&self, other: &Self) -> bool {
        match (self, other) {
            (this, Self::Full) => !this.is_empty(),
            (Self::Full, that) => !that.is_empty(),
            (Self::Partial(this), Self::Partial(that)) => this.overlap(that),
        }
    }

    pub fn union(&self, other: &Self) -> Self {
        match (self, other) {
            (_, Self::Full) => Self::Full,
            (Self::Full, _) => Self::Full,
            (Self::Partial(this), Self::Partial(that)) => {
                Self::Partial(this.union(that)).normalized()
            }
        }
    }

    pub fn intersection(&self, other: &Self) -> Self {
        match (self, other) {
            (this, Self::Full) => this.clone(),
            (Self::Full, that) => that.clone(),
            (Self::Partial(this), Self::Partial(that)) => {
                Self::Partial(this.intersection(that)).normalized()
            }
        }
    }

    pub fn size(&self) -> u32 {
        match self {
            Self::Full => u32::MAX,
            Self::Partial(intervals) => intervals.size(),
        }
    }

    pub fn print_arcs(&self, len: usize) {
        let arcs = self.intervals();
        println!("{} arcs", arcs.len());
        for (i, arc) in arcs.iter().enumerate() {
            println!(
                "{:>3}: |{}| {} {:?}",
                i,
                arc.to_ascii(len),
                arc.length(),
                arc.to_bounds_grouped(),
            );
        }
    }
}

impl From<&DhtArcRange> for DhtArcSet {
    fn from(arc: &DhtArcRange) -> Self {
        Self::from_interval(arc)
    }
}

impl From<DhtArcRange> for DhtArcSet {
    fn from(arc: DhtArcRange) -> Self {
        Self::from_interval(arc)
    }
}

impl From<&[DhtArcRange]> for DhtArcSet {
    fn from(arcs: &[DhtArcRange]) -> Self {
        arcs.iter()
            .map(Self::from)
            .fold(Self::new_empty(), |a, b| a.union(&b))
    }
}

impl From<Vec<DhtArcRange>> for DhtArcSet {
    fn from(arcs: Vec<DhtArcRange>) -> Self {
        arcs.iter()
            .map(Self::from)
            .fold(Self::new_empty(), |a, b| a.union(&b))
    }
}

impl From<Vec<(DhtLocation, DhtLocation)>> for DhtArcSet {
    fn from(pairs: Vec<(DhtLocation, DhtLocation)>) -> Self {
        pairs
            .into_iter()
            .map(|(a, b)| Self::from(&DhtArcRange::from_bounds(a, b)))
            .fold(Self::new_empty(), |a, b| a.union(&b))
    }
}

impl From<Vec<(u32, u32)>> for DhtArcSet {
    fn from(pairs: Vec<(u32, u32)>) -> Self {
        Self::from(
            pairs
                .into_iter()
                .map(|(a, b)| (DhtLocation::new(a), DhtLocation::new(b)))
                .collect::<Vec<_>>(),
        )
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn fullness() {
        assert_eq!(DhtArcSet::from(vec![(0, u32::MAX),]), DhtArcSet::Full,);
        assert_eq!(DhtArcSet::from(vec![(0, u32::MAX - 1),]), DhtArcSet::Full,);
        assert_ne!(DhtArcSet::from(vec![(0, u32::MAX - 2),]), DhtArcSet::Full,);

        assert_eq!(DhtArcSet::from(vec![(11, 10),]), DhtArcSet::Full,);

        assert_eq!(
            DhtArcSet::from(vec![(u32::MAX - 1, u32::MAX - 2),]),
            DhtArcSet::Full,
        );

        assert_eq!(
            DhtArcSet::from(vec![(u32::MAX, u32::MAX - 1),]),
            DhtArcSet::Full,
        );
    }

    #[test]
    fn single_overlap() {
        let first = DhtArcSet::from(vec![(0, 100)]);
        let second = DhtArcSet::from(vec![(50, 150)]);

        assert!(first.overlap(&second));
    }

    #[test]
    fn single_no_overlap() {
        let first = DhtArcSet::from(vec![(0, 100)]);
        let second = DhtArcSet::from(vec![(101, 150)]);

        assert!(!first.overlap(&second));
    }

    #[test]
    fn overlap_multi() {
        let first = DhtArcSet::from(vec![(0, 100), (200, 300)]);
        let second = DhtArcSet::from(vec![(250, 350)]);

        assert!(first.overlap(&second));
    }

    #[test]
    fn overlap_multi_out_of_order() {
        let first = DhtArcSet::from(vec![(200, 300), (0, 100)]);
        let second = DhtArcSet::from(vec![(500, 550), (250, 350)]);

        assert!(first.overlap(&second));
    }
}



================================================
File: crates/kitsune_p2p/dht_arc/src/dht_location.rs
================================================
use derive_more::From;
use derive_more::Into;
use num_traits::AsPrimitive;
use std::num::Wrapping;

/// Type for representing a location that can wrap around
/// a u32 dht arc
#[derive(
    Debug,
    Clone,
    Copy,
    serde::Serialize,
    serde::Deserialize,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    Hash,
    From,
    Into,
    derive_more::AsRef,
    derive_more::Deref,
    derive_more::Display,
)]
pub struct DhtLocation(pub Wrapping<u32>);

impl DhtLocation {
    pub const MIN: Self = Self(Wrapping(u32::MIN));
    pub const MAX: Self = Self(Wrapping(u32::MAX));

    pub fn new(loc: u32) -> Self {
        Self(Wrapping(loc))
    }

    pub fn as_u32(&self) -> u32 {
        self.0 .0
    }

    pub fn as_i64(&self) -> i64 {
        self.0 .0 as i64
    }

    #[cfg(any(test, feature = "test_utils"))]
    pub fn as_i32(&self) -> i32 {
        self.0 .0 as i32
    }
}

// This From impl exists to make it easier to construct DhtLocations near the
// maximum value in tests
#[cfg(any(test, feature = "test_utils"))]
impl From<i32> for DhtLocation {
    fn from(i: i32) -> Self {
        (i as u32).into()
    }
}

#[cfg(any(feature = "sqlite", feature = "sqlite-encrypted"))]
impl rusqlite::ToSql for DhtLocation {
    fn to_sql(&self) -> rusqlite::Result<rusqlite::types::ToSqlOutput> {
        Ok(rusqlite::types::ToSqlOutput::Owned(self.0 .0.into()))
    }
}

/// The maximum you can hold either side of the hash location
/// is half the circle.
/// This is half of the furthest index you can hold
/// 1 is added for rounding
/// 1 more is added to represent the middle point of an odd length array
pub const MAX_HALF_LENGTH: u32 = (u32::MAX / 2) + 1 + 1;

/// Maximum number of values that a u32 can represent.
pub(crate) const U32_LEN: u64 = u32::MAX as u64 + 1;

impl From<u32> for DhtLocation {
    fn from(a: u32) -> Self {
        Self(Wrapping(a))
    }
}

impl AsPrimitive<u32> for DhtLocation {
    fn as_(self) -> u32 {
        self.as_u32()
    }
}

impl num_traits::Num for DhtLocation {
    type FromStrRadixErr = <u32 as num_traits::Num>::FromStrRadixErr;

    fn from_str_radix(str: &str, radix: u32) -> Result<Self, Self::FromStrRadixErr> {
        u32::from_str_radix(str, radix).map(Self::new)
    }
}

impl std::ops::Add for DhtLocation {
    type Output = Self;

    fn add(self, rhs: Self) -> Self::Output {
        Self(self.0 + rhs.0)
    }
}

impl std::ops::Sub for DhtLocation {
    type Output = Self;

    fn sub(self, rhs: Self) -> Self::Output {
        Self(self.0 - rhs.0)
    }
}

impl std::ops::Mul for DhtLocation {
    type Output = Self;

    fn mul(self, rhs: Self) -> Self::Output {
        Self(self.0 * rhs.0)
    }
}

impl std::ops::Div for DhtLocation {
    type Output = Self;

    fn div(self, rhs: Self) -> Self::Output {
        Self(self.0 / rhs.0)
    }
}

impl std::ops::Rem for DhtLocation {
    type Output = Self;

    fn rem(self, rhs: Self) -> Self::Output {
        Self(self.0 % rhs.0)
    }
}

impl num_traits::Zero for DhtLocation {
    fn zero() -> Self {
        Self::new(0)
    }

    fn is_zero(&self) -> bool {
        self.0 .0 == 0
    }
}

impl num_traits::One for DhtLocation {
    fn one() -> Self {
        Self::new(1)
    }
}

impl interval::ops::Width for DhtLocation {
    type Output = u32;

    fn max_value() -> Self {
        u32::MAX.into()
    }

    fn min_value() -> Self {
        u32::MIN.into()
    }

    fn width(lower: &Self, upper: &Self) -> Self::Output {
        u32::width(&lower.0 .0, &upper.0 .0)
    }
}

impl From<DhtLocation> for u32 {
    fn from(l: DhtLocation) -> Self {
        (l.0).0
    }
}

/// Scale a number in a smaller space (specified by `len`) up into the `u32` space.
/// The number to scale can be negative, which is wrapped to a positive value via modulo
pub(crate) fn loc_upscale(len: usize, v: i32) -> u32 {
    let max = crate::FULL_LEN_F;
    let lenf = len as f64;
    let vf = v as f64;
    (max / lenf * vf) as i64 as u32
}

/// Scale a u32 DhtLocation down into a smaller space (specified by `len`)
pub(crate) fn loc_downscale(len: usize, d: DhtLocation) -> usize {
    let max = crate::FULL_LEN_F;
    let lenf = len as f64;
    ((lenf / max * (d.as_u32() as f64)) as usize) % len
}

#[test]
fn test_loc_upscale() {
    let m = crate::FULL_LEN_F;
    assert_eq!(loc_upscale(8, 0), DhtLocation::from(0).as_u32());
    assert_eq!(
        loc_upscale(8, 1),
        DhtLocation::from((m / 8.0) as u32).as_u32()
    );
    assert_eq!(
        loc_upscale(3, 1),
        DhtLocation::from((m / 3.0) as u32).as_u32()
    );
}



================================================
File: crates/kitsune_p2p/dht_arc/src/lib.rs
================================================
mod defaults;
pub use defaults::*;

mod dht_arc;
pub use dht_arc::*;

mod dht_arc_redundancy;
pub use dht_arc_redundancy::*;

mod dht_arc_set;
pub use dht_arc_set::*;

mod dht_location;
pub use dht_location::*;

#[cfg(any(test, feature = "test_utils"))]
pub mod loc8;

#[cfg(test)]
mod test;



================================================
File: crates/kitsune_p2p/dht_arc/src/loc8.rs
================================================
use std::collections::BTreeSet;

use crate::{loc_downscale, loc_upscale, DhtArcRange, DhtLocation};

/// A representation of DhtLocation in the u8 space. Useful for writing tests
/// that test the full range of possible locations while still working with small numbers.
/// A Loc8 can be constructed `From<i32>` within `-128 <= n <= 255`.
/// A negative number is wrapped to a positive number internally, and the `sign` is preserved
/// for display purposes.
///
/// Loc8 has custom `Eq`, `Ord`, and other impls which disregard the `sign`.
#[derive(Copy, Clone)]
pub struct Loc8 {
    /// The unsigned value
    val: u8,
    /// Designates whether this value was constructed with a negative number or not,
    /// so that it can be displayed as positive or negative accordingly.
    sign: bool,
}

impl From<i32> for Loc8 {
    fn from(i: i32) -> Self {
        if i >= 0 {
            Self {
                val: i as u8,
                sign: false,
            }
        } else {
            Self {
                val: i as i8 as u8,
                sign: true,
            }
        }
    }
}

impl PartialEq for Loc8 {
    fn eq(&self, other: &Self) -> bool {
        self.val == other.val
    }
}

impl Eq for Loc8 {}

impl PartialOrd for Loc8 {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for Loc8 {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        self.val.cmp(&other.val)
    }
}

impl std::hash::Hash for Loc8 {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.val.hash(state);
    }
}

impl std::fmt::Display for Loc8 {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.as_i32().fmt(f)
    }
}

impl std::fmt::Debug for Loc8 {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.as_i32().fmt(f)
    }
}

impl Loc8 {
    pub fn as_i8(&self) -> i8 {
        self.as_u8() as i8
    }

    pub fn as_u8(&self) -> u8 {
        self.val
    }

    pub fn as_i32(&self) -> i32 {
        if self.sign {
            self.as_i8() as i32
        } else {
            self.as_u8() as u32 as i32
        }
    }

    pub fn to_unsigned(mut self) -> Self {
        self.sign = false;
        self
    }

    pub fn set<L: Into<Loc8>, I: IntoIterator<Item = L>>(it: I) -> BTreeSet<Self> {
        it.into_iter().map(Into::into).collect()
    }

    pub fn upscale<L: Into<Loc8>>(v: L) -> u32 {
        let v: Loc8 = v.into();
        loc_upscale(256, v.as_i32())
    }

    pub fn downscale(v: u32) -> u8 {
        loc_downscale(256, DhtLocation::from(v)) as u8
    }
}

impl From<Loc8> for DhtLocation {
    fn from(i: Loc8) -> Self {
        DhtLocation::from(Loc8::upscale(i))
    }
}

impl DhtLocation {
    pub fn as_loc8(&self) -> Loc8 {
        Loc8 {
            val: Loc8::downscale(self.as_u32()),
            sign: false,
        }
    }

    /// Turn this location into a "representative" 36 byte vec,
    /// suitable for use as a hash type.
    #[cfg(feature = "test_utils")]
    pub fn to_representative_test_bytes_36(&self) -> Vec<u8> {
        let mut bytes: Vec<u8> = self
            .as_u32()
            .to_le_bytes()
            .iter()
            .cycle()
            .take(36)
            .copied()
            .collect();
        // to distinguish the 0 location from an empty hash
        bytes[0] = 255;
        bytes
    }
}

impl DhtArcRange {
    pub fn as_loc8(&self) -> DhtArcRange<Loc8> {
        match self {
            Self::Empty => DhtArcRange::Empty,
            Self::Full => DhtArcRange::Full,
            Self::Bounded(lo, hi) => DhtArcRange::Bounded(lo.as_loc8(), hi.as_loc8()),
        }
    }
}

impl<L: Copy> DhtArcRange<L>
where
    Loc8: From<L>,
{
    pub fn canonical(self) -> DhtArcRange {
        match self {
            DhtArcRange::Empty => DhtArcRange::Empty,
            DhtArcRange::Full => DhtArcRange::Full,
            DhtArcRange::Bounded(lo, hi) => DhtArcRange::from_bounds(
                DhtLocation::from(Loc8::from(lo)),
                DhtLocation::from(Loc8::from(hi)),
            ),
        }
    }
}

#[test]
fn scaling() {
    let f = 16777216i32;
    assert_eq!(Loc8::upscale(4) as i32, f * 4);
    assert_eq!(Loc8::upscale(-4) as i32, f * -4);

    assert_eq!(Loc8::downscale((f * 4) as u32), 4);
    assert_eq!(Loc8::downscale((f * -4) as u32) as i8, -4);
}



================================================
File: crates/kitsune_p2p/dht_arc/src/test/ascii.rs
================================================
//! Allows easy construction of small ranges via ASCII art, useful for testing

use crate::{DhtArcRange, DhtArcSet};

pub fn ascii(s: &str) -> DhtArcSet {
    let mut arcs = Vec::<DhtArcRange>::new();
    let mut i: usize = 0;

    loop {
        if i >= s.len() {
            break;
        }
        while i < s.len() && &s[i..=i] == " " {
            i += 1
        }
        if i >= s.len() {
            break;
        }
        let start = i;
        while i < s.len() && &s[i..=i] != " " {
            i += 1
        }
        let end = i - 1;
        arcs.push(DhtArcRange::from_bounds(start as u32, end as u32));
    }

    arcs.as_slice().into()
}

#[cfg(test)]
mod tests {

    use crate::DhtArcSet;

    use super::*;

    #[test]
    // @maackle Do you know why this is now failing?
    #[ignore = "Broken not sure how to fix"]
    fn sanity() {
        assert_eq!(
            DhtArcSet::from(
                vec![
                    DhtArcRange::from_bounds(0, 2).canonical(),
                    DhtArcRange::from_bounds(u32::MAX - 2, u32::MAX).canonical()
                ]
                .as_slice()
            )
            .intervals(),
            vec![DhtArcRange::from_bounds(u32::MAX - 2, 2).canonical()]
        );
        assert_eq!(
            ascii("ooo    oo ").intervals(),
            vec![
                DhtArcRange::from_bounds(0, 2).canonical(),
                DhtArcRange::from_bounds(7, 8).canonical()
            ]
        );
        assert_eq!(
            ascii("oo oo o   ").intervals(),
            vec![
                DhtArcRange::from_bounds(0, 1).canonical(),
                DhtArcRange::from_bounds(3, 4).canonical(),
                DhtArcRange::from_bounds(6, 6).canonical(),
            ]
        );
    }
}



================================================
File: crates/kitsune_p2p/dht_arc/src/test/intersection.rs
================================================
use super::ascii;
use crate::DhtArcSet;

const MAX: u32 = u32::MAX;

macro_rules! assert_intersection {
    ($a: expr, $b: expr, $e: expr $(,)?) => {
        let empty = $e.is_empty();
        assert_eq!(DhtArcSet::intersection(&$a, &$b), $e);
        assert_eq!(DhtArcSet::intersection(&$b, &$a), $e);
        if empty {
            assert!(!DhtArcSet::overlap(&$a, &$b));
            assert!(!DhtArcSet::overlap(&$b, &$a));
        } else {
            assert!(DhtArcSet::overlap(&$a, &$b));
            assert!(DhtArcSet::overlap(&$b, &$a));
        }
    };
}

#[test]
// @maackle Do you know why this is now failing?
#[ignore = "Broken not sure how to fix"]
fn test_intersection_at_limits() {
    assert_intersection!(
        DhtArcSet::from(vec![(0, MAX)]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX),]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX),]),
    );
    assert_intersection!(
        DhtArcSet::from(vec![(1, MAX - 1)]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX),]),
        DhtArcSet::from(vec![(1, MAX / 4), (MAX / 2, MAX - 1),]),
    );
    assert_intersection!(
        DhtArcSet::from(vec![(1, MAX)]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX),]),
        DhtArcSet::from(vec![(1, MAX / 4), (MAX / 2, MAX),]),
    );
}

#[test]
#[ignore = "This test should pass, but doesn't.
It seems to highlight a weird edge case in the underlying `intervallum` crate,
but shouldn't have an impact on our use"]
fn test_intersection_at_limits_bug() {
    assert_intersection!(
        DhtArcSet::from(vec![(0, MAX - 1)]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX),]),
        DhtArcSet::from(vec![(0, MAX / 4), (MAX / 2, MAX - 1),]),
    );
}

#[test]
fn test_intersection() {
    assert_intersection!(
        ascii("oo       o"),
        ascii("o       oo"),
        ascii("o        o"),
    );
    assert_intersection!(
        ascii("  ooo     "),
        ascii("    ooo   "),
        ascii("    o     "),
    );
    assert_intersection!(
        ascii("o o o o o "),
        ascii(" o o o o o"),
        ascii("          "),
    );
    assert_intersection!(
        ascii("oooooooooo"),
        ascii("          "),
        ascii("          "),
    );
}



================================================
File: crates/kitsune_p2p/dht_arc/src/test/mod.rs
================================================
mod ascii;
pub use ascii::ascii;

mod intersection;
mod test_ascii;
mod union;



================================================
File: crates/kitsune_p2p/dht_arc/src/test/test_ascii.rs
================================================
use crate::{loc_upscale, DhtArc};

use pretty_assertions::assert_eq;

#[test]
fn correct_ascii() {
    test_cases_scaled(
        16,
        [
            (0, 8, "@--------       "),
            (1, 8, " @-------       "),
            (2, 8, "  @------       "),
            (8, 0, "-       @-------"),
            (8, 1, "--      @-------"),
            (8, 2, "---     @-------"),
            (9, 5, "------   @------"),
            (9, 6, "-------  @------"),
            (9, 7, "-------- @------"),
            (8, 7, "--------@-------"),
            (1, 0, "-@--------------"),
            (0, 15, "@---------------"),
        ],
    );
}

#[test]
fn correct_ascii_regressions() {
    test_cases_raw(
        32,
        [
            (17331162, 4277636136, "@-------------------------------"),
            (82641416, 4212325882, "@-------------------------------"),
            (2213596780, 2081370516, "----------------@---------------"),
            (3832356631, 4267951689, "                            @---"),
        ],
    );
}

fn test_cases_raw<'a>(len: usize, cases: impl IntoIterator<Item = (u32, u32, &'a str)> + Clone) {
    let fmt_bounds = |lo, hi| format!("({:+}, {:+})", lo, hi);

    let expected: Vec<_> = cases
        .clone()
        .into_iter()
        .map(|(lo, hi, ascii)| (fmt_bounds(lo, hi), ascii.to_string()))
        .collect();

    let actual: Vec<_> = cases
        .into_iter()
        .map(|(lo, hi, _)| {
            let ascii = DhtArc::from_bounds(lo, hi).to_ascii(len);
            let bounds = fmt_bounds(lo, hi);
            assert_eq!(ascii.len(), len, "Wrong length for case {}", bounds);
            (bounds, ascii)
        })
        .collect();

    assert_eq!(expected, actual);
}

fn test_cases_scaled<'a>(len: usize, cases: impl IntoIterator<Item = (i32, i32, &'a str)> + Clone) {
    let fmt_bounds = |lo, hi| format!("({:+}, {:+})", lo, hi);

    let expected: Vec<_> = cases
        .clone()
        .into_iter()
        .map(|(lo, hi, ascii)| (fmt_bounds(lo, hi), ascii.to_string()))
        .collect();

    let actual: Vec<_> = cases
        .into_iter()
        .map(|(lo, hi, _)| {
            let ascii =
                DhtArc::from_bounds(loc_upscale(len, lo), loc_upscale(len, hi)).to_ascii(len);
            let bounds = fmt_bounds(lo, hi);
            assert_eq!(ascii.len(), len, "Wrong length for case {}", bounds);
            (bounds, ascii)
        })
        .collect();

    assert_eq!(expected, actual);
}



================================================
File: crates/kitsune_p2p/dht_arc/src/test/union.rs
================================================
use super::ascii;
use crate::DhtArcSet;

const MAX: u32 = u32::MAX;

macro_rules! assert_union {
    ($a: expr, $b: expr, $e: expr $(,)?) => {
        assert_eq!(DhtArcSet::union(&$a, &$b), $e);
        assert_eq!(DhtArcSet::union(&$b, &$a), $e);
    };
}

#[test]
// @maackle Do you know why this is now failing?
#[ignore = "Broken not sure how to fix"]
fn test_union_at_limits() {
    assert_union!(
        DhtArcSet::from(vec![(0, MAX / 2)]),
        DhtArcSet::from(vec![(MAX / 2, MAX)]),
        DhtArcSet::from(vec![(0, MAX),]),
    );
    assert_union!(
        DhtArcSet::from(vec![(0, MAX / 2)]),
        DhtArcSet::from(vec![(MAX / 2, MAX - 1)]),
        DhtArcSet::from(vec![(0, MAX - 1),]),
    );
    assert_union!(
        DhtArcSet::from(vec![(0, MAX / 2)]),
        DhtArcSet::from(vec![(MAX / 2, MAX - 1)]),
        DhtArcSet::from(vec![(0, MAX - 1),]),
    );
    assert_union!(
        DhtArcSet::from(vec![(0, MAX / 2)]),
        DhtArcSet::from(vec![(MAX / 2, MAX - 2)]),
        DhtArcSet::from(vec![(0, MAX - 2),]),
    );
}

#[test]
fn test_union() {
    assert_union!(
        ascii("          "),
        ascii("          "),
        ascii("          "),
    );
    assert_union!(
        ascii("    o     "),
        ascii("     o    "),
        ascii("    oo    "),
    );
    assert_union!(
        ascii("o o o o o "),
        ascii(" o o o o o"),
        ascii("oooooooooo"),
    );
    assert_union!(
        ascii("oooooooooo"),
        ascii("          "),
        ascii("oooooooooo"),
    );
}



================================================
File: crates/kitsune_p2p/fetch/README.md
================================================
# kitsune_p2p_fetch

Kitsune P2p Fetch Queue Logic

License: Apache-2.0



================================================
File: crates/kitsune_p2p/fetch/Cargo.toml
================================================
[package]
name = "kitsune_p2p_fetch"
version = "0.5.0-dev.9"
description = "Kitsune P2p Fetch Pool Logic"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_fetch"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"


# reminder - do not use workspace deps
[dependencies]
derive_more = "0.99"
kitsune_p2p_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../types" }
serde = { version = "1.0", features = ["derive"] }
tokio = { version = "1.27", features = ["full"] }
tracing = "0.1.29"
backon = "0.4.1"
indexmap = { version = "2.6.0", features = ["serde"] }

human-repr = { version = "1", optional = true }
proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }
rusqlite = { version = "0.32.1", optional = true }

[dev-dependencies]
kitsune_p2p_fetch = { path = ".", features = [
  "test_utils",
  "sqlite",
  "fuzzing",
] }

holochain_serialized_bytes = "=0.0.55"
holochain_trace = { version = "^0.5.0-dev.1", path = "../../holochain_trace" }
pretty_assertions = "1.4.0"
test-case = "3.3"
tokio = { version = "1.27", features = ["full", "test-util"] }
rand = "0.8.5"

[lints]
workspace = true

[features]
fuzzing = [
  "proptest",
  "proptest-derive",
  "kitsune_p2p_timestamp/fuzzing",
  "kitsune_p2p_types/fuzzing",
]

test_utils = ["human-repr", "kitsune_p2p_types/test_utils"]

default = []

sqlite-encrypted = [
  "rusqlite",
  "rusqlite/bundled-sqlcipher-vendored-openssl",
  "kitsune_p2p_types/sqlite-encrypted",
]
sqlite = ["rusqlite", "rusqlite/bundled", "kitsune_p2p_types/sqlite"]



================================================
File: crates/kitsune_p2p/fetch/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

- Enhance source backoff logic. The fetch pool used to give a source a 5 minute pause if it failed to serve an op before using the source again. Now the failures to serve by sources is tracked across the pool. Sources that fail too often will be put on a backoff to give them a chance to deal with their current workload before we use them again. For hosts that continue to not respond they will be dropped as sources for ops. Ops that end up with no sources will be dropped from the fetch pool. This means that we can stop using resources on ops we will never be able to fetch. If a source appears who is capable of serving the missing ops then they should be re-added to the fetch pool.

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

- Fix an issue with merging fetch contexts where merging an item with a context with an item that did not could result in the removal of the context.
- Fix an issue where duplicate fetch sources would be permitted for a single item.

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.1



================================================
File: crates/kitsune_p2p/fetch/proptest-regressions/rough_sized.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc d38afdf8ea0a4ed8bf47b5a04e2818061b6631e3612c05b4f7aab1a01cfba99c # shrinks to v = [32769]



================================================
File: crates/kitsune_p2p/fetch/src/backoff.rs
================================================
use tokio::time::{Duration, Instant};

use backon::{BackoffBuilder, FibonacciBackoff, FibonacciBuilder};

/// The number of times to retry a fetch before giving up.
pub const BACKOFF_RETRY_COUNT: usize = 8;

/// A backoff strategy for use when fetching data from remote nodes that appear to not be responding.
#[derive(Debug)]
pub struct FetchBackoff {
    backoff: FibonacciBackoff,
    current_wait: Duration,
    started_wait_at: Instant,
    expired: bool,
}

impl FetchBackoff {
    /// Create a new instance with the given initial delay.
    pub fn new(initial_delay: Duration) -> Self {
        let mut backoff = FibonacciBuilder::default()
            .with_jitter()
            .with_min_delay(initial_delay)
            .with_max_delay(6 * initial_delay)
            .with_max_times(BACKOFF_RETRY_COUNT)
            .build();

        Self {
            current_wait: backoff.next().expect("At least one backoff period"),
            started_wait_at: Instant::now(),
            backoff,
            expired: false,
        }
    }

    /// Check whether the backoff is ready to try again. It will return true once each time it is
    /// ready and then start the next delay so the consumer must make an attempt to use the backoff
    /// before calling this again.
    pub fn is_ready(&mut self) -> bool {
        if self.expired {
            return false;
        }

        if self.started_wait_at.elapsed() >= self.current_wait {
            self.advance();
            true
        } else {
            false
        }
    }

    /// Check whether the backoff has expired.
    pub fn is_expired(&self) -> bool {
        self.expired
    }

    fn advance(&mut self) {
        match self.backoff.next() {
            Some(d) => {
                self.current_wait = d;
                self.started_wait_at = Instant::now();
            }
            None => {
                self.expired = true;
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::backoff::BACKOFF_RETRY_COUNT;

    use super::FetchBackoff;
    use std::time::Duration;

    #[test]
    fn first_delay_is_initial_delay() {
        let initial_delay = Duration::from_secs(1);
        let mut backoff = FetchBackoff::new(initial_delay);

        // The backoff should not be ready due to the initial delay period
        assert!(!backoff.is_ready());

        // Account for jitter and check that the delay is roughly the initial delay
        assert!(initial_delay <= backoff.current_wait && backoff.current_wait < initial_delay * 2);
    }

    #[test]
    fn number_of_tries_is_limited() {
        let mut backoff = FetchBackoff::new(Duration::from_nanos(1));
        assert!(!backoff.is_expired());

        let mut num_tries = 0;
        for _ in 0..100 {
            if backoff.is_expired() {
                break;
            }

            while !backoff.is_ready() {
                std::thread::sleep(Duration::from_nanos(10));
            }

            num_tries += 1;
        }

        assert!(backoff.is_expired());
        assert!(!backoff.is_ready());
        assert_eq!(BACKOFF_RETRY_COUNT, num_tries);
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/lib.rs
================================================
#![deny(missing_docs)]
#![deny(unsafe_code)]

//! Kitsune P2p Fetch Queue Logic

use kitsune_p2p_types::{fetch_pool::GossipType, KOpHash, KSpace};

mod backoff;
mod pool;
mod queue;
mod respond;
mod rough_sized;
mod source;

#[cfg(any(test, feature = "test_utils"))]
pub mod test_utils;

pub use pool::*;
pub use respond::*;
pub use rough_sized::*;
pub use source::FetchSource;

/// Determine what should be fetched.
#[derive(
    Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, serde::Deserialize, serde::Serialize,
)]
#[serde(tag = "type", content = "key", rename_all = "camelCase")]
pub enum FetchKey {
    /// Fetch via op hash.
    Op(KOpHash),
}

/// A fetch "unit" that can be de-duplicated.
#[derive(Debug, Clone, PartialEq)]
pub struct FetchPoolPush {
    /// Description of what to fetch.
    pub key: FetchKey,

    /// The space this op belongs to
    pub space: KSpace,

    /// The source to fetch the op from
    pub source: FetchSource,

    /// The means by which this hash arrived, either via Publish or Gossip.
    pub transfer_method: TransferMethod,

    /// The approximate size of the item
    pub size: Option<RoughInt>,

    /// Opaque "context" to be provided and interpreted by the host.
    pub context: Option<FetchContext>,
}

/// The possible methods of transferring op hashes
#[derive(
    Clone,
    Copy,
    Debug,
    PartialEq,
    Eq,
    Hash,
    derive_more::Display,
    serde::Serialize,
    serde::Deserialize,
)]
pub enum TransferMethod {
    /// Transfer by publishing
    Publish,
    /// Transfer by gossiping
    Gossip(GossipType),
}

#[cfg(any(feature = "sqlite", feature = "sqlite-encrypted"))]
impl rusqlite::ToSql for TransferMethod {
    fn to_sql(&self) -> rusqlite::Result<rusqlite::types::ToSqlOutput> {
        let stage = match self {
            TransferMethod::Publish => 1,
            TransferMethod::Gossip(GossipType::Recent) => 2,
            TransferMethod::Gossip(GossipType::Historical) => 3,
        };
        Ok(rusqlite::types::ToSqlOutput::Owned(stage.into()))
    }
}

#[cfg(any(feature = "sqlite", feature = "sqlite-encrypted"))]
impl rusqlite::types::FromSql for TransferMethod {
    fn column_result(value: rusqlite::types::ValueRef<'_>) -> rusqlite::types::FromSqlResult<Self> {
        i32::column_result(value).and_then(|int| match int {
            1 => Ok(TransferMethod::Publish),
            2 => Ok(TransferMethod::Gossip(GossipType::Recent)),
            3 => Ok(TransferMethod::Gossip(GossipType::Historical)),
            _ => Err(rusqlite::types::FromSqlError::InvalidType),
        })
    }
}

/// Usage agnostic context data.
#[derive(
    Default,
    Debug,
    Clone,
    Copy,
    PartialEq,
    Eq,
    serde::Serialize,
    serde::Deserialize,
    derive_more::Deref,
    derive_more::From,
)]
pub struct FetchContext(pub u32);



================================================
File: crates/kitsune_p2p/fetch/src/pool.rs
================================================
//! The Fetch Pool: a structure to store ops-to-be-fetched.
//!
//! When we encounter an op hash that we have no record of, we store it as an item
//! at the end of the FetchPool. The items of the queue contain not only the op hash,
//! but also the source(s) to fetch it from, and other data including the last time
//! a fetch was attempted.
//!
//! The consumer of the queue can read items whose last fetch time is older than some interval
//! from the current moment. The items thus returned are not guaranteed to be returned in
//! order of last fetch time, but they are guaranteed to be at least as old as the specified
//! interval.

use indexmap::map::Entry;
use kitsune_p2p_timestamp::Timestamp;
use std::{collections::HashMap, sync::Arc};
use tokio::time::{Duration, Instant};

use kitsune_p2p_types::{tx_utils::ShareOpen, KSpace};

use crate::{
    queue::MapQueue,
    source::{FetchSource, SourceState, Sources},
    FetchContext, FetchKey, FetchPoolPush, RoughInt, TransferMethod,
};

mod pool_reader;
pub use pool_reader::*;

/// A FetchPool tracks a set of [`FetchKey`]s (op hashes) to be fetched,
/// each of which can have multiple sources associated with it.
///
/// When adding the same key twice, the sources are merged by appending the newest
/// source to the front of the list of sources, and the contexts are merged by the
/// method defined in [`FetchPoolConfig`].
///
/// Each item consists of a FetchKey (Op) and one or more sources (Agent) from which to fetch it.
/// Items can be retrieved in batches using [`FetchPool::get_items_to_fetch`]. Any items which
/// were considered while building the batch, either because they were still awaiting a response
/// or because they were returned in the batch, will be moved to the end of the queue. This makes
/// fetching items reasonably fair.
#[derive(Clone)]
pub struct FetchPool {
    config: FetchConfig,
    state: ShareOpen<State>,
}

impl std::fmt::Debug for FetchPool {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.state
            .share_ref(|state| f.debug_struct("FetchPool").field("state", state).finish())
    }
}

/// Alias
pub type FetchConfig = Arc<dyn FetchPoolConfig>;

/// Host-defined details about how the fetch queue should function
pub trait FetchPoolConfig: 'static + Send + Sync {
    /// How long between successive item fetches, regardless of source?
    /// This gives a source a fair chance to respond before proceeding with a
    /// different source.
    ///
    /// The most conservative setting for this is `2 * tuning_params.implicit_timeout`,
    /// since that is the maximum amount of time a successful response can take.
    /// Lower values will give up early and may result in duplicate data sent if the
    /// response takes a long time to come back.
    fn item_retry_delay(&self) -> Duration {
        Duration::from_secs(90)
    }

    /// How long to put a source on a backoff if it fails to respond to a fetch.
    /// This is an initial value for a backoff on the source and will be increased if the source remains unresponsive.
    ///
    /// With the default settings of 30s for this delay and 8 retries, the total retry period is around 20 minutes (with jitter) so that the
    /// time we keep sources in the pool is close to the default value for the TTL on agent info. This means if an agent goes offline then
    /// they should be removed from the fetch pool in a similar amount of time to other communication with them ceasing.
    fn source_retry_delay(&self) -> Duration {
        Duration::from_secs(30)
    }

    /// When a fetch key is added twice, this determines how the two different contexts
    /// get reconciled.
    fn merge_fetch_contexts(&self, a: u32, b: u32) -> u32;

    /// How many items should be returned for fetching per call to [`FetchPool::get_items_to_fetch`].
    fn fetch_batch_size(&self) -> usize {
        100
    }

    /// The number of times a source can fail to respond in time before it is put on a backoff.
    ///
    /// This is a total number of timeouts so if a source is unreliable over time then it will be put on a backoff even if it is currently responding.
    /// If the source responds after its timeout period then this counter will be reset and the source will be considered available again after
    /// a single backoff period.
    ///
    /// The reasoning behind this parameter is that we want to limit the amount of resources we sink into an unresponsive source,
    /// as well as limiting the load on the source itself, who may be unresponsive because they're already struggling with too much load.
    fn source_unavailable_timeout_threshold(&self) -> usize {
        30
    }
}

// TODO: move this to host, but for now, for convenience, we just use this one config
//       for every queue
struct FetchPoolConfigBitwiseOr;

impl FetchPoolConfig for FetchPoolConfigBitwiseOr {
    fn merge_fetch_contexts(&self, a: u32, b: u32) -> u32 {
        a | b
    }
}

/// The actual inner state of the FetchPool, from which items can be obtained
#[derive(Debug, Default)]
pub(crate) struct State {
    /// Items to be fetched, ordered by least recently considered for fetching.
    queue: MapQueue<FetchKey, FetchPoolItem>,

    /// The state of all sources that we have seen in [`FetchPoolPush`]es.
    ///
    /// Note that sources are put on a backoff if they fail to respond to enough fetches. If the backoff
    /// expires and the source is still not responding, it will be removed from this map.
    sources: HashMap<FetchSource, SourceState>,
}

impl FetchPool {
    /// Constructor
    pub fn new(config: FetchConfig) -> Self {
        Self {
            config,
            state: ShareOpen::new(State::default()),
        }
    }

    /// Constructor, using only the "hardcoded" config (TODO: remove)
    pub fn new_bitwise_or() -> Self {
        Self {
            config: Arc::new(FetchPoolConfigBitwiseOr),
            state: ShareOpen::new(State::default()),
        }
    }

    /// Add an item to the queue.
    /// If the FetchKey does not already exist, add it to the end of the queue.
    /// If the FetchKey exists, add the new source and merge the context in, without
    /// changing the position in the queue.
    pub fn push(&self, args: FetchPoolPush) {
        self.state.share_mut(|s| {
            tracing::debug!(
                "FetchPool (size = {}) item added: {:?}",
                s.queue.len() + 1,
                args
            );
            s.push(&*self.config, args);
        });
    }

    /// Check if an item is in the fetch pool and what its context is.
    pub fn check_item(&self, key: &FetchKey) -> (bool, Option<FetchContext>) {
        self.state.share_ref(|s| match s.queue.get(key) {
            Some(item) => (true, item.context),
            None => (false, None),
        })
    }

    /// When an item has been successfully fetched, we can remove it from the queue.
    pub fn remove(&self, key: &FetchKey) -> Option<FetchPoolItem> {
        self.state.share_mut(|s| {
            let removed = s.remove(key);
            tracing::debug!(
                "FetchPool (size = {}) item removed: key={:?} val={:?}",
                s.queue.len(),
                key,
                removed
            );
            removed
        })
    }

    /// Get a list of the next items to be fetched.
    pub fn get_items_to_fetch(&self) -> Vec<(FetchKey, KSpace, FetchSource, Option<FetchContext>)> {
        self.state
            .share_mut(|s| s.get_batch(self.config.clone()).clone())
    }

    /// Get the current size of the fetch pool. This is the number of outstanding items
    /// and may be different to the size of response from `get_items_to_fetch` because it
    /// ignores retry delays.
    pub fn len(&self) -> usize {
        self.state.share_ref(|s| s.queue.len())
    }

    /// Check whether the fetch pool is empty.
    pub fn is_empty(&self) -> bool {
        self.state.share_ref(|s| s.queue.is_empty())
    }

    /// Check the state of all sources and remove any that have expired. See the docs on State::check_sources for details.
    pub fn check_sources(&self) {
        self.state.share_mut(|s| {
            s.check_sources(self.config.clone());
        });
    }
}

impl State {
    /// Add an item to the queue.
    /// If the FetchKey does not already exist, add it to the end of the queue.
    /// If the FetchKey exists, add the new source and merge the context in, without
    /// changing the position in the queue.
    pub fn push(&mut self, config: &dyn FetchPoolConfig, args: FetchPoolPush) {
        let FetchPoolPush {
            key,
            context,
            space,
            source,
            size,
            transfer_method,
        } = args;

        // Register sources once as they are discovered, with a default initial state
        self.sources.entry(source.clone()).or_default();

        match self.queue.entry(key) {
            Entry::Vacant(e) => {
                let sources = Sources::new([source.clone()]);
                let item = FetchPoolItem {
                    sources,
                    space,
                    size,
                    first_transfer_info: (transfer_method, Timestamp::now()),
                    context,
                    pending_response: None,
                };
                e.insert(item);
            }
            Entry::Occupied(mut e) => {
                let v = e.get_mut();
                v.sources.add(source.clone());
                v.context = match (v.context.take(), context) {
                    (Some(a), Some(b)) => Some(config.merge_fetch_contexts(*a, *b).into()),
                    (Some(a), None) => Some(a),
                    (None, Some(b)) => Some(b),
                    (None, None) => None,
                }
            }
        }
    }

    /// Poll for a batch of queue items to fetch. The size of the batch is determined by [`FetchPoolConfig::fetch_batch_size`].
    /// Items which are accessed while trying to fill the batch will be moved to the end of the queue. This is the case
    /// even if the item was not returned in the batch because it was waiting for a response already.
    pub fn get_batch(
        &mut self,
        config: Arc<dyn FetchPoolConfig>,
    ) -> Vec<(FetchKey, KSpace, FetchSource, Option<FetchContext>)> {
        let batch_size = config.fetch_batch_size();

        let mut to_fetch = vec![];
        // The queue provides a `front` method which will repeatedly loop over all the items it contains to bound the
        // search by the size of the queue.
        for _ in 0..self.queue.len() {
            // If we have enough items, stop looking
            if to_fetch.len() >= batch_size {
                break;
            }

            // Get the next item from the queue
            let (key, item) = match self.queue.front() {
                Some(item) => item,
                None => continue,
            };

            // Check for a pending response on this item
            let should_fetch_item = match &item.pending_response {
                Some(pending_response) => {
                    if pending_response.when.elapsed() > config.item_retry_delay() {
                        if let Some(state) = self.sources.get_mut(&pending_response.source) {
                            state.record_timeout();
                        }
                        true
                    } else {
                        false
                    }
                }
                None => true,
            };

            if should_fetch_item {
                // Clear the last fetch state if it was set. Even if there are no sources and we don't do a fetch, we want to forget
                // the previous request if we're planning to make a new one.
                item.pending_response = None;

                // Find the next source for this item which is in good standing across other fetches
                if let Some(source) = item.sources.next(|source| {
                    match self.sources.get_mut(source) {
                        Some(state) => {
                            if state.should_use() {
                                return true;
                            }
                        }
                        _ => {
                            tracing::warn!(
                                "Not considering source because it is not registered: {:?}",
                                source
                            );
                        }
                    }

                    false
                }) {
                    let space = item.space.clone();
                    item.pending_response = Some(PendingItemResponse {
                        when: Instant::now(),
                        source: source.clone(),
                    });
                    to_fetch.push((key.clone(), space, source, item.context));
                }
            }
        }

        to_fetch
    }

    /// When an item has been successfully fetched, we can remove it from the queue.
    pub fn remove(&mut self, key: &FetchKey) -> Option<FetchPoolItem> {
        match self.queue.remove(key) {
            Some(item) => {
                if let Some(pending) = item.pending_response.as_ref() {
                    if let Some(state) = self.sources.get_mut(&pending.source) {
                        state.record_response();
                    }
                }

                Some(item)
            }
            None => None,
        }
    }

    /// Check for sources which have expired and remove them from the list of sources.
    /// Any ops which don't have any sources left will be removed from the queue.
    pub fn check_sources(&mut self, config: FetchConfig) {
        self.sources
            .retain(|_, source| source.is_valid(config.clone()));

        // Drop any sources we are no longer using from the sources used by items
        let keys: Vec<_> = self.queue.keys().cloned().collect();
        for key in keys {
            self.queue
                .get_mut(&key)
                .expect("Iterating keys")
                .sources
                .retain(|s| self.sources.contains_key(s));

            // If we've removed all sources from an item, remove the item
            if self
                .queue
                .get(&key)
                .expect("Iterating keys")
                .sources
                .is_empty()
            {
                self.queue.remove(&key);
            }
        }
    }

    /// Get a string summary of the queue's contents
    #[cfg(any(test, feature = "test_utils"))]
    #[allow(dead_code)]
    pub fn summary(&self) -> String {
        use human_repr::HumanCount;

        let table = self
            .queue
            .iter()
            .map(|(k, v)| {
                let key = match k {
                    FetchKey::Op(hash) => {
                        let h = hash.to_string();
                        format!("{}..{}", &h[0..4], &h[h.len() - 4..])
                    }
                };

                let size = v.size.unwrap_or_default().get();
                format!(
                    "{:10}  {:^6} {:^6} {:>6}",
                    key,
                    v.sources.len(),
                    v.pending_response
                        .as_ref()
                        .map(|t| format!("{:?}", t.when.elapsed()))
                        .unwrap_or_else(|| "-".to_string()),
                    size.human_count_bytes(),
                )
            })
            .collect::<Vec<_>>()
            .join("\n");
        format!("{}\n{} items total", table, self.queue.len())
    }

    /// The heading to go along with the summary
    #[cfg(any(test, feature = "test_utils"))]
    #[allow(dead_code)]
    pub fn summary_heading() -> String {
        format!("{:10}  {:>6} {:>6} {}", "key", "#src", "last", "size")
    }
}

/// An item in the queue, corresponding to a single op or region to fetch
#[derive(Debug, PartialEq, Eq)]
#[cfg_attr(test, derive(Clone))]
pub struct FetchPoolItem {
    /// Known sources from whom we can fetch this item.
    /// Sources will always be tried in order.
    sources: Sources,
    /// The space to retrieve this op from
    space: KSpace,
    /// Approximate size of the item. If set, the item will be counted towards overall progress.
    size: Option<RoughInt>,
    /// The timestamp of and method by which this item was first transferred.
    /// Does not capture the case where an item is transferred by multiple sources
    /// and multiple methods.
    first_transfer_info: (TransferMethod, Timestamp),
    /// Opaque user data specified by the host
    pub context: Option<FetchContext>,
    /// If there is a response pending for this item then track the source and when the request was made.
    pending_response: Option<PendingItemResponse>,
}

/// Tracks the source and when a request was made for a [`FetchPoolItem`]. This is used to track timeouts
/// for sources that don't respond before the configured timeout.
#[derive(Debug, PartialEq, Eq)]
#[cfg_attr(test, derive(Clone))]
pub struct PendingItemResponse {
    when: Instant,
    source: FetchSource,
}

#[cfg(test)]
mod tests {
    use crate::backoff::BACKOFF_RETRY_COUNT;
    use crate::test_utils::*;
    use crate::TransferMethod;
    use kitsune_p2p_types::fetch_pool::GossipType;
    use pretty_assertions::assert_eq;
    use rand::{Rng, RngCore};
    use std::collections::HashSet;
    use std::{sync::Arc, time::Duration};

    use kitsune_p2p_types::bin_types::{KitsuneBinType, KitsuneSpace};

    use super::*;

    pub(super) fn item(
        _cfg: Arc<dyn FetchPoolConfig>,
        sources: Vec<FetchSource>,
        context: Option<FetchContext>,
    ) -> FetchPoolItem {
        FetchPoolItem {
            sources: Sources::new(sources),
            space: Arc::new(KitsuneSpace::new(vec![0; 36])),
            context,
            first_transfer_info: (TransferMethod::Publish, Timestamp::now()),
            size: None,
            pending_response: None,
        }
    }

    fn create_test_sources(count: usize) -> Vec<FetchSource> {
        test_sources(std::iter::repeat_with(|| rand::thread_rng().gen()).take(count))
    }

    #[test]
    fn state_keeps_context_on_merge_if_new_is_none() {
        let mut q = State::default();
        let cfg = TestFetchConfig(1, 1);

        q.push(&cfg, test_req_op(1, test_ctx(1), test_source(1)));
        assert_eq!(test_ctx(1), q.queue.front().unwrap().1.context);

        // Same key but different source so that it will merge and no context set to check how that is merged
        q.push(&cfg, test_req_op(1, None, test_source(0)));
        assert_eq!(test_ctx(1), q.queue.front().unwrap().1.context);
    }

    #[test]
    fn state_adds_context_on_merge_if_current_is_none() {
        let mut q = State::default();
        let cfg = TestFetchConfig(1, 1);

        // Initially have no context
        q.push(&cfg, test_req_op(1, None, test_source(1)));
        assert_eq!(None, q.queue.front().unwrap().1.context);

        // Now merge with a context
        q.push(&cfg, test_req_op(1, test_ctx(1), test_source(0)));
        assert_eq!(test_ctx(1), q.queue.front().unwrap().1.context);
    }

    #[test]
    fn state_can_merge_two_items_without_contexts() {
        let mut q = State::default();
        let cfg = TestFetchConfig(1, 1);

        // Initially have no context
        q.push(&cfg, test_req_op(1, None, test_source(1)));
        assert_eq!(None, q.queue.front().unwrap().1.context);

        // Now merge with no context
        q.push(&cfg, test_req_op(1, None, test_source(0)));

        // Still no context
        assert_eq!(None, q.queue.front().unwrap().1.context);
        // but both sources are present
        assert_eq!(2, q.queue.front().unwrap().1.sources.len());
    }

    #[test]
    fn state_ignores_duplicate_sources_on_merge() {
        let mut q = State::default();
        let cfg = TestFetchConfig(1, 1);

        q.push(&cfg, test_req_op(1, test_ctx(1), test_source(1)));
        assert_eq!(1, q.queue.front().unwrap().1.sources.len());

        // Set a different context but otherwise the same operation as above
        q.push(&cfg, test_req_op(1, test_ctx(2), test_source(1)));
        assert_eq!(1, q.queue.front().unwrap().1.sources.len());
    }

    #[test]
    fn queue_push() {
        let mut q = State::default();
        let cfg = Arc::new(TestFetchConfig(1, 1));

        let ts = Timestamp::now();

        // note: new sources get added to the back of the list
        q.push(&*cfg, test_req_op(1, test_ctx(0), test_source(0)));
        q.push(&*cfg, test_req_op(1, test_ctx(1), test_source(1)));

        q.push(&*cfg, test_req_op(2, test_ctx(0), test_source(0)));

        fn update_timestamp(
            ts: Timestamp,
            q: &MapQueue<FetchKey, FetchPoolItem>,
        ) -> Vec<(FetchKey, FetchPoolItem)> {
            (**q)
                .clone()
                .into_iter()
                .map(|(key, mut item)| {
                    item.first_transfer_info.1 = ts;
                    (key, item)
                })
                .collect()
        }

        let expected_ready: MapQueue<FetchKey, FetchPoolItem> = [
            (
                test_key_op(1),
                item(cfg.clone(), test_sources(0..=1), test_ctx(1)),
            ),
            (test_key_op(2), item(cfg, test_sources([0]), test_ctx(0))),
        ]
        .into_iter()
        .map(|(key, mut item)| {
            // set the timestamp to match
            item.first_transfer_info.1 = ts;
            (key, item)
        })
        .collect();

        // Need to set timestamps equal for equality check
        assert_eq!(
            update_timestamp(ts, &q.queue),
            update_timestamp(ts, &expected_ready)
        )
    }

    #[tokio::test(start_paused = true)]
    async fn queue_next() {
        let cfg = Arc::new(TestFetchConfig(5, 10));
        let mut q = {
            let mut queue = [
                (
                    test_key_op(1),
                    item(cfg.clone(), test_sources(0..=2), test_ctx(1)),
                ),
                (
                    test_key_op(2),
                    item(cfg.clone(), test_sources(1..=3), test_ctx(1)),
                ),
                (
                    test_key_op(3),
                    item(cfg.clone(), test_sources(2..=4), test_ctx(1)),
                ),
            ];

            queue[1].1.pending_response = Some(PendingItemResponse {
                when: Instant::now() - Duration::from_secs(3),
                source: test_source(1),
            });

            let queue = queue.into_iter().collect();
            State {
                queue,
                sources: test_sources(0..=4)
                    .into_iter()
                    .map(|s| (s, SourceState::default()))
                    .collect(),
            }
        };

        // We can try fetching items one source at a time by waiting 1 sec in between

        assert_eq!(2, q.get_batch(cfg.clone()).len());

        tokio::time::advance(Duration::from_secs(3)).await;

        assert_eq!(1, q.get_batch(cfg.clone()).len());

        tokio::time::advance(Duration::from_secs(10)).await;

        assert_eq!(3, q.get_batch(cfg.clone()).len());
    }

    #[tokio::test(start_paused = true)]
    async fn uses_all_sources() {
        let cfg = Arc::new(TestFetchConfig(1, 10));
        let num_items = 10;

        let mut q = {
            let mut queue = vec![];
            let mut sources = vec![];
            for i in 0..num_items {
                let these_sources =
                    test_sources((i * num_items) as u8..(i * num_items + num_items) as u8);
                queue.push((
                    test_key_op(i as u8),
                    // Give each item a different set of sources
                    item(cfg.clone(), these_sources.clone(), test_ctx(1)),
                ));

                sources.extend(these_sources);
            }

            State {
                queue: queue.into_iter().collect(),
                sources: sources
                    .into_iter()
                    .map(|s| (s, SourceState::default()))
                    .collect(),
            }
        };

        let mut seen_sources = HashSet::new();
        for _ in 0..num_items {
            q.get_batch(cfg.clone())
                .into_iter()
                .map(|item| match item.2 {
                    FetchSource::Agent(a) => a.0.clone(),
                })
                .for_each(|source| {
                    seen_sources.insert(source);
                });

            // Move time forwards so everything will be ready to retry
            tokio::time::advance(Duration::from_secs(30)).await;
        }

        assert_eq!(num_items * num_items, seen_sources.len());
    }

    #[tokio::test(start_paused = true)]
    async fn remove_fetch_item() {
        holochain_trace::test_run();

        let cfg = Arc::new(TestFetchConfig(1, 10));
        let mut q: State = {
            let queue = [(
                test_key_op(1),
                item(cfg.clone(), test_sources([1]), test_ctx(1)),
            )];
            let queue = queue.into_iter().collect();

            let sources = [(test_source(1), SourceState::default())]
                .into_iter()
                .collect();
            State { queue, sources }
        };

        assert_eq!(1, q.get_batch(cfg.clone()).len());
        q.remove(&test_key_op(1));

        // Move time forwards to be able to retry the item
        tokio::time::advance(Duration::from_secs(30)).await;

        assert_eq!(0, q.get_batch(cfg).len());
    }

    #[tokio::test(start_paused = true)]
    async fn fetch_pool() {
        // Use a nearly real fetch config.
        struct TestFetchConfig {}
        impl FetchPoolConfig for TestFetchConfig {
            // Don't really care about this, but the default trait functions for timeouts are wanted for this test
            fn merge_fetch_contexts(&self, a: u32, b: u32) -> u32 {
                a | b
            }
        }

        // Create a fetch pool to test
        let fetch_pool = FetchPool::new(Arc::new(TestFetchConfig {}));

        // Some sources will be unavailable for blocks of time
        let unavailable_sources: HashSet<FetchSource> =
            create_test_sources(10).into_iter().collect();

        // Add one item that will never send
        fetch_pool.push(FetchPoolPush {
            key: test_key_op(220),
            space: test_space(rand::thread_rng().gen()),
            source: unavailable_sources.iter().last().cloned().unwrap(),
            size: None, // Not important for this test
            context: test_ctx(rand::thread_rng().next_u32()),
            transfer_method: TransferMethod::Gossip(GossipType::Recent),
        });

        let mut failed_count = 0;
        for i in (0..200).step_by(5) {
            // Add five items to fetch
            for _ in 0..5 {
                fetch_pool.push(FetchPoolPush {
                    key: test_key_op(i),
                    space: test_space(rand::thread_rng().gen()),
                    source: test_source(rand::thread_rng().gen()),
                    size: None, // Not important for this test
                    context: test_ctx(rand::thread_rng().next_u32()),
                    transfer_method: TransferMethod::Gossip(GossipType::Recent),
                });
            }

            // Try to process all items (because that's how this is used in practice)
            let items = fetch_pool.get_items_to_fetch();
            for item in items {
                // If the source is available the fetch succeeds and we remove the item, otherwise leave it in the pool
                if !unavailable_sources.contains(&item.2) {
                    fetch_pool.remove(&item.0);
                } else {
                    failed_count += 1;
                }
            }

            // Advance time to allow items retry with a difference source if necessary
            tokio::time::advance(fetch_pool.config.item_retry_delay()).await;
        }

        // We created an item that will always fail, so should have at least one left
        assert!(
            !fetch_pool.get_items_to_fetch().is_empty(),
            "Pool should have had at least one item but got \n {}",
            fetch_pool.state.share_ref(|s| format!(
                "{}\n{}",
                State::summary_heading(),
                s.summary()
            ))
        );

        // 10 accounted for by the item we've set up to never succeed, possible to get more but not guaranteed to not
        // asserting
        assert!(
            failed_count >= 10,
            "At least 10 items should have failed to be fetched but was {}",
            failed_count
        );
    }

    #[test]
    fn check_item_missing() {
        let fetch_pool = FetchPool::new(Arc::new(TestFetchConfig(1, 1)));
        assert_eq!((false, None), fetch_pool.check_item(&test_key_op(1)));
    }

    #[test]
    fn drain_fetch_pool() {
        // Use a nearly real fetch config.
        struct TestFetchConfig {}
        impl FetchPoolConfig for TestFetchConfig {
            // Don't really care about this, but the default trait functions for timeouts are wanted for this test
            fn merge_fetch_contexts(&self, a: u32, b: u32) -> u32 {
                a | b
            }
        }

        // Create a fetch pool to test
        let fetch_pool = FetchPool::new(Arc::new(TestFetchConfig {}));

        for i in (0..200).step_by(5) {
            for j in 0..5 {
                fetch_pool.push(FetchPoolPush {
                    key: test_key_op(i),
                    space: test_space(j),
                    source: test_source(j),
                    size: None, // Not important for this test
                    context: test_ctx(0),
                    transfer_method: TransferMethod::Gossip(GossipType::Recent),
                });
            }
        }

        for _ in 0..2 {
            for (key, _, _, _) in fetch_pool.get_items_to_fetch() {
                if fetch_pool.check_item(&key).0 {
                    fetch_pool.remove(&key);
                }
            }
        }

        assert!(fetch_pool.is_empty());
        assert_eq!(0, fetch_pool.get_items_to_fetch().len());
    }

    #[tokio::test(start_paused = true)]
    async fn drop_expired_sources() {
        let config = Arc::new(TestFetchConfig(1, 1));
        let fetch_pool = FetchPool::new(config.clone());

        // First op with one source
        fetch_pool.push(FetchPoolPush {
            key: test_key_op(1),
            space: test_space(1),
            source: test_source(1),
            size: None,
            context: test_ctx(0),
            transfer_method: TransferMethod::Gossip(GossipType::Recent),
        });

        // Second op with two sources
        fetch_pool.push(FetchPoolPush {
            key: test_key_op(2),
            space: test_space(1),
            source: test_source(1),
            size: None,
            context: test_ctx(0),
            transfer_method: TransferMethod::Gossip(GossipType::Recent),
        });

        // Add the second source to the op above
        fetch_pool.push(FetchPoolPush {
            key: test_key_op(2),
            space: test_space(1),
            source: test_source(2),
            size: None,
            context: test_ctx(0),
            transfer_method: TransferMethod::Gossip(GossipType::Recent),
        });

        // Send enough ops for the first source to be put on a backoff
        for _ in 0..(config.source_unavailable_timeout_threshold() + 1) {
            fetch_pool.get_items_to_fetch();

            // Wait long enough for items to be retried
            tokio::time::advance(2 * config.item_retry_delay()).await;
        }

        // Check sources to mark the first source on a backoff
        fetch_pool.check_sources();

        for _ in 0..BACKOFF_RETRY_COUNT {
            // Need to wait by both the source and item retry delays, accounting for source delays being increased in the backoff
            tokio::time::advance(1000 * config.source_retry_delay()).await;

            assert_eq!(2, fetch_pool.get_items_to_fetch().len());
        }

        let keep_source_two_alive_key = test_key_op(5);
        fetch_pool.push(FetchPoolPush {
            key: keep_source_two_alive_key.clone(),
            space: test_space(1),
            source: test_source(2),
            size: None,
            context: test_ctx(0),
            transfer_method: TransferMethod::Gossip(GossipType::Recent),
        });

        // Verify the item is in the pool and remove it again to mark a successful fetch for source 2
        assert!(fetch_pool.check_item(&keep_source_two_alive_key).0);
        fetch_pool.remove(&keep_source_two_alive_key);

        // Now check sources to remove source 1 which hasn't had a successful receive in the backoff period
        fetch_pool.check_sources();

        // Should have dropped source 1 from the pool, which means op 1 is gone and op 2 should only have 1 source
        assert_eq!(1, fetch_pool.len());

        // Wait for the first item to be ready again
        tokio::time::advance(2 * config.item_retry_delay()).await;

        let batch = fetch_pool.get_items_to_fetch();
        assert_eq!(1, batch.len());
        assert_eq!(test_source(2), batch.first().unwrap().2);
    }

    #[test]
    fn default_fetch_context_merge_maintains_flags_from_both_contexts() {
        const FLAG_1: u32 = 1 << 5;
        const FLAG_2: u32 = 1 << 10;

        let context_1 = FetchContext(FLAG_1);
        let context_2 = FetchContext(FLAG_2);

        let pool = FetchPool::new_bitwise_or();
        let merged = pool.config.merge_fetch_contexts(*context_1, *context_2);

        assert_eq!(FLAG_1, merged & FLAG_1);
        assert_eq!(FLAG_2, merged & FLAG_2);
        assert_eq!(0, merged ^ (FLAG_1 | FLAG_2)); // Clear FLAG_1 and FLAG_2 to check no other bits are set
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/queue.rs
================================================
use std::ops::Deref;

use indexmap::{map::Entry, IndexMap};

#[derive(Debug, Eq, PartialEq)]
pub(crate) struct MapQueue<K: Eq + std::hash::Hash, V> {
    inner: IndexMap<K, V>,
    index: usize,
}

impl<K, V> Default for MapQueue<K, V>
where
    K: Eq + std::hash::Hash,
{
    fn default() -> Self {
        Self::new()
    }
}

impl<K, V> Deref for MapQueue<K, V>
where
    K: Eq + std::hash::Hash,
{
    type Target = IndexMap<K, V>;

    fn deref(&self) -> &Self::Target {
        &self.inner
    }
}

impl<K, V> MapQueue<K, V>
where
    K: Eq + std::hash::Hash,
{
    pub(crate) fn new() -> Self {
        Self {
            inner: IndexMap::new(),
            index: 0,
        }
    }

    pub(crate) fn front(&mut self) -> Option<(&K, &mut V)> {
        if self.inner.is_empty() {
            return None;
        }

        let fetch_index = self.index;
        self.index = (self.index + 1) % self.inner.len();

        self.inner.get_index_mut(fetch_index)
    }

    pub(crate) fn entry(&mut self, key: K) -> Entry<K, V> {
        self.inner.entry(key)
    }

    pub(crate) fn remove(&mut self, key: &K) -> Option<V> {
        // Fast but does change order so the element that is currently last will take the position of this element
        self.inner.swap_remove(key)
    }

    pub(crate) fn get_mut(&mut self, key: &K) -> Option<&mut V> {
        self.inner.get_mut(key)
    }
}

impl<K, V> FromIterator<(K, V)> for MapQueue<K, V>
where
    K: Eq + std::hash::Hash,
{
    fn from_iter<T: IntoIterator<Item = (K, V)>>(iter: T) -> Self {
        Self {
            inner: iter.into_iter().collect(),
            index: 0,
        }
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/respond.rs
================================================
use kitsune_p2p_types::{KOpData, KSpace};
use std::sync::Arc;

/// Drop this when response sending is complete.
pub struct FetchResponseGuard(#[allow(dead_code)] tokio::sync::oneshot::Sender<()>);

#[cfg(any(test, feature = "test_utils"))]
impl FetchResponseGuard {
    /// Create a new FetchResponseGuard for testing.
    pub fn new(inner: tokio::sync::oneshot::Sender<()>) -> Self {
        Self(inner)
    }
}

/// Customization by code making use of the FetchResponseQueue.
pub trait FetchResponseConfig: 'static + Clone + Send + Sync {
    /// Data that is forwarded.
    type User: 'static + Send;

    /// Byte count allowed to be outstanding.
    /// Any ops requested to be enqueued over this amount
    /// will be dropped without responding.
    fn byte_limit(&self) -> u32 {
        64 * 1024 * 1024
    }

    /// Number of concurrent sends to allow.
    fn concurrent_send_limit(&self) -> u32 {
        1
    }

    /// Send this fetch response.
    fn respond(
        &self,
        space: KSpace,
        user: Self::User,
        completion_guard: FetchResponseGuard,
        op: KOpData,
    );
}

/// Manage responding to requests for data.
#[derive(Clone)]
pub struct FetchResponseQueue<C: FetchResponseConfig> {
    byte_limit: Arc<tokio::sync::Semaphore>,
    concurrent_send_limit: Arc<tokio::sync::Semaphore>,
    config: Arc<C>,
    /// For testing, track the number of bytes sent.
    #[cfg(feature = "test_utils")]
    pub bytes_sent: Arc<std::sync::atomic::AtomicUsize>,
}

impl<C: FetchResponseConfig> FetchResponseQueue<C> {
    /// Construct a new response queue.
    pub fn new(config: C) -> Self {
        let byte_limit = Arc::new(tokio::sync::Semaphore::new(config.byte_limit() as usize));
        let concurrent_send_limit = Arc::new(tokio::sync::Semaphore::new(
            config.concurrent_send_limit() as usize,
        ));
        let config = Arc::new(config);
        Self {
            byte_limit,
            concurrent_send_limit,
            config,
            #[cfg(feature = "test_utils")]
            bytes_sent: Arc::new(std::sync::atomic::AtomicUsize::new(0)),
        }
    }

    /// Enqueue an op to be sent to a remote.
    pub fn enqueue_op(&self, space: KSpace, user: C::User, op: KOpData) -> bool {
        let len = op.size();

        // Don't try to take more permits than the byte_limit has.
        if len > self.config.byte_limit() as usize {
            tracing::error!(
                "op size is over configured limit {}",
                self.config.byte_limit()
            );
            return false;
        }

        let len = len as u32;

        let byte_permit = match self.byte_limit.clone().try_acquire_many_owned(len) {
            Err(_) => {
                tracing::warn!(%len, "fetch responder overloaded, dropping op");
                return false;
            }
            Ok(permit) => permit,
        };

        #[cfg(feature = "test_utils")]
        self.bytes_sent
            .fetch_add(len as usize, std::sync::atomic::Ordering::SeqCst);

        let c_limit = self.concurrent_send_limit.clone();
        let config = self.config.clone();
        tokio::task::spawn(async move {
            let _byte_permit = byte_permit;

            let _c_permit = match c_limit.acquire_owned().await {
                Err(_) => {
                    tracing::error!("Unexpected closed semaphore for concurrent_send_limit");
                    return;
                }
                Ok(permit) => permit,
            };

            let (s, r) = tokio::sync::oneshot::channel();

            let guard = FetchResponseGuard(s);

            config.respond(space, user, guard, op);

            // we don't care about the response... in fact
            // it's *always* an error, because we drop it.
            let _ = r.await;
        });

        true
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use kitsune_p2p_types::bin_types::{KitsuneBinType, KitsuneSpace};
    use std::sync::Mutex;

    struct TestConfInner {
        pub byte_limit: u32,
        pub concurrent_send_limit: u32,
        pub responds: Vec<(KSpace, &'static str, FetchResponseGuard, KOpData)>,
    }

    #[derive(Clone)]
    struct TestConf(Arc<Mutex<TestConfInner>>);

    impl TestConf {
        pub fn new(byte_limit: u32, concurrent_send_limit: u32) -> Self {
            Self(Arc::new(Mutex::new(TestConfInner {
                byte_limit,
                concurrent_send_limit,
                responds: Vec::new(),
            })))
        }

        pub fn drain_responds(&self) -> Vec<(KSpace, &'static str, FetchResponseGuard, KOpData)> {
            std::mem::take(&mut self.0.lock().unwrap().responds)
        }
    }

    impl FetchResponseConfig for TestConf {
        type User = &'static str;

        fn byte_limit(&self) -> u32 {
            self.0.lock().unwrap().byte_limit
        }

        fn concurrent_send_limit(&self) -> u32 {
            self.0.lock().unwrap().concurrent_send_limit
        }

        fn respond(&self, space: KSpace, user: Self::User, g: FetchResponseGuard, op: KOpData) {
            self.0.lock().unwrap().responds.push((space, user, g, op));
        }
    }

    #[test]
    fn config_provides_defaults() {
        #[derive(Clone)]
        struct DefaultConf;
        impl FetchResponseConfig for DefaultConf {
            type User = ();

            fn respond(
                &self,
                _space: KSpace,
                _user: Self::User,
                _completion_guard: FetchResponseGuard,
                _op: KOpData,
            ) {
                unreachable!()
            }
        }

        let config = DefaultConf;
        assert!(config.byte_limit() > 0);
        assert!(config.concurrent_send_limit() > 0);
    }

    #[test]
    fn queue_uses_input_config() {
        let config = TestConf::new(1024, 1);
        let queue = FetchResponseQueue::new(config.clone());

        // Check that the queue config is based on the input config.
        assert_eq!(
            config.byte_limit(),
            queue.byte_limit.available_permits() as u32
        );
        assert_eq!(
            config.concurrent_send_limit(),
            queue.concurrent_send_limit.available_permits() as u32
        );

        // Check that updating the input config DOES NOT update the queue config.
        // TODO They may as well be properties rather than functions
        config.0.lock().unwrap().byte_limit = 1;
        config.0.lock().unwrap().concurrent_send_limit = 2;

        assert_ne!(
            config.byte_limit(),
            queue.byte_limit.available_permits() as u32
        );
        assert_ne!(
            config.concurrent_send_limit(),
            queue.concurrent_send_limit.available_permits() as u32
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn enqueue_op_single() {
        let config = TestConf::new(1024, 1);

        let q = FetchResponseQueue::new(config.clone());
        assert_eq!(0, config.drain_responds().len());

        assert!(q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "noodle",
            Arc::new(b"hello".to_vec().into()),
        ));

        tokio::time::sleep(std::time::Duration::from_millis(10)).await;

        assert_eq!(1, config.drain_responds().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn enqueue_op_drops_large_op() {
        let config = TestConf::new(1024, 1);
        let q = FetchResponseQueue::new(config.clone());

        assert!(!q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "lots-of-bytes",
            Arc::new([0; 1040].to_vec().into()),
        ));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn enqueue_op_with_insufficient_capacity_remaining() {
        let config = TestConf::new(1024, 1);
        let q = FetchResponseQueue::new(config.clone());

        assert!(q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "lots-of-bytes",
            Arc::new([0; 1000].to_vec().into()),
        ));

        assert!(!q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "lots-of-bytes",
            Arc::new([0; 100].to_vec().into()),
        ));
    }

    // TODO This situation is never communicated back to the caller because `enqueue_op` is effectively fire and forget
    //      but it is actually a fatal condition.
    #[tokio::test(flavor = "multi_thread")]
    async fn handles_closed_semaphore() {
        let config = TestConf::new(1024, 1);
        let q = FetchResponseQueue::new(config.clone());

        assert!(q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "lots-of-bytes",
            Arc::new([0; 100].to_vec().into()),
        ));

        // Give the op time to queue
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;

        q.concurrent_send_limit.close();

        // The semaphore is closed but we only find that out inside the inner task so the enqueue should succeed.
        assert!(q.enqueue_op(
            Arc::new(KitsuneSpace::new(vec![0; 36])),
            "lots-of-bytes",
            Arc::new([0; 100].to_vec().into()),
        ));

        tokio::time::sleep(std::time::Duration::from_millis(10)).await;

        // But there will only be one op in the queue
        assert_eq!(1, config.drain_responds().len());
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/rough_sized.rs
================================================
use kitsune_p2p_types::KOpHash;
use std::hash::Hash;

/// The granularity once we're > i16::MAX
const GRAN: usize = 4096;

// TODO: try a u32 -> u16 mapping based on phi, the golden ratio, which keeps the expected error constant
//       at any scale. These are some initial calculations to try.
// const G: f64 = 1690.0; // x / (ln_phi 128000000)
// const LOW: f64 = 15.0 * G;
// const THRESH: u16 = 30000;

/// Roughly track an approximate integer value.
#[derive(Clone, Copy, Default, PartialEq, Eq, Hash, serde::Deserialize, serde::Serialize)]
pub struct RoughInt(i16);

impl std::fmt::Debug for RoughInt {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.get())
    }
}

impl RoughInt {
    /// Maximum value representable by RoughInt, currently 134_213_632
    pub const MAX: usize = i16::MAX as usize * GRAN;

    /// Get the full value from the rough int
    pub fn get(&self) -> usize {
        if self.0 > 0 {
            // positive is exact value
            self.0 as usize
        } else {
            // negative is in chunks of size GRAN
            (-self.0) as usize * GRAN
        }
    }

    /// Set the rough int from the full value
    pub fn set(&mut self, v: usize) -> Self {
        if v <= i16::MAX as usize {
            // if we're under i16::MAX, we can store the exact value
            self.0 = v as i16
        } else {
            // otherwise, divide by GRAN and store as negative
            self.0 = -(std::cmp::min(i16::MAX as usize, v / GRAN) as i16);
        }
        *self
    }
}

impl From<usize> for RoughInt {
    fn from(v: usize) -> Self {
        Self::default().set(v)
    }
}

/// An op hash combined with an approximate size of the op
pub type OpHashSized = RoughSized<KOpHash>;

/// Some data which has a RoughInt assigned for its size
#[derive(
    Clone,
    Debug,
    serde::Deserialize,
    serde::Serialize,
    derive_more::From,
    derive_more::Into,
    derive_more::Constructor,
    derive_more::Deref,
)]
pub struct RoughSized<T> {
    /// The data to be sized
    #[deref]
    data: T,
    /// The approximate size of the hash.
    // TODO: remove the option, which will require adding sizes for Recent gossip as well
    size: Option<RoughInt>,
}

impl<T> RoughSized<T> {
    /// Break into constituent parts
    pub fn into_inner(self) -> (T, Option<RoughInt>) {
        (self.data, self.size)
    }

    /// Accessor
    pub fn data_ref(&self) -> &T {
        &self.data
    }

    /// Accessor
    pub fn maybe_size(&self) -> Option<RoughInt> {
        self.size
    }

    /// Accessor
    pub fn size(&self) -> RoughInt {
        self.size.unwrap_or_default()
    }
}

impl<T: Clone> RoughSized<T> {
    /// Accessor
    pub fn data(&self) -> T {
        self.data.clone()
    }
}

impl<T: Hash> Hash for RoughSized<T> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        // size is omitted from the hash
        self.data.hash(state);
    }
}

impl<T: PartialEq> PartialEq for RoughSized<T> {
    fn eq(&self, other: &Self) -> bool {
        // size is omitted from the equality
        self.data == other.data
    }
}

impl<T: Eq> Eq for RoughSized<T> {}

#[cfg(test)]
mod tests {
    use super::*;
    use proptest::prelude::*;

    /// The percent error is always less than 12%, and the max error occurs right around
    /// i16::MAX, tapering off as values grow larger
    #[test]
    fn error_upper_bound() {
        let m16 = i16::MAX as usize;
        for m in [
            m16,
            m16 + 2,
            m16 + GRAN - 1,
            m16 + GRAN * 10 - 1,
            RoughInt::MAX - 1,
            RoughInt::MAX,
        ] {
            let r = RoughInt::from(m).get();
            let error = r.abs_diff(m) as f64 / m as f64;
            dbg!(r, m, error);
            assert!(error < 0.13);
        }
    }

    proptest! {
        #[test]
        fn roughint_roundtrip(v: usize) {
            let r = RoughInt::from(v);
            let v = r.get();
            assert_eq!(r, RoughInt::from(v));
        }

        #[test]
        fn roughint_always_underestimates(actual: usize) {
            let rough = RoughInt::from(actual);
            assert!(rough.get() <= actual);
        }

        /// Test that the sum of roughints is less than the max error for a single roughint,
        /// even in the most problematic range
        #[test]
        fn roughint_sum_error_problematic_range(
            real in proptest::collection::vec(i16::MAX as usize..i16::MAX as usize + GRAN, 1..10)
        ) {
            let rough = real.iter().copied().map(RoughInt::from).map(|r| r.get());
            let both: Vec<(usize, usize)> = real.iter().copied().zip(rough).collect();
            let real_sum: usize = both.iter().map(|(r, _)| r).sum();
            let rough_sum: usize = both.iter().map(|(_, r)| r).sum();

            if real_sum == 0 || rough_sum == 0 {
                unreachable!("zero sum");
            }

            let error = (rough_sum.abs_diff(real_sum)) as f64 / real_sum as f64;
            dbg!(error);
            assert!(error <= 0.13);
        }

        /// Test that the sum of roughints is less than the max error for a single roughint,
        /// across the range of all possible values
        fn roughint_sum_error_full_range(
            real in proptest::collection::vec(1..RoughInt::MAX, 1..10)
        ) {
            let rough = real.iter().copied().map(RoughInt::from).map(|r| r.get());
            let both: Vec<(usize, usize)> = real.iter().copied().zip(rough).collect();
            let real_sum: usize = both.iter().map(|(r, _)| r).sum();
            let rough_sum: usize = both.iter().map(|(_, r)| r).sum();

            if real_sum == 0 || rough_sum == 0 {
                unreachable!("zero sum");
            }

            let error = (rough_sum.abs_diff(real_sum)) as f64 / real_sum as f64;
            dbg!(error);
            assert!(error <= 0.13);
        }
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/source.rs
================================================
use indexmap::IndexSet;
use kitsune_p2p_types::KAgent;
use std::{default, ops::Deref};

use crate::{backoff::FetchBackoff, FetchConfig};

/// The number of times to probe a source between backoff attempts. This needs to be enough to reasonably allow
/// a source which might be slow to respond to one or two requests to respond to at least one of the probes but
/// not so high that it wastes time for this node trying to talk to a source that is not responding.
const NUM_PROBE_ATTEMPTS: u32 = 10;

/// A source to fetch from: either a node, or an agent on a node
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum FetchSource {
    /// An agent on a node
    Agent(KAgent),
}

/// Fetch item within the fetch queue state.
#[derive(Debug, PartialEq, Eq)]
#[cfg_attr(test, derive(Clone))]
pub(crate) struct Sources {
    inner: IndexSet<FetchSource>,
    index: usize,
}

impl Deref for Sources {
    type Target = IndexSet<FetchSource>;

    fn deref(&self) -> &Self::Target {
        &self.inner
    }
}

impl Sources {
    pub(crate) fn new(queue: impl IntoIterator<Item = FetchSource>) -> Self {
        Self {
            inner: queue.into_iter().collect(),
            index: 0,
        }
    }

    pub(crate) fn next(
        &mut self,
        mut state_filter: impl FnMut(&FetchSource) -> bool,
    ) -> Option<FetchSource> {
        for _ in 0..self.inner.len() {
            let fetch_index = self.index;
            self.index = (self.index + 1) % self.inner.len();

            if let Some(source) = self.inner.get_index(fetch_index) {
                if state_filter(source) {
                    return Some(source.clone());
                }
            }
        }

        None
    }

    pub(crate) fn add(&mut self, source: FetchSource) {
        self.inner.insert(source);
    }

    pub(crate) fn retain(&mut self, filter: impl Fn(&FetchSource) -> bool) {
        self.inner.retain(filter);

        // Ensure the index is still valid
        if !self.inner.is_empty() {
            self.index %= self.inner.len();
        }
    }
}

/// The state of a source
#[derive(Debug, Default)]
pub(crate) struct SourceState {
    /// The current state of the source
    current_state: SourceCurrentState,
}

impl SourceState {
    /// Check whether this source should be used. If this source is currently considered available then it will always be usable.
    /// Otherwise, when the source is in a backoff state, it will only be usable if the backoff is ready. The backoff will be ready
    /// a fixed number of times to probe the source before going back into a backoff state. If any fetches from the
    /// probe attempts succeed then the source will be considered available again.
    pub fn should_use(&mut self) -> bool {
        match &mut self.current_state {
            SourceCurrentState::Available(_) => true,
            SourceCurrentState::Backoff(backoff) => backoff.is_ready(),
        }
    }

    /// Check the state of this source. If the source has had too many timeouts then it is still considered valid but it will be put into a backoff state.
    /// If the source is in a backoff state and the backoff has expired, then the check fails and this source should be dropped.
    pub fn is_valid(&mut self, config: FetchConfig) -> bool {
        match &self.current_state {
            SourceCurrentState::Available(num_timed_out) => {
                if *num_timed_out > config.source_unavailable_timeout_threshold() {
                    self.current_state = SourceCurrentState::Backoff(FetchSourceBackoff::new(
                        FetchBackoff::new(config.source_retry_delay()),
                        NUM_PROBE_ATTEMPTS,
                    ));
                }

                true
            }
            SourceCurrentState::Backoff(ref backoff) => !backoff.is_expired(),
        }
    }

    /// Notify the state that a request to this source has timed out.
    pub fn record_timeout(&mut self) {
        if let SourceCurrentState::Available(num_timeouts) = &mut self.current_state {
            *num_timeouts += 1;
        }
    }

    /// Notify the state that a request to this source has succeeded.
    /// If the source is in a backoff state then it will be considered available again.
    pub fn record_response(&mut self) {
        match &self.current_state {
            SourceCurrentState::Backoff(_) => {
                self.current_state = SourceCurrentState::Available(0);
            }
            SourceCurrentState::Available(_) => (),
        }
    }
}

/// The state of a source
#[derive(Debug)]
pub enum SourceCurrentState {
    /// As far as we know, this source is available and responding.
    ///
    /// The inner value tracks the number of requests to this source that have timed out.
    /// Note that these failures do not age out, so if a source is unreliable it will get put on a timeout
    /// briefly after it fails to respond too many times. This isn't a bad thing if the source is
    /// not responding because it is overwhelmed.
    Available(usize),

    /// The source has been unavailable and we're waiting for a backoff period to expire before trying again.
    Backoff(FetchSourceBackoff),
}

impl default::Default for SourceCurrentState {
    fn default() -> Self {
        Self::Available(0)
    }
}

/// a struct
#[derive(Debug)]
pub struct FetchSourceBackoff {
    backoff: FetchBackoff,
    probe_limit: u32,
    probes: u32,
}

impl FetchSourceBackoff {
    fn new(backoff: FetchBackoff, probe_limit: u32) -> Self {
        Self {
            backoff,
            probe_limit,
            probes: 0,
        }
    }

    fn is_ready(&mut self) -> bool {
        if self.backoff.is_ready() {
            self.probes = self.probe_limit - 1; // Grant more probes for this retry
            true
        } else if self.probes > 0 {
            self.probes -= 1;
            true
        } else {
            // Probes exhausted, wait for the backoff to expire and grant more probes
            false
        }
    }

    fn is_expired(&self) -> bool {
        self.backoff.is_expired()
    }
}

#[cfg(test)]
mod tests {
    use std::{sync::Arc, time::Duration};

    use super::{SourceState, Sources, NUM_PROBE_ATTEMPTS};
    use crate::{
        backoff::{FetchBackoff, BACKOFF_RETRY_COUNT},
        source::FetchSourceBackoff,
        test_utils::*,
        FetchPoolConfig,
    };

    #[test]
    fn single_source() {
        let mut sources = Sources::new([test_source(1)]);

        // The first source is returned
        assert_eq!(Some(test_source(1)), sources.next(|_| true));
        // The first source is returned a second time with no delay
        assert_eq!(Some(test_source(1)), sources.next(|_| true));
        // The first source can be filtered out
        assert_eq!(None, sources.next(|_| false));
    }

    #[test]
    fn source_rotation() {
        let mut sources = Sources::new([]);
        sources.add(test_source(1));
        sources.add(test_source(2));

        assert_eq!(Some(test_source(1)), sources.next(|_| true));
        assert_eq!(Some(test_source(2)), sources.next(|_| true));
        assert_eq!(Some(test_source(1)), sources.next(|_| true));

        sources.add(test_source(3));
        assert_eq!(Some(test_source(2)), sources.next(|_| true));
        assert_eq!(Some(test_source(3)), sources.next(|_| true));
        assert_eq!(Some(test_source(1)), sources.next(|_| true));
    }

    #[tokio::test]
    async fn fetch_source_backoff() {
        let mut backoff = FetchSourceBackoff::new(FetchBackoff::new(Duration::from_millis(10)), 3);
        assert!(!backoff.is_ready());

        let mut num_tries = 0;
        tokio::time::timeout(Duration::from_secs(2), async {
            loop {
                if backoff.is_ready() {
                    num_tries += 1;
                } else if backoff.is_expired() {
                    break;
                }
            }
        })
        .await
        .unwrap();

        // Number of probes per ready (3), multiplied by the number of attempts that FetchBackoff allows (BACKOFF_RETRY_COUNT)
        assert_eq!(3 * BACKOFF_RETRY_COUNT, num_tries);
    }

    #[test]
    fn happy_path_source_state() {
        let mut source_state: SourceState = Default::default();
        let config = Arc::new(TestFetchConfig(1, 1));

        for i in 0..500 {
            assert!(source_state.should_use());
            source_state.record_response();

            if i % 100 == 0 {
                assert!(source_state.is_valid(config.clone()));
            }
        }

        assert!(source_state.should_use());
    }

    #[tokio::test(start_paused = true)]
    async fn source_state_single_backoff_then_recover() {
        let mut source_state: SourceState = Default::default();
        let config = Arc::new(TestFetchConfig(1, 1));

        assert!(source_state.should_use());

        // Exhaust the retries
        for _ in 0..=config.source_unavailable_timeout_threshold() {
            // The source should continue being used even with timeouts. It's only when we hit the limit that it shouldn't.
            assert!(source_state.should_use());

            // The check should keep passing
            source_state.is_valid(config.clone());

            // Record another timeout
            source_state.record_timeout();
        }

        // The source is still ready because it hasn't been checked
        assert!(source_state.should_use());

        // Now it goes into a backoff state
        source_state.is_valid(config.clone());
        assert!(!source_state.should_use());

        tokio::time::advance(Duration::from_secs(2)).await;

        // Now the backoff should go into a ready state and permit a number of probes
        for _ in 0..NUM_PROBE_ATTEMPTS {
            assert!(source_state.should_use());
        }

        // The probes have all been used and the backoff should be waiting again
        assert!(!source_state.should_use());

        // Now get a single successful response
        source_state.record_response();

        // Go back to a ready state
        assert!(source_state.should_use());
    }

    #[tokio::test(start_paused = true)]
    async fn source_state_backoff_to_expiry() {
        let mut source_state: SourceState = Default::default();
        let config = Arc::new(TestFetchConfig(1, 1));

        assert!(source_state.should_use());

        // Exhaust the retries
        for _ in 0..=config.source_unavailable_timeout_threshold() {
            source_state.record_timeout();
        }
        source_state.is_valid(config.clone());
        assert!(!source_state.should_use());

        for _ in 0..BACKOFF_RETRY_COUNT {
            // Just move by a lot to guarantee that we're back in a ready state, without hitting probes because that's irrelevant for this test
            tokio::time::advance(100 * config.source_retry_delay()).await;

            assert!(source_state.should_use());
        }

        // The source state is now dead and should be removed.
        assert!(!source_state.is_valid(config.clone()));
    }
}



================================================
File: crates/kitsune_p2p/fetch/src/test_utils.rs
================================================
//! Test utilities for the fetch crate.

use crate::source::FetchSource;
use crate::{FetchContext, FetchKey, FetchPoolPush, TransferMethod};
use kitsune_p2p_types::bin_types::{KitsuneAgent, KitsuneBinType, KitsuneOpHash, KitsuneSpace};
use kitsune_p2p_types::{KOpHash, KSpace};
use std::sync::Arc;

#[cfg(test)]
pub(super) struct TestFetchConfig(pub u32, pub u32);

#[cfg(test)]
impl crate::FetchPoolConfig for TestFetchConfig {
    fn item_retry_delay(&self) -> std::time::Duration {
        std::time::Duration::from_secs(self.0 as u64)
    }

    fn source_retry_delay(&self) -> std::time::Duration {
        std::time::Duration::from_secs(self.1 as u64)
    }

    fn merge_fetch_contexts(&self, a: u32, b: u32) -> u32 {
        (a + b).min(1)
    }
}

/// Create a sample op hash.
pub fn test_key_hash(n: u8) -> KOpHash {
    Arc::new(KitsuneOpHash::new(vec![n; 36]))
}

/// Create a sample FetchKey::Op.
pub fn test_key_op(n: u8) -> FetchKey {
    FetchKey::Op(test_key_hash(n))
}

/// Create a sample FetchPoolPush keyed with a FetchKey::Op.
pub fn test_req_op(n: u8, context: Option<FetchContext>, source: FetchSource) -> FetchPoolPush {
    FetchPoolPush {
        key: test_key_op(n),
        context,
        space: test_space(0),
        source,
        size: None,
        transfer_method: TransferMethod::Publish,
    }
}

/// Create a sample space.
pub fn test_space(i: u8) -> KSpace {
    Arc::new(KitsuneSpace::new(vec![i; 36]))
}

/// Create a sample FetchSource.
pub fn test_source(i: u8) -> FetchSource {
    FetchSource::Agent(Arc::new(KitsuneAgent::new(vec![i; 36])))
}

/// Create multiple sample [FetchSource]s at once.
pub fn test_sources(ix: impl IntoIterator<Item = u8>) -> Vec<FetchSource> {
    ix.into_iter().map(test_source).collect()
}

/// Create a sample [FetchContext].
pub fn test_ctx(c: u32) -> Option<FetchContext> {
    Some(c.into())
}



================================================
File: crates/kitsune_p2p/fetch/src/pool/pool_reader.rs
================================================
use std::collections::HashSet;

use kitsune_p2p_types::{fetch_pool::FetchPoolInfo, KSpace};

use crate::FetchPool;

/// Read-only access to the queue
#[derive(Debug, Clone, derive_more::From)]
pub struct FetchPoolReader(FetchPool);

impl FetchPoolReader {
    /// Get info about the queue, filtered by space
    pub fn info(&self, spaces: HashSet<KSpace>) -> FetchPoolInfo {
        let (count, bytes) = self.0.state.share_ref(|s| {
            s.queue
                .values()
                .filter(|v| spaces.contains(&v.space))
                .filter_map(|v| v.size.map(|s| s.get()))
                .fold((0, 0), |(c, s), t| (c + 1, s + t))
        });

        FetchPoolInfo {
            op_bytes_to_fetch: bytes,
            num_ops_to_fetch: count,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::test_utils::*;
    use crate::{pool::tests::*, State};
    use kitsune_p2p_types::tx_utils::ShareOpen;
    use std::sync::Arc;

    #[test]
    fn queue_info_empty() {
        let fetch_pool_reader = FetchPoolReader(FetchPool {
            config: Arc::new(TestFetchConfig(1, 1)),
            state: ShareOpen::new(Default::default()),
        });

        let info = fetch_pool_reader.info([test_space(0), test_space(1)].into_iter().collect());
        assert_eq!(0, info.op_bytes_to_fetch);
        assert_eq!(0, info.num_ops_to_fetch);
    }

    #[test]
    fn queue_info_fetch_no_spaces() {
        let cfg = Arc::new(TestFetchConfig(1, 1));
        let q = {
            let mut queue = [(
                test_key_op(1),
                item(cfg.clone(), test_sources(0..=2), test_ctx(1)),
            )];

            queue[0].1.size = Some(100.into());

            let queue = queue.into_iter().collect();
            FetchPoolReader(FetchPool {
                config: cfg,
                state: ShareOpen::new(State {
                    queue,
                    ..Default::default()
                }),
            })
        };

        let info = q.info([].into_iter().collect());

        assert_eq!(0, info.num_ops_to_fetch);
        assert_eq!(0, info.op_bytes_to_fetch);
    }

    #[test]
    fn queue_info() {
        let cfg = Arc::new(TestFetchConfig(1, 1));
        let q = {
            let mut queue = [
                (
                    test_key_op(1),
                    item(cfg.clone(), test_sources(0..=2), test_ctx(1)),
                ),
                (
                    test_key_op(2),
                    item(cfg.clone(), test_sources(1..=3), test_ctx(1)),
                ),
                (
                    test_key_op(3),
                    item(cfg.clone(), test_sources(2..=4), test_ctx(1)),
                ),
            ];

            queue[0].1.size = Some(100.into());
            queue[1].1.size = Some(1000.into());

            let queue = queue.into_iter().collect();
            FetchPoolReader(FetchPool {
                config: cfg,
                state: ShareOpen::new(State {
                    queue,
                    ..Default::default()
                }),
            })
        };

        let info = q.info([test_space(0)].into_iter().collect());
        // The item without a size is not returned.
        assert_eq!(info.num_ops_to_fetch, 2);
        assert_eq!(info.op_bytes_to_fetch, 1100);
    }

    #[test]
    fn queue_info_filter_spaces() {
        let cfg = Arc::new(TestFetchConfig(1, 1));
        let q = {
            let mut item_for_space_1 = item(cfg.clone(), test_sources(0..=2), test_ctx(1));
            item_for_space_1.space = test_space(1);
            item_for_space_1.size = Some(100.into());

            let mut item_for_space_2 = item(cfg.clone(), test_sources(0..=2), test_ctx(1));
            item_for_space_2.space = test_space(2);
            item_for_space_2.size = Some(500.into());

            let queue = [
                (test_key_op(1), item_for_space_1),
                (test_key_op(2), item_for_space_2),
            ];

            let queue = queue.into_iter().collect();
            FetchPoolReader(FetchPool {
                config: cfg,
                state: ShareOpen::new(State {
                    queue,
                    ..Default::default()
                }),
            })
        };

        let info_space_1 = q.info([test_space(1)].into_iter().collect());
        assert_eq!(info_space_1.num_ops_to_fetch, 1);
        assert_eq!(info_space_1.op_bytes_to_fetch, 100);

        let info_space_2 = q.info([test_space(2)].into_iter().collect());
        assert_eq!(info_space_2.num_ops_to_fetch, 1);
        assert_eq!(info_space_2.op_bytes_to_fetch, 500);

        let info_space_2 = q.info([test_space(1), test_space(2)].into_iter().collect());
        assert_eq!(info_space_2.num_ops_to_fetch, 2);
        assert_eq!(info_space_2.op_bytes_to_fetch, 600);
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/README.md
================================================
# kitsune_p2p

[![Project](https://img.shields.io/badge/project-holochain-blue.svg?style=flat-square)](http://holochain.org/)
[![Forum](https://img.shields.io/badge/chat-forum%2eholochain%2enet-blue.svg?style=flat-square)](https://forum.holochain.org)
[![Chat](https://img.shields.io/badge/chat-chat%2eholochain%2enet-blue.svg?style=flat-square)](https://chat.holochain.org)

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)

Current version: 0.0.1

P2p / dht communication framework.

## Contribute
Holochain is an open source project.  We welcome all sorts of participation and are actively working on increasing surface area to accept it.  Please see our [contributing guidelines](/CONTRIBUTING.md) for our general practices and protocols on participating in the community, as well as specific expectations around things like code formatting, testing practices, continuous integration, etc.

* Connect with us on our [forum](https://forum.holochain.org)

## License
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)

Copyright (C) 2019 - 2024, Holochain Foundation

This program is free software: you can redistribute it and/or modify it under the terms of the license
provided in the LICENSE file (Apache 2.0).  This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE.



================================================
File: crates/kitsune_p2p/kitsune_p2p/Cargo.toml
================================================
[package]
name = "kitsune_p2p"
version = "0.5.0-dev.13"
description = "p2p / dht communication framework"
license = "CAL-1.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
arrayref = "0.3.6"
base64 = "0.22"
bloomfilter = { version = "1.0.5", features = ["serde"] }
bytes = "1.4.0"
derive_more = "0.99"
futures = "0.3"
ghost_actor = "=0.3.0-alpha.6"
governor = "0.3.2"
kitsune_p2p_fetch = { version = "^0.5.0-dev.9", path = "../fetch" }
kitsune_p2p_mdns = { version = "^0.5.0-dev.2", path = "../mdns" }
kitsune_p2p_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp", features = [
  "now",
] }
kitsune_p2p_block = { version = "^0.5.0-dev.5", path = "../block" }
kitsune_p2p_bootstrap_client = { version = "^0.5.0-dev.11", path = "../bootstrap_client" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../bin_data" }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../types", default-features = false }
must_future = "0.1.1"
nanoid = "0.4"
num-traits = "0.2"
holochain_trace = { version = "^0.5.0-dev.1", path = "../../holochain_trace" }
once_cell = "1.4.1"
opentelemetry_api = { version = "=0.20.0", features = ["metrics"] }
parking_lot = "0.12.1"
rand = "0.8.5"
serde = { version = "1.0", features = ["derive"] }
serde_bytes = "0.11"
serde_json = { version = "1.0.51", features = ["preserve_order"] }
thiserror = "1.0.22"
tokio = { version = "1.36.0", features = ["full"] }
tracing = "0.1"
tokio-stream = "0.1"
tx5 = { version = "=0.1.5-beta" }
url2 = "0.0.6"

kitsune2_api = "0.0.1-alpha.1"

# fuzzing
contrafact = { version = "0.2.0-rc.1", optional = true }
proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }

blake2b_simd = { version = "1.0", optional = true }
maplit = { version = "1", optional = true }
mockall = { version = "0.11.3", optional = true }
sbd-server = { version = "=0.0.8-alpha", optional = true }
tx5-signal-srv = { version = "=0.0.16-alpha", optional = true }
fixt = { path = "../../fixt", version = "^0.5.0-dev.1", optional = true }

[dev-dependencies]
# include self with test_utils feature, to allow integration tests to run properly
kitsune_p2p = { path = ".", features = ["test_utils", "fuzzing", "sqlite"] }

kitsune_p2p_fetch = { path = "../fetch", features = ["test_utils"] }
kitsune_p2p_bootstrap = { path = "../bootstrap", features = ["sqlite"] }
kitsune_p2p_timestamp = { path = "../timestamp", features = [
  "now",
] }
kitsune_p2p_types = { path = "../types", features = ["test_utils"] }
maplit = "1"
pretty_assertions = "1.4.0"
sbd-server = "=0.0.8-alpha"
test-case = "3.3"
tokio = { version = "1.11", features = ["full", "test-util"] }
tracing-subscriber = "0.3.16"
itertools = "0.12"

[lints]
workspace = true

[features]

default = []

fuzzing = [
  "contrafact",
  "proptest",
  "proptest-derive",
  "kitsune_p2p_fetch/fuzzing",
  "kitsune_p2p_timestamp/fuzzing",
  "kitsune_p2p_types/fuzzing",
]

test_utils = [
  "blake2b_simd",
  "tokio/test-util",
  "ghost_actor/test_utils",
  "kitsune_p2p_types/test_utils",
  "kitsune_p2p_types/fixt",
  "kitsune_p2p_bin_data/fixt",
  "maplit",
  "mockall",
  "sbd-server",
  "tx5-signal-srv",
  "fixt",
]

mock_network = ["kitsune_p2p_types/test_utils", "mockall"]

instrument = []

unstable-sharding = [
  "kitsune_p2p_types/unstable-sharding",
]

sqlite-encrypted = [
  "kitsune_p2p_fetch/sqlite-encrypted",
  "kitsune_p2p_timestamp/sqlite-encrypted",
  "kitsune_p2p_block/sqlite-encrypted",
  "kitsune_p2p_types/sqlite-encrypted",
]
sqlite = [
  "kitsune_p2p_fetch/sqlite",
  "kitsune_p2p_timestamp/sqlite",
  "kitsune_p2p_block/sqlite",
  "kitsune_p2p_types/sqlite",
]



================================================
File: crates/kitsune_p2p/kitsune_p2p/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

- Prevent TODO comments from being rendered in cargo docs.

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

- Removed `network_type` from `KitsuneP2pConfig`

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

## 0.4.0-dev.17

- minor sbd rate limiting fix - tx5 webrtc buffer size fix - tx5 webrtc connection close fix - better tx5 connection tracing [\#4184](https://github.com/holochain/holochain/pull/4184)

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

- Change the default `DbSyncStrategy` from `Fast` to `Resilient` which should reduce database corruptions. You can override this setting in your conductor config if you wish to go faster and are willing to rebuild your database if it gets corrupted. \#4010

## 0.4.0-dev.8

- **BREAKING** Bumped KITSUNE\_PROTOCOL\_VERSION. This *should* have been bumped with https://github.com/holochain/holochain/pull/3842, but better late than never. [\#3984](https://github.com/holochain/holochain/pull/3984)

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

- Fix an issue with delegated publish where delegates were publishing to nodes near the target basis, rather than nodes covering the basis.

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.40

- **BREAKING** - AgentInfo uses a quantized arc instead of an arc half-length to represent DHT coverage. This is a breaking protocol change, nodes on different versions will not be able to gossip with each other.

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

- Kitsune will now close connections in two cases. Firstly, when receiving an updated agent info that indicates that the peer URL for an agent has changed. If there is an open connection to the old peer URL then it will be closed. Secondly, when if a message from a peer fails to decode. This is likely sign that communication between two conductors is not going to work so the connection is closed.

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

- *BREAKING* Adds a preflight check in tx5 which requires the equality of two things: a `KITSUNE_PROTOCOL_VERSION`, which is incremented every time there is a breaking protocol change in Kitsune, and an opaque `user_data` passed in by the host, which allows the host to specify its own compatibility requirements. This allows protocol incompatibilities to be explicitly handled and logged, rather than letting things silently and unpredictably fail in case of a mismatch in protocol datatypes.

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

- *BREAKING* Updates tx5 to a version using new endpoint state logic and a new incompatible protocol. [\#3287](https://github.com/holochain/holochain/pull/3287)

## 0.3.0-beta-dev.30

- Performance improvement by reducing the number of `query_agents` calls used by Kitsune. The host (Holochain conductor) responds to these queries using an in-memory store which is fast but all the queries go through the `ghost_actor` so making an excessive number of calls for the same information reduces the availability of the host for other calls. For a test which sets up 10 spaces (equivalent to a happ running on the host) this change takes the number of host queries for agent info from ~13k to ~1.4k. The removed calls were largely redundant since Kitsune refreshes agent info every 1s anyway so it shouldnt need to make many further calls between refreshes.

- Minor optimisation when delegate broadcasting ops, the delegated broadcasts will now avoid connecting back to the source. There is currently no way to prevent other agents that were delegated to from connecting to each other but this change takes care of one case.

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

- Gossip send failures and target expired events are now logged as warnings rather than errors, and have additional text for clarity. [\#2974](https://github.com/holochain/holochain/pull/2974)

## 0.3.0-beta-dev.22

- Update to a tx5 version that includes go code that is statically linked for all platform that support it. Windows and Android will remain dynamically linked. [\#2967](https://github.com/holochain/holochain/pull/2967)
- Change the license from Apache-2.0 to CAL-1.0.
- Fixed spammy Recorded initiate|accept with current round already set warning. [\#3060](https://github.com/holochain/holochain/pull/3060)

## 0.3.0-beta-dev.21

- There were some places where parsing an invalid URL would crash kitsune. This is now fixed. [\#2689](https://github.com/holochain/holochain/pull/2689)

## 0.3.0-beta-dev.20

- Augment network stats with holochain agent info correlation [\#2953](https://github.com/holochain/holochain/pull/2953)
- Adjust bootstrap max\_delay from 60 minutes -\> 5 minutes [\#2948](https://github.com/holochain/holochain/pull/2948)

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

- Add additional configuration options to network\_tuning for setting the allowed ephemeral port range for tx5 connections: tx5\_min\_ephemeral\_udp\_port and tx5\_max\_ephemeral\_udp\_port

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

- Resolves several cases where the meta net task would not stop on fatal errors and would not correctly handle other errors [\#2762](https://github.com/holochain/holochain/pull/2762)
- Resolves an issue where a `FetchOp` could skip processing op hashes if getting a topology for the space from the host failed [\#2737](https://github.com/holochain/holochain/pull/2737)
- Adds a warning log if incoming op data pushes are dropped due to a hashing failure on the host [\#2737](https://github.com/holochain/holochain/pull/2737)
- Fixes an issue where sending an unexpected request payload would cause the process to crash [\#2737](https://github.com/holochain/holochain/pull/2737)

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

- Fixes bug where authored data cannot be retrieved locally if the storage arc is not covering that data [\#2425](https://github.com/holochain/holochain/pull/2425)

## 0.3.0-beta-dev.0

- Bump tx5 to include https://github.com/holochain/tx5/pull/31 which should fix the network loop halting on certain error types, like Ban on data send. [\#2315](https://github.com/holochain/holochain/pull/2315)
- Removes the experimental `gossip_single_storage_arc_per_space` tuning param
- Fixes sharded gossip issue where storage arcs are not properly quantized in multi-agent-per-node sharded scenarios. [\#2332](https://github.com/holochain/holochain/pull/2332)
- Add `gossip_arc_clamping` Kitsune tuning param, allowing initial options to set all storage arcs to empty or full. [\#2352](https://github.com/holochain/holochain/pull/2352)
- Changes to arc resizing algorithm to ensure that nodes pick up the slack for freeloading nodes with zero storage arcs. [\#2352](https://github.com/holochain/holochain/pull/2352)
- Disables gossip when using `gossip_arc_clamping = "empty"`: when the arc is clamped to empty, the gossip module doesnt even activate. [\#2380](https://github.com/holochain/holochain/pull/2380)

## 0.2.0

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

- Adds feature flipper `tx5` which enables experimental integration with holochains WebRTC networking backend. This is not enabled by default. [\#1741](https://github.com/holochain/holochain/pull/1741)

## 0.1.0

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

- Fixes some bad logic around leaving spaces, which can cause problems upon rejoining [\#1744](https://github.com/holochain/holochain/pull/1744)
  - When an agent leaves a space, an `AgentInfoSigned` with an empty arc is published before leaving. Previously, this empty-arc agent info was also persisted to the database, but this is inappropriate because upon rejoining, they will start with an empty arc. Now, the agent info is removed from the database altogether upon leaving.

## 0.1.0-beta-rc.0

- **BREAKING CHANGE:** The gossip and publishing algorithms have undergone a significant rework, making this version incompatible with previous versions. Rather than gossiping and publishing entire Ops, only hashes are sent, which the recipient uses to maintain a queue of items which need to be fetched from various other sources on the DHT. This allows for finer-grained control over receiving Ops from multiple sources, and allows each node to manage their own incoming data flow. [\#1662](https://github.com/holochain/holochain/pull/1662)
- **BREAKING CHANGE:** `AppRequest::GossipInfo` is renamed to `AppRequest::NetworkInfo`, and the fields have changed. Since ops are no longer sent during gossip, there is no way to track overall gossip progress over a discrete time interval. There is now only a description of the total number of ops and total number of bytes waiting to be received. As ops are received, these numbers decrement.

## 0.0.52

- The soft maximum gossip batch size has been lowered to 1MB (entries larger than this will just be in a batch alone), and the default timeouts have been increased from 30 seconds to 60 seconds. This is NOT a breaking change, though the usefulness is negated unless the majority of peers are running with the same settings.  [\#1659](https://github.com/holochain/holochain/pull/1659)

## 0.0.51

- `rpc_multi` now only actually makes a single request. This greatly simplifies the code path and eliminates a source of network bandwidth congestion, but removes the redundancy of aggregating the results of multiple peers. [\#1651](https://github.com/holochain/holochain/pull/1651)

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

- Fixes a regression where a node can prematurely end a gossip round if their partner signals that they are done sending data, even if the node itself still has more data to send, which can lead to persistent timeouts between the two nodes. [\#1553](https://github.com/holochain/holochain/pull/1553)

## 0.0.43

- Increases all gossip bandwidth rate limits to 10mbps, up from 0.1mbps, allowing for gossip of larger entries
- Adds `gossip_burst_ratio` to `KitsuneTuningParams`, allowing tuning of bandwidth bursts
- Fixes a bug where a too-large gossip payload could put the rate limiter into an infinite loop

## 0.0.42

## 0.0.41

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

- Allow TLS session keylogging via tuning param `danger_tls_keylog` = `env_keylog`, and environment variable `SSLKEYLOGFILE` (See kitsune\_p2p crate api documentation). [\#1261](https://github.com/holochain/holochain/pull/1261)

## 0.0.25

- BREAKING: Gossip messages no longer contain the hash of the ops being gossiped. This is a breaking protocol change.
- Removed the unmaintained simple-bloom gossip module in favor of sharded-gossip

## 0.0.24

## 0.0.23

- Fixes D-01415 holochain panic on startup [\#1206](https://github.com/holochain/holochain/pull/1206)

## 0.0.22

## 0.0.21

## 0.0.20

## 0.0.19

## 0.0.18

## 0.0.17

- Agent info is now published as well as gossiped. [\#1115](https://github.com/holochain/holochain/pull/1115)
- BREAKING: Network wire message has changed format so will not be compatible with older versions. [1143](https://github.com/holochain/holochain/pull/1143).
- Fixes to gossip that allows batching of large amounts of data. [1143](https://github.com/holochain/holochain/pull/1143).

## 0.0.16

## 0.0.15

- BREAKING: Wire message `Call` no longer takes `from_agent`. [\#1091](https://github.com/holochain/holochain/pull/1091)

## 0.0.14

## 0.0.13

## 0.0.12

- BREAKING: Return `ShardedGossipWire::Busy` if we are overloaded with incoming gossip. [\#1076](https://github.com/holochain/holochain/pull/1076)
  - This breaks the current network protocol and will not be compatible with other older versions of holochain (no manual action required).

## 0.0.11

## 0.0.10

- Check local agents for basis when doing a RPCMulti call. [\#1009](https://github.com/holochain/holochain/pull/1009).

## 0.0.9

- Fix rpc\_multi bug that caused all request to wait 3 seconds. [\#1009](https://github.com/holochain/holochain/pull/1009/)
- Fix to gossips round initiate. We were not timing out a round if there was no response to an initiate message. [\#1014](https://github.com/holochain/holochain/pull/1014).
- Make gossip only initiate with agents that have info that is not expired. [\#1014](https://github.com/holochain/holochain/pull/1014).

## 0.0.8

### Changed

- `query_gossip_agents`, `query_agent_info_signed`, and `query_agent_info_signed_near_basis` are now unified into a single `query_agents` call in `KitsuneP2pEvent`

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip.rs
================================================
//! Various gossip strategies for Kitsune.
//!
//! Gossip is one of the main methods for nodes to exchange data in a Kitsune network.
//! During a gossip round, there are two things gossiped about:
//! info about other Agents in the network, and Ops, which are opaque chunks of data
//! used by the user of Kitsune.
//!
//! There are two types of gossip, Recent and Historical.
//!
//! Recent gossip is covers the last N minutes (currently N =  5) of Op activity. It uses bloom filters
//! to convey which ops are held and to discover which ops need to be transmitted during this round.
//! It is "pessimistic" in that we *a priori* expect our gossip partner to have different ops than we do.
//! Recent gossip is also solely responsible for gossiping info about other Agents, which it does using
//! the same method of employing bloom filters.
//!
//! Historical gossip is for everything else, namely for Ops which were created more than N minutes ago.
//! It is "optimistic" in that it expects ops to be mostly the same between nodes. Rather than bloom
//! filters, it uses a novel method of splitting the possible hash space into a number of regions with
//! deterministic hashes associated with each based on the contents, which are sent to the gossip partner.
//! For regions which mismatch, the ops in those regions will be exchanged between partners. For regions
//! which match, no data will be transferred.

pub mod sharded_gossip;

mod common;
pub use common::*;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/host_api.rs
================================================
use crate::dht::prelude::ArqSet;
use kitsune_p2p_fetch::{OpHashSized, RoughSized, TransferMethod};
use kitsune_p2p_timestamp::Timestamp;
use must_future::MustBoxFuture;
use std::sync::Arc;

use kitsune_p2p_types::{
    bin_types::KitsuneSpace,
    dependencies::lair_keystore_api,
    dht::{
        region::{Region, RegionCoords},
        region_set::RegionSetLtcs,
        spacetime::Topology,
    },
    dht_arc::DhtArcSet,
    KOpData, KOpHash,
};

use crate::event::GetAgentInfoSignedEvt;

/// A boxed future result with dynamic error type
pub type KitsuneHostResult<'a, T> =
    MustBoxFuture<'a, Result<T, Box<dyn Send + Sync + std::error::Error>>>;

/// The interface to be implemented by the host, which handles various requests
/// for data
pub trait KitsuneHost: 'static + Send + Sync + std::fmt::Debug {
    /// We are requesting a block.
    fn block(&self, input: kitsune_p2p_block::Block) -> KitsuneHostResult<()>;

    /// We are requesting an unblock.
    fn unblock(&self, input: kitsune_p2p_block::Block) -> KitsuneHostResult<()>;

    /// We want to know if a target is blocked.
    fn is_blocked(
        &self,
        input: kitsune_p2p_block::BlockTargetId,
        timestamp: Timestamp,
    ) -> KitsuneHostResult<bool>;

    /// We need to get previously stored agent info.
    fn get_agent_info_signed(
        &self,
        input: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>>;

    /// Remove an agent info from storage
    fn remove_agent_info_signed(&self, input: GetAgentInfoSignedEvt) -> KitsuneHostResult<bool>;

    /// Extrapolated Peer Coverage.
    fn peer_extrapolated_coverage(
        &self,
        space: Arc<KitsuneSpace>,
        dht_arc_set: DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>>;

    /// Query aggregate dht op data to form an LTCS set of region data.
    fn query_region_set(
        &self,
        space: Arc<KitsuneSpace>,
        arq_set: ArqSet,
    ) -> KitsuneHostResult<RegionSetLtcs>;

    /// Given an input list of regions, return a list of equal or greater length
    /// such that each region's size is less than the `size_limit`, by recursively
    /// subdividing regions which are over the size limit.
    fn query_size_limited_regions(
        &self,
        space: Arc<KitsuneSpace>,
        size_limit: u32,
        regions: Vec<Region>,
    ) -> KitsuneHostResult<Vec<Region>>;

    /// Get all op hashes within a region
    fn query_op_hashes_by_region(
        &self,
        space: Arc<KitsuneSpace>,
        region: RegionCoords,
    ) -> KitsuneHostResult<Vec<OpHashSized>>;

    /// Record a set of metric records.
    fn record_metrics(
        &self,
        space: Arc<KitsuneSpace>,
        records: Vec<MetricRecord>,
    ) -> KitsuneHostResult<()>;

    /// Get the quantum Topology associated with this Space.
    fn get_topology(&self, space: Arc<KitsuneSpace>) -> KitsuneHostResult<Topology>;

    /// Hashing function to get an op_hash from op_data.
    fn op_hash(&self, op_data: KOpData) -> KitsuneHostResult<KOpHash>;

    /// Check which hashes we have data for.
    fn check_op_data(
        &self,
        space: Arc<KitsuneSpace>,
        op_hash_list: Vec<KOpHash>,
        _context: Option<kitsune_p2p_fetch::FetchContext>,
    ) -> KitsuneHostResult<Vec<bool>> {
        let _space = space;
        futures::FutureExt::boxed(
            async move { Ok(op_hash_list.into_iter().map(|_| false).collect()) },
        )
        .into()
    }

    /// Do something whenever a batch of op hashes was received and stored in the FetchPool
    fn handle_op_hash_received(
        &self,
        _space: &KitsuneSpace,
        _op_hash: &RoughSized<KOpHash>,
        _transfer_method: TransferMethod,
    ) {
    }

    /// Do something whenever a batch of op hashes was sent to another node
    fn handle_op_hash_transmitted(
        &self,
        _space: &KitsuneSpace,
        _op_hash: &RoughSized<KOpHash>,
        _transfer_method: TransferMethod,
    ) {
    }

    /// Get the lair "tag" identifying the id seed to use for crypto signing.
    /// (this is currently only used in tx5/WebRTC if that feature is enabled.)
    fn lair_tag(&self) -> Option<Arc<str>> {
        None
    }

    /// Get the lair client to use as the backend keystore.
    /// (this is currently only used in tx5/WebRTC if that feature is enabled.)
    fn lair_client(&self) -> Option<lair_keystore_api::LairClient> {
        None
    }
}

/// Trait object for the host interface
pub type HostApi = std::sync::Arc<dyn KitsuneHost>;

/// A HostApi paired with a ghost_actor sender (legacy)
/// When all legacy functions have been moved to the API,
/// this type can be replaced by `HostApi`.
#[derive(Clone, Debug, derive_more::Constructor, derive_more::Deref, derive_more::Into)]
pub struct HostApiLegacy {
    /// The new API
    #[deref]
    pub api: HostApi,
    /// The old ghost_actor sender based API
    pub legacy: futures::channel::mpsc::Sender<crate::event::KitsuneP2pEvent>,
}

// Test-only stub which mostly panics
#[cfg(any(test, feature = "test_utils"))]
mod host_stub;
#[cfg(any(test, feature = "test_utils"))]
pub use host_stub::*;

#[cfg(any(test, feature = "test_utils"))]
mod host_default_error;
#[cfg(any(test, feature = "test_utils"))]
pub use host_default_error::*;
use kitsune_p2p_types::metrics::MetricRecord;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/lib.rs
================================================
#![deny(missing_docs)]

//! P2p / dht communication framework.
//!
//! ### TLS session key logging
//!
//! To use a tool like wireshark to debug kitsune QUIC communications,
//! enable keylogging via tuning_param:
//!
//! ```
//! # use kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams;
//! # let mut tuning_params = KitsuneP2pTuningParams::default();
//! tuning_params.danger_tls_keylog = "env_keylog".to_string();
//! ```
//!
//! The tuning param by itself will do nothing, you also must specify
//! the file target via the environment variable `SSLKEYLOGFILE`, e.g.:
//!
//! ```no_compile
//! SSLKEYLOGFILE="$(pwd)/keylog" my-kitsune-executable
//! ```
//!
//! As QUIC support within wireshark is in-progress, you'll need a newer
//! version. This documentation was tested with version `3.6.2`.
//!
//! Tell wireshark about your keylog file at:
//!
//! `[Edit] -> [Preferences...] -> [Protocols] -> [TLS] -> [(Pre)-Master-Secret log filename]`
//!
//! Your capture should now include `QUIC` protocol packets, where the
//! `Protected Payload` variants will be able to display internals,
//! such as `STREAM([id])` decrypted content.
//!
//! Also see [https://github.com/quiclog/pcap2qlog](https://github.com/quiclog/pcap2qlog)

/// re-exported dependencies
pub mod dependencies {
    pub use ::kitsune_p2p_fetch;
    pub use ::kitsune_p2p_timestamp;
    pub use ::kitsune_p2p_types;
    pub use ::url2;
}

/// This value determines protocol compatibility.
/// Any time there is a protocol breaking change, this number must be incremented.
pub use kitsune_p2p_timestamp::KITSUNE_PROTOCOL_VERSION;

pub mod metrics;

mod types;
pub use types::*;

pub mod gossip;
pub use gossip::sharded_gossip::KitsuneDiagnostics;

mod spawn;
pub use spawn::*;

mod host_api;
pub use host_api::*;

pub use meta_net::PreflightUserData;

#[allow(missing_docs)]
#[cfg(feature = "test_utils")]
pub mod test_util;

#[cfg(test)]
mod test;

#[cfg(feature = "fuzzing")]
pub use kitsune_p2p_timestamp::noise::NOISE;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/metrics.rs
================================================
//! metrics tracked by kitsune_p2p spaces

use std::collections::HashMap;
use std::collections::HashSet;
use std::collections::VecDeque;
use std::sync::Arc;
use std::time::Duration;

use kitsune_p2p_types::GossipType;
use tokio::time::Instant;

use crate::gossip::sharded_gossip::NodeId;
use crate::gossip::sharded_gossip::RegionDiffs;
use crate::gossip::sharded_gossip::RoundState;
use crate::types::*;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::agent_info::AgentInfoSigned;

use num_traits::*;

use kitsune_p2p_types::metrics::{MetricRecord, MetricRecordKind};
use once_cell::sync::Lazy;

pub(crate) static METRIC_MSG_OUT_BYTE: Lazy<opentelemetry_api::metrics::Histogram<u64>> =
    Lazy::new(|| {
        opentelemetry_api::global::meter("kitsune")
            .u64_histogram("kitsune.peer.send.byte.count")
            .with_description("Outgoing p2p network messages byte count")
            .with_unit(opentelemetry_api::metrics::Unit::new("By"))
            .init()
    });

pub(crate) static METRIC_MSG_OUT_TIME: Lazy<opentelemetry_api::metrics::Histogram<f64>> =
    Lazy::new(|| {
        opentelemetry_api::global::meter("kitsune")
            .f64_histogram("kitsune.peer.send.duration")
            .with_description("Outgoing p2p network messages seconds")
            .with_unit(opentelemetry_api::metrics::Unit::new("s"))
            .init()
    });

pub(crate) static GENERATE_OP_BLOOMS_TIME: Lazy<opentelemetry_api::metrics::Histogram<f64>> =
    Lazy::new(|| {
        opentelemetry_api::global::meter("kitsune")
            .f64_histogram("kitsune.gossip.generate_op_blooms.duration")
            .with_description("Time taken to generate op blooms for gossip")
            .with_unit(opentelemetry_api::metrics::Unit::new("s"))
            .init()
    });

pub(crate) static GENERATE_OP_REGION_SET_TIME: Lazy<opentelemetry_api::metrics::Histogram<f64>> =
    Lazy::new(|| {
        opentelemetry_api::global::meter("kitsune")
            .f64_histogram("kitsune.gossip.generate_op_region_set.duration")
            .with_description("Time taken to generate region set for gossip")
            .with_unit(opentelemetry_api::metrics::Unit::new("s"))
            .init()
    });

/// how long historical metric records should be kept
/// (currently set to 1 week)
const HISTORICAL_RECORD_EXPIRE_DURATION_MICROS: i64 = 1000 * 1000 * 60 * 60 * 24 * 7;

/// Running average that prioritizes memory and cpu efficiency
/// over strict accuracy.
/// For metrics where we can't afford the memory of tracking samples
/// for every remote we might talk to, this running average is
/// accurate enough and uses only 5 bytes of memory.
#[derive(Debug, Clone, Copy)]
pub struct RunAvg(f32, u8);

impl Default for RunAvg {
    fn default() -> Self {
        Self(0.0, 0)
    }
}

impl RunAvg {
    /// Push a new data point onto the running average
    pub fn push<V: AsPrimitive<f32>>(&mut self, v: V) {
        self.push_n(v, 1);
    }

    /// Push multiple entries (up to 255) of the same value onto the average
    pub fn push_n<V: AsPrimitive<f32>>(&mut self, v: V, count: u8) {
        self.1 = self.1.saturating_add(count);
        self.0 = (self.0 * (self.1 - count) as f32 + (v.as_() * count as f32)) / self.1 as f32;
    }
}

macro_rules! mk_from {
    ($($t:ty,)*) => {$(
        impl From<$t> for RunAvg {
            fn from(o: $t) -> Self {
                Self(o as f32, 1)
            }
        }
    )*};
}

mk_from! {
    i8,
    u8,
    i16,
    u16,
    i32,
    u32,
    i64,
    u64,
    f32,
    f64,
}

impl std::ops::Deref for RunAvg {
    type Target = f32;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl AsRef<f32> for RunAvg {
    fn as_ref(&self) -> &f32 {
        &self.0
    }
}

impl std::borrow::Borrow<f32> for RunAvg {
    fn borrow(&self) -> &f32 {
        &self.0
    }
}

impl std::fmt::Display for RunAvg {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.0.fmt(f)
    }
}

/// The maximum number of different nodes that will be
/// gossiped with if gossip is triggered.
const MAX_TRIGGERS: u8 = 2;

/// Maximum amount of history we will track
/// per remote node.
const MAX_HISTORY: usize = 10;

#[derive(Debug, Clone, Default)]
/// The history of gossip with an agent on a remote node.
/// We record metrics per agent,
pub struct PeerAgentHistory {
    /// Successful and unsuccessful messages from the remote
    /// can be combined to estimate a "reachability quotient"
    /// between 1 (or 0 if empty) and 100. Errors are weighted
    /// heavier because we retry less frequently.
    pub reachability_quotient: RunAvg,
    /// Running average for latency microseconds for any direct
    /// request/response calls to remote agent.
    pub latency_micros: RunAvg,
    /// Times we recorded successful initiates to this node (they accepted).
    pub initiates: VecDeque<RoundMetric>,
    /// Times we recorded initiates from this node (we accepted).
    pub accepts: VecDeque<RoundMetric>,
    /// Times we recorded complete rounds for this node.
    pub successes: VecDeque<RoundMetric>,
    /// Times we recorded errors for this node.
    pub errors: VecDeque<RoundMetric>,
    /// Is this node currently in an active round?
    pub current_rounds: HashSet<GossipType>,
}

/// Detailed info about the history of gossip with this node
#[derive(Debug, Clone, Default)]
pub struct PeerNodeHistory {
    /// The most recent list of remote agents reported by this node
    pub remote_agents: Vec<Arc<KitsuneAgent>>,

    /// Detailed info about the ongoing round with this node
    pub current_round: Option<CurrentRound>,

    /// Detailed info about rounds completed with this node
    pub completed_rounds: VecDeque<CompletedRound>,
}

/// Info about a completed gossip round
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct RoundMetric {
    /// The time this metric was recorded
    pub instant: Instant,
    /// The type of gossip module
    pub gossip_type: GossipModuleType,
}

/// Metrics about a completed gossip round
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct CompletedRound {
    /// Unique string id
    pub id: String,
    /// The type of gossip module
    pub gossip_type: GossipModuleType,
    /// The start time of the round
    pub start_time: Instant,
    /// The end time of the round
    pub end_time: Instant,
    /// This round ended in an error
    pub error: bool,
    /// If historical, the region diffs
    pub region_diffs: RegionDiffs,
}

impl CompletedRound {
    /// Total duration of this round, from start to end
    pub fn duration(&self) -> Duration {
        self.end_time.duration_since(self.start_time)
    }
}

/// Metrics about an ongoing gossip round
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct CurrentRound {
    /// Unique string id
    pub id: String,
    /// The type of gossip module
    pub gossip_type: GossipModuleType,
    /// Last time this was updated
    pub last_touch: Instant,
    /// The start time of the round
    pub start_time: Instant,
    /// If historical, the region diffs
    pub region_diffs: RegionDiffs,
}

impl CurrentRound {
    /// Constructor
    pub fn new(id: String, gossip_type: GossipModuleType, start_time: Instant) -> Self {
        Self {
            id,
            gossip_type,
            start_time,
            last_touch: Instant::now(),
            region_diffs: Default::default(),
        }
    }

    /// Update status based on an existing round
    pub fn update(&mut self, round_state: &RoundState) {
        self.last_touch = Instant::now();
        self.region_diffs.clone_from(&round_state.region_diffs);
    }

    /// Convert to a CompletedRound
    pub fn completed(self, error: bool) -> CompletedRound {
        CompletedRound {
            id: self.id,
            gossip_type: self.gossip_type,
            start_time: self.start_time,
            end_time: Instant::now(),
            error,
            region_diffs: self.region_diffs,
        }
    }
}

impl RoundMetric {
    /// Time elapsed since this round was recorded
    pub fn elapsed(&self) -> Duration {
        self.instant.elapsed()
    }
}

impl PartialOrd for RoundMetric {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for RoundMetric {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        self.instant.cmp(&other.instant)
    }
}

impl PartialOrd for CompletedRound {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for CompletedRound {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        match self.start_time.cmp(&other.start_time) {
            core::cmp::Ordering::Equal => {}
            ord => return ord,
        }
        self.end_time.cmp(&other.end_time)
    }
}

impl PartialOrd for CurrentRound {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for CurrentRound {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        self.start_time.cmp(&other.start_time)
    }
}

#[derive(Debug, Default)]
/// Metrics tracking for remote nodes to help
/// choose which remote node to initiate the next round with.
pub struct Metrics {
    /// Map of remote agents and gossip history with each.
    agent_history: HashMap<Arc<KitsuneAgent>, PeerAgentHistory>,

    /// Map of remote nodes and gossip history with each
    node_history: HashMap<NodeId, PeerNodeHistory>,

    /// Aggregate Extrapolated Dht Coverage
    agg_extrap_cov: RunAvg,

    // Number of times we need to force initiate
    // the next round.
    pub(crate) force_initiates: u8,
}

/// Outcome of a gossip round.
#[derive(Debug, PartialOrd, Ord, PartialEq, Eq)]
pub enum RoundOutcome {
    /// Success outcome
    Success(RoundMetric),
    /// Error outcome
    Error(RoundMetric),
}

/// Accept differing key types
pub enum AgentLike<'lt> {
    /// An agent info
    Info(&'lt AgentInfoSigned),
    /// A raw agent pubkey
    PubKey(&'lt Arc<KitsuneAgent>),
}

impl<'lt> From<&'lt AgentInfoSigned> for AgentLike<'lt> {
    fn from(i: &'lt AgentInfoSigned) -> Self {
        Self::Info(i)
    }
}

impl<'lt> From<&'lt Arc<KitsuneAgent>> for AgentLike<'lt> {
    fn from(pk: &'lt Arc<KitsuneAgent>) -> Self {
        Self::PubKey(pk)
    }
}

impl AgentLike<'_> {
    /// Get a raw agent pubkey from any variant type
    pub fn agent(&self) -> &Arc<KitsuneAgent> {
        match self {
            Self::Info(i) => &i.agent,
            Self::PubKey(pk) => pk,
        }
    }
}

impl Metrics {
    /// Dump historical metrics for recording to db.
    pub fn dump_historical(&self) -> Vec<MetricRecord> {
        let now = Timestamp::now();

        let expires_at =
            Timestamp::from_micros(now.as_micros() + HISTORICAL_RECORD_EXPIRE_DURATION_MICROS);

        let mut out = Vec::new();

        for (agent, node) in self.agent_history.iter() {
            out.push(MetricRecord {
                kind: MetricRecordKind::ReachabilityQuotient,
                agent: Some(agent.clone()),
                recorded_at_utc: now,
                expires_at_utc: expires_at,
                data: serde_json::json!(*node.reachability_quotient),
            });

            out.push(MetricRecord {
                kind: MetricRecordKind::LatencyMicros,
                agent: Some(agent.clone()),
                recorded_at_utc: now,
                expires_at_utc: expires_at,
                data: serde_json::json!(*node.latency_micros),
            });
        }

        out.push(MetricRecord {
            kind: MetricRecordKind::AggExtrapCov,
            agent: None,
            recorded_at_utc: now,
            expires_at_utc: expires_at,
            data: serde_json::json!(*self.agg_extrap_cov),
        });

        out
    }

    /// Dump json encoded metrics
    pub fn dump(&self) -> serde_json::Value {
        let agents: serde_json::Value = self
            .agent_history
            .iter()
            .map(|(a, i)| {
                (
                    a.to_string(),
                    serde_json::json!({
                        "reachability_quotient": *i.reachability_quotient,
                        "latency_micros": *i.latency_micros,
                    }),
                )
            })
            .collect::<serde_json::map::Map<String, serde_json::Value>>()
            .into();

        serde_json::json!({
            "aggExtrapCov": *self.agg_extrap_cov,
            "agents": agents,
        })
    }

    /// Record an individual extrapolated coverage event
    /// (either from us or a remote)
    /// and add it to our running aggregate extrapolated coverage metric.
    pub fn record_extrap_cov_event(&mut self, extrap_cov: f32) {
        self.agg_extrap_cov.push(extrap_cov);
    }

    /// Sucessful and unsuccessful messages from the remote
    /// can be combined to estimate a "reachability quotient"
    /// between 1 (or 0 if empty) and 100. Errors are weighted
    /// heavier because we retry less frequently.
    /// Call this to register a reachability event.
    /// Note, `record_success` and `record_error` below invoke this
    /// function internally, you don't need to call it again.
    pub fn record_reachability_event<'a, T, I>(&mut self, success: bool, remote_agent_list: I)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        for agent_info in remote_agent_list {
            let info = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            if success {
                info.reachability_quotient.push(100);
            } else {
                info.reachability_quotient.push_n(1, 5);
            }
        }
    }

    /// Running average for latency microseconds for any direct
    /// request/response calls to remote agent.
    pub fn record_latency_micros<'a, T, I, V>(&mut self, micros: V, remote_agent_list: I)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
        V: AsPrimitive<f32>,
    {
        for agent_info in remote_agent_list {
            let history = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            history.latency_micros.push(micros);
        }
    }

    /// Record a gossip round has been initiated by us.
    pub fn record_initiate<'a, T, I>(&mut self, remote_agent_list: I, gossip_type: GossipModuleType)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        for agent_info in remote_agent_list {
            let history = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            let round = RoundMetric {
                instant: Instant::now(),
                gossip_type,
            };
            record_item(&mut history.initiates, round);
            if history.current_rounds.contains(&gossip_type.into()) {
                tracing::warn!("Recorded initiate with current round already set");
            } else {
                history.current_rounds.insert(gossip_type.into());
            }
        }
    }

    /// Record a gossip round has been initiated by a peer.
    pub fn record_accept<'a, T, I>(&mut self, remote_agent_list: I, gossip_type: GossipModuleType)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        for agent_info in remote_agent_list {
            let history = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            let round = RoundMetric {
                instant: Instant::now(),
                gossip_type,
            };
            record_item(&mut history.accepts, round);
            if history.current_rounds.contains(&gossip_type.into()) {
                tracing::warn!("Recorded accept with current round already set");
            } else {
                history.current_rounds.insert(gossip_type.into());
            }
        }
    }

    /// Record a gossip round has completed successfully.
    pub fn record_success<'a, T, I>(&mut self, remote_agent_list: I, gossip_type: GossipModuleType)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        let mut should_dec_force_initiates = false;

        for agent_info in remote_agent_list {
            let history = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            history.reachability_quotient.push(100);
            let round = RoundMetric {
                instant: Instant::now(),
                gossip_type,
            };
            record_item(&mut history.successes, round);

            let removed = history.current_rounds.remove(&gossip_type.into());
            if !removed {
                tracing::warn!("Recorded success without record of gossip round");
            }

            if history.is_initiate_round() {
                should_dec_force_initiates = true;
            }
        }

        if should_dec_force_initiates {
            self.force_initiates = self.force_initiates.saturating_sub(1);
        }

        tracing::debug!(
            "recorded success in metrics. force_initiates={}",
            self.force_initiates
        );
    }

    /// Record a gossip round has finished with an error.
    pub fn record_error<'a, T, I>(&mut self, remote_agent_list: I, gossip_type: GossipModuleType)
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        for agent_info in remote_agent_list {
            let history = self
                .agent_history
                .entry(agent_info.into().agent().clone())
                .or_default();
            history.reachability_quotient.push_n(1, 5);
            let round = RoundMetric {
                instant: Instant::now(),
                gossip_type,
            };
            record_item(&mut history.errors, round);

            let removed = history.current_rounds.remove(&gossip_type.into());
            if !removed {
                tracing::warn!("Recorded error without record of gossip round");
            }
        }
        tracing::debug!(
            "recorded error in metrics. force_initiates={}",
            self.force_initiates
        );
    }

    /// Update node-level info about a current round, or create one if it doesn't exist
    pub fn update_current_round(
        &mut self,
        peer: &NodeId,
        gossip_type: GossipModuleType,
        round_state: &RoundState,
    ) {
        let remote_agents = round_state
            .remote_agent_list
            .clone()
            .into_iter()
            .map(|a| a.agent())
            .collect();
        let history = self.node_history.entry(peer.clone()).or_default();
        history.remote_agents = remote_agents;
        if let Some(r) = &mut history.current_round {
            r.update(round_state);
        } else {
            history.current_round = Some(CurrentRound::new(
                round_state.id.clone(),
                gossip_type,
                Instant::now(),
            ));
        }
    }

    /// Remove the current round info once it's complete, and put it into the history list
    pub fn complete_current_round(&mut self, node: &NodeId, error: bool) {
        let history = self.node_history.entry(node.clone()).or_default();
        let r = history.current_round.take();
        if let Some(r) = r {
            history.completed_rounds.push_back(r.completed(error))
        }
    }

    /// Record that we should force initiate the next few rounds.
    pub fn record_force_initiate(&mut self) {
        self.force_initiates = MAX_TRIGGERS;
    }

    /// Get the last successful round time.
    pub fn last_success<'a, T, I>(&self, remote_agent_list: I) -> Option<&RoundMetric>
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        remote_agent_list
            .into_iter()
            .filter_map(|agent_info| self.agent_history.get(agent_info.into().agent()))
            .filter_map(|info| info.successes.back())
            .min_by_key(|r| r.instant)
    }

    /// Is this node currently in an active round?
    pub fn is_current_round<'a, T, I>(&self, remote_agent_list: I) -> bool
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        remote_agent_list
            .into_iter()
            .filter_map(|agent_info| self.agent_history.get(agent_info.into().agent()))
            .any(|info| !info.current_rounds.is_empty())
    }

    /// What was the last outcome for this node's gossip round?
    pub fn last_outcome<'a, T, I>(&self, remote_agent_list: I) -> Option<RoundOutcome>
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        #[allow(clippy::map_flatten)]
        remote_agent_list
            .into_iter()
            .filter_map(|agent_info| self.agent_history.get(agent_info.into().agent()))
            .map(|info| {
                [
                    info.errors.back().map(|x| RoundOutcome::Error(x.clone())),
                    info.successes
                        .back()
                        .map(|x| RoundOutcome::Success(x.clone())),
                ]
            })
            .flatten()
            .flatten()
            .max()
    }

    /// Should we force initiate the next round?
    pub fn forced_initiate(&self) -> bool {
        self.force_initiates > 0
    }

    /// Return the average (mean) reachability quotient for the
    /// supplied remote agents.
    pub fn reachability_quotient<'a, T, I>(&self, remote_agent_list: I) -> f32
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        let (sum, cnt) = remote_agent_list
            .into_iter()
            .filter_map(|agent_info| self.agent_history.get(agent_info.into().agent()))
            .map(|info| *info.reachability_quotient)
            .fold((0.0, 0.0), |acc, x| (acc.0 + x, acc.1 + 1.0));
        if cnt <= 0.0 {
            0.0
        } else {
            sum / cnt
        }
    }

    /// Return the average (mean) latency microseconds for the
    /// supplied remote agents.
    pub fn latency_micros<'a, T, I>(&self, remote_agent_list: I) -> f32
    where
        T: Into<AgentLike<'a>>,
        I: IntoIterator<Item = T>,
    {
        let (sum, cnt) = remote_agent_list
            .into_iter()
            .filter_map(|agent_info| self.agent_history.get(agent_info.into().agent()))
            .map(|info| *info.latency_micros)
            .fold((0.0, 0.0), |acc, x| (acc.0 + x, acc.1 + 1.0));
        if cnt <= 0.0 {
            0.0
        } else {
            sum / cnt
        }
    }

    /// Getter
    pub fn peer_agent_histories(&self) -> &HashMap<Arc<KitsuneAgent>, PeerAgentHistory> {
        &self.agent_history
    }

    /// Getter
    pub fn peer_node_histories(&self) -> &HashMap<NodeId, PeerNodeHistory> {
        &self.node_history
    }
}

impl PeerAgentHistory {
    /// Was the last round for this node initiated by us?
    fn is_initiate_round(&self) -> bool {
        match (self.accepts.back(), self.initiates.back()) {
            (None, None) | (Some(_), None) => false,
            (None, Some(_)) => true,
            (Some(remote), Some(initiate)) => initiate > remote,
        }
    }
}

fn record_item<T>(buffer: &mut VecDeque<T>, item: T) {
    if buffer.len() > MAX_HISTORY {
        buffer.pop_front();
    }
    buffer.push_back(item);
}

impl std::fmt::Display for Metrics {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        static TRACE: once_cell::sync::Lazy<bool> = once_cell::sync::Lazy::new(|| {
            std::env::var("GOSSIP_METRICS").map_or(false, |s| s == "trace")
        });
        let trace = *TRACE;
        write!(f, "Metrics:")?;
        let mut average_last_completion = std::time::Duration::default();
        let mut max_last_completion = std::time::Duration::default();
        let mut average_completion_frequency = std::time::Duration::default();
        let mut complete_rounds = 0;
        let mut min_complete_rounds = usize::MAX;
        for (key, info) in &self.agent_history {
            let completion_frequency: std::time::Duration =
                info.successes.iter().map(|i| i.elapsed()).sum();
            let completion_frequency = completion_frequency
                .checked_div(info.successes.len() as u32)
                .unwrap_or_default();
            let last_completion = info
                .successes
                .back()
                .map(|i| i.elapsed())
                .unwrap_or_default();
            average_last_completion += last_completion;
            max_last_completion = max_last_completion.max(last_completion);
            average_completion_frequency += completion_frequency;
            if !info.successes.is_empty() {
                complete_rounds += 1;
            }
            min_complete_rounds = min_complete_rounds.min(info.successes.len());
            if trace {
                write!(f, "\n\t{:?}:", key)?;
                write!(
                    f,
                    "\n\t\tErrors: {}, Last: {:?}",
                    info.errors.len(),
                    info.errors.back().map(|i| i.elapsed()).unwrap_or_default()
                )?;
                write!(
                    f,
                    "\n\t\tInitiates: {}, Last: {:?}",
                    info.initiates.len(),
                    info.initiates
                        .back()
                        .map(|i| i.elapsed())
                        .unwrap_or_default()
                )?;
                write!(
                    f,
                    "\n\t\tRemote Rounds: {}, Last: {:?}",
                    info.accepts.len(),
                    info.accepts.back().map(|i| i.elapsed()).unwrap_or_default()
                )?;
                write!(
                    f,
                    "\n\t\tComplete Rounds: {}, Last: {:?}, Average completion Frequency: {:?}",
                    info.successes.len(),
                    last_completion,
                    completion_frequency
                )?;
                write!(f, "\n\t\tCurrent Rounds: {:?}", info.current_rounds)?;
            }
        }
        write!(
            f,
            "\n\tNumber of remote nodes complete {} out of {}. Min per node: {}.",
            complete_rounds,
            self.agent_history.len(),
            min_complete_rounds
        )?;
        write!(
            f,
            "\n\tAverage time since last completion: {:?}",
            average_last_completion
                .checked_div(self.agent_history.len() as u32)
                .unwrap_or_default()
        )?;
        write!(
            f,
            "\n\tMax time since last completion: {:?}",
            max_last_completion
        )?;
        write!(
            f,
            "\n\tAverage completion frequency: {:?}",
            average_completion_frequency
                .checked_div(self.agent_history.len() as u32)
                .unwrap_or_default()
        )?;
        write!(f, "\n\tForce Initiate: {}", self.force_initiates)?;
        Ok(())
    }
}

/// Synchronization primitive around the Metrics struct.
#[derive(Clone)]
pub struct MetricsSync(Arc<parking_lot::RwLock<Metrics>>);

impl Default for MetricsSync {
    fn default() -> Self {
        Self(Arc::new(parking_lot::RwLock::new(Metrics::default())))
    }
}

impl std::fmt::Debug for MetricsSync {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.0.read().fmt(f)
    }
}

impl std::fmt::Display for MetricsSync {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.0.read().fmt(f)
    }
}

impl MetricsSync {
    /// Get a read lock for the metrics store.
    pub fn read(&self) -> parking_lot::RwLockReadGuard<Metrics> {
        match self.0.try_read_for(std::time::Duration::from_millis(100)) {
            Some(g) => g,
            // This won't block if a writer is waiting.
            // NOTE: This is a bit of a hack to work around a lock somewhere that is errant-ly
            // held over another call to lock. Really we should fix that error,
            // potentially by using a closure pattern here to ensure the lock cannot
            // be held beyond the access logic.
            None => self.0.read_recursive(),
        }
    }

    /// Get a write lock for the metrics store.
    pub fn write(&self) -> parking_lot::RwLockWriteGuard<Metrics> {
        match self.0.try_write_for(std::time::Duration::from_secs(100)) {
            Some(g) => g,
            None => {
                eprintln!("Metrics lock likely deadlocked");
                self.0.write()
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_run_avg() {
        let mut a1 = RunAvg::default();
        a1.push(100);
        a1.push(1);
        a1.push(1);
        a1.push(1);
        assert_eq!(25.75, *a1);

        let mut a2 = RunAvg::default();
        a2.push_n(100, 1);
        a2.push_n(1, 3);
        assert_eq!(25.75, *a2);

        let mut a3 = RunAvg::default();
        a3.push_n(100, 255);
        a3.push(1);
        assert_eq!(99.61176, *a3);

        let mut a4 = RunAvg::default();
        a4.push_n(100, 255);
        a4.push_n(1, 128);
        assert_eq!(50.30588, *a4);

        let mut a5 = RunAvg::default();
        a5.push_n(100, 255);
        a5.push_n(1, 255);
        assert_eq!(1.0, *a5);
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn.rs
================================================
use crate::actor::*;
use crate::event::*;
use crate::HostApi;
use crate::HostApiLegacy;
use actor::create_meta_net;
use actor::KitsuneP2pActor;
use kitsune_p2p_types::config::KitsuneP2pConfig;

mod actor;

pub(crate) use actor::meta_net;

#[cfg(feature = "test_utils")]
pub use actor::MockKitsuneP2pEventHandler;

use self::actor::Internal;
use self::meta_net::PreflightUserData;

/// Spawn a new KitsuneP2p actor.
pub async fn spawn_kitsune_p2p(
    config: KitsuneP2pConfig,
    tls_config: kitsune_p2p_types::tls::TlsConfig,
    host: HostApi,
    preflight_user_data: PreflightUserData,
) -> KitsuneP2pResult<(
    ghost_actor::GhostSender<KitsuneP2p>,
    KitsuneP2pEventReceiver,
)> {
    #[cfg(not(feature = "unstable-sharding"))]
    if config.tuning_params.arc_clamping().is_none() {
        tracing::warn!(
            "\
            gossip_arc_clamping network tuning parameter is not set. \
            This is not permitted without \"unstable-sharding\" feature enabled. \
            Please choose either \"empty\" or \"full\""
        );
        return Err("gossip_arc_clamping must be set".into());
    }

    let (evt_send, evt_recv) = futures::channel::mpsc::channel(10);
    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let channel_factory = builder.channel_factory().clone();

    let internal_sender = channel_factory.create_channel::<Internal>().await?;

    let sender = channel_factory.create_channel::<KitsuneP2p>().await?;
    let host = HostApiLegacy::new(host, evt_send);

    // Create a `HostApiLegacy` that is configured to talk to the `KitsuneP2pActor` rather than directly to the Kitsune host.
    let self_host_api = {
        let (evt_send, evt_recv) = futures::channel::mpsc::channel(10);
        let self_host_api = HostApiLegacy::new(host.api.clone(), evt_send);
        channel_factory.attach_receiver(evt_recv).await?;

        self_host_api
    };

    // Create the network. Any events it sends will have to wait to be processed until Kitsune has finished initialising
    // but everything that is needed to construct the network is available now.
    let (ep_hnd, ep_evt, bootstrap_net, maybe_peer_url) = create_meta_net(
        &config,
        tls_config,
        internal_sender.clone(),
        self_host_api.clone(),
        preflight_user_data,
    )
    .await?;

    tokio::task::spawn(
        builder.spawn(
            KitsuneP2pActor::new(
                config,
                channel_factory,
                internal_sender,
                host,
                self_host_api,
                ep_hnd,
                ep_evt,
                bootstrap_net,
                maybe_peer_url,
            )
            .await?,
        ),
    );

    Ok((sender, evt_recv))
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test.rs
================================================
/*
#[cfg(test)]
mod tests {
    use crate::test_util::*;
    use crate::types::actor::KitsuneP2pSender;
    use crate::*;
    use ghost_actor::dependencies::tracing;
    use ghost_actor::GhostControlSender;
    use std::sync::Arc;

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_transport_coms() {
        holochain_trace::test_run();
        holochain_trace::metrics::init();
        let (harness, _evt) = spawn_test_harness_mem().await.unwrap();

        let space = harness.add_space().await.unwrap();
        let (a1, p2p1) = harness.add_direct_agent("one".into()).await.unwrap();
        let (a2, p2p2) = harness.add_direct_agent("two".into()).await.unwrap();

        // needed until we have some way of bootstrapping
        harness.magic_peer_info_exchange().await.unwrap();

        let r1 = p2p1
            .rpc_single(space.clone(), a1.clone(), b"m1".to_vec(), None)
            .await
            .unwrap();
        let s = std::time::Instant::now();
        let r2 = match p2p2
            .rpc_single(space.clone(), a2, b"m2".to_vec(), None)
            .await
        {
            Err(_) => {
                panic!("TIMEOUT AFTER: {} ms", s.elapsed().as_millis());
            }
            Ok(r) => r,
        };
        assert_eq!(b"echo: m1".to_vec(), r1);
        assert_eq!(b"echo: m2".to_vec(), r2);
        harness.ghost_actor_shutdown().await.unwrap();
        crate::types::metrics::print_all_metrics();
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_peer_info_store() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, evt) = spawn_test_harness_mem().await?;
        let mut recv = evt.receive();

        harness.add_space().await?;
        let (agent, _p2p) = harness.add_direct_agent("DIRECT".into()).await?;

        harness.ghost_actor_shutdown().await?;

        let mut agent_info_signed = None;

        use tokio_stream::StreamExt;
        while let Some(item) = recv.next().await {
            if let HarnessEventType::StoreAgentInfo { agent, .. } = item.ty {
                agent_info_signed = Some((agent,));
            }
        }

        if let Some(i) = agent_info_signed {
            assert_eq!(i.0, Slug::from(agent));
            return Ok(());
        }

        panic!("Failed to receive agent_info_signed")
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_transport_binding() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;

        // Create a p2p config with a local proxy that rejects proxying anyone else
        // and binds to `kitsune-quic://0.0.0.0:0`.
        // This allows the OS to assign an interface / port.
        harness.add_space().await?;
        let (_, p2p) = harness.add_direct_agent("DIRECT".into()).await?;

        // List the bindings and assert that we have one binding that is a
        // kitsune-proxy scheme with a kitsune-quic url.
        let bindings = p2p.list_transport_bindings().await?;
        tracing::warn!("BINDINGS: {:?}", bindings);
        assert_eq!(1, bindings.len());
        let binding = &bindings[0];
        assert_eq!("kitsune-proxy", binding.scheme());
        assert_eq!(
            "kitsune-quic",
            binding.path_segments().unwrap().next().unwrap()
        );

        harness.ghost_actor_shutdown().await?;
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_request_workflow() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;
        let space = harness.add_space().await?;
        let (a1, p2p) = harness.add_direct_agent("DIRECT".into()).await?;
        // TODO when networking works, just add_*_agent again...
        // but for now, we need the two agents to be on the same node:
        let a2: Arc<KitsuneAgent> = TestVal::test_val();
        p2p.join(space.clone(), a2.clone(), None).await?;

        let res = p2p.rpc_single(space, a1, b"hello".to_vec(), None).await?;
        assert_eq!(b"echo: hello".to_vec(), res);

        harness.ghost_actor_shutdown().await?;
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_multi_request_workflow() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;

        let space = harness.add_space().await?;
        let (a1, p2p) = harness.add_direct_agent("DIRECT".into()).await?;
        // TODO when networking works, just add_*_agent again...
        // but for now, we need the two agents to be on the same node:
        let a2: Arc<KitsuneAgent> = TestVal::test_val();
        p2p.join(space.clone(), a2.clone(), None).await?;
        let a3: Arc<KitsuneAgent> = TestVal::test_val();
        p2p.join(space.clone(), a3.clone(), None).await?;

        let mut input = actor::RpcMulti::new(
            &Default::default(),
            space,
            TestVal::test_val(),
            b"test-multi-request".to_vec(),
        );
        input.max_remote_agent_count = 2;
        input.max_timeout = kitsune_p2p_types::KitsuneTimeout::from_millis(2000);
        let res = p2p.rpc_multi(input).await.unwrap();

        harness.ghost_actor_shutdown().await?;

        assert_eq!(1, res.len());
        for r in res {
            let data = String::from_utf8_lossy(&r.response);
            assert_eq!("echo: test-multi-request", &data);
            assert!(r.agent == a1 || r.agent == a2 || r.agent == a3);
        }

        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_single_agent_multi_request_workflow() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;

        let space = harness.add_space().await?;
        let (a1, p2p) = harness.add_direct_agent("DIRECT".into()).await?;

        let mut input = actor::RpcMulti::new(
            &Default::default(),
            space,
            TestVal::test_val(),
            b"test-multi-request".to_vec(),
        );
        input.max_remote_agent_count = 1;
        input.max_timeout = kitsune_p2p_types::KitsuneTimeout::from_millis(2000);
        let res = p2p.rpc_multi(input).await.unwrap();

        assert_eq!(1, res.len());
        for r in res {
            let data = String::from_utf8_lossy(&r.response);
            assert_eq!("echo: test-multi-request", &data);
            assert!(r.agent == a1);
        }

        harness.ghost_actor_shutdown().await.unwrap();
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_gossip_workflow() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;

        let space = harness.add_space().await?;
        let (a1, p2p) = harness.add_direct_agent("DIRECT".into()).await?;
        // TODO when networking works, just add_*_agent again...
        // but for now, we need the two agents to be on the same node:
        let a2: Arc<KitsuneAgent> = TestVal::test_val();
        p2p.join(space.clone(), a2.clone(), None).await?;

        let op1 = harness
            .inject_gossip_data(a1.clone(), "agent-1-data".to_string())
            .await?;

        // TODO - This doesn't work on fake nodes
        //        we need to actually add_*_agent to do this
        //let op2 = harness.inject_gossip_data(a2, "agent-2-data".to_string()).await?;

        tokio::time::sleep(std::time::Duration::from_millis(200)).await;

        let res = harness.dump_local_gossip_data(a1).await?;
        let (op_hash, data) = res.into_iter().next().unwrap();
        assert_eq!(op1, op_hash);
        assert_eq!("agent-1-data", &data);

        // TODO - This doesn't work on fake nodes
        //        we need to actually add_*_agent to do this
        //let res = harness.dump_local_gossip_data(a2).await?;
        //let (op_hash, data) = res.into_iter().next().unwrap();
        //assert_eq!(op2, op_hash);
        //assert_eq!("agent-2-data", &data);

        harness.ghost_actor_shutdown().await.unwrap();
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_peer_data_workflow() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_quic().await?;

        let space = harness.add_space().await?;
        let (a1, p2p) = harness.add_direct_agent("DIRECT".into()).await?;

        let res = harness.dump_local_peer_data(a1.clone()).await?;
        let num_agent_info = res.len();
        let (agent_hash, _agent_info) = res.into_iter().next().unwrap();
        assert_eq!(a1, agent_hash);
        assert_eq!(num_agent_info, 1);

        let a2: Arc<KitsuneAgent> = TestVal::test_val();
        p2p.join(space.clone(), a2.clone(), None).await?;

        tokio::time::sleep(std::time::Duration::from_millis(200)).await;

        let res = harness.dump_local_peer_data(a1.clone()).await?;
        let num_agent_info = res.len();

        assert!(res.contains_key(&a1));
        assert!(res.contains_key(&a2));
        assert_eq!(num_agent_info, 2);

        harness.ghost_actor_shutdown().await.unwrap();
        Ok(())
    }

    /// Test that we can gossip across a in memory transport layer.
    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "(david.b) these tests are becoming irrelevant, worth it to maintain?"]
    async fn test_gossip_transport() -> Result<(), KitsuneP2pError> {
        holochain_trace::test_run();
        let (harness, _evt) = spawn_test_harness_mem().await?;

        harness.add_space().await?;

        // - Add the first agent
        let (a1, _) = harness.add_direct_agent("one".into()).await?;

        // - Insert some data for agent 1
        let op1 = harness
            .inject_gossip_data(a1.clone(), "agent-1-data".to_string())
            .await?;

        // - Check agent one has the data
        let res = harness.dump_local_gossip_data(a1.clone()).await?;
        let num_gossip = res.len();
        let data = res.get(&op1);
        assert_eq!(Some(&"agent-1-data".to_string()), data);
        assert_eq!(num_gossip, 1);

        // - Add the second agent
        let (a2, _) = harness.add_direct_agent("two".into()).await?;

        // - Insert some data for agent 2
        let op2 = harness
            .inject_gossip_data(a2.clone(), "agent-2-data".to_string())
            .await?;

        // - Check agent two only has this data
        let res = harness.dump_local_gossip_data(a2.clone()).await?;
        let num_gossip = res.len();
        let data = res.get(&op2);
        assert_eq!(Some(&"agent-2-data".to_string()), data);
        assert_eq!(num_gossip, 1);

        // TODO: remove when we have bootstrapping for tests
        // needed until we have some way of bootstrapping
        harness.magic_peer_info_exchange().await?;

        // TODO - a better way to await gossip??
        tokio::time::sleep(std::time::Duration::from_millis(1500)).await;

        // - Check agent one now has all the data
        let res = harness.dump_local_gossip_data(a1.clone()).await?;
        let num_gossip = res.len();
        let data = res.get(&op1);
        assert_eq!(Some(&"agent-1-data".to_string()), data);
        let data = res.get(&op2);
        assert_eq!(Some(&"agent-2-data".to_string()), data);
        assert_eq!(num_gossip, 2);

        // - Check agent two now has all the data
        let res = harness.dump_local_gossip_data(a2.clone()).await?;
        let num_gossip = res.len();
        let data = res.get(&op1);
        assert_eq!(Some(&"agent-1-data".to_string()), data);
        let data = res.get(&op2);
        assert_eq!(Some(&"agent-2-data".to_string()), data);
        assert_eq!(num_gossip, 2);

        harness.ghost_actor_shutdown().await?;
        Ok(())
    }

    /// Test that we can publish agent info.
    #[tokio::test(flavor = "multi_thread")]
    // @freesig Can anyone think of a better way to do this?
    #[ignore = "Need a better way then waiting 6 minutes to test this"]
    async fn test_publish_agent_info() {
        holochain_trace::test_run();

        let (harness, _evt) = spawn_test_harness_mem().await.unwrap();

        harness.add_space().await.unwrap();

        // - Add the first agent
        let (a1, _) = harness.add_publish_only_agent("one".into()).await.unwrap();

        let peer_data = harness
            .dump_local_peer_data(dbg!(a1.clone()))
            .await
            .unwrap();
        let a1_peer_info = peer_data[&a1].clone();
        // - Add the second agent
        let (a2, _) = harness.add_publish_only_agent("two".into()).await.unwrap();

        // - Add the second agent
        let (a3, _) = harness
            .add_publish_only_agent("three".into())
            .await
            .unwrap();

        harness
            .inject_peer_info(a3.clone(), a1_peer_info.clone())
            .await
            .unwrap();

        // There's no way to trigger a publishing of peer data without waiting
        // for > five minutes.
        tokio::time::sleep(std::time::Duration::from_secs(6 * 60)).await;

        let a1_peers = harness.dump_local_peer_data(a1.clone()).await.unwrap();
        let a2_peers = harness.dump_local_peer_data(a2.clone()).await.unwrap();
        let a3_peers = harness.dump_local_peer_data(a3.clone()).await.unwrap();

        // a1 and a2 have each others peer info.
        assert!(a1_peers.get(&a3).is_some());
        assert!(a3_peers.get(&a1).is_some());

        // a2 doesn't have anyone's info and no one has a2's info.
        assert!(a1_peers.get(&a2).is_none());
        assert!(a3_peers.get(&a2).is_none());
        assert!(a2_peers.get(&a1).is_none());
        assert!(a2_peers.get(&a3).is_none());

        harness.ghost_actor_shutdown().await.unwrap();
    }
}
*/



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util.rs
================================================
//! Utilities to make kitsune testing a little more sane.

#![allow(dead_code)]

use crate::types::actor::*;
use crate::types::agent_store::*;
use crate::types::event::*;
use crate::*;
use futures::future::FutureExt;
use ghost_actor::dependencies::tracing;
use std::collections::HashMap;
use std::sync::Arc;

/// Utility trait for test values
pub trait TestVal: Sized {
    /// Create the test val
    fn test_val() -> Self;
}

/// Boilerplate shortcut for implementing TestVal on an item
#[macro_export]
macro_rules! test_val  {
    ($($item:ty => $code:block,)*) => {$(
        impl TestVal for $item { fn test_val() -> Self { $code } }
    )*};
}

/// internal helper to generate randomized kitsune data items
fn rand36<F: KitsuneBinType>() -> Arc<F> {
    use rand::Rng;
    let mut out = vec![0; 36];
    rand::thread_rng().fill(&mut out[..]);
    Arc::new(F::new(out))
}

// setup randomized TestVal::test_val() impls for kitsune data items
test_val! {
    Arc<KitsuneSpace> => { rand36() },
    Arc<KitsuneAgent> => { rand36() },
    Arc<KitsuneBasis> => { rand36() },
    Arc<KitsuneOpHash> => { rand36() },
}

/// Create a handler task and produce a Sender for interacting with it
pub async fn spawn_handler<H: KitsuneP2pEventHandler + ghost_actor::GhostControlHandler>(
    h: H,
) -> (
    futures::channel::mpsc::Sender<event::KitsuneP2pEvent>,
    tokio::task::JoinHandle<ghost_actor::GhostResult<()>>,
) {
    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();
    let (tx, rx) = futures::channel::mpsc::channel(4096);
    builder.channel_factory().attach_receiver(rx).await.unwrap();
    let driver = builder.spawn(h);
    (tx, tokio::task::spawn(driver))
}

pub fn hash_op_data(data: &[u8]) -> Arc<KitsuneOpHash> {
    Arc::new(KitsuneOpHash::new(
        blake2b_simd::Params::new()
            .hash_length(32)
            .hash(data)
            .as_bytes()
            .to_vec(),
    ))
}

/// Start a test signal server
pub async fn start_signal_srv() -> (std::net::SocketAddr, sbd_server::SbdServer) {
    let server = sbd_server::SbdServer::new(Arc::new(sbd_server::Config {
        bind: vec!["127.0.0.1:0".to_string(), "[::1]:0".to_string()],
        limit_clients: 100,
        ..Default::default()
    }))
    .await
    .unwrap();

    (*server.bind_addrs().first().unwrap(), server)
}

mod harness_event;
pub(crate) use harness_event::*;

mod harness_agent;
pub(crate) use harness_agent::*;

mod harness_actor;
#[allow(unused_imports)]
pub(crate) use harness_actor::*;

pub(crate) mod scenario_def_local;

pub mod data;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types.rs
================================================
use std::sync::Arc;

/// KitsuneP2p Error Type.
#[derive(Debug, thiserror::Error)]
#[non_exhaustive]
pub enum KitsuneP2pError {
    /// GhostError
    #[error(transparent)]
    GhostError(#[from] ghost_actor::GhostError),

    /// Base Kitsune Error
    #[error(transparent)]
    KitsuneError(#[from] kitsune_p2p_types::KitsuneError),

    /// RoutingSpaceError
    #[error("Routing Space Error: {0:?}")]
    RoutingSpaceError(Arc<KitsuneSpace>),

    /// RoutingAgentError
    #[error("Routing Agent Error: {0:?}")]
    RoutingAgentError(Arc<KitsuneAgent>),

    /// DecodingError
    #[error("Decoding Error: {0}")]
    DecodingError(Box<str>),

    /// std::io::Error
    #[error(transparent)]
    StdIoError(#[from] std::io::Error),

    /// Bootstrap call failed.
    #[error(transparent)]
    Bootstrap(#[from] BootstrapClientError),

    /// Other
    #[error("Other: {0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

pub use crate::actor::KitsuneP2pResult;

impl KitsuneP2pError {
    /// promote a custom error type to a KitsuneP2pError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self::Other(e.into())
    }

    /// generate a decoding error from a string
    pub fn decoding_error(s: String) -> Self {
        Self::DecodingError(s.into_boxed_str())
    }
}

impl From<String> for KitsuneP2pError {
    fn from(s: String) -> Self {
        #[derive(Debug, thiserror::Error)]
        struct OtherError(String);
        impl std::fmt::Display for OtherError {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                write!(f, "{}", self.0)
            }
        }

        KitsuneP2pError::other(OtherError(s))
    }
}

impl From<&str> for KitsuneP2pError {
    fn from(s: &str) -> Self {
        s.to_string().into()
    }
}

use kitsune_p2p_bootstrap_client::prelude::BootstrapClientError;
pub use kitsune_p2p_types::bin_types::*;

/// Data structures to be stored in the agent/peer database.
pub mod agent_store {
    pub use kitsune_p2p_types::agent_info::*;
}

pub mod actor;
pub mod event;
pub(crate) mod gossip;
#[allow(missing_docs)]
pub mod wire;

pub use gossip::GossipModuleType;
pub use kitsune_p2p_types::dht;
pub use kitsune_p2p_types::dht_arc;

#[allow(missing_docs)]
pub mod metrics;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/common.rs
================================================
use crate::agent_store::AgentInfoSigned;
use crate::meta_net::*;
use crate::types::*;
use kitsune_p2p_types::tx_utils::*;
use std::sync::Arc;

/// A bloom filter of Kitsune hash types
pub type BloomFilter = bloomfilter::Bloom<MetaOpKey>;

#[derive(Clone, Debug)]
pub(crate) enum HowToConnect {
    /// The connection handle and the url that this handle has been connected to.
    /// If the connection handle closes the url can change so we need to track it.
    Con(MetaNetCon, String),
    Url(String),
}

/// The key to use for referencing items in a bloom filter
#[derive(Debug, Clone, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum MetaOpKey {
    /// data key type
    Op(Arc<KitsuneOpHash>),

    /// agent key type
    Agent(Arc<KitsuneAgent>, u64),
}

/// The actual data added to a bloom filter
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub enum MetaOpData {
    /// data chunk type
    Op(Arc<KitsuneOpHash>, Vec<u8>),

    /// agent chunk type
    Agent(AgentInfoSigned),
}

pub(crate) fn encode_bloom_filter(bloom: &BloomFilter) -> PoolBuf {
    let bitmap: Vec<u8> = bloom.bitmap();
    let bitmap_bits: u64 = bloom.number_of_bits();
    let k_num: u32 = bloom.number_of_hash_functions();
    let sip_keys = bloom.sip_keys();
    let k1: u64 = sip_keys[0].0;
    let k2: u64 = sip_keys[0].1;
    let k3: u64 = sip_keys[1].0;
    let k4: u64 = sip_keys[1].1;

    let size = bitmap.len()
        + 8 // bitmap bits
        + 4 // k_num
        + (8 * 4) // k1-4
        ;

    let mut buf = PoolBuf::new();
    buf.reserve(size);

    buf.extend_from_slice(&bitmap_bits.to_le_bytes());
    buf.extend_from_slice(&k_num.to_le_bytes());
    buf.extend_from_slice(&k1.to_le_bytes());
    buf.extend_from_slice(&k2.to_le_bytes());
    buf.extend_from_slice(&k3.to_le_bytes());
    buf.extend_from_slice(&k4.to_le_bytes());
    buf.extend_from_slice(&bitmap);

    buf
}

pub(crate) fn decode_bloom_filter(bloom: &[u8]) -> BloomFilter {
    let bitmap_bits = u64::from_le_bytes(*arrayref::array_ref![bloom, 0, 8]);
    let k_num = u32::from_le_bytes(*arrayref::array_ref![bloom, 8, 4]);
    let k1 = u64::from_le_bytes(*arrayref::array_ref![bloom, 12, 8]);
    let k2 = u64::from_le_bytes(*arrayref::array_ref![bloom, 20, 8]);
    let k3 = u64::from_le_bytes(*arrayref::array_ref![bloom, 28, 8]);
    let k4 = u64::from_le_bytes(*arrayref::array_ref![bloom, 36, 8]);
    let sip_keys = [(k1, k2), (k3, k4)];
    bloomfilter::Bloom::from_existing(&bloom[44..], bitmap_bits, k_num, sip_keys)
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip.rs
================================================
//! The main (and only) Sharded gossiping strategy

#![warn(missing_docs)]

use crate::agent_store::AgentInfoSigned;
use crate::gossip::{decode_bloom_filter, encode_bloom_filter};
use crate::types::event::*;
use crate::types::gossip::*;
use crate::types::*;
use crate::{meta_net::*, HostApiLegacy};
use fetch_pool::GossipType;
use ghost_actor::dependencies::tracing;
use governor::clock::DefaultClock;
use governor::state::{InMemoryState, NotKeyed};
use governor::RateLimiter;
use kitsune_p2p_fetch::{FetchPool, FetchPoolReader, FetchSource, OpHashSized, TransferMethod};
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::codec::Codec;
use kitsune_p2p_types::config::*;
use kitsune_p2p_types::dht::arq::ArqSet;
use kitsune_p2p_types::dht::region::{Region, RegionData};
use kitsune_p2p_types::dht::region_set::RegionSetLtcs;
use kitsune_p2p_types::dht::ArqBounds;
use kitsune_p2p_types::dht_arc::DhtArcSet;
use kitsune_p2p_types::metrics::*;
use kitsune_p2p_types::tx_utils::*;
use kitsune_p2p_types::*;
use std::collections::{HashMap, HashSet, VecDeque};
use std::convert::{TryFrom, TryInto};
use std::sync::atomic::AtomicBool;
use std::sync::Arc;
use std::time::{Duration, SystemTime};
use tokio::time::Instant;

pub use self::bandwidth::BandwidthThrottle;
use self::ops::OpsBatchQueue;
use self::state_map::RoundStateMap;
use self::store::AgentInfoSession;
use crate::metrics::MetricsSync;

use super::{HowToConnect, MetaOpKey};

pub use bandwidth::BandwidthThrottles;

/// How quickly to run a gossip iteration which attempts to initiate
/// with a new target.
const GOSSIP_LOOP_INTERVAL: Duration = Duration::from_millis(100);

const AGENT_LIST_FETCH_INTERVAL: Duration = Duration::from_secs(1);

#[cfg(any(test, feature = "test_utils"))]
#[allow(missing_docs)]
pub mod test_utils;

mod accept;
mod agents;
mod bloom;
mod initiate;
mod ops;
mod state_map;
mod store;

mod bandwidth;
mod next_target;

// dead_code and unused_imports are allowed here because when compiling this
// code path due to test_utils, the helper functions defined in this module
// are not used due to the tests themselves not being compiled, so it's easier
// to do this than to annotate each function as `#[cfg(test)]`
#[cfg(test)]
pub(crate) mod tests;

/// max send buffer size (keep it under 16384 with a little room for overhead)
/// (this is not a tuning_param because it must be coordinated
/// with the constant in PoolBuf which cannot be set at runtime)
/// ^^ obviously we're no longer following the above advice..
///    in the case of the pool buf management, any gossips larger than
///    16384 will now be shrunk resulting in additional memory thrashing
const MAX_SEND_BUF_BYTES: usize = 16_000_000;

type BloomFilter = bloomfilter::Bloom<MetaOpKey>;

#[derive(Debug)]
struct TimedBloomFilter {
    /// The bloom filter for the time window.
    /// If this is none then we have no hashes
    /// for this time window.
    bloom: Option<BloomFilter>,
    /// The time window for this bloom filter.
    time: TimeWindow,
}

/// The entry point for the sharded gossip strategy.
///
/// This struct encapsulates the network communication concerns, mainly
/// managing the incoming and outgoing gossip queues. It contains a struct
/// which handles all other (local) aspects of gossip.
pub struct ShardedGossip {
    /// ShardedGossipLocal handles the non-networking concerns of gossip
    gossip: ShardedGossipLocal,
    // The endpoint to use for all outgoing comms
    ep_hnd: MetaNet,
    /// The internal mutable state
    pub(crate) state: Share<ShardedGossipState>,
    /// Bandwidth for incoming and outgoing gossip.
    bandwidth: Arc<BandwidthThrottle>,
}

impl std::fmt::Debug for ShardedGossip {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("ShardedGossip{...}").finish()
    }
}

/// Basic statistic for gossip loop processing performance.
struct Stats {
    start: Instant,
    last: Option<tokio::time::Instant>,
    avg_processing_time: std::time::Duration,
