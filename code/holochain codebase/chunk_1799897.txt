- Adds a new `DisabledAppReason::NotStartedAfterProvidingMemproofs` variant which effectively allows a new app status, corresponding to the specific state where a UI has just called `AppRequest::ProvideMemproofs`, but the app has not yet been Enabled for the first time.
- Adds a new app interface method `AppRequest::EnableAfterMemproofsProvided`, which allows enabling an app only if the app is in the `AppStatus::Disabled(DisabledAppReason::NotStartedAfterProvidingMemproofs)` state. Attempting to enable the app from other states (other than Running) will fail.
- Warrants are used under-the-hood in more places now:
  - When gossiping amongst authorities, if an authority has a warrant for some data being requested, they will send the warrant instead of the data to indicate the invalid status of that data
  - When requesting data through must\_get calls, warrants will be returned with the data. The data returned to the client remains the same, but under the hood any warrants will be cached for later use.
- Adds a `lineage` field to the DNA manifest, which declares forward compatibility for any hash in that list with this DNA
- Adds a `AdminRequest::GetCompatibleCells` method which returns CellId for all installed cells which use a DNA that is forward-compatible with a given DNA hash. This can be used to find a compatible cell for use with the `UseExisting` cell provisioning method (still to be implemented)

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

- Warrants: When an authority rejects another agent’s authored data, that authority creates a Warrant which is gossiped to the offending agent’s Agent Activity Authority, who then serves that warrant along with any `get_agent_activity` request.
- The `warrants` field of `AgentActivity` is now populated with warrants for that agent.
- Authorities author ChainFork warrants when detecting two actions by the same author with the same `prev_action`

## 0.4.0-dev.8

## 0.4.0-dev.7

- App manifest now includes a new `membrane_proofs_deferred: bool` field, which allows the membrane proofs for the app’s cells to be provided at a time after installation, allowing the app’s UI to guide the process of creating membrane proofs.
- Adds new `AppStatus::AwaitingMemproofs` to indicate an app which was installed with `membrane_proofs_deferred`
- Adds new app websocket method `ProvideMemproofs` for use with `membrane_proofs_deferred`

## 0.4.0-dev.6

## 0.4.0-dev.5

- Moved the WASM cache from the data directory to a subdirectory of the data directory named `wasm-cache`. Old content won’t be removed and WASMs will have to be recompiled into the new cache. \#3920
- Remove deprecated functions `consistency_10s` and `consistency_60s`. Use `await_consistency` instead.
- Remove deprecated type `SweetEasyInline`. Use `SweetInlineZomes` instead.
- Remove deprecated methods `SweetInlineZomes::callback` and `SweetInlineZomes::integrity_callback`. Use `SweetInlineZomes::function` and `SweetInlineZomes::integrity_function` instead.

## 0.4.0-dev.4

- Rename feature `sweetest` in Holochain crate to `sweettest` to match the crate name.
- App validation workflow: Reduce interval to re-trigger when dependencies are missing from 10 seconds to 100-1000 ms, according to number of missing dependencies.

## 0.4.0-dev.3

- App validation workflow: Fix bug where ops were stuck in app validation when multiple ops were requiring the same action or entry hash. Such ops were erroneously filtered out from validation for being marked as ops awaiting hashes and not unmarked as awaiting once the hashes had arrived.

## 0.4.0-dev.2

- System validation: Added a new rule that no new actions are allowed following a chain close action.
- App validation workflow: Add module-level documentation.
- Validation: Remove unused type `DhtOpOrder`. This type is superseded by `OpOrder`.

## 0.4.0-dev.1

- **BREAKING** - Serialization: Update of serialization packages `holochain-serialization` and `holochain-wasmer-*` leads to general message format change for enums. Previously an enum value like

<!-- end list -->

``` rust
enum Enum {
  Variant1,
  Variant2,
}
let value = Enum::Variant1;
```

was serialized as (JSON representation)

``` json
{
  "value": {
    "variant1": null
  }
}
```

Now it serializes to

``` json
{
  "value": "variant1"
}
```

- Adds a new admin interface call `RevokeAppAuthenticationToken` to revoke issued app authentication tokens. \#3765
- App validation workflow: Validate ops in sequence instead of in parallel. Ops validated one after the other have a higher chance of being validated if they depend on earlier ops. When validated in parallel, they potentially needed to await a next workflow run when the dependent op would have been validated.

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.48

## 0.3.0-beta-dev.47

- Connections to Holochain app interfaces are now app specific, so anywhere that you used to have to provide an `installed_app_id` or `app_id` in requests, that is no longer required and has been removed. For example, `AppRequest::AppInfo` no longer takes any parameters and will return information about the app the connection is authenticated with. \#3643
- Signals are now only sent to clients that are connected to the app emitting the signal. When a cell is created by the conductor, it gets the ability to broadcast signals to any clients that are connected to the app that the cell is part of. When a client authenticates a connection to an app interface, the broadcaster for that app is found and attached to the connection. Previously all connected clients saw all signals, and there was no requirement to authenticate before receiving them. This is important to be aware of - if you connect to an app interface for signals only, you will still have to authenticate before receiving signals. \#3643
- App websocket connections now require authentication. There is a new admin operation `AdminRequest::IssueAppAuthenticationToken` which must be used to issue a connection token for a specific app. That token can be used with any app interface that will permit a connection to that app. After establishing a client connection, the first message must be an Authenticate message (rather than Request or Signal) and contain an `AppAuthenticationRequest` as its payload. \#3622
- When creating an app interface with `AdminRequest::AttachAppInterface` it is possible to specify an `installed_app_id` which will require that connections to that app interface are for the specified app. \#3622
- `AdminRequest::ListAppInterfaces` has been changed from returning a list of ports to return a list of `AppInterfaceInfo` which includes the port as well as the `installed_app_id` and `allowed_origins` for that interface. \#3622

## 0.3.0-beta-dev.46

## 0.3.0-beta-dev.45

- App validation workflow: Mock network in unit tests using new type `GenericNetwork` to properly test `must_get_agent_activity`. Previously that was not possible, as all peers in a test case were authorities for each other and `must_get_agent_activity` would therefore not send requests to the network.
- App validation workflow: Skip ops that have missing dependencies. If an op is awaiting dependencies to be fetched, it will be excluded from app validation.
- App validation workflow: Integration workflow is only triggered when some ops have been validated (either accepted or rejected).
- App validation workflow: While op dependencies are missing and being fetched, the workflow is re-triggering itself periodically. It’ll terminate this re-triggering after an interval in which no more missing dependencies could be fetched.

## 0.3.0-beta-dev.44

- App validation workflow: Refactored to not wait for ops that the op being validated depends on, that are being fetched and thus keep the workflow occupied. The workflow no longer awaits the dependencies and instead sends off fetch requests in the background.
- `consistency_10s` and `consistency_60s` from `holochain::sweettest` are deprecated. Use `await_consistency` instead.

## 0.3.0-beta-dev.43

- BREAKING: Holochain websockets now require an `allowed_origins` configuration to be provided. When connecting to the websocket a matching origin must be specified in the connection request `Origin` header. [\#3460](https://github.com/holochain/holochain/pull/3460)
  - The `ConductorConfiguration` has been changed so that specifying an admin interface requires an `allowed_origins` as well as the port it already required.
  - `AdminRequest::AddAdminInterfaces` has been updated as per the previous point.
  - `AdminRequest::AttachAppInterface` has also been updated so that attaching app ports requires an `allowed_origins` as well as the port it already required.
- BREAKING: Split the authored database by author. It was previous partitioned by DNA only and each agent that shared a DB because they were running the same DNA would have to share the write lock. This is a pretty serious bottleneck when the same app is being run for multiple agents on the same conductor. They are now separate files on disk and writes can proceed independently. There is no migration path for this change, if you have existing databases they will not be found. [\#3450](https://github.com/holochain/holochain/pull/3450)

## 0.3.0-beta-dev.42

## 0.3.0-beta-dev.41

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

- Some of the function signatures around SweetConductor app installation have changed slightly. You may need to use a slice (`&[x]`) instead of a collection of references (`[&x]`), or vice versa, in some places. If this is cumbersome please open an issue. [\#3310](https://github.com/holochain/holochain/pull/3310)
- Start refactoring app validation workflow by simplifying main validation loop. All op validations are awaited at once now instead of creating a stream of tasks and processing it in the background.

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

- Added `lair_keystore_version_req` to the output of `--build-info` for Holochain.
- BREAKING: Changed `post_commit` behavior so that it only gets called after a commit to the source chain. Previously, it would get called after every zome call, regardless of if a commit happened. [\#3302](https://github.com/holochain/holochain/pull/3302)
- Fixed a performance bug: various extra tasks were being triggered after every zome call which are only necessary if the zome call resulted in commits to the source chain. The fix should improve performance for read-only zome calls. [\#3302](https://github.com/holochain/holochain/pull/3302)
- Fixed a bug during the admin call `GrantZomeCallCapability`, where if the source chain had not yet been initialized, it was possible to create a capability grant before the `init()` callback runs. Now, `init()` is guaranteed to run before any cap grants are created.
- Updates sys validation to allow the timestamps of two actions on the same chain to be equal, rather than requiring them to strictly increasing.

## 0.3.0-beta-dev.35

- There is no longer a notion of “joining the network”. Previously, apps could fail to be enabled, accompanied by an error “Timed out trying to join the network” or “Error while trying to join the network”. Now, apps never fail to start for this reason. If the network cannot be reached, the app starts anyway. It is up to the UI to determine whether the node is in an “online” state via `AppRequest::NetworkInfo` (soon-to-be improved with richer information).
- CellStatus is deprecated and only remains in areas where deserialization would break if it were removed. The only valid CellStatus now is `CellStatus::Joined`.

## 0.3.0-beta-dev.34

- Fix: Wasmer cache was deserializing modules for every zome call which slowed them down. Additionally the instance cache that was supposed to store callable instances of modules was not doing that correctly. A cache for deserialized modules has been re-introduced and the instance cache was removed, following recommendation from the wasmer team regarding caching.
- Fix: Call contexts of internal callbacks like `validate` were not cleaned up from an in-memory map. Now external as well as internal callbacks remove the call contexts from memory. This is covered by a test.
- **BREAKING CHANGE:** Wasmer-related items from `holochain_types` have been moved to crate `holochain_wasmer_host::module`.
- Refactor: Every ribosome used to create a separate wasmer module cache. During app installation of multiple agents on the same conductor, the caches weren’t used, regardless of whether that DNA is already registered or not. The module cache is now moved to the conductor and kept there as a single instance.

## 0.3.0-beta-dev.33

- Make sqlite-encrypted a default feature

- Sys validation will no longer check the integrity with the previous action for StoreRecord or StoreEntry ops. These ‘store record’ checks are now only done for RegisterAgentActivity ops which we are sent when we are responsible for validating an agents whole chain. This avoids fetching and caching ops that we don’t actually need.

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

- Sys validation will now validate that a DeleteLink points to an action which is a CreateLink through the `link_add_address` of the delete.

## 0.3.0-beta-dev.28

- Fix an issue where app validation for StoreRecord ops with a Delete or DeleteLink action were always passed to all zomes. These ops are now only passed to the zome which defined the entry type of the op that is being deleted. [\#3107](https://github.com/holochain/holochain/pull/3107)
- Wasmer bumped from 4.2.2 to 4.2.4 [\#3025](https://github.com/holochain/holochain/pull/3025)
- Compiled wasms are now persisted to the file system so no longer need to be recompiled on subsequent loads [\#3025](https://github.com/holochain/holochain/pull/3025)
- **BREAKING CHANGE** Several changes to the file system [\#3025](https://github.com/holochain/holochain/pull/3025):
  - The environment path in config file is now called `data_root_path`
  - The `data_root_path` is no longer optional so MUST be specified in config
  - Interactive mode is no longer supported, so paths MUST be provided in config
  - The database is in a `databases` subdirectory of the `data_root_path`
  - The keystore now consistently uses a `ks` directory, was previously inconsistent between `ks` and `keystore`
  - The compiled wasm cache now exists and puts artifacts in the `wasm` subdirectory of the `data_root_path`

## 0.3.0-beta-dev.27

- Refactor: Remove shadowing glob re-exports that were shadowing other exports.

- Fix: Countersigning test `lock_chain` which ensures that source chain is locked while in a countersigning session.

- Major refactor of the sys validation workflow to improve reliability and performance:
  
  - Reliability: The workflow will now prioritise validating ops that have their dependencies available locally. As soon as it has finished with those it will trigger app validation before dealing with missing dependencies.
  - Reliability: For ops which have dependencies we aren’t holding locally, the network get will now be retried. This was a cause of undesirable behaviour for validation where a failed get would result in validation for ops with missing dependencies not being retried until new ops arrived. The workflow now retries the get on an interval until it finds dependencies and can proceed with validation.
  - Performance and correctness: A feature which captured and processed ops that were discovered during validation has been removed. This had been added as an attempt to avoid deadlocks within validation but if that happens there’s a bug somewhere else. Sys validation needs to trust that Holochain will correctly manage its current arc and that we will get that data eventually through publishing or gossip. This probably wasn’t doing a lot of harm but it was uneccessary and doing database queries so it should be good to have that gone.
  - Performance: In-memory caching for sys validation dependencies. When we have to wait to validate an op because it has a missing dependency, any other actions required by that op will be held in memory rather than being refetched from the database. This has a fairly small memory footprint because actions are relatively small but saves repeatedly hitting the cascade for the same data if it takes a bit of time to find a dependency on the network.

- **BREAKING* CHANGE*: The `ConductorConfig` has been updated to add a new option for configuring conductor behaviour. This should be compatible with existing conductor config YAML files but if you are creating the struct directly then you will need to include the new field. Currently this just has one setting which controls how fast the sys validation workflow will retry network gets for missing dependencies. It’s likely this option will change in the near future.

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

- Fix: In many cases app validation would not be retriggered for ops that failed validation. Previously the app validation workflow had been retriggered only when the number of concurrent ops to be validated (50) was reached. Now the workflow will be retriggered whenever any ops could not be validated.

- Added a new check to system validation to ensure that the `original_entry_address` of an update points to the same entry hash that the original action pointed to. [3023](https://github.com/holochain/holochain/pull/3023)

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

- Fix an issue where enough validation receipts being received would not prevent the publish workflow from continuing to run. This was a terrible waste of data and compute and would build up over time as Holochain is used. [2931](https://github.com/holochain/holochain/pull/2931)
- Improve log output for op publishing to accurately reflect the number of ops to be published. The number published which is logged later is accurate and it was confusing to see more ops published than were supposed to be. [2922](https://github.com/holochain/holochain/pull/2922)
- Fix an issue which prevented the publish loop for a cell from suspending if there was either 1. publish activity pending for other cells or 2. enough validation receipts received. [2922](https://github.com/holochain/holochain/pull/2922)

## 0.3.0-beta-dev.21

- Fix an issue where receiving incoming ops can accidentally filter out some DHT data until Holochain is restarted. The state management for in-flight DHT ops is now guaranteed by a `Drop` implementation which will clean up state when the `incoming_dht_ops_workflow` finishes. [2913](https://github.com/holochain/holochain/pull/2913)
- Performance improvement when sending validation receipts. When a batch of DHT ops is being processed and an author is unreachable it will no longer spend time trying to send more receipts to that author in serial and instead it sends receipts as a single batch per author. [2848](https://github.com/holochain/holochain/pull/2848)
- Resilience improvement with handling keystore errors in the validation receipt workflow. Previously, all errors caused the workflow to restart from the beginning. This was good for transient errors such as the keystore being unavailable but it also meant that a single validation receipt failing to be signed (e.g. due to a local agent key being removed from the keystore) would prevent any more validation receipts being sent by that conductor. [2848](https://github.com/holochain/holochain/pull/2848)
- **BREAKING CHANGE** Addressed an outstanding technical debt item to make the validation receipt workflow send a network notification (fire and forget) rather than waiting for a response. When the validation receipt workflow was written this functionality wasn’t available but now that it is, sending validation receipts can be sped up by not waiting for a peer to respond. The format has also been changed from sending one receipt at a time to sending batches so it was not possible to maintain backwards compatibility here. [2848](https://github.com/holochain/holochain/pull/2848)

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

- Fix: App interfaces are persisted when shutting down conductor. After restart, app interfaces without connected receiver websocket had signal emission fail altogether. Send errors are only logged now instead.

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

- Change `GenesisFailed` error to include `CellId` so that genesis failures can be correlated with the cells that failed. [2616](https://github.com/holochain/holochain/pull/2616)

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

- **BREAKING CHANGE** updating the project lock file to use the latest version of `serde` at `1.0.185` has changed how enums get serialized and as a knock on effect it has changed some hashes. This will make databases from previous versions incompatible with the next version of Holochain.

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

- Improves error messages when validation fails with an InvalidCommit error
- Fixed bug where if signature verification fails due to the lair service being unavailable, validation could fail. Now, that failure is treated as a normal error, so validation cannot proceed. [\#2604](https://github.com/holochain/holochain/pull/2604)

## 0.3.0-beta-dev.10

- Adds experimental Chain Head Coordinator feature, allowing multiple machines to share the same source chain. Holochain must be built with the `chc` feature flag (disabled by default).

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

- Fixes race condition which caused network instability. Newly joined nodes can get temporarily blocked by other nodes, causing connections to be repeatedly dropped. [\#2534](https://github.com/holochain/holochain/pull/2534)

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

- **BREAKING CHANGE**: The DhtOp validation rules have been significantly expanded upon, and some logic around what ops are produced when has been altered. Your existing app may experience rejected ops due to these more strict rules.

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

- The feature `test_utils` is no longer a default feature. To consume `sweetest` from this crate please now use `default-features = false` and the feature `sweetest`.

## 0.2.0

## 0.2.0-beta-rc.7

## 0.2.0-beta-rc.6

- Feature renaming from `no-deps` to `sqlite` and `db-encryption` to `sqlite-encrypted`. It should not be necessary to configure these unless you are packaging `holochain` or have imported it as a dependency without default features. In the latter case, please update any references to the old feature names.

## 0.2.0-beta-rc.5

- Implements the `clone_only` cell provisioning strategy, desgined for situations where no cell should be installed upon app installation but clones may be created later, via `roles[].provisioning.strategy` in the app manifest [\#2243](https://github.com/holochain/holochain/pull/2243)

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

- BREAKING CHANGE - Removes conductor networking types “Proxy” (“proxy”) and “Quic” (“quic”). Please transition to “WebRTC” (“webrtc”). [\#2208](https://github.com/holochain/holochain/pull/2208)
- Adds `DumpNetworkStats` api to admin websocket [\#2182](https://github.com/holochain/holochain/pull/2182).
- System validation now ensures that all records in a source chain are by the same author [\#2189](https://github.com/holochain/holochain/pull/2189)

## 0.2.0-beta-rc.2

- Fixes bug where supplying a `network_seed` during an `InstallApp` call does not actually update the network seed for roles whose `provisioning` is set to `None` in the manifest. Now the network seed is correctly updated. [\#2102](https://github.com/holochain/holochain/pull/2102)

- If AppManifest specifies an `installed_hash` for a DNA, it will check the conductor for an already-registered DNA at that hash, ignoring the DNA passed in as part of the bundle. Note that this means you can install apps without passing in any DNA, if the DNAs are already installed in the conductor. [\#2157](https://github.com/holochain/holochain/pull/2157)

- Adds new functionality to the conductor admin API which returns disk storage information. The storage used by apps is broken down into blobs which are being used by one or more app.

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

- When uninstalling an app, local data is now cleaned up where appropriate. [\#1805](https://github.com/holochain/holochain/pull/1805)
  - Detail: any time an app is uninstalled, if the removal of that app’s cells would cause there to be no cell installed which uses a given DNA, the databases for that DNA space are deleted. So, if you have an app installed twice under two different agents and uninstall one of them, no data will be removed, but if you uninstall both, then all local data will be cleaned up. If any of your data was gossiped to other peers though, it will live on in the DHT, and even be gossiped back to you if you reinstall that same app with a new agent.
- Renames `OpType` to `FlatOp`, and `Op::to_type()` to `Op::flattened()`. Aliases for the old names still exist, so this is not a breaking change. [\#1909](https://github.com/holochain/holochain/pull/1909)
- Fixed a [problem with validation of Ops with private entry data](https://github.com/holochain/holochain/issues/1861), where  `Op::to_type()` would fail for private `StoreEntry` ops. [\#1910](https://github.com/holochain/holochain/pull/1910)

## 0.1.0

## 0.1.0-beta-rc.4

- Fix: Disabled clone cells are no longer started when conductor restarts. [\#1775](https://github.com/holochain/holochain/pull/1775)

## 0.1.0-beta-rc.3

- Fix: calling `emit_signal` from the `post_commit` callback caused a panic, this is now fixed [\#1749](https://github.com/holochain/holochain/pull/1749)
- Fix: When you install an app with a cell that already exists for the same agent, the installation will error now. [\#1773](https://github.com/holochain/holochain/pull/1773)
- Fixes problem where disabling and re-enabling an app causes all of its cells to become unresponsive to any `get*` requests. [\#1744](https://github.com/holochain/holochain/pull/1744)
- Fixes problem where a disabled cell can continue to respond to zome calls and transmit data until the conductor is restarted. [\#1761](https://github.com/holochain/holochain/pull/1761)
- Adds Ctrl+C handling, so that graceful conductor shutdown is possible. [\#1761](https://github.com/holochain/holochain/pull/1761)
- BREAKING CHANGE - Added zome name to the signal emitted when using `emit_signal`.

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

- All zome calls must now be signed by the provenance, the signature is of the hash of the unsigned zome call, a unique nonce and expiry is also required [1510](https://github.com/holochain/holochain/pull/1510/files)

## 0.0.175

- BREAKING CHANGE - `ZomeId` and `zome_id` renamed to `ZomeIndex` and `zome_index` [\#1667](https://github.com/holochain/holochain/pull/1667)
- BREAKING CHANGE - `AppEntryType.id` renamed to `AppEntryType.entry_index` [\#1667](https://github.com/holochain/holochain/pull/1667)
- BREAKING CHANGE - `AppEntryType` renamed to `AppEntryDef` [\#1667](https://github.com/holochain/holochain/pull/1667)
- BREAKING CHANGE - `AppEntryDefName` renamed to `AppEntryName` [\#1667](https://github.com/holochain/holochain/pull/1667)
- BREAKING CHANGE - `AppRoleId` renamed to `RoleName` [\#1667](https://github.com/holochain/holochain/pull/1667)

## 0.0.174

- BREAKING CHANGE - The max entry size has been lowered to 4MB (strictly 4,000,000 bytes) [\#1659](https://github.com/holochain/holochain/pull/1659)
- BREAKING CHANGE - `emit_signal` permissions are changed so that it can be called during `post_commit`, which previously was not allowed [\#1661](https://github.com/holochain/holochain/pull/1661)

## 0.0.173

## 0.0.172

- BREAKING CHANGE - Update wasmer crate dependency [\#1620](https://github.com/holochain/holochain/pull/1620)
- Adds GossipInfo app interface method, which returns data about historical gossip progress which can be used to implement a progress bar in app UIs. [\#1649](https://github.com/holochain/holochain/pull/1649)
- BREAKING CHANGE - Add `quantum_time` as a DNA modifier. The default is set to 5 minutes, which is what it was previously hardcoded to. DNA manifests do not need to be updated, but this will change the DNA hash of all existing DNAs.

## 0.0.171

## 0.0.170

- Add call to authorize a zome call signing key to Admin API [\#1641](https://github.com/holochain/holochain/pull/1641)
- Add call to request DNA definition to Admin API [\#1641](https://github.com/holochain/holochain/pull/1641)

## 0.0.169

## 0.0.168

- Fixes bug that causes crash when starting a conductor with a clone cell installed

## 0.0.167

- Adds `SweetConductorConfig`, which adds a few builder methods for constructing variations of the standard ConductorConfig

## 0.0.166

- Fix restore clone cell by cell id. This used to fail with a “CloneCellNotFound” error. [\#1603](https://github.com/holochain/holochain/pull/1603)

## 0.0.165

- Revert requiring DNA modifiers when registering a DNA. These modifiers were optional before and were made mandatory by accident.

## 0.0.164

- Add App API call to archive an existing clone cell. [\#1578](https://github.com/holochain/holochain/pull/1578)
- Add Admin API call to restore an archived clone cell. [\#1578](https://github.com/holochain/holochain/pull/1578)
- Add Admin API call to delete all archived clone cells of an app’s role. For example, there is a base cell with role `document` and clones `document.0`, `document.1` etc.; this call deletes all clones permanently that have been archived before. This is not reversable; clones cannot be restored afterwards. [\#1578](https://github.com/holochain/holochain/pull/1578)

## 0.0.163

- Fixed rare “arc is not quantizable” panic, issuing a warning instead. [\#1577](https://github.com/holochain/holochain/pull/1577)

## 0.0.162

- **BREAKING CHANGE**: Implement App API call `CreateCloneCell`. **Role ids must not contain a dot `.` any more.** Clone ids make use of the dot as a delimiter to separate role id and clone index. [\#1547](https://github.com/holochain/holochain/pull/1547)
- Remove conductor config legacy keystore config options. These config options have been broken since we removed legacy lair in \#1518, hence this fix itself is not a breaking change. Also adds the `lair_server_in_proc` keystore config option as the new default to run an embedded lair server inside the conductor process, no longer requiring a separate system process. [\#1571](https://github.com/holochain/holochain/pull/1571)

## 0.0.161

## 0.0.160

## 0.0.159

- Updates TLS certificate handling so that multiple conductors can share the same lair, but use different TLS certificates by storing a “tag” in the conductor state database. This should not be a breaking change, but *will* result in a new TLS certificate being used per conductor. [\#1519](https://github.com/holochain/holochain/pull/1519)

## 0.0.158

## 0.0.157

## 0.0.156

- Effectively disable Wasm metering by setting the cranelift cost\_function to always return 0. This is meant as a temporary stop-gap and give us time to figure out a configurable approach. [\#1535](https://github.com/holochain/holochain/pull/1535)

## 0.0.155

- **BREAKING CHANGE** - Removes legacy lair. You must now use lair-keystore \>= 0.2.0 with holochain. It is recommended to abandon your previous holochain agents, as there is not a straight forward migration path. To migrate: [dump the old keys](https://github.com/holochain/lair/blob/v0.0.11/crates/lair_keystore/src/bin/lair-keystore/main.rs#L38) -\> [write a utility to re-encode them](https://github.com/holochain/lair/tree/hc_seed_bundle-v0.1.2/crates/hc_seed_bundle) -\> [then import them to the new lair](https://github.com/holochain/lair/tree/lair_keystore-v0.2.0/crates/lair_keystore#lair-keystore-import-seed---help) – [\#1518](https://github.com/holochain/holochain/pull/1518)
- New solution for adding `hdi_version_req` field to the output of `--build-info` argument. [\#1523](https://github.com/holochain/holochain/pull/1523)

## 0.0.154

- Revert: “Add the `hdi_version_req` key:value field to the output of the `--build-info` argument” because it broke. [\#1521](https://github.com/holochain/holochain/pull/1521)
  
  Reason: it causes a build failure of the *holochain*  crate on crates.io

## 0.0.153

- Add the `hdi_version_req` key:value field to the output of the `--build-info` argument

## 0.0.152

- Adds `AdminRequest::UpdateCoordinators` that allows swapping coordinator zomes for a running happ.

## 0.0.151

- BREAKING CHANGE - Refactor: Property `integrity.uid` of DNA Yaml files renamed to `integrity.network_seed`. Functionality has not changed. [\#1493](https://github.com/holochain/holochain/pull/1493)
- Allow deterministic bindings (dna\_info() & zome\_info()) to the genesis self check [\#1491](https://github.com/holochain/holochain/pull/1491).

## 0.0.150

## 0.0.149

## 0.0.148

- Added networking logic for enzymatic countersigning [\#1472](https://github.com/holochain/holochain/pull/1472)
- Countersigning authority response network message changed to a session negotiation enum [/\#1472](https://github.com/holochain/holochain/pull/1472)

## 0.0.147

## 0.0.146

## 0.0.145

**MAJOR BREAKING CHANGE\!** This release includes a rename of two Holochain core concepts, which results in a LOT of changes to public APIs and type names:

- “Element” has been renamed to “Record”
- “Header” has been renamed to “Action”

All names which include these words have also been renamed accordingly.

As Holochain has evolved, the meaning behind these concepts, as well as our understanding of them, has evolved as well, to the point that the original names are no longer adequate descriptors. We chose new names to help better reflect what these concepts mean, to bring more clarity to how we write and talk about Holochain.

## 0.0.144

- Add functional stub for `x_salsa20_poly1305_shared_secret_create_random` [\#1410](https://github.com/holochain/holochain/pull/1410)
- Add functional stub for `x_salsa20_poly1305_shared_secret_export` [\#1410](https://github.com/holochain/holochain/pull/1410)
- Add functional stub for `x_salsa20_poly1305_shared_secret_ingest` [\#1410](https://github.com/holochain/holochain/pull/1410)
- Limit conductor calls to `10_000_000_000` Wasm operations [\#1386](https://github.com/holochain/holochain/pull/1386)

## 0.0.143

## 0.0.142

## 0.0.141

## 0.0.140

## 0.0.139

- Udpate lair to 0.1.3 - largely just documentation updates, but also re-introduces some dependency pinning to fix mismatch client/server version check [\#1377](https://github.com/holochain/holochain/pull/1377)

## 0.0.138

## 0.0.137

- Docs: Fix intra-doc links in all crates [\#1323](https://github.com/holochain/holochain/pull/1323)
- Update legacy lair to 0.0.10 - allowing “panicky” flag [\#1349](https://github.com/holochain/holochain/pull/1349)
- Udpate lair to 0.1.1 - allowing usage in path with whitespace [\#1349](https://github.com/holochain/holochain/pull/1349)

## 0.0.136

## 0.0.135

## 0.0.134

## 0.0.133

## 0.0.132

## 0.0.131

- When joining the network set arc size to previous value if available instead of full to avoid network load [1287](https://github.com/holochain/holochain/pull/1287)

## 0.0.130

- Workflow errors generally now log rather than abort the current app [1279](https://github.com/holochain/holochain/pull/1279/files)

- Fixed broken links in Rust docs [\#1284](https://github.com/holochain/holochain/pull/1284)

## 0.0.129

## 0.0.128

- Proxy server chosen from bootstrap server proxy\_list [1242](https://github.com/holochain/holochain/pull/1242)

<!-- end list -->

``` yaml
network:
  transport_pool:
    - type: proxy
      proxy_config:
        type: remote_proxy_client_from_bootstrap
        bootstrap_url: https://bootstrap.holo.host
        fallback_proxy_url: ~
```

## 0.0.127

- **BREAKING CHANGE** App validation callbacks are now run per `Op`. There is now only a single validation callback `fn validate(op: Op) -> ExternResult<ValidateCallbackResult>` that is called for each `Op`. See the documentation for `Op` for more details on what data is passed to the callback. There are example use cases in `crates/test_utils/wasm/wasm_workspace/`. For example in the `validate` test wasm. To update an existing app, you to this version all `validate_*` callbacks including `validate_create_link` must be changed to the new `validate(..)` callback. [\#1212](https://github.com/holochain/holochain/pull/1212).

- `RegisterAgentActivity` ops are now validated by app validation.

- Init functions can now make zome calls. [\#1186](https://github.com/holochain/holochain/pull/1186)

- Adds header hashing to `hash` host fn [1227](https://github.com/holochain/holochain/pull/1227)

- Adds blake2b hashing to `hash` host fn [1228](https://github.com/holochain/holochain/pull/1228)

## 0.0.126

## 0.0.125

## 0.0.124

## 0.0.123

- Fixes issue where holochain could get stuck in infinite loop when trying to send validation receipts. [\#1181](https://github.com/holochain/holochain/pull/1181).
- Additional networking metric collection and associated admin api `DumpNetworkMetrics { dna_hash: Option<DnaHash> }` for inspection of metrics [\#1160](https://github.com/holochain/holochain/pull/1160)
- **BREAKING CHANGE** - Schema change for metrics database. Holochain will persist historical metrics once per hour, if you do not clear the metrics database it will crash at that point. [\#1183](https://github.com/holochain/holochain/pull/1183)

## 0.0.122

- Adds better batching to validation workflows for much faster validation. [\#1167](https://github.com/holochain/holochain/pull/1167).

## 0.0.121

- **BREAKING CHANGE** Removed `app_info` from HDK [1108](https://github.com/holochain/holochain/pull/1108)
- Permissions on host functions now return an error instead of panicking [1141](https://github.com/holochain/holochain/pull/1141)
- Add `--build-info` CLI flag for displaying various information in JSON format. [\#1163](https://github.com/holochain/holochain/pull/1163)

## 0.0.120

## 0.0.119

## 0.0.118

- **BREAKING CHANGE** - Gossip now exchanges local peer info with `initiate` and `accept` request types. [\#1114](https://github.com/holochain/holochain/pull/1114).

## 0.0.117

## 0.0.116

## 0.0.115

- Fix [issue](https://github.com/holochain/holochain/issues/1100) where private dht ops were being leaked through the incoming ops sender. [1104](https://github.com/holochain/holochain/pull/1104).
- Kitsune now attempts to rebind the network interface in the event of endpoint shutdown. Note, it’s still recommended to bind to `0.0.0.0` as the OS provides additional resiliency for interfaces coming and going. [\#1083](https://github.com/holochain/holochain/pull/1083)
- **BREAKING CHANGE** current chain head including recent writes available in agent info [\#1079](https://github.com/holochain/holochain/pull/1079)
- **BREAKING (If using new lair)** If you are using the new (non-legacy) `lair_server` keystore, you will need to rebuild your keystore, we now pre-hash the passphrase used to access it to mitigate some information leakage. [\#1094](https://github.com/holochain/holochain/pull/1094)
- Better lair signature fallback child process management. The child process will now be properly restarted if it exits. (Note this can take a few millis on Windows, and may result in some signature errors.) [\#1094](https://github.com/holochain/holochain/pull/1094)

## 0.0.114

- `remote_signal` has always been a fire-and-forget operation. Now it also uses the more efficient fire-and-forget “notify” low-level networking plumbing. [\#1075](https://github.com/holochain/holochain/pull/1075)

- **BREAKING CHANGE** `entry_defs` added to `zome_info` and referenced by macros [PR1055](https://github.com/holochain/holochain/pull/1055)

- **BREAKING CHANGE**: The notion of “cell nicknames” (“nicks”) and “app slots” has been unified into the notion of “app roles”. This introduces several breaking changes. In general, you will need to rebuild any app bundles you are using, and potentially update some usages of the admin interface. In particular:
  
  - The `slots` field in App manifests is now called `roles`
  - The `InstallApp` admin method now takes a `role_id` field instead of a `nick` field
  - In the return value for any admin method which lists installed apps, e.g. `ListEnabledApps`, any reference to `"slots"` is now named `"roles"`
  - See [\#1045](https://github.com/holochain/holochain/pull/1045)

- Adds test utils for creating simulated networks. [\#1037](https://github.com/holochain/holochain/pull/1037).

- Conductor can take a mocked network for testing simulated networks. [\#1036](https://github.com/holochain/holochain/pull/1036)

- Added `DumpFullState` to the admin interface, as a more complete form of `DumpState` which returns full `Vec<DhtOp>` instead of just their count, enabling more introspection of the state of the cell [\#1065](https://github.com/holochain/holochain/pull/1065).

- **BREAKING CHANGE** Added function name to call info in HDK. [\#1078](https://github.com/holochain/holochain/pull/1078).

## 0.0.113

- Post commit is now infallible and expects no return value [PR1049](https://github.com/holochain/holochain/pull/1049)
- Always depend on `itertools` to make `cargo build --no-default-features` work [\#1060](https://github.com/holochain/holochain/pull/1060)
- `call_info` includes provenance and cap grant information [PR1063](https://github.com/holochain/holochain/pull/1063)
- Always depend on `itertools` to make `cargo build --no-default-features` work [\#1060](https://github.com/holochain/holochain/pull/1060)

## 0.0.112

- Always depend on `itertools` to make `cargo build --no-default-features` work [\#1060](https://github.com/holochain/holochain/pull/1060)

## 0.0.111

- `call_info` is now implemented [1047](https://github.com/holochain/holochain/pull/1047)

- `dna_info` now returns `DnaInfo` correctly [\#1044](https://github.com/holochain/holochain/pull/1044)
  
  - `ZomeInfo` no longer includes what is now on `DnaInfo`
  - `ZomeInfo` renames `zome_name` and `zome_id` to `name` and `id`
  - `DnaInfo` includes `name`, `hash`, `properties`

- `post_commit` hook is implemented now [PR 1000](https://github.com/holochain/holochain/pull/1000)

- Bump legacy lair version to 0.0.8 fixing a crash when error message was too long [\#1046](https://github.com/holochain/holochain/pull/1046)

- Options to use new lair keystore [\#1040](https://github.com/holochain/holochain/pull/1040)

<!-- end list -->

``` yaml
keystore:
  type: danger_test_keystore
```

or

``` yaml
keystore:
  type: lair_server
  connection_url: "unix:///my/path/socket?k=Foo"
```

## 0.0.110

- Publish now runs on a loop if there are ops still needing receipts. [\#1024](https://github.com/holochain/holochain/pull/1024)
- Batch peer store write so we use less transactions. [\#1007](https://github.com/holochain/holochain/pull/1007/).
- Preparation for new lair api [\#1017](https://github.com/holochain/holochain/pull/1017)
  - there should be no functional changes with this update.
  - adds new lair as an additional dependency and begins preparation for a config-time switch allowing use of new api lair keystore.
- Add method `SweetDnaFile::from_bundle_with_overrides` [\#1030](https://github.com/holochain/holochain/pull/1030)
- Some `SweetConductor::setup_app_*` methods now take anything iterable, instead of array slices, for specifying lists of agents and DNAs [\#1030](https://github.com/holochain/holochain/pull/1030)
- BREAKING conductor config changes [\#1031](https://github.com/holochain/holochain/pull/1031)

Where previously, you might have had:

``` yaml
use_dangerous_test_keystore: false
keystore_path: /my/path
passphrase_service:
  type: danger_insecure_from_config
  passphrase: "test-passphrase"
```

now you will use:

``` yaml
keystore:
  type: lair_server_legacy_deprecated
  keystore_path: /my/path
  danger_passphrase_insecure_from_config: "test-passphrase"
```

or:

``` yaml
keystore:
  type: danger_test_keystore_legacy_deprecated
```

## 0.0.109

- Make validation run concurrently up to 50 DhtOps. This allows us to make progress on other ops when waiting for the network. [\#1005](https://github.com/holochain/holochain/pull/1005)
- FIX: Prevent the conductor from trying to join cells to the network that are already in the process of joining. [\#1006](https://github.com/holochain/holochain/pull/1006)

## 0.0.108

- Refactor conductor to use parking lot rw lock instead of tokio rw lock. (Faster and prevents deadlocks.). [\#979](https://github.com/holochain/holochain/pull/979).

### Changed

- The scheduler should work now

## 0.0.107

## 0.0.106

### Changed

- All Holochain `Timestamp`s (including those in Headers) are now at the precision of microseconds rather than nanoseconds. This saves 4 bytes per timestamp in memory and on disk.
- Various database field names changed. **Databases created in prior versions will be incompatible.**
- HDK `sys_time` now returns a `holochain_zome_types::prelude::Timestamp` instead of a `core::time::Duration`.
- Exposes `UninstallApp` in the conductor admin API.

## 0.0.105

## 0.0.104

- Updates lair to 0.0.4 which pins rcgen to 0.8.11 to work around [https://github.com/est31/rcgen/issues/63](https://github.com/est31/rcgen/issues/63)

## 0.0.103

### Fixed

- This release solves the issues with installing happ bundles or registering DNA via the admin API concurrently. [\#881](https://github.com/holochain/holochain/pull/881).

### Changed

- Header builder now uses chain top timestamp for new headers if in the future
- Timestamps in headers require strict inequality in sys validation

## 0.0.102

### Known Issues :exclamation:

- We’ve become aware of a bug that locks up the conductor when installing happ bundles or registering DNA via the admin API concurrently. Please perform these actions sequentially until we’ve resolved the bug.

### Fixed

- Concurrent zome calls could cause the `init()` zome callback to run multiple times concurrently, causing `HeadMoved` errors. This is fixed, so that `init()` can only ever run once.
  - If a zome call has been waiting for another zome call to finish running `init()` for longer than 30 seconds, it will timeout.

### Changed

- Apps now have a more complex status. Apps now can be either enabled/disabled as well as running/stopped, the combination of which is captured by three distinctly named states:
  - “Running” (enabled + running) -\> The app is running normally
  - “Paused” (enabled + stopped) -\> The app is currently stopped due to some minor problem in one of its cells such as failed network access, but will start running again as soon as it’s able. Some Cells may still be running normally.
  - “Disabled” (disabled + stopped) -\> The app is stopped and will remain so until explicitly enabled via `EnableApp` admin method. Apps can be disabled manually via `DisableApp`, or automatically due to an unrecoverable error in a Cell.
- Some admin methods are deprecated due to the app status changes:
  - `ActivateApp` is deprecated in favor of `EnableApp`
  - `DeactivateApp` is deprecated in favor of `DisableApp`
- Apps will be automatically Paused if not all of their cells are able to join the network during startup

### Added

- `InstallAppBundle` command added to admin conductor API. [\#665](https://github.com/holochain/holochain/pull/665)
- `DnaSource` in conductor\_api `RegisterDna` call now can take a `DnaBundle` [\#665](https://github.com/holochain/holochain/pull/665)
- New admin interface methods:
  - `EnableApp` (replaces `ActivateApp`)
  - `DisableApp` (replaces `DeactivateApp`)
  - `StartApp` (used to attempt to manually restart a Paused app)
- Using the 3 level PLRU instance cache from latest holochain wasmer `v0.0.72`

## 0.0.101

This version contains breaking changes to the conductor API as well as a major upgrade to the underlying Wasm runtime.

***:exclamation: Performance impact***

The version of wasmer that is used in this holochain release contains bugs in the scoping of wasmer modules vs. instances, such that it blocks the proper release of memory and slows down execution of concurrent Wasm instances. While we were able to at least mitigate these effects and are coordinating with wasmer to find a proper solution as soon as possible.

The severity of these issues increases with cell concurrency, i.e. using multiple cells with the same DNA. Application development with a single conductor and a few cells are expected to work well unless your machine has serious resource restrictions.

### Added

- `InstallAppBundle` command added to admin conductor API. [\#665](https://github.com/holochain/holochain/pull/665)
- `DnaSource` in conductor\_api `RegisterDna` call now can take a `DnaBundle` [\#665](https://github.com/holochain/holochain/pull/665)

### Removed

- BREAKING:  `InstallAppDnaPayload` in admin conductor API `InstallApp` command now only accepts a hash.  Both properties and path have been removed as per deprecation warning.  Use either `RegisterDna` or `InstallAppBundle` instead. [\#665](https://github.com/holochain/holochain/pull/665)
- BREAKING: `DnaSource(Path)` in conductor\_api `RegisterDna` call now must point to `DnaBundle` as created by `hc dna pack` not a `DnaFile` created by `dna_util` [\#665](https://github.com/holochain/holochain/pull/665)

### CHANGED

- Updated to a version of `holochain_wasmer` that includes a migration to wasmer v2+. [\#773](https://github.com/holochain/holochain/pull/773/files), [\#801](https://github.com/holochain/holochain/pull/80), [\#836](https://github.com/holochain/holochain/pull/836)
- Introduced a simple instance cache to mitigate and potentially outweigh the effects of the aforementioned wasmer conditions [\#848](https://github.com/holochain/holochain/pull/848)

## 0.0.100

This is the first version number for the version of Holochain with a refactored state model (you may see references to it as Holochain RSM).

## 0.0.52-alpha2

*Note: Versions 0.0.52-alpha2 and older are belong to previous iterations of the Holochain architecture and are not tracked here.*



================================================
File: crates/holochain/benches/bench.rs
================================================
use ::fixt::prelude::*;
use criterion::criterion_group;
use criterion::BenchmarkId;
use criterion::Criterion;
use criterion::Throughput;
use hdk::prelude::*;
use holo_hash::fixt::AgentPubKeyFixturator;
use holochain::core::ribosome::ZomeCallInvocation;
use holochain_wasm_test_utils::TestWasm;
use holochain_wasm_test_utils::TestZomes;
use once_cell::sync::Lazy;
use std::sync::Mutex;

mod websocket;

static TOKIO_RUNTIME: Lazy<Mutex<tokio::runtime::Runtime>> = Lazy::new(|| {
    Mutex::new(
        tokio::runtime::Builder::new_multi_thread()
            .enable_all()
            .build()
            .unwrap(),
    )
});

static REAL_RIBOSOME: Lazy<Mutex<holochain::core::ribosome::real_ribosome::RealRibosome>> =
    Lazy::new(|| {
        Mutex::new(
            holochain::fixt::RealRibosomeFixturator::new(holochain::fixt::curve::Zomes(vec![
                TestWasm::Bench,
            ]))
            .next()
            .unwrap(),
        )
    });

static CELL_ID: Lazy<Mutex<holochain_zome_types::cell::CellId>> = Lazy::new(|| {
    Mutex::new(
        holochain_types::fixt::CellIdFixturator::new(Unpredictable)
            .next()
            .unwrap(),
    )
});

static CAP: Lazy<Mutex<holochain_zome_types::capability::CapSecret>> =
    Lazy::new(|| Mutex::new(CapSecretFixturator::new(Unpredictable).next().unwrap()));

static AGENT_KEY: Lazy<Mutex<AgentPubKey>> =
    Lazy::new(|| Mutex::new(AgentPubKeyFixturator::new(Unpredictable).next().unwrap()));

static HOST_ACCESS_FIXTURATOR: Lazy<
    Mutex<holochain::fixt::ZomeCallHostAccessFixturator<Unpredictable>>,
> = Lazy::new(|| {
    Mutex::new(holochain::fixt::ZomeCallHostAccessFixturator::new(
        Unpredictable,
    ))
});

pub fn wasm_call_n(c: &mut Criterion) {
    let mut group = c.benchmark_group("wasm_call_n");

    for n in [
        1,         // 1 byte
        1_000,     // 1 kB
        1_000_000, // 1 MB
    ] {
        group.throughput(Throughput::Bytes(n as _));

        group.bench_function(BenchmarkId::from_parameter(n), |b| {
            // bytes
            let bytes = vec![0; n];
            let _g = TOKIO_RUNTIME.lock().unwrap().enter();
            let ha = HOST_ACCESS_FIXTURATOR.lock().unwrap().next().unwrap();

            b.iter(|| {
                let zome: Zome = TestZomes::from(TestWasm::Bench).coordinator.erase_type();
                let i = ZomeCallInvocation {
                    cell_id: CELL_ID.lock().unwrap().clone(),
                    zome: zome.clone(),
                    cap_secret: Some(*CAP.lock().unwrap()),
                    fn_name: "echo_bytes".into(),
                    payload: ExternIO::encode(&bytes).unwrap(),
                    provenance: AGENT_KEY.lock().unwrap().clone(),
                    expires_at: Timestamp::now(),
                    nonce: [0; 32].into(),
                };
                let ribosome = REAL_RIBOSOME.lock().unwrap().clone();
                let fut = ribosome.maybe_call(ha.clone().into(), &i, zome, i.fn_name.clone());
                futures::executor::block_on(fut).unwrap();
            });
        });
    }

    group.finish();
}

criterion_group!(wasm, wasm_call_n);

fn main() {}

// @todo fix after fixing new InstallApp tests
// criterion_main!(wasm, websocket::websocket);



================================================
File: crates/holochain/benches/consistency.rs
================================================
use criterion::criterion_group;
use criterion::criterion_main;
use criterion::BenchmarkId;
use criterion::Criterion;

use holo_hash::EntryHash;
use holo_hash::EntryHashes;
use holochain::sweettest::*;
use holochain_test_wasm_common::AnchorInput;
use holochain_test_wasm_common::ManyAnchorInput;
use holochain_wasm_test_utils::TestWasm;
use tokio::runtime::Builder;
use tokio::runtime::Runtime;

// TODO: Produce a high data version of this bench.
// TODO: Add profile function to queries that need optimizing.
// TODO: Research indexing.

criterion_group!(benches, consistency);

criterion_main!(benches);

fn consistency(bench: &mut Criterion) {
    holochain_trace::test_run();
    let mut group = bench.benchmark_group("consistency");
    group.sample_size(
        std::env::var_os("BENCH_SAMPLE_SIZE")
            .and_then(|s| s.to_string_lossy().parse::<usize>().ok())
            .unwrap_or(100),
    );
    let runtime = rt();

    let (mut producer, mut consumer, others) = runtime.block_on(setup());
    if let Some(n) = std::env::var_os("BENCH_NUM_OPS") {
        let num_ops = n.to_string_lossy().parse::<usize>().unwrap();
        runtime.block_on(async {
            producer.fill(num_ops).await;
            let mut cells = vec![&consumer.cell, &producer.cell];
            cells.extend(others.cells.iter());
            await_consistency(50, cells).await.unwrap();
            // holochain_state::prelude::dump_tmp(consumer.cell.env());
        });
    }
    let mut cells = vec![consumer.cell.clone(), producer.cell.clone()];
    cells.extend(others.cells.clone());
    runtime.spawn(async move {
        producer.run().await;
        producer.conductor.shutdown().await;
    });
    group.bench_function(BenchmarkId::new("test", "test".to_string()), |b| {
        b.iter(|| {
            runtime.block_on(async { consumer.run(&cells[..]).await });
        });
    });
    runtime.block_on(async move {
        // The line below was added when migrating to rust edition 2021, per
        // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
        let _ = &others;
        consumer.conductor.shutdown().await;
        drop(consumer);
        for mut c in others.conductors {
            c.shutdown().await;
            drop(c);
        }
    });
    runtime.shutdown_background();
}

struct Producer {
    conductor: SweetConductor,
    cell: SweetCell,
    rx: tokio::sync::mpsc::Receiver<usize>,
}

struct Consumer {
    conductor: SweetConductor,
    cell: SweetCell,
    last: usize,
    tx: tokio::sync::mpsc::Sender<usize>,
}

struct Others {
    conductors: Vec<SweetConductor>,
    cells: Vec<SweetCell>,
}

impl Producer {
    async fn run(&mut self) {
        while let Some(mut i) = self.rx.recv().await {
            i += 1;
            let _: EntryHash = self
                .conductor
                .call(
                    &self.cell.zome("anchor"),
                    "anchor",
                    AnchorInput("alice".to_string(), i.to_string()),
                )
                .await;
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    async fn fill(&mut self, num_ops: usize) {
        let inputs: Vec<_> = (0..num_ops)
            .map(|i| AnchorInput("alice_fill".to_string(), i.to_string()))
            .collect();
        let _: Vec<EntryHash> = self
            .conductor
            .call(
                &self.cell.zome("anchor"),
                "anchor_many",
                ManyAnchorInput(inputs),
            )
            .await;
        // holochain_state::prelude::dump_tmp(self.cell.env());
    }
}

impl Consumer {
    async fn run(&mut self, cells: &[SweetCell]) {
        let start = std::time::Instant::now();
        let mut num = self.last;
        while num <= self.last {
            let hashes: EntryHashes = self
                .conductor
                .call(
                    &self.cell.zome("anchor"),
                    "list_anchor_addresses",
                    "alice".to_string(),
                )
                .await;
            num = hashes.0.len();
            if start.elapsed().as_secs() > 1 {
                for cell in cells {
                    await_consistency(1, [cell]).await.unwrap();
                }
            }
            // dump_tmp(self.cell.env());
            // dump_tmp(prod.env());
        }
        self.last = num;
        self.tx.send(num).await.unwrap();
    }
}

async fn setup() -> (Producer, Consumer, Others) {
    let (tx, rx) = tokio::sync::mpsc::channel(1);
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Anchor]).await;
    let config = SweetConductorConfig::standard().no_publish();
    let configs = vec![config; 5];
    let mut conductors = SweetConductorBatch::from_configs(configs.clone()).await;
    let apps = conductors.setup_app("app", [&dna]).await.unwrap();
    let mut cells = apps
        .into_inner()
        .into_iter()
        .map(|c| c.into_cells().into_iter().next().unwrap());
    let alice = cells.next().unwrap();
    let bobbo = cells.next().unwrap();

    conductors.exchange_peer_info().await;
    let mut conductors = conductors.into_inner().into_iter();
    tx.send(0).await.unwrap();

    (
        Producer {
            conductor: conductors.next().unwrap(),
            cell: alice,
            rx,
        },
        Consumer {
            conductor: conductors.next().unwrap(),
            cell: bobbo,
            tx,
            last: 0,
        },
        Others {
            conductors: conductors.collect(),
            cells: cells.collect(),
        },
    )
}

pub fn rt() -> Runtime {
    Builder::new_multi_thread().enable_all().build().unwrap()
}



================================================
File: crates/holochain/benches/websocket.rs
================================================
#![allow(unused_imports, unused_variables)]

use criterion::criterion_group;
use criterion::criterion_main;
use criterion::BenchmarkId;
use criterion::Criterion;
use criterion::Throughput;
use futures::StreamExt;
use holochain_types::prelude::*;
use holochain_util::tokio_helper;
use holochain_wasm_test_utils::TestWasm;
use std::time::Duration;
use tempfile::TempDir;

#[path = "../tests/tests/test_utils/mod.rs"]
mod test_utils;

use test_utils::*;
use tracing::debug;

// @todo fix after fixing new InstallApp tests
// pub fn websocket_concurrent_install(c: &mut Criterion) {
//     holochain_trace::test_run();

//     static REQ_TIMEOUT_MS: u64 = 15000;
//     static NUM_DNA_CONCURRENCY: &[(u16, usize)] = &[(1, 1), (8, 4), (64, 10)];
//     let admin_port = std::sync::atomic::AtomicUsize::new(9910);

//     let mut group = c.benchmark_group("websocket");
//     for (i, j) in NUM_DNA_CONCURRENCY {
//         group.throughput(Throughput::Elements(*i as u64 * *j as u64));

//         group.sample_size(10);
//         group.measurement_time(Duration::from_secs(20));

//         let bench_id = format!("{}_{}", i, j);
//         let bench_fn = group.bench_function(BenchmarkId::from_parameter(bench_id.clone()), |b| {
//             // separate the holochain spawn time from the measured time
//             b.iter_batched(
//                 || {
//                     tokio_helper::block_forever_on(async {
//                         let admin_port =
//                             admin_port.fetch_add(1, std::sync::atomic::Ordering::SeqCst) as u16;
//                         let tmp_dir = tempfile::tempdir().unwrap();

//                         let path = tmp_dir.path().to_path_buf();
//                         let environment_path = path.clone();
//                         let config = create_config(admin_port, environment_path);
//                         let config_path = write_config(path, &config);
//                         let holochain = start_holochain(config_path.clone()).await;

//                         let (client, _) = websocket_client_by_port(admin_port).await.unwrap();

//                         let zomes = vec![(TestWasm::Foo.into(), TestWasm::Foo.into())];

//                         (client, holochain, zomes)
//                     })
//                 },
//                 |(client, _holochain, zomes)| {
//                     tokio_helper::block_forever_on(async move {
//                         // without this holochain gets dropped too early
//                         let _holochain = _holochain;

//                         let install_tasks_stream =
//                             futures::stream::iter((0..*i).into_iter().map(|g| {
//                                 let mut client = client.clone();
//                                 let zomes = zomes.clone();

//                                 tokio::spawn(async move {
//                                     let agent_key =
//                                         generate_agent_pubkey(&mut client, REQ_TIMEOUT_MS).await;
//                                     debug!("[{}] Agent pub key generated: {}", g, agent_key);

//                                     // Install Dna
//                                     let name = format!("fake_dna_{}", g);
//                                     let dna = fake_dna_zomes_named(
//                                         &uuid::Uuid::new_v4().to_string(),
//                                         &name,
//                                         zomes,
//                                     );

//                                     let original_dna_hash = dna.dna_hash().clone();
//                                     let (fake_dna_path, _tmpdir) =
//                                         write_fake_dna_file(dna.clone()).await.unwrap();
//                                     let dna_hash = register_and_install_dna_named(
//                                         &mut client,
//                                         original_dna_hash.clone(),
//                                         agent_key,
//                                         fake_dna_path.clone(),
//                                         None,
//                                         name.clone(),
//                                         name.clone(),
//                                         REQ_TIMEOUT_MS,
//                                     )
//                                     .await;

//                                     debug!(
//                                         "[{}] installed dna with hash {} and name {}",
//                                         g, dna_hash, name
//                                     );
//                                 })
//                             }))
//                             .buffer_unordered(*j);

//                         let install_tasks =
//                             futures::StreamExt::collect::<Vec<_>>(install_tasks_stream);

//                         for r in install_tasks.await {
//                             r.unwrap();
//                         }
//                     })
//                 },
//                 criterion::BatchSize::LargeInput,
//             );
//         });
//     }

//     group.finish();
// }

// criterion_group!(websocket, websocket_concurrent_install);



================================================
File: crates/holochain/examples/hc-stress-test-behavior-1.rs
================================================
use holochain::test_utils::hc_stress_test::*;

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    let test = LocalBehavior1::new();

    loop {
        tokio::time::sleep(std::time::Duration::from_secs(30)).await;

        println!("{:#?}", &*test.lock().unwrap());
    }
}



================================================
File: crates/holochain/examples/hc-stress-test-behavior-2.rs
================================================
use holochain::test_utils::hc_stress_test::*;

use clap::Parser;

#[derive(Parser, Debug)]
struct Args {
    /// Number of DNAs to install on each conductor node.
    #[arg(long)]
    dna_count: u8,

    /// Number of conductor nodes to run for this test.
    #[arg(long)]
    node_count: u8,
}

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    holochain_trace::test_run();

    let args = Args::parse();

    let test = LocalBehavior2::new(args.dna_count, args.node_count);

    loop {
        tokio::time::sleep(std::time::Duration::from_secs(30)).await;

        println!("{:#?}", &*test.lock().unwrap());
    }
}



================================================
File: crates/holochain/examples/websocket_install_dna.rs
================================================
// use futures::StreamExt;
// use holochain_types::prelude::fake_dna_zomes_named;
// use holochain_types::prelude::write_fake_dna_file;
// use holochain_wasm_test_utils::TestWasm;

#[path = "../tests/tests/test_utils/mod.rs"]
mod test_utils;

// use test_utils::*;

// @todo fix test by using new InstallApp call
#[tokio::main]
pub async fn main() {
    //     static NUM_DNA: u8 = 100;
    //     static NUM_CONCURRENT_INSTALLS: usize = 2;
    //     static REQ_TIMEOUT_MS: u64 = 30000;

    //     holochain_trace::test_run();
    //     // NOTE: This is a full integration test that
    //     // actually runs the holochain binary

    //     let admin_port = 9211;

    //     let zomes = vec![(TestWasm::Foo.into(), TestWasm::Foo.into())];
    //     let (client, _) = websocket_client_by_port(admin_port).await.unwrap();

    //     let install_tasks_stream = futures::stream::iter((0..NUM_DNA).into_iter().map(|i| {
    //         let mut client = client.clone();
    //         let zomes = zomes.clone();

    //         tokio::spawn(async move {
    //             let agent_key = generate_agent_pubkey(&mut client, REQ_TIMEOUT_MS).await;
    //             println!("[{}] Agent pub key generated: {}", i, agent_key);

    //             // Install Dna
    //             let name = format!("fake_dna_{}", i);
    //             let dna = fake_dna_zomes_named(&uuid::Uuid::new_v4().to_string(), &name, zomes);

    //             let original_dna_hash = dna.dna_hash().clone();
    //             let (fake_dna_path, _tmpdir) = write_fake_dna_file(dna.clone()).await.unwrap();
    //             let dna_hash = register_and_install_dna_named(
    //                 &mut client,
    //                 original_dna_hash.clone(),
    //                 agent_key,
    //                 fake_dna_path.clone(),
    //                 None,
    //                 name.clone(),
    //                 name.clone(),
    //                 REQ_TIMEOUT_MS,
    //             )
    //             .await;

    //             println!(
    //                 "[{}] installed dna with hash {} and name {}",
    //                 i, dna_hash, name
    //             );
    //         })
    //     }))
    //     .buffer_unordered(NUM_CONCURRENT_INSTALLS);

    //     let install_tasks = futures::StreamExt::collect::<Vec<_>>(install_tasks_stream);

    //     for r in install_tasks.await {
    //         r.unwrap();
    //     }
}



================================================
File: crates/holochain/src/conductor.rs
================================================
//! A Conductor manages interactions between its contained [Cell]s, as well as
//! interactions with the outside world. It is primarily a mediator of messages.
//!
//! The Conductor exposes two types of external interfaces:
//! - App interface: used by Holochain app UIs to drive the behavior of Cells,
//! - Admin interface: used to modify the Conductor itself, including adding and removing Cells
//!
//! It also exposes an internal interface to Cells themselves, allowing Cells
//! to call zome functions on other Cells, as well as to send Signals to the
//! outside world

#![deny(missing_docs)]

// TODO: clean up allow(missing_docs) once parent is fully documented

pub mod api;
mod cell;
#[cfg(feature = "chc")]
pub mod chc;
#[allow(clippy::module_inception)]
#[allow(missing_docs)]
pub mod conductor;
#[allow(missing_docs)]
pub mod config;
pub mod entry_def_store;
#[allow(missing_docs)]
pub mod error;
pub mod interface;
pub mod kitsune_host_impl;
pub mod manager;
mod metrics;
pub mod p2p_agent_store;
pub mod paths;
#[allow(missing_docs)]
pub mod ribosome_store;
pub mod space;
pub mod state;

pub use cell::error::CellError;
pub use cell::Cell;
pub use conductor::Conductor;
pub use conductor::ConductorBuilder;
pub use conductor::ConductorHandle;
pub use conductor::{full_integration_dump, integration_dump};

#[cfg(test)]
mod tests;



================================================
File: crates/holochain/src/core.rs
================================================
//! Defines the core Holochain workflows

#![deny(missing_docs)]

pub mod queue_consumer;
#[allow(missing_docs)]
pub mod ribosome;
mod validation;
#[allow(missing_docs)]
pub mod workflow;

mod metrics;
mod share;
mod sys_validate;

pub use sys_validate::*;



================================================
File: crates/holochain/src/docs.rs
================================================
//! Holochain conductor usage documentation.

/// How holochain conductor makes use of rust tracing/logging.
///
/// ### `RUST_LOG` Environment Variable
///
/// See the [tracing-subscriber](https://crates.io/crates/tracing-subscriber)
/// crate's documentation for usage of the EnvFilter. Essentially you can
/// specify tracing/logging levels via the environment variable RUST_LOG.
///
/// Examples:
/// - `RUST_LOG=warn` - only print warn and error levels
/// - `RUST_LOG=trace` - print ALL very verbose tracing logs
/// - `RUST_LOG=off,NETAUDIT=trace` - print ONLY the "NETAUDIT" target tracing
///
/// ### Conductor Config `tracing_override` Field
///
/// If, for some reason you cannot specify an environment variable, you
/// can also set the tracing level via the conductor config
/// `tracing_override` field.
///
/// ### `NETAUDIT` Target
///
/// The special `NETAUDIT` target is a cross-crate tracing target for
/// getting a handle on what conductor is doing with remote communications
/// under the hood. Specific traces are output in:
/// - sbd-client
/// - tx5-connection
/// - tx5
/// - kitsune_p2p
/// - and holochain itself
///
/// Where appropriate, try to set some standardized properties on the trace:
///
/// - `m` - module or crate in which the trace is defined
/// - `t` - type or additional internal context for making sense of the trace
/// - `a` - action or event described by the trace
///
/// E.g.: `m="tx5" t="signal" a="connected"`
///
/// To see the output, use a tracing configuration such as
/// `RUST_LOG=off,NETAUDIT=trace`.
pub mod tracing {}



================================================
File: crates/holochain/src/fixt.rs
================================================
pub mod curve;

use crate::conductor::api::CellConductorReadHandle;
use crate::conductor::api::DpkiApi;
use crate::conductor::api::MockCellConductorReadHandleT;
use crate::core::ribosome::guest_callback::entry_defs::EntryDefsHostAccess;
use crate::core::ribosome::guest_callback::entry_defs::EntryDefsInvocation;
use crate::core::ribosome::guest_callback::init::InitHostAccess;
use crate::core::ribosome::guest_callback::init::InitInvocation;
use crate::core::ribosome::guest_callback::post_commit::PostCommitHostAccess;
use crate::core::ribosome::guest_callback::post_commit::PostCommitInvocation;
use crate::core::ribosome::guest_callback::validate::ValidateHostAccess;
#[cfg(feature = "wasmer_sys")]
use crate::core::ribosome::real_ribosome::ModuleCacheLock;
use crate::core::ribosome::real_ribosome::RealRibosome;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::FnComponents;
use crate::core::ribosome::HostContext;
use crate::core::ribosome::InvocationAuth;
use crate::core::ribosome::ZomeCallHostAccess;
use crate::core::ribosome::ZomesToInvoke;
use crate::sweettest::SweetDnaFile;
use crate::test_utils::fake_genesis;
use ::fixt::prelude::*;
pub use holo_hash::fixt::*;
use holo_hash::WasmHash;
use holochain_keystore::test_keystore;
use holochain_keystore::MetaLairClient;
use holochain_p2p::HolochainP2pDnaFixturator;
use holochain_state::host_fn_workspace::HostFnWorkspace;
use holochain_state::host_fn_workspace::HostFnWorkspaceRead;
use holochain_types::db_cache::DhtDbQueryCache;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;
#[cfg(feature = "wasmer_sys")]
use holochain_wasmer_host::module::ModuleCache;
use rand::seq::IteratorRandom;
use rand::thread_rng;
use rand::Rng;
use std::collections::BTreeMap;
use std::sync::Arc;
use strum::IntoEnumIterator;
use tokio::sync::broadcast;

pub use holochain_types::fixt::*;

newtype_fixturator!(FnComponents<Vec<String>>);

fixturator!(
    RealRibosome;
    constructor fn empty(DnaFile);
);

impl Iterator for RealRibosomeFixturator<curve::Zomes> {
    type Item = RealRibosome;

    fn next(&mut self) -> Option<Self::Item> {
        let input = self.0.curve.0.clone();
        let uuid = StringFixturator::new(Unpredictable).next().unwrap();
        let (dna_file, _, _) = tokio_helper::block_forever_on(async move {
            SweetDnaFile::from_test_wasms(uuid, input, Default::default()).await
        });

        #[cfg(feature = "wasmer_wamr")]
        let module_cache = None;
        #[cfg(feature = "wasmer_sys")]
        let module_cache = Some(Arc::new(ModuleCacheLock::new(ModuleCache::new(None))));

        let ribosome =
            tokio_helper::block_forever_on(RealRibosome::new(dna_file, module_cache)).unwrap();

        // warm the module cache for each wasm in the ribosome
        for zome in self.0.curve.0.clone() {
            let mut call_context = CallContextFixturator::new(Empty).next().unwrap();
            call_context.zome = CoordinatorZome::from(zome).erase_type();
            tokio_helper::block_forever_on(ribosome.build_module(call_context.zome.zome_name()))
                .unwrap();
        }

        self.0.index += 1;

        Some(ribosome)
    }
}

fixturator!(
    DnaWasm;
    // note that an empty wasm will not compile
    curve Empty DnaWasm { code: Default::default() };
    curve Unpredictable TestWasm::iter().choose(&mut thread_rng()).unwrap().into();
    curve Predictable TestWasm::iter().cycle().nth(get_fixt_index!()).unwrap().into();
);

fixturator!(
    WasmMap;
    curve Empty BTreeMap::new().into();
    curve Unpredictable {
        let mut rng = rand::thread_rng();
        let number_of_wasms = rng.gen_range(0..5);

        let mut wasms = BTreeMap::new();
        let mut dna_wasm_fixturator = DnaWasmFixturator::new(Unpredictable);
        for _ in 0..number_of_wasms {
            let wasm = dna_wasm_fixturator.next().unwrap();
            wasms.insert(
                tokio_helper::block_forever_on(
                    async { WasmHash::with_data(&wasm).await },
                ),
                wasm,
            );
        }
        wasms.into()
    };
    curve Predictable {
        let mut wasms = BTreeMap::new();
        let mut dna_wasm_fixturator = DnaWasmFixturator::new_indexed(Predictable, get_fixt_index!());
        for _ in 0..3 {
            let wasm = dna_wasm_fixturator.next().unwrap();
            wasms.insert(
                tokio_helper::block_forever_on(
                    async { WasmHash::with_data(&wasm).await },
                ),
                wasm,
            );
        }
        wasms.into()
    };
);

fixturator!(
    DnaFile;
    curve Empty {
        DnaFile::from_parts(
            DnaDefFixturator::new(Empty).next().unwrap().into_hashed(),
            WasmMapFixturator::new(Empty).next().unwrap(),
        )
    };
    curve Unpredictable {
        // align the wasm hashes across the file and def
        let mut zome_name_fixturator = ZomeNameFixturator::new(Unpredictable);
        let wasms = WasmMapFixturator::new(Unpredictable).next().unwrap();
        let mut zomes: IntegrityZomes = Vec::new();
        for (hash, _) in wasms {
            zomes.push((
                zome_name_fixturator.next().unwrap(),
                IntegrityZomeDef::from_hash(
                    hash.to_owned()
                ),
            ));
        }
        let mut dna_def = DnaDefFixturator::new(Unpredictable).next().unwrap();
        dna_def.integrity_zomes = zomes;
        let dna = dna_def.into_hashed();
        DnaFile::from_parts(dna, WasmMapFixturator::new(Unpredictable).next().unwrap())
    };
    curve Predictable {
        // align the wasm hashes across the file and def
        let mut zome_name_fixturator =
            ZomeNameFixturator::new_indexed(Predictable, get_fixt_index!());
        let wasms = WasmMapFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap();
        let mut zomes: IntegrityZomes = Vec::new();
        for (hash, _) in wasms {
            zomes.push((
                zome_name_fixturator.next().unwrap(),
                IntegrityZomeDef::from_hash(
                    hash.to_owned()
                ),
            ));
        }
        let mut dna_def = DnaDefFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap();
        dna_def.integrity_zomes = zomes;
        let dna = dna_def.into_hashed();
        DnaFile::from_parts(
            dna,
            WasmMapFixturator::new_indexed(Predictable, get_fixt_index!())
                .next()
                .unwrap(),
        )
    };
);

// fixturator!(
//     LinkMetaVal;
//     constructor fn new(ActionHash, EntryHash, Timestamp, u8, LinkTag);
// );

// impl Iterator for LinkMetaValFixturator<(EntryHash, LinkTag)> {
//     type Item = LinkMetaVal;
//     fn next(&mut self) -> Option<Self::Item> {
//         let mut f = fixt!(LinkMetaVal);
//         f.target = self.0.curve.0.clone();
//         f.tag = self.0.curve.1.clone();
//         Some(f)
//     }
// }

fixturator!(
    MetaLairClient;
    curve Empty {
        tokio_helper::block_forever_on(async {
            // an empty keystore
            holochain_keystore::spawn_test_keystore().await.unwrap()
        })
    };
    curve Unpredictable {
        // TODO: Make this unpredictable
        tokio_helper::block_forever_on(async {
            holochain_keystore::spawn_test_keystore().await.unwrap()
        })
    };
    // a prepopulate keystore with hardcoded agents in it
    curve Predictable test_keystore();
);

// XXX: This may not be great to just grab an environment for this purpose.
//      It is assumed that this value is never really used in any "real"
//      way, because previously, it was implemented as a null pointer
//      wrapped in an UnsafeZomeCallWorkspace
fixturator!(
    HostFnWorkspace;
    curve Empty {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db();
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            fake_genesis(authored_db.to_db(), dht_db.to_db(), keystore.clone()).await.unwrap();
            HostFnWorkspace::new(
                authored_db.to_db(),
                dht_db.to_db(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(fixt!(AgentPubKey, Predictable, get_fixt_index!())),
                Arc::new(fixt!(DnaDef))
            ).await.unwrap()
        })
    };
    curve Unpredictable {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db();
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            fake_genesis(authored_db.to_db(), dht_db.to_db(), keystore.clone()).await.unwrap();
            HostFnWorkspace::new(
                authored_db.to_db(),
                dht_db.to_db(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(fixt!(AgentPubKey, Predictable, get_fixt_index!())),
                Arc::new(fixt!(DnaDef))
            ).await.unwrap()
        })
    };
    curve Predictable {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db_with_id(get_fixt_index!() as u8);
        let agent = fixt!(AgentPubKey, Predictable, get_fixt_index!());
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            crate::test_utils::fake_genesis_for_agent(authored_db.to_db(), dht_db.to_db(), agent.clone(), keystore.clone()).await.unwrap();
            HostFnWorkspace::new(
                authored_db.to_db(),
                dht_db.to_db(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(agent),
                Arc::new(fixt!(DnaDef))
            ).await.unwrap()
        })
    };
);

fixturator!(
    HostFnWorkspaceRead;
    curve Empty {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db();
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            fake_genesis(authored_db.to_db(), dht_db.to_db(), keystore.clone()).await.unwrap();
            HostFnWorkspaceRead::new(
                authored_db.to_db().into(),
                dht_db.to_db().into(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(fixt!(AgentPubKey, Predictable, get_fixt_index!())),
                Arc::new(fixt!(DnaDef))
            ).await.unwrap()
        })
    };
    curve Unpredictable {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db();
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            fake_genesis(authored_db.to_db(), dht_db.to_db(), keystore.clone()).await.unwrap();
            HostFnWorkspaceRead::new(
                authored_db.to_db().into(),
                dht_db.to_db().into(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(fixt!(AgentPubKey, Predictable, get_fixt_index!())),
                Arc::new(fixt!(DnaDef)),
            ).await.unwrap()
        })
    };
    curve Predictable {
        let authored_db = holochain_state::test_utils::test_authored_db_with_id(get_fixt_index!() as u8);
        let dht_db = holochain_state::test_utils::test_dht_db_with_id(get_fixt_index!() as u8);
        let cache = holochain_state::test_utils::test_cache_db_with_id(get_fixt_index!() as u8);
        let agent = fixt!(AgentPubKey, Predictable, get_fixt_index!());
        let keystore = holochain_keystore::test_keystore();
        tokio_helper::block_forever_on(async {
            crate::test_utils::fake_genesis_for_agent(authored_db.to_db(), dht_db.to_db(), agent.clone(), keystore.clone()).await.unwrap();
            HostFnWorkspaceRead::new(
                authored_db.to_db().into(),
                dht_db.to_db().into(),
                DhtDbQueryCache::new(dht_db.to_db().into()),
                cache.to_db(),
                keystore,
                Some(agent),
                Arc::new(fixt!(DnaDef))
            ).await.unwrap()
        })
    };
);

fn make_call_zome_handle() -> CellConductorReadHandle {
    Arc::new(MockCellConductorReadHandleT::new())
}

fixturator!(
    CellConductorReadHandle;
    vanilla fn make_call_zome_handle();
);

fixturator!(
    ZomeCallHostAccess;
    curve Empty ZomeCallHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Empty).next().unwrap(),
        // No DPKI fixturator
        dpki: None,
        keystore: MetaLairClientFixturator::new(Empty).next().unwrap(),
        network: HolochainP2pDnaFixturator::new(Empty).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new(Empty).next().unwrap(),
    };
    curve Unpredictable ZomeCallHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Unpredictable).next().unwrap(),
        // No DPKI fixturator
        dpki: None,
        keystore: MetaLairClientFixturator::new(Unpredictable).next().unwrap(),
        network: HolochainP2pDnaFixturator::new(Unpredictable).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new(Unpredictable).next().unwrap(),
    };
    curve Predictable ZomeCallHostAccess {
        workspace: HostFnWorkspaceFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        // No DPKI fixturator
        dpki: None,
        keystore: MetaLairClientFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        network: HolochainP2pDnaFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
    };
);

fixturator!(
    EntryDefsInvocation;
    constructor fn new();
);

fixturator!(
    EntryDefsHostAccess;
    constructor fn new();
);

fixturator!(
    InitInvocation;
    constructor fn new(DnaDef);
);

fixturator!(
    InitHostAccess;
    curve Empty InitHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Empty).next().unwrap(),
        keystore: MetaLairClientFixturator::new(Empty).next().unwrap(),
        // DPKI cannot be fixturated.
        dpki: None,
        network: HolochainP2pDnaFixturator::new(Empty).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new(Empty).next().unwrap(),
    };
    curve Unpredictable InitHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Unpredictable).next().unwrap(),
        keystore: MetaLairClientFixturator::new(Unpredictable).next().unwrap(),
        // DPKI cannot be fixturated.
        dpki: None,
        network: HolochainP2pDnaFixturator::new(Unpredictable).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new(Unpredictable).next().unwrap(),
    };
    curve Predictable InitHostAccess {
        workspace: HostFnWorkspaceFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        keystore: MetaLairClientFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        // DPKI cannot be fixturated.
        dpki: None,
        network: HolochainP2pDnaFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        signal_tx: broadcast::channel(50).0,
        call_zome_handle: CellConductorReadHandleFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
    };
);

fixturator!(
    PostCommitInvocation;
    constructor fn new(CoordinatorZome, SignedActionHashedVec);
);

fixturator!(
    PostCommitHostAccess;
    curve Empty PostCommitHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Empty).next().unwrap(),
        keystore: MetaLairClientFixturator::new(Empty).next().unwrap(),
        network: HolochainP2pDnaFixturator::new(Empty).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
    };
    curve Unpredictable PostCommitHostAccess {
        workspace: HostFnWorkspaceFixturator::new(Unpredictable).next().unwrap(),
        keystore: MetaLairClientFixturator::new(Unpredictable).next().unwrap(),
        network: HolochainP2pDnaFixturator::new(Unpredictable).next().unwrap(),
        signal_tx: broadcast::channel(50).0,
    };
    curve Predictable PostCommitHostAccess {
        workspace: HostFnWorkspaceFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        keystore: MetaLairClientFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        network: HolochainP2pDnaFixturator::new_indexed(Predictable, get_fixt_index!())
            .next()
            .unwrap(),
        signal_tx: broadcast::channel(50).0,
    };
);

fixturator!(
    ZomesToInvoke;
    constructor fn one(Zome);
);

// DPKI service itself cannot be fixturated. This is just needed for the ValidateHostAccess
// fixturator.
fixturator!(
    DpkiApi;
    curve Empty None;
    curve Unpredictable None;
    curve Predictable None;
);

fixturator!(
    ValidateHostAccess;
    constructor fn new(HostFnWorkspace, HolochainP2pDna, DpkiApi, bool);
);

fixturator!(
    HostContext;
    variants [
        ZomeCall(ZomeCallHostAccess)
        Validate(ValidateHostAccess)
        Init(InitHostAccess)
        EntryDefs(EntryDefsHostAccess)
        PostCommit(PostCommitHostAccess)
    ];
);

fixturator!(
    InvocationAuth;
    constructor fn new(AgentPubKey, CapSecret);
);

fixturator!(
    CallContext;
    constructor fn new(Zome, FunctionName, HostContext, InvocationAuth);
);



================================================
File: crates/holochain/src/lib.rs
================================================
//! All the components you need to build a Holochain Conductor

// TODO investigate this lint
#![allow(clippy::result_large_err)]
// We have a lot of usages of type aliases to `&String`, which clippy objects to.
#![allow(clippy::ptr_arg)]
#![recursion_limit = "256"]

#[cfg(doc)]
pub mod docs;

#[cfg(feature = "hdk")]
pub use hdk::HDI_VERSION;

#[cfg(feature = "hdk")]
pub use hdk::HDK_VERSION;

/// Current Holochain Conductor rust crate version.
pub const HOLOCHAIN_VERSION: &str = env!("CARGO_PKG_VERSION");

pub mod conductor;
#[allow(missing_docs)]
pub mod core;
#[allow(missing_docs)]
#[cfg(feature = "test_utils")]
pub mod fixt;

#[cfg(any(test, feature = "test_utils"))]
#[deny(missing_docs)]
pub mod sweettest;
#[cfg(any(test, feature = "test_utils"))]
#[deny(missing_docs)]
pub mod test_utils;

// this is here so that wasm ribosome macros can reference it
pub use holochain_wasmer_host;
pub use tracing;

// TODO can probably move these to integration test once
// we work out the test utils stuff
#[cfg(test)]
mod local_network_tests;

pub mod prelude {
    pub use holo_hash;
    pub use holochain_p2p::{AgentPubKeyExt, DhtOpHashExt, DnaHashExt, HolochainP2pSender};

    #[cfg(feature = "hdk")]
    pub use hdk::link::GetLinksInputBuilder;

    pub use holochain_types::prelude::{fixt, *};

    #[cfg(feature = "fuzzing")]
    pub use kitsune_p2p::{NOISE, *};

    #[cfg(feature = "test_utils")]
    pub use holochain_types::inline_zome::*;
}

#[cfg(all(feature = "wasmer_sys", feature = "wasmer_wamr"))]
compile_error!(
    "feature \"wasmer_sys\" and feature \"wasmer_wamr\" cannot be enabled at the same time"
);

#[cfg(all(not(feature = "wasmer_sys"), not(feature = "wasmer_wamr"),))]
compile_error!("One of: `wasmer_sys`, `wasmer_wamr` features must be enabled. Please, pick one.");



================================================
File: crates/holochain/src/local_network_tests.rs
================================================
use crate::sweettest::*;
use futures::StreamExt;
use holo_hash::ActionHash;
use holochain_wasm_test_utils::TestWasm;
use test_case::test_case;

#[test_case(2)]
#[test_case(4)]
#[tokio::test(flavor = "multi_thread")]
async fn conductors_call_remote(num_conductors: usize) {
    holochain_trace::test_run();

    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;

    let config = SweetConductorConfig::rendezvous(true);

    let mut conductors = SweetConductorBatch::from_config_rendezvous(num_conductors, config).await;

    let apps = conductors.setup_app("app", [&dna]).await.unwrap();
    let cells: Vec<_> = apps
        .into_inner()
        .into_iter()
        .map(|c| c.into_cells().into_iter().next().unwrap())
        .collect();

    // Make sure that genesis records are integrated now that conductors have discovered each other. This makes it
    // more likely that Kitsune knows about all the agents in the network to be able to make remote calls to them.
    await_consistency(60, cells.iter()).await.unwrap();

    let agents: Vec<_> = cells.iter().map(|c| c.agent_pubkey().clone()).collect();

    let iter = cells
        .clone()
        .into_iter()
        .zip(conductors.into_inner().into_iter());
    let keep = std::sync::Mutex::new(Vec::new());
    let keep = &keep;
    futures::stream::iter(iter)
        .for_each_concurrent(20, |(cell, conductor)| {
            let agents = agents.clone();
            async move {
                for agent in agents {
                    if agent == *cell.agent_pubkey() {
                        continue;
                    }
                    let _: ActionHash = conductor
                        .call(
                            &cell.zome(TestWasm::Create),
                            "call_create_entry_remotely_no_rec",
                            agent,
                        )
                        .await;
                }
                keep.lock().unwrap().push(conductor);
            }
        })
        .await;

    // Ensure that all the create requests were received and published.
    await_consistency(60, cells.iter()).await.unwrap();
}

// TODO - rewrite all these tests to use local sweettest

/*
#[test_case(2, 1, 1)]
#[test_case(5, 1, 1)]
#[test_case(1, 5, 5)]
#[test_case(5, 5, 5)]
#[test_case(1, 10, 1)]
#[test_case(1, 1, 1)]
#[test_case(10, 10, 10)]
#[test_case(8, 8, 8)]
#[test_case(10, 10, 1)]
#[ignore = "Don't want network tests running on ci"]
fn conductors_local_gossip(num_committers: usize, num_conductors: usize, new_conductors: usize) {
    let mut network = KitsuneP2pConfig::empty();
    network.transport_pool = vec![TransportConfig::Quic {
        bind_to: None,
        override_host: None,
        override_port: None,
    }];
    let f = conductors_gossip_inner(
        num_committers,
        num_conductors,
        new_conductors,
        network,
        true,
    );
    tokio_helper::block_forever_on(f);
}

#[test_case(2, 1, 1)]
#[test_case(5, 1, 1)]
#[test_case(1, 5, 5)]
#[test_case(5, 5, 5)]
#[test_case(1, 10, 1)]
#[test_case(1, 1, 1)]
#[test_case(10, 10, 10)]
#[test_case(8, 8, 8)]
#[test_case(10, 10, 1)]
#[ignore = "Don't want network tests running on ci"]
fn conductors_boot_gossip(num_committers: usize, num_conductors: usize, new_conductors: usize) {
    let mut network = KitsuneP2pConfig::empty();
    network.bootstrap_service = Some(url2::url2!("https://bootstrap-staging.holo.host"));
    network.transport_pool = vec![TransportConfig::Quic {
        bind_to: None,
        override_host: None,
        override_port: None,
    }];
    let f = conductors_gossip_inner(
        num_committers,
        num_conductors,
        new_conductors,
        network,
        false,
    );
    tokio_helper::block_forever_on(f);
}

#[test_case(2, 1, 1)]
#[test_case(5, 1, 1)]
#[test_case(1, 5, 5)]
#[test_case(5, 5, 5)]
#[test_case(1, 10, 1)]
#[test_case(1, 1, 1)]
#[test_case(10, 10, 10)]
#[test_case(8, 8, 8)]
#[test_case(10, 10, 1)]
#[ignore = "Don't want network tests running on ci"]
fn conductors_local_boot_gossip(
    num_committers: usize,
    num_conductors: usize,
    new_conductors: usize,
) {
    let mut network = KitsuneP2pConfig::empty();
    network.bootstrap_service = Some(url2::url2!("http://localhost:8787"));
    network.transport_pool = vec![TransportConfig::Quic {
        bind_to: None,
        override_host: None,
        override_port: None,
    }];
    let f = conductors_gossip_inner(
        num_committers,
        num_conductors,
        new_conductors,
        network,
        false,
    );
    tokio_helper::block_forever_on(f);
}

#[test_case(2, 1, 1)]
#[test_case(5, 1, 1)]
#[test_case(1, 5, 5)]
#[test_case(5, 5, 5)]
#[test_case(1, 10, 1)]
#[test_case(1, 1, 1)]
#[test_case(10, 10, 10)]
#[test_case(8, 8, 8)]
#[test_case(10, 10, 1)]
#[ignore = "Don't want network tests running on ci"]
fn conductors_remote_gossip(num_committers: usize, num_conductors: usize, new_conductors: usize) {
    let mut network = KitsuneP2pConfig::empty();
    let transport = TransportConfig::Quic {
        bind_to: None,
        override_port: None,
        override_host: None,
    };
    let proxy_config = if let Some(proxy_addr) = std::env::var_os("KIT_PROXY") {
        holochain_p2p::kitsune_p2p::ProxyConfig::RemoteProxyClient {
            // Real proxy
            proxy_url: url2::url2!("{}", proxy_addr.into_string().unwrap()),
        }
    } else {
        holochain_p2p::kitsune_p2p::ProxyConfig::RemoteProxyClient{
            // Real proxy
            proxy_url: url2::url2!("kitsune-proxy://CIW6PxKxsPPlcuvUCbMcKwUpaMSmB7kLD8xyyj4mqcw/kitsune-quic/h/proxy.holochain.org/p/5778/--"),
            // Local proxy
            // proxy_url: url2::url2!("kitsune-proxy://h5_sQGIdBB7OnWVc1iuYZ-QUzb0DowdCA73PA0oOcv4/kitsune-quic/h/192.168.1.6/p/58451/--"),
            // Other machine proxy
            // proxy_url: url2::url2!("kitsune-proxy://h5_sQGIdBB7OnWVc1iuYZ-QUzb0DowdCA73PA0oOcv4/kitsune-quic/h/192.168.1.68/p/58451/--"),
        }
    };

    network.transport_pool = vec![TransportConfig::Proxy {
        sub_transport: transport.into(),
        proxy_config,
    }];
    let f = conductors_gossip_inner(
        num_committers,
        num_conductors,
        new_conductors,
        network,
        true,
    );
    tokio_helper::block_forever_on(f);
}

#[test_case(2, 1, 1)]
#[test_case(5, 1, 1)]
#[test_case(1, 5, 5)]
#[test_case(5, 5, 5)]
#[test_case(1, 10, 1)]
#[test_case(1, 1, 1)]
#[test_case(10, 10, 10)]
#[test_case(8, 8, 8)]
#[test_case(10, 10, 1)]
#[ignore = "Don't want network tests running on ci"]
fn conductors_remote_boot_gossip(
    num_committers: usize,
    num_conductors: usize,
    new_conductors: usize,
) {
    let mut network = KitsuneP2pConfig::empty();
    let transport = TransportConfig::Quic {
        bind_to: None,
        override_port: None,
        override_host: None,
    };
    network.bootstrap_service = Some(url2::url2!("https://bootstrap-staging.holo.host/"));
    let proxy_config = holochain_p2p::kitsune_p2p::ProxyConfig::RemoteProxyClient{
        proxy_url: url2::url2!("kitsune-proxy://CIW6PxKxsPPlcuvUCbMcKwUpaMSmB7kLD8xyyj4mqcw/kitsune-quic/h/proxy.holochain.org/p/5778/--"),
    };
    network.transport_pool = vec![TransportConfig::Proxy {
        sub_transport: transport.into(),
        proxy_config,
    }];
    let f = conductors_gossip_inner(
        num_committers,
        num_conductors,
        new_conductors,
        network,
        false,
    );
    tokio_helper::block_forever_on(f);
}

async fn conductors_gossip_inner(
    num_committers: usize,
    num_conductors: usize,
    new_conductors: usize,
    network: KitsuneP2pConfig,
    share_peers: bool,
) {
    holochain_trace::test_run();
    let network_seed = nanoid::nanoid!().to_string();

    let zomes = vec![TestWasm::Create];
    let handles = setup(
        zomes.clone(),
        Some(network.clone()),
        num_committers,
        network_seed.clone(),
    )
    .await;

    let actions = init_all(&handles[..]).await;

    let second_handles = setup(
        zomes.clone(),
        Some(network.clone()),
        num_conductors,
        network_seed.clone(),
    )
    .await;

    let mut envs = Vec::with_capacity(handles.len() + second_handles.len());
    for h in handles.iter().chain(second_handles.iter()) {
        let space = h.cell_id.dna_hash();
        envs.push(h.get_p2p_db(space));
    }

    if share_peers {
        exchange_peer_info(envs.clone()).await;
    }

    // for _ in 0..600 {
    //     check_peers(envs.clone());
    //     tokio::time::sleep(std::time::Duration::from_millis(100)).await;
    // }

    let all_handles = handles
        .iter()
        .chain(second_handles.iter())
        .collect::<Vec<_>>();

    // 3 ops per create plus 7 for genesis + 2 for init + 2 for cap
    let mut expected_count = num_committers * (3 + 7 + 2 + 2) + num_conductors * 7;
    for (i, handle) in second_handles.iter().enumerate() {
        check_gossip(handle, &all_handles, &actions, expected_count, line!(), i).await;
        // Add 4 ops for each init
        expected_count += 4;
    }

    shutdown(handles).await;

    let third_handles = setup(
        zomes.clone(),
        Some(network.clone()),
        new_conductors,
        network_seed,
    )
    .await;

    let mut envs = Vec::with_capacity(third_handles.len() + second_handles.len());
    for h in third_handles.iter().chain(second_handles.iter()) {
        let space = h.cell_id.dna_hash();
        envs.push(h.get_p2p_db(space));
    }

    if share_peers {
        exchange_peer_info(envs.clone()).await;
    }

    let all_handles = third_handles
        .iter()
        .chain(second_handles.iter())
        .collect::<Vec<_>>();

    expected_count += new_conductors * 7;
    for (i, handle) in third_handles.iter().enumerate() {
        check_gossip(handle, &all_handles, &actions, expected_count, line!(), i).await;
        // Add 4 ops for each init
        expected_count += 4;
    }

    shutdown(second_handles).await;

    let all_handles = third_handles.iter().collect::<Vec<_>>();

    for (i, handle) in third_handles.iter().enumerate() {
        check_gossip(handle, &all_handles, &actions, expected_count, line!(), i).await;
    }

    shutdown(third_handles).await;
}

async fn init_all(handles: &[TestHandle]) -> Vec<ActionHash> {
    let mut futures = Vec::with_capacity(handles.len());
    for (i, h) in handles.iter().cloned().enumerate() {
        let f = async move {
            let large_msg = std::iter::repeat(b"a"[0]).take(20_000).collect::<Vec<_>>();
            let invocation = new_zome_call(
                h.keystore(),
                &h.cell_id,
                "create_post",
                Post(format!("{}{}", i, String::from_utf8_lossy(&large_msg))),
                TestWasm::Create,
            )
            .await
            .unwrap();
            h.call_zome(invocation).await.unwrap().unwrap()
        };
        let f = tokio::task::spawn(f);
        futures.push(f);
    }
    let mut actions = Vec::with_capacity(handles.len());
    for f in futures {
        let result = f.await.unwrap();
        let result: ActionHash = unwrap_to::unwrap_to!(result => ZomeCallResponse::Ok)
            .decode()
            .unwrap();
        actions.push(result);
    }
    actions
}

async fn check_gossip(
    handle: &TestHandle,
    all_handles: &[&TestHandle],
    posts: &[ActionHash],
    expected_count: usize,
    line: u32,
    i: usize,
) {
    const NUM_ATTEMPTS: usize = 600;
    const DELAY_PER_ATTEMPT: std::time::Duration = std::time::Duration::from_millis(100);

    let mut others = Vec::with_capacity(all_handles.len());
    for other in all_handles {
        let other = other.get_dht_db(other.cell_id.dna_hash()).unwrap().into();
        others.push(other);
    }
    let others_ref = others.iter().collect::<Vec<_>>();

    wait_for_integration_with_others(
        &handle.get_dht_db(handle.cell_id.dna_hash()).unwrap(),
        &others_ref,
        expected_count,
        NUM_ATTEMPTS,
        DELAY_PER_ATTEMPT.clone(),
        None,
    )
    .await;
    for hash in posts {
        let invocation = new_zome_call(
            handle.keystore(),
            &handle.cell_id,
            "get_post",
            hash,
            TestWasm::Create,
        )
        .await
        .unwrap();
        let result = handle.call_zome(invocation).await.unwrap().unwrap();
        let result: Option<Record> = unwrap_to::unwrap_to!(result => ZomeCallResponse::Ok)
            .decode()
            .unwrap();
        let s = debug_span!("check_gossip", ?line, ?i, ?hash);
        let _g = s.enter();
        tracing::debug!("Checking hash {:?} for {}", hash, i);
        tracing::debug!(?result);
        assert_matches!(result, Some(_));
    }
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(envs)))]
async fn check_peers(envs: Vec<DbWrite<DbKindP2pAgents>>) {
    for (i, a) in envs.iter().enumerate() {
        let peers = all_agent_infos(a.clone().into()).await.unwrap();
        let num_peers = peers.len();
        let peers = peers
            .into_iter()
            .map(|a| a.agent.clone())
            .collect::<Vec<_>>();
        tracing::debug!(?i, ?num_peers, ?peers);
    }
}

#[derive(Shrinkwrap, Clone)]
struct TestHandle {
    #[shrinkwrap(main_field)]
    handle: ConductorHandle,
    cell_id: CellId,
    _db_dir: Arc<TempDir>,
}

impl TestHandle {
    async fn shutdown(self) {
        self.handle.shutdown().await.unwrap().unwrap();
    }
}

async fn shutdown(handles: Vec<TestHandle>) {
    for h in handles {
        h.shutdown().await;
    }
}

async fn setup(
    zomes: Vec<TestWasm>,
    network: Option<KitsuneP2pConfig>,
    num_conductors: usize,
    network_seed: NetworkSeed,
) -> Vec<TestHandle> {
    let dna_file = DnaFile::new(
        DnaDef {
            name: "conductor_test".to_string(),
            modifiers: DnaModifiers {
                network_seed,
                properties: SerializedBytes::try_from(()).unwrap(),
                origin_time: Timestamp::HOLOCHAIN_EPOCH,
                quantum_time: STANDARD_QUANTUM_TIME,
            },
            integrity_zomes: zomes
                .clone()
                .into_iter()
                .map(TestZomes::from)
                .map(|z| z.integrity.into_inner())
                .collect(),
            coordinator_zomes: zomes
                .clone()
                .into_iter()
                .map(TestZomes::from)
                .map(|z| z.coordinator.into_inner())
                .collect(),
        },
        zomes.into_iter().map(Into::into),
    )
    .await;

    let mut handles = Vec::with_capacity(num_conductors);
    for _ in 0..num_conductors {
        let dnas = vec![dna_file.clone()];
        let (_db_dir, _, handle) =
            setup_app_with_network(vec![], vec![], network.clone().unwrap_or_default()).await;

        let agent_key = AgentPubKey::new_random(handle.keystore()).await.unwrap();
        let cell_id = CellId::new(dna_file.dna_hash().to_owned(), agent_key.clone());
        let app = InstalledCell::new(cell_id.clone(), "cell_handle".into());
        install_app("test_app", vec![(app, None)], dnas.clone(), handle.clone()).await;
        handles.push(TestHandle {
            _db_dir: Arc::new(_db_dir),
            cell_id,
            handle,
        });
    }
    handles
}
*/



================================================
File: crates/holochain/src/perf.rs
================================================



================================================
File: crates/holochain/src/test_utils.rs
================================================
//! Utils for Holochain tests
use crate::conductor::api::AppInterfaceApi;
use crate::conductor::config::AdminInterfaceConfig;
use crate::conductor::config::ConductorConfig;
use crate::conductor::config::InterfaceDriver;
use crate::conductor::integration_dump;
use crate::conductor::p2p_agent_store;
use crate::conductor::ConductorBuilder;
use crate::conductor::ConductorHandle;
use crate::core::queue_consumer::TriggerSender;
use crate::core::ribosome::ZomeCallInvocation;
use ::fixt::prelude::*;
use hdk::prelude::ZomeName;
use holo_hash::fixt::*;
use holo_hash::*;
use holochain_conductor_api::conductor::paths::DataRootPath;
use holochain_conductor_api::IntegrationStateDump;
use holochain_conductor_api::IntegrationStateDumps;
use holochain_conductor_api::ZomeCallParamsSigned;
use holochain_keystore::MetaLairClient;
use holochain_nonce::fresh_nonce;
use holochain_p2p::actor::HolochainP2pRefToDna;
use holochain_p2p::dht::prelude::Topology;
use holochain_p2p::dht::ArqStrat;
use holochain_p2p::dht::PeerViewQ;
use holochain_p2p::event::HolochainP2pEvent;
use holochain_p2p::spawn_holochain_p2p;
use holochain_p2p::HolochainP2pDna;
use holochain_p2p::HolochainP2pRef;
use holochain_p2p::HolochainP2pSender;
use holochain_p2p::NetworkCompatParams;
use holochain_serialized_bytes::SerializedBytesError;
use holochain_sqlite::prelude::DatabaseResult;
use holochain_state::prelude::test_db_dir;
use holochain_state::prelude::SourceChainResult;
use holochain_state::prelude::StateQueryResult;
use holochain_state::source_chain;
use holochain_types::db_cache::DhtDbQueryCache;
use holochain_types::prelude::*;
use holochain_types::test_utils::fake_dna_file;
use holochain_types::test_utils::fake_dna_zomes;
use holochain_wasm_test_utils::TestWasm;
use kitsune_p2p_types::config::KitsuneP2pConfig;
use kitsune_p2p_types::ok_fut;
use rusqlite::named_params;
use std::collections::HashMap;
use std::collections::HashSet;
use std::fmt::Write;
use std::sync::Arc;
use std::time::Duration;
use tempfile::TempDir;
use tokio::sync::mpsc;

pub use itertools;

pub mod consistency;
pub mod hc_stress_test;
pub mod host_fn_caller;
pub mod inline_zomes;
pub mod network_simulation;

mod wait_for;
pub use wait_for::*;

mod big_stack_test;

use holochain_types::websocket::AllowedOrigins;

use self::consistency::request_published_ops;

/// Produce file and line number info at compile-time
#[macro_export]
macro_rules! here {
    ($test: expr) => {
        concat!($test, " !!!_LOOK HERE:---> ", file!(), ":", line!())
    };
}

/// Create metadata mocks easily by passing in
/// expected functions, return data and with_f checks
#[macro_export]
macro_rules! meta_mock {
    () => {{
        holochain_state::metadata::MockMetadataBuf::new()
    }};
    ($fun:ident) => {{
        let d: Vec<holochain_types::metadata::TimedActionHash> = Vec::new();
        meta_mock!($fun, d)
    }};
    ($fun:ident, $data:expr) => {{
        let mut metadata = holochain_state::metadata::MockMetadataBuf::new();
        metadata.$fun().returning({
            move |_| {
                Ok(Box::new(fallible_iterator::convert(
                    $data
                        .clone()
                        .into_iter()
                        .map(holochain_types::metadata::TimedActionHash::from)
                        .map(Ok),
                )))
            }
        });
        metadata
    }};
    ($fun:ident, $data:expr, $match_fn:expr) => {{
        let mut metadata = holochain_state::metadata::MockMetadataBuf::new();
        metadata.$fun().returning({
            move |a| {
                if $match_fn(a) {
                    Ok(Box::new(fallible_iterator::convert(
                        $data
                            .clone()
                            .into_iter()
                            .map(holochain_types::metadata::TimedActionHash::from)
                            .map(Ok),
                    )))
                } else {
                    let mut data = $data.clone();
                    data.clear();
                    Ok(Box::new(fallible_iterator::convert(
                        data.into_iter()
                            .map(holochain_types::metadata::TimedActionHash::from)
                            .map(Ok),
                    )))
                }
            }
        });
        metadata
    }};
}

/// A running test network with a joined cell.
/// Will shutdown on drop.
pub struct TestNetwork {
    network: Option<HolochainP2pRef>,
    respond_task: Option<tokio::task::JoinHandle<()>>,
    dna_network: HolochainP2pDna,

    /// List of arguments used for `check_op_data` calls
    #[allow(clippy::type_complexity)]
    pub check_op_data_calls: Arc<
        std::sync::Mutex<
            Vec<(
                kitsune_p2p_types::KSpace,
                Vec<kitsune_p2p_types::KOpHash>,
                Option<kitsune_p2p::dependencies::kitsune_p2p_fetch::FetchContext>,
            )>,
        >,
    >,
}

impl TestNetwork {
    /// Create a new test network
    #[allow(clippy::type_complexity)]
    fn new(
        network: HolochainP2pRef,
        respond_task: tokio::task::JoinHandle<()>,
        dna_network: HolochainP2pDna,
        check_op_data_calls: Arc<
            std::sync::Mutex<
                Vec<(
                    kitsune_p2p_types::KSpace,
                    Vec<kitsune_p2p_types::KOpHash>,
                    Option<kitsune_p2p::dependencies::kitsune_p2p_fetch::FetchContext>,
                )>,
            >,
        >,
    ) -> Self {
        Self {
            network: Some(network),
            respond_task: Some(respond_task),
            dna_network,
            check_op_data_calls,
        }
    }

    /// Get the holochain p2p network
    pub fn network(&self) -> HolochainP2pRef {
        self.network
            .as_ref()
            .expect("Tried to use network while it was shutting down")
            .clone()
    }

    /// Get the cell network
    pub fn dna_network(&self) -> HolochainP2pDna {
        self.dna_network.clone()
    }
}

impl Drop for TestNetwork {
    fn drop(&mut self) {
        use ghost_actor::GhostControlSender;
        let network = self.network.take().unwrap();
        let respond_task = self.respond_task.take().unwrap();
        tokio::task::spawn(async move {
            network.ghost_actor_shutdown_immediate().await.ok();
            respond_task.await.ok();
        });
    }
}

/// Convenience constructor for cell networks
pub async fn test_network(
    dna_hash: Option<DnaHash>,
    agent_key: Option<AgentPubKey>,
) -> TestNetwork {
    test_network_inner::<fn(&HolochainP2pEvent) -> bool>(dna_hash, agent_key, None).await
}

/// Convenience constructor for cell networks
/// where you need to filter some events into a channel
pub async fn test_network_with_events<F>(
    dna_hash: Option<DnaHash>,
    agent_key: Option<AgentPubKey>,
    filter: F,
    evt_send: mpsc::Sender<HolochainP2pEvent>,
) -> TestNetwork
where
    F: Fn(&HolochainP2pEvent) -> bool + Send + 'static,
{
    test_network_inner(dna_hash, agent_key, Some((filter, evt_send))).await
}

async fn test_network_inner<F>(
    dna_hash: Option<DnaHash>,
    agent_key: Option<AgentPubKey>,
    mut events: Option<(F, mpsc::Sender<HolochainP2pEvent>)>,
) -> TestNetwork
where
    F: Fn(&HolochainP2pEvent) -> bool + Send + 'static,
{
    let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;
    let mut config = holochain_p2p::kitsune_p2p::dependencies::kitsune_p2p_types::config::KitsuneP2pConfig::from_signal_addr(signal_url);
    let mut tuning =
        kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
    tuning.tx5_implicit_timeout_ms = 500;
    let tuning = std::sync::Arc::new(tuning);
    let cutoff = tuning.danger_gossip_recent_threshold();
    config.tuning_params = tuning;

    let check_op_data_calls = Arc::new(std::sync::Mutex::new(Vec::new()));

    let test_host = {
        let check_op_data_calls = check_op_data_calls.clone();
        kitsune_p2p::HostStub::with_check_op_data(Box::new(move |space, list, ctx| {
            let out = list.iter().map(|_| false).collect();
            check_op_data_calls.lock().unwrap().push((space, list, ctx));
            futures::FutureExt::boxed(async move { Ok(out) }).into()
        }))
    };

    let (network, mut recv) = spawn_holochain_p2p(
        config,
        holochain_p2p::kitsune_p2p::dependencies::kitsune_p2p_types::tls::TlsConfig::new_ephemeral(
        )
        .await
        .unwrap(),
        test_host,
        NetworkCompatParams::default(),
    )
    .await
    .unwrap();
    let respond_task = tokio::task::spawn(async move {
        use tokio_stream::StreamExt;
        while let Some(evt) = recv.next().await {
            if let Some((filter, tx)) = &mut events {
                if filter(&evt) {
                    tx.send(evt).await.unwrap();
                    continue;
                }
            }
            use holochain_p2p::event::HolochainP2pEvent::*;
            match evt {
                SignNetworkData { respond, .. } => {
                    respond.r(ok_fut(Ok([0; 64].into())));
                }
                PutAgentInfoSigned { respond, .. } => {
                    respond.r(ok_fut(Ok(vec![])));
                }
                QueryAgentInfoSigned { respond, .. } => {
                    respond.r(ok_fut(Ok(vec![])));
                }
                QueryAgentInfoSignedNearBasis { respond, .. } => {
                    respond.r(ok_fut(Ok(vec![])));
                }
                QueryGossipAgents { respond, .. } => {
                    respond.r(ok_fut(Ok(vec![])));
                }
                QueryPeerDensity { respond, .. } => {
                    respond.r(ok_fut(Ok(PeerViewQ::new(
                        Topology::standard_epoch(cutoff),
                        ArqStrat::default(),
                        vec![],
                    )
                    .into())));
                }
                oth => tracing::warn!(?oth, "UnhandledEvent"),
            }
        }
    });
    let dna = dna_hash.unwrap_or_else(|| fixt!(DnaHash));
    let mut key_fixt = AgentPubKeyFixturator::new(Predictable);
    let agent_key = agent_key.unwrap_or_else(|| key_fixt.next().unwrap());
    let dna_network = network.to_dna(dna.clone(), None);
    network
        .join(dna.clone(), agent_key, None, None)
        .await
        .unwrap();
    TestNetwork::new(network, respond_task, dna_network, check_op_data_calls)
}

/// Do what's necessary to install an app
pub async fn install_app(
    name: &str,
    agent: AgentPubKey,
    data: &[(DnaFile, Option<MembraneProof>)],
    conductor_handle: ConductorHandle,
) {
    for (dna, _) in data.iter() {
        conductor_handle.register_dna(dna.clone()).await.unwrap();
    }
    conductor_handle
        .clone()
        .install_app_minimal(name.to_string(), Some(agent), data, None)
        .await
        .unwrap();

    conductor_handle
        .clone()
        .enable_app(name.to_string())
        .await
        .unwrap();

    let errors = conductor_handle
        .reconcile_cell_status_with_app_status()
        .await
        .unwrap();

    assert!(errors.is_empty(), "{:?}", errors);
}

/// Payload for installing cells
pub type DnasWithProofs = Vec<(DnaFile, Option<MembraneProof>)>;

/// One of various ways to setup an app, used somewhere...
pub async fn setup_app_in_new_conductor(
    installed_app_id: InstalledAppId,
    agent: Option<AgentPubKey>,
    dnas: DnasWithProofs,
) -> (Arc<TempDir>, AppInterfaceApi, ConductorHandle, AgentPubKey) {
    let db_dir = test_db_dir();
    let conductor_handle = ConductorBuilder::new()
        .with_data_root_path(db_dir.path().to_path_buf().into())
        .test(&[])
        .await
        .unwrap();

    let agent =
        install_app_in_conductor(conductor_handle.clone(), installed_app_id, agent, &dnas).await;

    let handle = conductor_handle.clone();

    (
        Arc::new(db_dir),
        AppInterfaceApi::new(conductor_handle),
        handle,
        agent,
    )
}

/// Install an app into an existing conductor instance
pub async fn install_app_in_conductor(
    conductor_handle: ConductorHandle,
    installed_app_id: InstalledAppId,
    agent: Option<AgentPubKey>,
    dnas_with_proofs: &[(DnaFile, Option<MembraneProof>)],
) -> AgentPubKey {
    for (dna, _) in dnas_with_proofs {
        conductor_handle.register_dna(dna.clone()).await.unwrap();
    }

    let agent = conductor_handle
        .clone()
        .install_app_minimal(installed_app_id.clone(), agent, dnas_with_proofs, None)
        .await
        .unwrap();

    conductor_handle
        .clone()
        .enable_app(installed_app_id)
        .await
        .unwrap();

    let errors = conductor_handle
        .clone()
        .reconcile_cell_status_with_app_status()
        .await
        .unwrap();

    assert!(errors.is_empty());

    agent
}

/// Setup an app for testing
/// apps_data is a vec of app nicknames with vecs of their cell data
pub async fn setup_app_with_names(
    agent: AgentPubKey,
    apps_data: Vec<(&str, DnasWithProofs)>,
) -> (TempDir, AppInterfaceApi, ConductorHandle) {
    let dir = test_db_dir();
    let (iface, handle) =
        setup_app_inner(dir.path().to_path_buf().into(), agent, apps_data, None).await;
    (dir, iface, handle)
}

/// Setup an app with a custom network config for testing
/// apps_data is a vec of app nicknames with vecs of their cell data.
pub async fn setup_app_with_network(
    agent: AgentPubKey,
    apps_data: Vec<(&str, DnasWithProofs)>,
    network: KitsuneP2pConfig,
) -> (TempDir, AppInterfaceApi, ConductorHandle) {
    let dir = test_db_dir();
    let (iface, handle) = setup_app_inner(
        dir.path().to_path_buf().into(),
        agent,
        apps_data,
        Some(network),
    )
    .await;
    (dir, iface, handle)
}

/// Setup an app with full configurability
pub async fn setup_app_inner(
    data_root_path: DataRootPath,
    agent: AgentPubKey,
    apps_data: Vec<(&str, DnasWithProofs)>,
    network: Option<KitsuneP2pConfig>,
) -> (AppInterfaceApi, ConductorHandle) {
    let config = ConductorConfig {
        data_root_path: Some(data_root_path.clone()),
        admin_interfaces: Some(vec![AdminInterfaceConfig {
            driver: InterfaceDriver::Websocket {
                port: 0,
                allowed_origins: AllowedOrigins::Any,
            },
        }]),
        network: network.unwrap_or_else(KitsuneP2pConfig::mem),
        ..Default::default()
    };
    let conductor_handle = ConductorBuilder::new()
        .config(config)
        .test(&[])
        .await
        .unwrap();

    for (app_name, cell_data) in apps_data {
        install_app(
            app_name,
            agent.clone(),
            &cell_data,
            conductor_handle.clone(),
        )
        .await;
    }

    let handle = conductor_handle.clone();

    (AppInterfaceApi::new(conductor_handle), handle)
}

/// If HC_WASM_CACHE_PATH is set warm the cache
pub fn warm_wasm_tests() {
    if let Some(_path) = std::env::var_os("HC_WASM_CACHE_PATH") {
        let wasms: Vec<_> = TestWasm::iter().collect();
        crate::fixt::RealRibosomeFixturator::new(crate::fixt::curve::Zomes(wasms))
            .next()
            .unwrap();
    }
}

/// Consistency was failed to be reached. Here's a report.
#[derive(derive_more::From)]
pub struct ConsistencyError(String);

impl std::fmt::Debug for ConsistencyError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

/// Alias
pub type ConsistencyResult = Result<(), ConsistencyError>;

async fn delay(elapsed: Duration) {
    let delay = if elapsed > Duration::from_secs(10) {
        CONSISTENCY_DELAY_HIGH
    } else if elapsed > Duration::from_secs(1) {
        CONSISTENCY_DELAY_MID
    } else {
        CONSISTENCY_DELAY_LOW
    };
    tokio::time::sleep(delay).await
}

/// Extra conditions that must be satisfied for consistency to be reached.
///
/// Without supplying extra conditions, it's expected that at the time of beginning
/// the consistency awaiting, all ops which will be published have already been published.
/// However, in cases where more publishing is expected, such as when warrants will be authored
/// due to recently publishing invalid ops, these conditions can be used to make sure that
/// the consistency check will not proceed until all publishing expectations have occurred.
#[derive(Debug, Default, Clone)]
pub struct ConsistencyConditions {
    /// This many warrants must have been published against the keyed agent.
    warrants_issued: HashMap<AgentPubKey, usize>,
}

impl From<()> for ConsistencyConditions {
    fn from(_: ()) -> Self {
        Self::default()
    }
}

impl From<Vec<(AgentPubKey, usize)>> for ConsistencyConditions {
    fn from(items: Vec<(AgentPubKey, usize)>) -> Self {
        Self {
            warrants_issued: items.iter().cloned().collect(),
        }
    }
}

impl ConsistencyConditions {
    fn check<'a>(
        &self,
        published_ops: impl Iterator<Item = &'a DhtOp>,
    ) -> Result<bool, ConsistencyError> {
        let mut checked = self.warrants_issued.clone();
        for v in checked.values_mut() {
            *v = 0;
        }

        for op in published_ops {
            if let DhtOp::WarrantOp(op) = op {
                let author = &op.action_author();
                if let Some(count) = checked.get_mut(author) {
                    *count += 1;
                    if *count > *self.warrants_issued.get(author).unwrap() {
                        return Err(format!(
                            "Expected exactly {} warrants to be published against agent {author}, but found more",
                            self.warrants_issued.get(author).unwrap(),
                        )
                    .into());
                    }
                }
            }
        }

        Ok(checked == self.warrants_issued)
    }

    /// Return the total number of warrants expected to be published
    pub fn num_warrants(&self) -> usize {
        self.warrants_issued.values().sum()
    }
}

/// Wait for all cell envs to reach consistency, meaning that every op
/// published by every cell has been integrated by every node
pub async fn wait_for_integration_diff<AuthorDb, DhtDb>(
    cells: &[(&AgentPubKey, &AuthorDb, Option<&DhtDb>)],
    timeout: Duration,
    conditions: ConsistencyConditions,
) -> ConsistencyResult
where
    AuthorDb: ReadAccess<DbKindAuthored>,
    DhtDb: ReadAccess<DbKindDht>,
{
    let start = tokio::time::Instant::now();
    let mut done = HashSet::new();
    let mut integrated = vec![HashSet::new(); cells.len()];
    let mut published = HashSet::new();
    let mut publish_complete = false;

    while start.elapsed() < timeout {
        if !publish_complete {
            published = HashSet::new();
            for (_author, db, _) in cells.iter() {
                // Providing the author is redundant
                let p = request_published_ops(*db, None /*Some((*author).to_owned())*/)
                    .await
                    .unwrap()
                    .into_iter()
                    .map(|(_, _, op)| op);

                // Assert that there are no duplicates
                let expected = p.len() + published.len();
                published.extend(p);
                assert_eq!(published.len(), expected);
            }
        }

        let prev_publish_complete = publish_complete;
        publish_complete = conditions.check(published.iter())?;

        if publish_complete {
            if !prev_publish_complete {
                tracing::info!("*** All expected ops were published ***");
            }
            // Compare the published ops to the integrated ops for each node
            for (i, (_, _, dht_db)) in cells.iter().enumerate() {
                if done.contains(&i) {
                    continue;
                }
                if let Some(db) = dht_db.as_ref() {
                    integrated[i] = get_integrated_ops(*db).await.into_iter().collect();

                    if integrated[i] == published {
                        done.insert(i);
                        tracing::debug!(i, "Node reached consistency");
                    } else {
                        let total_time_waited = start.elapsed();
                        let queries = query_integration(*db).await;
                        let num_integrated = integrated.len();
                        tracing::debug!(i, ?num_integrated, ?total_time_waited, counts = ?queries, "consistency-status");
                    }
                } else {
                    // If the DHT db is not provided, don't check integration
                    done.insert(i);
                }
            }
        }

        // If all nodes reached consistency, exit successfully
        if done.len() == cells.len() {
            return Ok(());
        }

        let total_time_waited = start.elapsed();
        delay(total_time_waited).await;
    }

    let header = format!(
        "{:53} {:>3} {:53} {:53} {}\n{}",
        "author",
        "seq",
        "op_hash",
        "action_hash",
        "op_type (action_type)",
        "-".repeat(53 + 3 + 53 + 53 + 4 + 21)
    );

    if !publish_complete {
        let published = published
            .iter()
            .map(display_op)
            .collect::<Vec<_>>()
            .join("\n");
        return Err(format!("There are still ops which were expected to have been published which weren't:\n{header}\n{published}").into());
    }

    let mut report = String::new();
    let not_consistent = (0..cells.len())
        .filter(|i| !done.contains(i))
        .collect::<Vec<_>>();

    writeln!(
        report,
        "{} cells did not reach consistency: {:?}",
        not_consistent.len(),
        not_consistent
    )
    .unwrap();

    if not_consistent.is_empty() {
        unreachable!("At least one node must not have reached consistency");
    }

    for c in &not_consistent {
        let integrated = integrated[*c].clone();

        eprintln!("Agent {} is not consistent", cells[*c].0);

        let (unintegrated, unpublished) = diff_ops(published.iter(), integrated.iter());
        let diff = diff_report(unintegrated, unpublished);

        #[allow(clippy::comparison_chain)]
        if integrated.len() > published.len() {
            eprintln!(
                "{report}\nnum integrated ops ({}) > num published ops ({}), meaning you may not be accounting for all nodes in this test. Consistency may not be complete. Report:\n\n{header}\n{diff}",
                integrated.len(),
                published.len()
            );
        } else if integrated.len() < published.len() {
            let db = cells[*c].2.as_ref().expect("DhtDb must be provided");
            let integration_dump = integration_dump(*db).await.unwrap();

            eprintln!(
                "{}\nConsistency not achieved after {:?}. Expected {} ops, but only {} integrated. Report:\n\n{}\n{}\n\n{:?}",
                report,
                timeout,
                published.len(),
                integrated.len(),
                header,
                diff,
                integration_dump
            );
        } else {
            unreachable!()
        }
    }

    Err(ConsistencyError(format!(
        "{} agents were inconsistent",
        not_consistent.len()
    )))
}

const CONSISTENCY_DELAY_LOW: Duration = Duration::from_millis(100);
const CONSISTENCY_DELAY_MID: Duration = Duration::from_millis(500);
const CONSISTENCY_DELAY_HIGH: Duration = Duration::from_millis(1000);

fn diff_ops<'a>(
    published: impl Iterator<Item = &'a DhtOp>,
    integrated: impl Iterator<Item = &'a DhtOp>,
) -> (Vec<String>, Vec<String>) {
    let mut published: Vec<_> = published.map(display_op).collect();
    let mut integrated: Vec<_> = integrated.map(display_op).collect();
    published.sort();
    integrated.sort();

    let mut unintegrated = vec![];
    let mut unpublished = vec![];

    for d in diff::slice(&published, &integrated) {
        match d {
            diff::Result::Left(l) => unintegrated.push(l.to_owned()),
            diff::Result::Right(r) => unpublished.push(r.to_owned()),
            _ => (),
        }
    }

    (unintegrated, unpublished)
}

fn diff_report(unintegrated: Vec<String>, unpublished: Vec<String>) -> String {
    let unintegrated = if unintegrated.is_empty() {
        "".to_string()
    } else {
        format!("Unintegrated:\n\n{}\n", unintegrated.join("\n"))
    };
    let unpublished = if unpublished.is_empty() {
        "".to_string()
    } else {
        format!("Unpublished:\n\n{}\n", unpublished.join("\n"))
    };

    format!("{}{}", unintegrated, unpublished)
}

fn display_op(op: &DhtOp) -> String {
    match op {
        DhtOp::ChainOp(op) => format!(
            "{} {:>3} {} {} {} ({})",
            op.action().author(),
            op.action().action_seq(),
            op.to_hash(),
            op.action().to_hash(),
            op.get_type(),
            op.action().action_type(),
        ),
        DhtOp::WarrantOp(op) => {
            format!("{} WARRANT ({})", op.author, op.get_type(),)
        }
    }
}

/// Wait for num_attempts * delay, or until all published ops have been integrated.
#[cfg_attr(feature = "instrument", tracing::instrument(skip(db)))]
pub async fn wait_for_integration<Db: ReadAccess<DbKindDht>>(
    db: &Db,
    num_published: usize,
    num_attempts: usize,
    delay: Duration,
) -> Result<(), String> {
    let mut num_integrated = 0;
    for i in 0..num_attempts {
        num_integrated = get_integrated_count(db).await;
        if num_integrated >= num_published {
            if num_integrated > num_published {
                tracing::warn!("num integrated ops > num published ops, meaning you may not be accounting for all nodes in this test.
                Consistency may not be complete.")
            }
            return Ok(());
        } else {
            let total_time_waited = delay * i as u32;
            tracing::debug!(?num_integrated, ?total_time_waited, counts = ?query_integration(db).await, "consistency-status");
        }
        tokio::time::sleep(delay).await;
    }

    Err(format!(
        "Consistency not achieved after {num_attempts} attempts. Expected {num_published} ops, but only {num_integrated} integrated.",
    ))
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(envs)))]
/// Show authored data for each cell environment
pub async fn show_authored<Db: ReadAccess<DbKindAuthored>>(envs: &[&Db]) {
    for (i, &db) in envs.iter().enumerate() {
        db.read_async(move |txn| -> DatabaseResult<()> {
            txn.prepare("SELECT DISTINCT Action.seq, Action.type, Action.entry_hash FROM Action JOIN DhtOp ON Action.hash = DhtOp.hash")
            .unwrap()
            .query_map([], |row| {
                let action_type: String = row.get("type")?;
                let seq: u32 = row.get("seq")?;
                let entry: Option<EntryHash> = row.get("entry_hash")?;
                Ok((action_type, seq, entry))
            })
            .unwrap()
            .for_each(|r|{
                let (action_type, seq, entry) = r.unwrap();
                tracing::debug!(chain = %i, %seq, ?action_type, ?entry);
            });

            Ok(())
        }).await.unwrap();
    }
}

/// Get multiple db states with compact Display representation
pub async fn get_integration_dumps<Db: ReadAccess<DbKindDht>>(
    dbs: &[&Db],
) -> IntegrationStateDumps {
    let mut output = Vec::new();
    for db in dbs {
        let db = *db;
        output.push(query_integration(db).await);
    }
    IntegrationStateDumps(output)
}

/// Show the current db state.
pub async fn query_integration<Db: ReadAccess<DbKindDht>>(db: &Db) -> IntegrationStateDump {
    crate::conductor::integration_dump(&db.clone().into())
        .await
        .unwrap()
}

async fn get_integrated_count<Db: ReadAccess<DbKindDht>>(db: &Db) -> usize {
    db.read_async(move |txn| -> DatabaseResult<usize> {
        Ok(txn.query_row(
            "SELECT COUNT(hash) FROM DhtOp WHERE DhtOp.when_integrated IS NOT NULL",
            [],
            |row| row.get(0),
        )?)
    })
    .await
    .unwrap()
}

/// Get count of ops that have been successfully validated but not integrated
pub async fn get_valid_and_not_integrated_count<Db: ReadAccess<DbKindDht>>(db: &Db) -> usize {
    db.read_async(move |txn| -> DatabaseResult<usize> {
        Ok(txn.query_row(
            "SELECT COUNT(hash) FROM DhtOp WHERE when_integrated IS NULL AND validation_status = :status",
            named_params!{
                ":status": ValidationStatus::Valid,
            },
            |row| row.get(0),
        )?)
    })
    .await
    .unwrap()
}

/// Get count of ops that have been successfully validated and integrated
pub async fn get_valid_and_integrated_count<Db: ReadAccess<DbKindDht>>(db: &Db) -> usize {
    db.read_async(move |txn| -> DatabaseResult<usize> {
        Ok(txn.query_row(
            "SELECT COUNT(hash) FROM DhtOp WHERE when_integrated IS NOT NULL AND validation_status = :status",
            named_params!{
                ":status": ValidationStatus::Valid,
            },
            |row| row.get(0),
        )?)
    })
    .await
    .unwrap()
}

/// Get all [`DhtOps`](holochain_types::prelude::DhtOp) integrated by this node
pub async fn get_integrated_ops<Db: ReadAccess<DbKindDht>>(db: &Db) -> Vec<DhtOp> {
    db.read_async(move |txn| -> StateQueryResult<Vec<DhtOp>> {
        txn.prepare(
            "
            SELECT
            DhtOp.type, 
            Action.author as author, 
            Action.blob as action_blob, 
            Entry.blob as entry_blob
            FROM DhtOp
            JOIN
            Action ON DhtOp.action_hash = Action.hash
            LEFT JOIN
            Entry ON Action.entry_hash = Entry.hash
            WHERE
            DhtOp.when_integrated IS NOT NULL
            ORDER BY DhtOp.rowid ASC
        ",
        )
        .unwrap()
        .query_and_then(named_params! {}, |row| {
            Ok(holochain_state::query::map_sql_dht_op(true, "type", row).unwrap())
        })
        .unwrap()
        .collect::<StateQueryResult<_>>()
    })
    .await
    .unwrap()
}

/// Helper for displaying agent infos stored on a conductor
pub async fn display_agent_infos(conductor: &ConductorHandle) {
    for cell_id in conductor.running_cell_ids() {
        let space = cell_id.dna_hash();
        let db = conductor.get_p2p_db(space);
        let info = p2p_agent_store::dump_state(db.into(), Some(cell_id))
            .await
            .unwrap();
        tracing::debug!(%info);
    }
}

/// Helper to create a signed zome invocation for tests
pub async fn new_zome_call<P, Z: Into<ZomeName>>(
    keystore: &MetaLairClient,
    cell_id: &CellId,
    func: &str,
    payload: P,
    zome: Z,
) -> Result<ZomeCallParamsSigned, SerializedBytesError>
where
    P: serde::Serialize + std::fmt::Debug,
{
    let zome_call_params = new_zome_call_params(cell_id, func, payload, zome)?;
    Ok(
        ZomeCallParamsSigned::try_from_params(keystore, zome_call_params)
            .await
            .unwrap(),
    )
}

/// Helper to create an unsigned zome invocation for tests
pub fn new_zome_call_params<P, Z>(
    cell_id: &CellId,
    func: &str,
    payload: P,
    zome: Z,
) -> Result<ZomeCallParams, SerializedBytesError>
where
    P: serde::Serialize + std::fmt::Debug,
    Z: Into<ZomeName>,
{
    let (nonce, expires_at) = fresh_nonce(Timestamp::now()).unwrap();
    Ok(ZomeCallParams {
        cell_id: cell_id.clone(),
        zome_name: zome.into(),
        cap_secret: Some(CapSecretFixturator::new(Unpredictable).next().unwrap()),
        fn_name: func.into(),
        payload: ExternIO::encode(payload)?,
        provenance: cell_id.agent_pubkey().clone(),
        nonce,
        expires_at,
    })
}

/// Helper to create a zome invocation for tests
pub async fn new_invocation<P, Z>(
    cell_id: &CellId,
    func: &str,
    payload: P,
    zome: Z,
) -> Result<ZomeCallInvocation, SerializedBytesError>
where
    P: serde::Serialize + std::fmt::Debug,
    Z: Into<Zome> + Clone,
{
    let ZomeCallParams {
        cell_id,
        cap_secret,
        fn_name,
        payload,
        provenance,
        nonce,
        expires_at,
        ..
    } = new_zome_call_params(cell_id, func, payload, zome.clone().into())?;
    Ok(ZomeCallInvocation {
        cell_id,
        zome: zome.into(),
        cap_secret,
        fn_name,
        payload,
        provenance,
        nonce,
        expires_at,
    })
}

/// A fixture example dna for unit testing.
pub fn fake_valid_dna_file(network_seed: &str) -> DnaFile {
    fake_dna_zomes(
        network_seed,
        vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
    )
}

/// Run genesis on the source chain for testing.
pub async fn fake_genesis(
    vault: DbWrite<DbKindAuthored>,
    dht_db: DbWrite<DbKindDht>,
    keystore: MetaLairClient,
) -> SourceChainResult<()> {
    fake_genesis_for_agent(vault, dht_db, fake_agent_pubkey_1(), keystore).await
}

/// Run genesis on the source chain for a specific agent for testing.
pub async fn fake_genesis_for_agent(
    vault: DbWrite<DbKindAuthored>,
    dht_db: DbWrite<DbKindDht>,
    agent: AgentPubKey,
    keystore: MetaLairClient,
) -> SourceChainResult<()> {
    let dna = fake_dna_file("cool dna");
    let dna_hash = dna.dna_hash().clone();

    source_chain::genesis(
        vault,
        dht_db.clone(),
        &DhtDbQueryCache::new(dht_db.clone().into()),
        keystore,
        dna_hash,
        agent,
        None,
        None,
    )
    .await
}

/// Force all dht ops without enough validation receipts to be published.
pub async fn force_publish_dht_ops(
    vault: &DbWrite<DbKindAuthored>,
    publish_trigger: &mut TriggerSender,
) -> DatabaseResult<()> {
    vault
        .write_async(|txn| {
            DatabaseResult::Ok(txn.execute(
                "UPDATE DhtOp SET last_publish_time = NULL WHERE receipts_complete IS NULL",
                [],
            )?)
        })
        .await?;
    publish_trigger.trigger(&"force_publish_dht_ops");
    Ok(())
}



================================================
File: crates/holochain/src/bin/holochain/main.rs
================================================
use holochain::conductor::config::ConductorConfig;
use holochain::conductor::manager::handle_shutdown;
use holochain::conductor::Conductor;
use holochain::conductor::ConductorHandle;
use holochain_conductor_api::conductor::paths::DataRootPath;
use holochain_conductor_api::conductor::process::ERROR_CODE;
use holochain_conductor_api::conductor::ConductorConfigError;
use holochain_conductor_api::config::conductor::paths::ConfigRootPath;
use holochain_conductor_api::config::conductor::KeystoreConfig;
use holochain_trace::Output;
use holochain_util::tokio_helper;
#[cfg(unix)]
use sd_notify::{notify, NotifyState};
use std::path::PathBuf;
use structopt::StructOpt;
use tracing::*;

const MAGIC_CONDUCTOR_READY_STRING: &str = "Conductor ready.";

#[derive(Debug, StructOpt)]
#[structopt(name = "holochain", about = "The Holochain Conductor.")]
struct Opt {
    #[structopt(
        long,
        help = "Outputs structured json from logging:
    - None: No logging at all (fastest)
    - Log: Output logs to stdout with spans (human readable)
    - Compact: Same as Log but with less information
    - Json: Output logs as structured json (machine readable)
    ",
        default_value = "Log"
    )]
    structured: Output,

    #[structopt(
        short = "c",
        long,
        help = "Path to a YAML file containing conductor configuration"
    )]
    config_path: Option<PathBuf>,

    #[structopt(long, help = "Print out the conductor config's json schema")]
    config_schema: bool,

    /// Instead of the normal "interactive" method of passphrase
    /// retrieval, read the passphrase from stdin. Be careful
    /// how you make use of this, as it could be less secure,
    /// for example, make sure it is not saved in your
    /// `~/.bash_history`.
    #[structopt(short = "p", long)]
    pub piped: bool,

    #[structopt(
        long,
        help = "Display version information such as git revision and HDK version"
    )]
    build_info: bool,

    #[structopt(long, help = "Create default conductor configuration.")]
    create_config: bool,

    /// WARNING!! DANGER!! This exposes your database decryption secrets!
    /// Print the database decryption secrets to stderr.
    /// With these PRAGMA commands, you'll be able to run sqlcipher
    /// directly to manipulate holochain databases.
    #[structopt(long)]
    pub danger_print_db_secrets: bool,
}

fn main() {
    // the async_main function should only end if our program is done
    tokio_helper::block_forever_on(async_main());
}

async fn async_main() {
    // Sets up a human-readable panic message with a request for bug reports
    //
    // See https://docs.rs/human-panic/1.0.3/human_panic/
    human_panic::setup_panic!();

    let opt = Opt::from_args();

    if opt.build_info {
        println!("{}", option_env!("BUILD_INFO").unwrap_or("{}"));
        return;
    }

    if opt.config_schema {
        let schema = schemars::schema_for!(ConductorConfig);
        let schema_string = serde_json::to_string_pretty(&schema).unwrap();
        println!("{}", schema_string);
        return;
    }

    if opt.create_config {
        holochain_conductor_config::generate::generate(
            None,
            std::env::current_dir().ok(),
            None,
            true,
            0,
            #[cfg(feature = "unstable-dpki")]
            true,
            #[cfg(feature = "unstable-dpki")]
            None,
            #[cfg(feature = "chc")]
            None,
        )
        .inspect_err(|e| tracing::error!("Failed to generate configurations: {}", e))
        .unwrap();
        return;
    }

    let config_path = opt.config_path.clone().map(ConfigRootPath::from);

    let config = load_config(config_path);

    if let Some(t) = &config.tracing_override {
        std::env::set_var("CUSTOM_FILTER", t);
    }

    holochain_trace::init_fmt(opt.structured.clone()).expect("Failed to start contextual logging");
    debug!("holochain_trace initialized");

    let data_root_path: DataRootPath = config.data_root_path_or_die();

    holochain_metrics::HolochainMetricsConfig::new(data_root_path.as_ref())
        .init()
        .await;

    kitsune_p2p_types::metrics::init_sys_info_poll();

    info!("Conductor startup: metrics loop spawned.");

    let conductor = conductor_handle_from_config(&opt, config).await;

    info!("Conductor successfully initialized.");

    // This println has special meaning. Other processes can detect it and know
    // that the conductor has been initialized, in particular that the admin
    // interfaces are running, and can be connected to.
    println!("{}", MAGIC_CONDUCTOR_READY_STRING);

    // Lets systemd units know that holochain is ready via sd_notify socket
    // Requires NotifyAccess=all and Type=notify attributes on holochain systemd unit
    // and NotifyAccess=all on dependant systemd unit
    #[cfg(unix)]
    let _ = notify(true, &[NotifyState::Ready]);

    // wait for a unix signal or ctrl-c instruction to
    // shutdown holochain
    tokio::signal::ctrl_c()
        .await
        .unwrap_or_else(|e| tracing::error!("Could not handle termination signal: {:?}", e));
    tracing::info!("Gracefully shutting down conductor...");
    let shutdown_result = conductor.shutdown().await;
    handle_shutdown(shutdown_result);
}

async fn conductor_handle_from_config(opt: &Opt, config: ConductorConfig) -> ConductorHandle {
    // read the passphrase to prepare for usage
    let passphrase = match &config.keystore {
        KeystoreConfig::DangerTestKeystore => None,
        KeystoreConfig::LairServer { .. } | KeystoreConfig::LairServerInProc { .. } => {
            if opt.piped {
                holochain_util::pw::pw_set_piped(true);
            }

            Some(holochain_util::pw::pw_get().unwrap())
        }
    };

    // Check if database is present
    // In interactive mode give the user a chance to create it, otherwise create it automatically
    let env_path = config.data_root_path_or_die();
    if !env_path.is_dir() {
        let result = std::fs::create_dir_all(env_path.as_ref());
        match result {
            Ok(()) => println!("Created database at {}.", env_path.display()),
            Err(e) => {
                println!("Couldn't create database: {}", e);
                std::process::exit(ERROR_CODE);
            }
        }
    }

    // Initialize the Conductor
    match Conductor::builder()
        .config(config)
        .passphrase(passphrase)
        .danger_print_db_secrets(opt.danger_print_db_secrets)
        .build()
        .await
    {
        Err(err) => panic!(
            "Could not initialize Conductor from configuration: {:?}",
            err
        ),
        Ok(res) => res,
    }
}

/// Load config, throw friendly error on failure
fn load_config(maybe_config_root_path: Option<ConfigRootPath>) -> ConductorConfig {
    if let Some(ref config_root_path) = maybe_config_root_path {
        match ConductorConfig::load_yaml(config_root_path.as_ref()) {
            Err(ConductorConfigError::ConfigMissing(_)) => {
                display_friendly_missing_config_message(maybe_config_root_path.as_ref());
                std::process::exit(ERROR_CODE);
            }
            Err(ConductorConfigError::SerializationError(err)) => {
                display_friendly_malformed_config_message(config_root_path, err);
                std::process::exit(ERROR_CODE);
            }
            result => result.expect("Could not load conductor config"),
        }
    } else {
        display_friendly_missing_config_message(maybe_config_root_path.as_ref());
        std::process::exit(ERROR_CODE);
    }
}

fn display_friendly_missing_config_message(maybe_config_root_path: Option<&ConfigRootPath>) {
    if let Some(config_root_path) = maybe_config_root_path {
        println!(
            "
    Error: You asked to load configuration from the path:

        {path}

    but this file doesn't exist. Please create a YAML config file at this path or run the following 
    command to generate starter configurations.

        holochain --create-config
            ",
            path = config_root_path.display(),
        );
    } else {
        println!(
            "
    Error: You tried to load a conductor config file, but didn't specify a path.
    Please run this command again with the -c flag, like this:

        holochain -c path/to/conductor-config.yml
        "
        );
    }
}

fn display_friendly_malformed_config_message(
    config_root_path: &ConfigRootPath,
    error: serde_yaml::Error,
) {
    println!(
        "
The specified config file ({})
could not be parsed, because it is not valid YAML. Please check and fix the
file. Details:

    {}

    ",
        config_root_path.display(),
        error
    )
}



================================================
File: crates/holochain/src/conductor/api.rs
================================================
#![deny(missing_docs)]

//! Defines the three Conductor APIs by which other code can communicate
//! with a [`Conductor`](super::Conductor):
//!
//! - [`CellConductorApi`], for Cells to communicate with their Conductor
//! - [`AppInterfaceApi`], for external UIs to e.g. call zome functions on a Conductor
//! - [`AdminInterfaceApi`], for external processes to e.g. modify ConductorState
//!
//! Each type of API uses a [`ConductorHandle`](super::ConductorHandle) as its exclusive means of conductor access

mod api_cell;
mod api_dpki;
mod api_external;
#[allow(missing_docs)]
pub mod error;

pub use api_cell::*;
pub use api_dpki::*;
pub use api_external::*;



================================================
File: crates/holochain/src/conductor/cell.rs
================================================
//! A Cell is an "instance" of Holochain DNA.
//!
//! It combines an AgentPubKey with a Dna to create a SourceChain, upon which
//! Records can be added. A constructed Cell is guaranteed to have a valid
//! SourceChain which has already undergone Genesis.

use std::hash::Hash;
use std::hash::Hasher;
use std::sync::Arc;

use futures::future::FutureExt;
use holochain_serialized_bytes::SerializedBytes;
use rusqlite::OptionalExtension;
use tokio::sync::broadcast;
use tracing::*;
use tracing_futures::Instrument;

use error::CellError;
use holo_hash::*;
use holochain_cascade::authority;
use holochain_chc::ChcImpl;
use holochain_nonce::fresh_nonce;
use holochain_p2p::HolochainP2pDna;
use holochain_sqlite::prelude::*;
use holochain_state::host_fn_workspace::SourceChainWorkspace;
use holochain_state::prelude::*;
use holochain_state::schedule::live_scheduled_fns;
use holochain_types::db_cache::DhtDbQueryCache;

use crate::conductor::api::CellConductorApi;
use crate::conductor::cell::error::CellResult;
use crate::core::queue_consumer::spawn_queue_consumer_tasks;
use crate::core::queue_consumer::InitialQueueTriggers;
use crate::core::queue_consumer::QueueTriggers;
use crate::core::ribosome::guest_callback::init::InitResult;
use crate::core::ribosome::real_ribosome::RealRibosome;
use crate::core::ribosome::ZomeCallInvocation;
use crate::core::workflow::call_zome_workflow;
use crate::core::workflow::genesis_workflow::genesis_workflow;
use crate::core::workflow::initialize_zomes_workflow;
use crate::core::workflow::CallZomeWorkflowArgs;
use crate::core::workflow::GenesisWorkflowArgs;
use crate::core::workflow::GenesisWorkspace;
use crate::core::workflow::InitializeZomesWorkflowArgs;
use crate::core::workflow::ZomeCallResult;
use crate::{conductor::api::error::ConductorApiError, core::ribosome::RibosomeT};
#[cfg(feature = "unstable-countersigning")]
use {
    crate::core::workflow::countersigning_workflow::countersigning_success,
    crate::core::workflow::witnessing_workflow::receive_incoming_countersigning_ops,
    holochain_p2p::event::CountersigningSessionNegotiationMessage,
};

use super::api::CellConductorHandle;
use super::conductor::zome_call_signature_verification::is_valid_signature;
use super::space::Space;
use super::ConductorHandle;

pub const INIT_MUTEX_TIMEOUT_SECS: u64 = 30;

#[allow(missing_docs)]
pub mod error;

#[cfg(test)]
mod gossip_test;

#[cfg(test)]
mod test;

impl Hash for Cell {
    fn hash<H>(&self, state: &mut H)
    where
        H: Hasher,
    {
        self.id.hash(state);
    }
}

impl PartialEq for Cell {
    fn eq(&self, other: &Self) -> bool {
        self.id == other.id
    }
}

/// A Cell is a grouping of the resources necessary to run workflows
/// on behalf of an agent. It does not have a lifetime of its own aside
/// from the lifetimes of the resources which it holds references to.
/// Any work it does is through running a workflow, passing references to
/// the resources needed to complete that workflow.
///
/// A Cell is guaranteed to contain a Source Chain which has undergone
/// Genesis.
///
/// The [`Conductor`](super::Conductor) manages a collection of Cells, and will call functions
/// on the Cell when a Conductor API method is called (either a
/// [`CellConductorApi`](super::api::CellConductorApi) or an [`AppInterfaceApi`](super::api::AppInterfaceApi))
pub struct Cell {
    id: CellId,
    conductor_api: CellConductorHandle,
    // NOTE: this got snuck in here, the original purpose was that the Cell would have limited access to
    // the full Conductor via `CellConductorHandle`. As it stands, it's redundant to have both, but it
    // may make it easier to a cleanup of the Conductor monolith later if we don't completely remove
    // the encapsulation of CellConductorHandle, even though the encapsulation is not complete.
    conductor_handle: ConductorHandle,
    space: Space,
    holochain_p2p_cell: HolochainP2pDna,
    queue_triggers: QueueTriggers,
    signal_tx: broadcast::Sender<Signal>,
    init_mutex: tokio::sync::Mutex<()>,
}

impl Cell {
    /// Constructor for a Cell, which ensure the Cell is fully initialized
    /// before returning.
    ///
    /// If it hasn't happened already, a SourceChain will be created, and
    /// genesis will be run. If these have already happened, those steps are
    /// skipped.
    ///
    /// No Cell will be created if the SourceChain is not ready to be used.
    pub async fn create(
        id: CellId,
        conductor_handle: ConductorHandle,
        space: Space,
        holochain_p2p_cell: HolochainP2pDna,
        signal_tx: broadcast::Sender<Signal>,
    ) -> CellResult<(Self, InitialQueueTriggers)> {
        let conductor_api = Arc::new(CellConductorApi::new(conductor_handle.clone(), id.clone()));
        let authored_db = space.get_or_create_authored_db(id.agent_pubkey().clone())?;

        // check if genesis has been run
        let has_genesis = {
            // check if genesis ran.
            GenesisWorkspace::new(authored_db.clone(), space.dht_db.clone())?
                .has_genesis(id.agent_pubkey().clone())
                .await?
        };

        if has_genesis {
            let (queue_triggers, initial_queue_triggers) = spawn_queue_consumer_tasks(
                id.clone(),
                holochain_p2p_cell.clone(),
                &space,
                conductor_handle.clone(),
            )
            .await
            .map_err(Box::new)?;

            Ok((
                Self {
                    id,
                    conductor_api,
                    conductor_handle,
                    space,
                    holochain_p2p_cell,
                    queue_triggers,
                    signal_tx,
                    init_mutex: Default::default(),
                },
                initial_queue_triggers,
            ))
        } else {
            Err(CellError::CellWithoutGenesis(id))
        }
    }

    /// Performs the Genesis workflow for the Cell, ensuring that its initial
    /// records are committed. This is a prerequisite for any other interaction
    /// with the SourceChain
    #[allow(clippy::too_many_arguments)]
    pub async fn genesis<Ribosome>(
        cell_id: CellId,
        conductor_handle: ConductorHandle,
        authored_db: DbWrite<DbKindAuthored>,
        dht_db: DbWrite<DbKindDht>,
        dht_db_cache: DhtDbQueryCache,
        ribosome: Ribosome,
        membrane_proof: Option<MembraneProof>,
        chc: Option<ChcImpl>,
    ) -> CellResult<()>
    where
        Ribosome: RibosomeT + 'static,
    {
        // get the dna
        let dna_file = conductor_handle
            .get_dna_file(cell_id.dna_hash())
            .ok_or_else(|| DnaError::DnaMissing(cell_id.dna_hash().to_owned()))?;

        let conductor_api = CellConductorApi::new(conductor_handle.clone(), cell_id.clone());

        // run genesis
        let workspace = GenesisWorkspace::new(authored_db, dht_db)
            .map_err(ConductorApiError::from)
            .map_err(Box::new)?;

        // exit early if genesis has already run
        if workspace
            .has_genesis(cell_id.agent_pubkey().clone())
            .await?
        {
            return Ok(());
        }

        let args = GenesisWorkflowArgs::new(
            dna_file,
            cell_id.agent_pubkey().clone(),
            membrane_proof,
            ribosome,
            dht_db_cache,
            chc,
        );

        genesis_workflow(workspace, conductor_api, args)
            .await
            .map_err(ConductorApiError::from)
            .map_err(Box::new)?;

        if let Some(trigger) = conductor_handle
            .get_queue_consumer_workflows()
            .integration_trigger(Arc::new(cell_id.dna_hash().clone()))
        {
            trigger.trigger(&"genesis");
        }
        Ok(())
    }

    fn dna_hash(&self) -> &DnaHash {
        self.id.dna_hash()
    }

    #[allow(unused)]
    fn agent_pubkey(&self) -> &AgentPubKey {
        self.id.agent_pubkey()
    }

    /// Accessor
    pub fn id(&self) -> &CellId {
        &self.id
    }

    /// Access a network sender that is partially applied to this cell's DnaHash/AgentPubKey
    pub fn holochain_p2p_dna(&self) -> &holochain_p2p::HolochainP2pDna {
        &self.holochain_p2p_cell
    }

    pub(super) async fn dispatch_scheduled_fns(self: Arc<Self>, now: Timestamp) {
        let authored_db = match self.get_or_create_authored_db() {
            Ok(db) => db,
            Err(e) => {
                error!(
                    "error getting authored db, cannot dispatch scheduled functions: {:?}",
                    e
                );
                return;
            }
        };

        let author = self.id.agent_pubkey().clone();
        let live_fns = authored_db
            .write_async(move |txn| {
                // Rescheduling should not fail as the data in the database
                // should be valid schedules only.
                reschedule_expired(txn, now, &author)?;
                let lives = live_scheduled_fns(txn, now, &author);
                // We know what to run so we can delete the ephemerals.
                if lives.is_ok() {
                    // Failing to delete should rollback this attempt.
                    delete_live_ephemeral_scheduled_fns(txn, now, &author)?;
                }
                lives
            })
            .await;

        match live_fns {
            // Cannot proceed if we don't know what to run.
            Err(e) => {
                error!("error calling scheduled fn: {:?}", e);
            }
            Ok(live_fns) => {
                let mut tasks = vec![];
                for (scheduled_fn, schedule) in &live_fns {
                    // Failing to encode a schedule should never happen.
                    // If it does log the error and bail.
                    let payload = match ExternIO::encode(schedule) {
                        Ok(payload) => payload,
                        Err(e) => {
                            error!(
                                "error encoding scheduled fn: {:?} error: {:?}",
                                scheduled_fn, e
                            );
                            continue;
                        }
                    };
                    let provenance = self.id.agent_pubkey().clone();
                    let (nonce, expires_at) = match fresh_nonce(now) {
                        Ok(v) => v,
                        Err(e) => {
                            error!(
                                "error creating nonce for fn: {:?} error: {:?}",
                                scheduled_fn, e
                            );
                            continue;
                        }
                    };
                    let zome_call_params = ZomeCallParams {
                        provenance,
                        cell_id: self.id.clone(),
                        zome_name: scheduled_fn.zome_name().clone(),
                        fn_name: scheduled_fn.fn_name().clone(),
                        cap_secret: None,
                        payload,
                        nonce,
                        expires_at,
                    };

                    tasks.push(self.call_zome(zome_call_params, None));
                }
                let results: Vec<CellResult<ZomeCallResult>> =
                    futures::future::join_all(tasks).await;

                let author = self.id.agent_pubkey().clone();
                // We don't do anything with errors in here.
                let _ = authored_db
                    .write_async(move |txn| {
                        for ((scheduled_fn, _), result) in live_fns.iter().zip(results.iter()) {
                            match result {
                                Ok(Ok(ZomeCallResponse::Ok(extern_io))) => {
                                    let next_schedule: Schedule = match extern_io.decode() {
                                        Ok(Some(v)) => v,
                                        Ok(None) => {
                                            continue;
                                        }
                                        Err(e) => {
                                            error!("scheduled zome call error in ExternIO::decode: {:?}", e);
                                            continue;
                                        }
                                    };
                                    // Ignore errors so that failing to schedule
                                    // one function doesn't error others.
                                    // For example if a zome returns a bad cron.
                                    if let Err(e) = schedule_fn(
                                        txn,
                                        &author,
                                        scheduled_fn.clone(),
                                        Some(next_schedule),
                                        now,
                                    ) {
                                        error!("scheduled zome call error in schedule_fn: {:?}", e);
                                        continue;
                                    }
                                }
                                errorish => error!("scheduled zome call error: {:?}", errorish),
                            }
                        }
                        Result::<(), DatabaseError>::Ok(())
                    })
                    .await;
            }
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, evt)))]
    /// Entry point for incoming messages from the network that need to be handled
    //
    // TODO: when we had CellStatus to track whether a cell had joined the network or not,
    // we would disallow zome calls for cells which had not joined. If we want that behavior,
    // we can do that check at the time of this function call, rather than at the time of trying
    // to access the Cell itself, as it was previously done.
    pub async fn handle_holochain_p2p_event(
        &self,
        evt: holochain_p2p::event::HolochainP2pEvent,
    ) -> CellResult<()> {
        use holochain_p2p::event::HolochainP2pEvent::*;
        match evt {
            PutAgentInfoSigned { .. }
            | QueryAgentInfoSigned { .. }
            | QueryGossipAgents { .. }
            | QueryOpHashes { .. }
            | QueryAgentInfoSignedNearBasis { .. }
            | QueryPeerDensity { .. }
            | Publish { .. }
            | FetchOpData { .. } => {
                // These events are aggregated over a set of cells, so need to be handled at the conductor level.
                unreachable!()
            }

            CallRemote {
                span_context: _,
                zome_call_params_serialized,
                signature,
                respond,
                ..
            } => {
                async {
                    let res = self
                        .handle_call_remote(zome_call_params_serialized, signature)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("call_remote"))
                .await;
            }

            Get {
                span_context: _,
                respond,
                dht_hash,
                options,
                ..
            } => {
                async {
                    let res = self
                        .handle_get(dht_hash, options)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_get"))
                .await;
            }

            GetMeta {
                span_context: _,
                respond,
                dht_hash,
                options,
                ..
            } => {
                async {
                    let res = self
                        .handle_get_meta(dht_hash, options)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_get_meta"))
                .await;
            }

            GetLinks {
                span_context: _,
                respond,
                link_key,
                options,
                ..
            } => {
                async {
                    let res = self
                        .handle_get_links(link_key, options)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_get_links"))
                .await;
            }

            CountLinks {
                span_context: _,
                respond,
                query,
                ..
            } => {
                async {
                    let res = self
                        .handle_count_links(query)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_count_links"))
                .await;
            }

            GetAgentActivity {
                span_context: _,
                respond,
                agent,
                query,
                options,
                ..
            } => {
                async {
                    let res = self
                        .handle_get_agent_activity(agent, query, options)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_get_agent_activity"))
                .await;
            }

            MustGetAgentActivity {
                span_context: _,
                respond,
                author,
                filter,
                ..
            } => {
                async {
                    let res = self
                        .handle_must_get_agent_activity(author, filter)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_must_get_agent_activity"))
                .await;
            }

            ValidationReceiptsReceived {
                span_context: _,
                respond,
                receipts,
                ..
            } => {
                async {
                    let res = self
                        .handle_validation_receipts(receipts)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_validation_receipt_received"))
                .await;
                // We got a receipt so we must be connected to the network
                // and should reset the publish back off loop to its minimum.
                self.queue_triggers.publish_dht_ops.reset_back_off();
            }

            SignNetworkData {
                span_context: _,
                respond,
                ..
            } => {
                async {
                    let res = self
                        .handle_sign_network_data()
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_sign_network_data"))
                .await;
            }

            #[allow(unused_variables)]
            CountersigningSessionNegotiation {
                respond, message, ..
            } => {
                #[cfg(feature = "unstable-countersigning")]
                async {
                    let res = self
                        .handle_countersigning_session_negotiation(message)
                        .await
                        .map_err(holochain_p2p::HolochainP2pError::other);
                    respond.respond(Ok(async move { res }.boxed().into()));
                }
                .instrument(debug_span!("cell_handle_countersigning_response"))
                .await;
            }
        }
        Ok(())
    }

    #[cfg(feature = "unstable-countersigning")]
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    /// we are receiving a response from a countersigning authority
    async fn handle_countersigning_session_negotiation(
        &self,
        message: CountersigningSessionNegotiationMessage,
    ) -> CellResult<()> {
        match message {
            CountersigningSessionNegotiationMessage::EnzymePush(chain_op) => {
                let ops = vec![*chain_op]
                    .into_iter()
                    .map(|op| {
                        let hash = DhtOpHash::with_data_sync(&op);
                        (hash, op)
                    })
                    .collect();
                receive_incoming_countersigning_ops(
                    ops,
                    &self.space.witnessing_workspace,
                    self.queue_triggers.witnessing.clone(),
                )
                .map_err(Box::new)?;
                Ok(())
            }
            CountersigningSessionNegotiationMessage::AuthorityResponse(signed_actions) => {
                countersigning_success(
                    self.space.clone(),
                    self.id.agent_pubkey().clone(),
                    signed_actions,
                    self.queue_triggers.countersigning.clone(),
                )
                .await;

                Ok(())
            }
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    /// a remote node is asking us for entry data
    async fn handle_get(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: holochain_p2p::event::GetOptions,
    ) -> CellResult<WireOps> {
        debug!("handling get");
        // TODO: Later we will need more get types but for now
        // we can just have these defaults depending on whether or not
        // the hash is an entry or action.
        // In the future we should use GetOptions to choose which get to run.
        let mut r = match dht_hash.into_primitive() {
            AnyDhtHashPrimitive::Entry(hash) => self
                .handle_get_entry(hash, options)
                .await
                .map(WireOps::Entry),
            AnyDhtHashPrimitive::Action(hash) => self
                .handle_get_record(hash, options)
                .await
                .map(WireOps::Record),
        };
        if let Err(e) = &mut r {
            error!(msg = "Error handling a get", ?e, agent = ?self.id.agent_pubkey());
        }
        r
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    async fn handle_get_entry(
        &self,
        hash: EntryHash,
        options: holochain_p2p::event::GetOptions,
    ) -> CellResult<WireEntryOps> {
        let db = self.space.dht_db.clone();
        authority::handle_get_entry(db.into(), hash, options)
            .await
            .map_err(Into::into)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    async fn handle_get_record(
        &self,
        hash: ActionHash,
        options: holochain_p2p::event::GetOptions,
    ) -> CellResult<WireRecordOps> {
        let db = self.space.dht_db.clone();
        authority::handle_get_record(db.into(), hash, options)
            .await
            .map_err(Into::into)
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, _dht_hash, _options))
    )]
    /// a remote node is asking us for metadata
    async fn handle_get_meta(
        &self,
        _dht_hash: holo_hash::AnyDhtHash,
        _options: holochain_p2p::event::GetMetaOptions,
    ) -> CellResult<MetadataSet> {
        unimplemented!()
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    /// a remote node is asking us for links
    // TODO: Right now we are returning all the full actions
    // We could probably send some smaller types instead of the full actions
    // if we are careful.
    async fn handle_get_links(
        &self,
        link_key: WireLinkKey,
        options: holochain_p2p::event::GetLinksOptions,
    ) -> CellResult<WireLinkOps> {
        debug!(id = ?self.id());
        let db = self.space.dht_db.clone();
        authority::handle_get_links(db.into(), link_key, options)
            .await
            .map_err(Into::into)
    }

    /// a remote node is asking us to count links
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    async fn handle_count_links(&self, query: WireLinkQuery) -> CellResult<CountLinksResponse> {
        let db = self.space.dht_db.clone();
        Ok(CountLinksResponse::new(
            authority::handle_get_links_query(db.into(), query)
                .await?
                .into_iter()
                .map(|l| l.create_link_hash)
                .collect::<Vec<_>>(),
        ))
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, options)))]
    async fn handle_get_agent_activity(
        &self,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: holochain_p2p::event::GetActivityOptions,
    ) -> CellResult<AgentActivityResponse> {
        let db = self.space.dht_db.clone();
        authority::handle_get_agent_activity(db.into(), agent, query, options)
            .await
            .map_err(Into::into)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    async fn handle_must_get_agent_activity(
        &self,
        author: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> CellResult<MustGetAgentActivityResponse> {
        let db = self.space.dht_db.clone();
        authority::handle_must_get_agent_activity(db.into(), author, filter)
            .await
            .map_err(Into::into)
    }

    /// A remote agent is sending us a validation receipt bundle.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, receipts)))]
    async fn handle_validation_receipts(
        &self,
        receipts: ValidationReceiptBundle,
    ) -> CellResult<()> {
        for receipt in receipts.into_iter() {
            debug!(from = ?receipt.receipt.validators, to = ?self.id.agent_pubkey(), hash = ?receipt.receipt.dht_op_hash);

            // Get the action for this op so we can check the entry type.
            let hash = receipt.receipt.dht_op_hash.clone();
            let action: Option<SignedAction> = self
                .get_or_create_authored_db()?
                .read_async(move |txn| {
                    let h: Option<Vec<u8>> = txn
                        .query_row(
                            "SELECT Action.blob as action_blob
                    FROM DhtOp
                    JOIN Action ON Action.hash = DhtOp.action_hash
                    WHERE DhtOp.hash = :hash",
                            named_params! {
                                ":hash": hash,
                            },
                            |row| row.get("action_blob"),
                        )
                        .optional()?;
                    match h {
                        Some(h) => from_blob(h),
                        None => Ok(None),
                    }
                })
                .await?;

            // If the action has an app entry type get the entry def
            // from the conductor.
            let required_receipt_count = match action.as_ref().and_then(|h| h.entry_type()) {
                Some(EntryType::App(AppEntryDef {
                    zome_index,
                    entry_index,
                    ..
                })) => {
                    let ribosome = self.conductor_api.get_this_ribosome().map_err(Box::new)?;
                    let zome = ribosome.get_integrity_zome(zome_index);
                    match zome {
                        Some(zome) => self
                            .conductor_api
                            .get_entry_def(&EntryDefBufferKey::new(
                                zome.into_inner().1,
                                *entry_index,
                            ))
                            .map(|e| u8::from(e.required_validations)),
                        None => None,
                    }
                }
                _ => None,
            };

            // If no required receipt count was found then fallback to the default.
            let required_validation_count = required_receipt_count.unwrap_or(
                crate::core::workflow::publish_dht_ops_workflow::DEFAULT_RECEIPT_BUNDLE_SIZE,
            );

            let receipt_op_hash = receipt.receipt.dht_op_hash.clone();

            let receipt_count = self
                .space
                .dht_db
                .write_async({
                    let receipt_op_hash = receipt_op_hash.clone();
                    move |txn| -> StateMutationResult<usize> {
                        // Add the new receipts to the db
                        add_if_unique(txn, receipt)?;

                        // Get the current count for this DhtOp.
                        let receipt_count: usize = txn.query_row(
                            "SELECT COUNT(rowid) FROM ValidationReceipt WHERE op_hash = :op_hash",
                            named_params! {
                                ":op_hash": receipt_op_hash,
                            },
                            |row| row.get(0),
                        )?;

                        if receipt_count >= required_validation_count as usize {
                            // If we have enough receipts then set receipts to complete.
                            //
                            // Don't fail here if this doesn't work, it's only informational. Getting
                            // the same flag set in the authored db is what will stop the publish
                            // workflow from republishing this op.
                            set_receipts_complete_redundantly_in_dht_db(
                                txn,
                                &receipt_op_hash,
                                true,
                            )
                            .ok();
                        }

                        Ok(receipt_count)
                    }
                })
                .await?;

            // If we have enough receipts then set receipts to complete.
            if receipt_count >= required_validation_count as usize {
                // Note that the flag is set in the authored db because that's what the publish workflow checks to decide
                // whether to republish the op for more validation receipts.
                self.get_or_create_authored_db()?
                    .write_async(move |txn| -> StateMutationResult<()> {
                        set_receipts_complete(txn, &receipt_op_hash, true)
                    })
                    .await?;
            }
        }

        Ok(())
    }

    /// the network module would like this cell/agent to sign some data
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    async fn handle_sign_network_data(&self) -> CellResult<Signature> {
        Ok([0; 64].into())
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, from_agent, fn_name, cap_secret, payload))
    )]
    #[allow(clippy::too_many_arguments)]
    /// a remote agent is attempting a "call_remote" on this cell.
    async fn handle_call_remote(
        &self,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> CellResult<SerializedBytes> {
        let zome_call_params = zome_call_params_serialized.decode::<ZomeCallParams>()?;
        if !is_valid_signature(
            &zome_call_params.provenance,
            zome_call_params_serialized.as_bytes(),
            &signature,
        )
        .await
        .map_err(|err| CellError::ConductorApiError(Box::new(err)))?
        {
            return Err(CellError::ZomeCallAuthenticationFailed(
                signature,
                zome_call_params.provenance,
            ));
        }

        // double ? because
        // - ConductorApiResult
        // - ZomeCallResult
        Ok(self.call_zome(zome_call_params, None).await??.try_into()?)
    }

    /// Function called by the Conductor
    //
    // TODO: when we had CellStatus to track whether a cell had joined the network or not,
    // we would disallow zome calls for cells which had not joined. If we want that behavior,
    // we can do that check at the time of the zome call, rather than at the time of trying
    // to access the Cell itself, as it was previously done.
    pub async fn call_zome(
        &self,
        params: ZomeCallParams,
        workspace_lock: Option<SourceChainWorkspace>,
    ) -> CellResult<ZomeCallResult> {
        // Only check if init has run if this call is not coming from
        // an already running init call.
        if workspace_lock
            .as_ref()
            .map_or(true, |w| !w.called_from_init())
        {
            // Check if init has run if not run it
            self.check_or_run_zome_init().await?;
        }

        // The "init" function is a reserved function name that is only allowed to be called once
        // by the `check_or_run_zome_init` function.
        // It is acceptable to call it to get the conductor to call it once but after that we will
        // just return a success with the same output that the init function would have returned.
        if params.fn_name.as_ref() == "init" {
            return Ok(Ok(ZomeCallResponse::Ok(ExternIO::encode(
                InitCallbackResult::Pass,
            )?)));
        }

        let keystore = self.conductor_api.keystore().clone();

        let conductor_handle = self.conductor_handle.clone();
        let ribosome = self.get_ribosome()?;
        let invocation =
            ZomeCallInvocation::try_from_params(self.conductor_api.clone(), params).await?;

        let dna_def = ribosome.dna_def().as_content().clone();
        // If there is no existing zome call then this is the root zome call
        let is_root_zome_call = workspace_lock.is_none();
        let workspace_lock = match workspace_lock {
            Some(l) => l,
            None => {
                SourceChainWorkspace::new(
                    self.get_or_create_authored_db()?,
                    self.dht_db().clone(),
                    self.space.dht_query_cache.clone(),
                    self.cache().clone(),
                    keystore.clone(),
                    self.id.agent_pubkey().clone(),
                    Arc::new(dna_def),
                )
                .await?
            }
        };
        let args = CallZomeWorkflowArgs {
            cell_id: self.id.clone(),
            ribosome,
            invocation,
            signal_tx: self.signal_tx.clone(),
            conductor_handle,
            is_root_zome_call,
        };
        Ok(call_zome_workflow(
            workspace_lock,
            self.holochain_p2p_cell.clone(),
            keystore,
            args,
            self.queue_triggers.publish_dht_ops.clone(),
            self.queue_triggers.integrate_dht_ops.clone(),
            self.queue_triggers.countersigning.clone(),
        )
        .await
        .map_err(Box::new)?)
    }

    /// Check if each Zome's init callback has been run, and if not, run it.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub(crate) async fn check_or_run_zome_init(&self) -> CellResult<()> {
        // Ensure that only one init check is run at a time
        let _guard = tokio::time::timeout(
            std::time::Duration::from_secs(INIT_MUTEX_TIMEOUT_SECS),
            self.init_mutex.lock(),
        )
        .await
        .map_err(|_| CellError::InitTimeout)?;

        // If not run it
        let keystore = self.conductor_api.keystore().clone();
        let id = self.id.clone();
        let conductor_handle = self.conductor_handle.clone();

        // get the dna
        let ribosome = self.get_ribosome()?;

        let dna_def = ribosome.dna_def().clone();

        // Create the workspace
        let workspace = SourceChainWorkspace::init_as_root(
            self.get_or_create_authored_db()?,
            self.dht_db().clone(),
            self.space.dht_query_cache.clone(),
            self.cache().clone(),
            keystore.clone(),
            id.agent_pubkey().clone(),
            Arc::new(dna_def.into_content()),
        )
        .await?;

        // Check if initialization has run
        if workspace.source_chain().zomes_initialized().await? {
            return Ok(());
        }
        trace!("running init");

        // Run the workflow
        let args = InitializeZomesWorkflowArgs {
            ribosome,
            conductor_handle,
            signal_tx: self.signal_tx.clone(),
            cell_id: self.id.clone(),
            integrate_dht_ops_trigger: self.queue_triggers.integrate_dht_ops.clone(),
        };
        let init_result =
            initialize_zomes_workflow(workspace, self.holochain_p2p_cell.clone(), keystore, args)
                .await
                .map_err(Box::new)?;
        trace!(?init_result);
        match init_result {
            InitResult::Pass => {}
            r => return Err(CellError::InitFailed(r)),
        }
        Ok(())
    }

    /// Clean up long-running managed tasks.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(cell_id = ?self.id())))]
    pub async fn cleanup(&self) -> CellResult<()> {
        use holochain_p2p::HolochainP2pDnaT;
        let shutdown = self
            .conductor_handle
            .task_manager()
            .stop_cell_tasks(self.id().clone())
            .map(|r| CellResult::Ok(r?))
            .in_current_span();
        let leave = self
            .holochain_p2p_dna()
            .leave(self.id.agent_pubkey().clone())
            .map(|r| CellResult::Ok(r?))
            .in_current_span();
        let (shutdown, leave) = futures::future::join(shutdown, leave).await;
        shutdown?;
        leave?;
        tracing::info!("Cell cleaned up and removed: {:?}", self.id());
        Ok(())
    }

    /// Instantiate a Ribosome for use by this Cell's workflows
    pub(crate) fn get_ribosome(&self) -> CellResult<RealRibosome> {
        Ok(self
            .conductor_handle
            .get_ribosome(self.dna_hash())
            .map_err(|_| DnaError::DnaMissing(self.dna_hash().to_owned()))?)
    }

    /// Accessor for the p2p_agents_db backing this Cell
    pub(crate) fn p2p_agents_db(&self) -> &DbWrite<DbKindP2pAgents> {
        &self.space.p2p_agents_db
    }

    /// Accessor for the authored database backing this Cell
    pub(crate) fn get_or_create_authored_db(&self) -> CellResult<DbWrite<DbKindAuthored>> {
        Ok(self
            .space
            .get_or_create_authored_db(self.id.agent_pubkey().clone())?)
    }

    /// Accessor for the authored database backing this Cell
    pub(crate) fn dht_db(&self) -> &DbWrite<DbKindDht> {
        &self.space.dht_db
    }

    pub(crate) fn cache(&self) -> &DbWrite<DbKindCache> {
        &self.space.cache_db
    }

    pub(crate) fn notify_authored_ops_moved_to_limbo(&self) {
        self.queue_triggers
            .integrate_dht_ops
            .trigger(&"notify_authored_ops_moved_to_limbo");
    }

    pub(crate) fn publish_authored_ops(&self) {
        self.queue_triggers
            .publish_dht_ops
            .trigger(&"publish_authored_ops");
    }

    #[cfg(any(test, feature = "test_utils"))]
    pub(crate) fn triggers(&self) -> &QueueTriggers {
        &self.queue_triggers
    }
}

impl std::fmt::Debug for Cell {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("Cell").field("id", &self.id()).finish()
    }
}



================================================
File: crates/holochain/src/conductor/chc.rs
================================================
//! Types for Chain Head Coordination

use holochain_keystore::MetaLairClient;
use holochain_zome_types::prelude::*;
use once_cell::sync::Lazy;
use std::{collections::HashMap, sync::Arc};
use url::Url;

pub use holochain_chc::*;

/// Storage for the local CHC implementations
pub static CHC_LOCAL_MAP: Lazy<parking_lot::Mutex<HashMap<CellId, ChcImpl>>> =
    Lazy::new(|| parking_lot::Mutex::new(HashMap::new()));

/// The URL which indicates that the fake local CHC service should be used,
/// instead of a remote service via HTTP
pub const CHC_LOCAL_MAGIC_URL: &str = "local:";

/// Build the appropriate CHC implementation.
///
/// In particular, if the url is the magic string "local:", then a [`ChcLocal`]
/// implementation will be used. Otherwise, if the url is set, and the CellId
/// is "CHC-enabled", then a [`ChcRemote`] will be produced.
pub fn build_chc(
    base_url: Option<&Url>,
    keystore: MetaLairClient,
    cell_id: &CellId,
) -> Option<ChcImpl> {
    // TODO: check if the agent key is Holo-hosted, otherwise return none
    let is_holo_agent = true;
    if is_holo_agent {
        base_url.map(|url| {
            #[cfg(feature = "chc")]
            {
                fn chc_local(keystore: MetaLairClient, cell_id: CellId) -> ChcImpl {
                    let agent = cell_id.agent_pubkey().clone();
                    let mut m = CHC_LOCAL_MAP.lock();
                    m.entry(cell_id)
                        .or_insert_with(|| Arc::new(chc_local::ChcLocal::new(keystore, agent)))
                        .clone()
                }

                fn chc_remote(
                    base_url: Url,
                    keystore: MetaLairClient,
                    cell_id: &CellId,
                ) -> ChcImpl {
                    Arc::new(chc_http::ChcHttp::new(base_url, keystore, cell_id))
                }

                if url.as_str() == CHC_LOCAL_MAGIC_URL {
                    chc_local(keystore, cell_id.clone())
                } else {
                    chc_remote(url.clone(), keystore, cell_id)
                }
            }

            #[cfg(not(feature = "chc"))]
            panic!("CHC is not enabled in this build. Rebuild with the `chc` feature enabled.")
        })
    } else {
        None
    }
}

#[cfg(test)]
mod tests {

    use std::sync::atomic::Ordering::SeqCst;
    use std::sync::{atomic::AtomicBool, Arc};

    use hdk::prelude::*;
    use holochain_chc::*;
    use holochain_conductor_api::conductor::{ConductorConfig, DpkiConfig};
    use holochain_keystore::MetaLairClient;
    use holochain_state::prelude::SourceChainError;
    use holochain_types::record::SignedActionHashedExt;
    use holochain_wasm_test_utils::TestWasm;

    use crate::conductor::CellError;
    use crate::core::workflow::WorkflowError;
    use crate::{
        conductor::{
            api::error::ConductorApiError,
            chc::{CHC_LOCAL_MAGIC_URL, CHC_LOCAL_MAP},
            error::ConductorError,
        },
        sweettest::*,
    };

    /// A CHC implementation that can be set up to error
    struct FlakyChc {
        chc: chc_local::ChcLocal,
        pub fail: AtomicBool,
    }

    #[async_trait::async_trait]
    impl ChainHeadCoordinator for FlakyChc {
        type Item = SignedActionHashed;

        async fn add_records_request(&self, request: AddRecordsRequest) -> ChcResult<()> {
            if self.fail.load(SeqCst) {
                Err(ChcError::Other("bad".to_string()))
            } else {
                self.chc.add_records_request(request).await
            }
        }

        async fn get_record_data_request(
            &self,
            request: GetRecordsRequest,
        ) -> ChcResult<Vec<(SignedActionHashed, Option<(Arc<EncryptedEntry>, Signature)>)>>
        {
            if self.fail.load(SeqCst) {
                Err(ChcError::Other("bad".to_string()))
            } else {
                self.chc.get_record_data_request(request).await
            }
        }
    }

    impl ChainHeadCoordinatorExt for FlakyChc {
        fn signing_info(&self) -> (MetaLairClient, AgentPubKey) {
            unimplemented!()
        }
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn simple_chc_sync() {
        use holochain::test_utils::inline_zomes::simple_crud_zome;

        let config = ConductorConfig {
            chc_url: Some(url2::Url2::parse(CHC_LOCAL_MAGIC_URL)),
            ..Default::default()
        };
        let mut conductor = SweetConductor::from_config(config).await;

        let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(simple_crud_zome()).await;
