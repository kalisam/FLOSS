    /// If there is no place to graft the incoming actions, then the incoming actions list entirely
    /// specifies the new chain. i.e., if the first incoming record's previous hash matches none of
    /// the existing hashes, then return an empty existing list and the full incoming list.
    ///
    /// If the first incoming record's previous hash matches the last existing hash,
    /// then we return both lists unchanged.
    ///
    /// If the first incoming record's previous hash matches one of the existing hashes
    /// other than the existing top, then:
    /// - from the first existing hash to match, walk forwards, checking if existing
    ///   hashes match the incoming actions. For each existing record which matches an incoming
    ///   record, keep that hash in the existing list and remove it from the incoming list,
    ///   so that it doesn't get committed twice.
    pub fn rebalance(self) -> Self {
        let (pivot, overlap) = self.pivot_and_overlap();
        if let Some(pivot) = pivot {
            Self {
                existing: self.existing[pivot - overlap..].to_vec(),
                incoming: self.incoming[overlap..].to_vec(),
            }
        } else {
            Self {
                existing: vec![],
                incoming: self.incoming,
            }
        }
    }

    fn pivot(&self) -> Pivot {
        if let Some(first) = self.incoming.first() {
            if first.as_ref().prev_hash().is_none() {
                // If the first incoming item is a root item, then there is no existing
                // item to use as the pivot, therefore we need to handle that case separately
                Pivot::NewRoot
            } else {
                self.existing
                    .iter()
                    .position(|e| {
                        Some(e.get_hash()) == first.as_ref().prev_hash()
                            && e.seq() + 1 == first.as_ref().seq()
                    })
                    .map(Pivot::Index)
                    .unwrap_or(Pivot::None)
            }
        } else {
            Pivot::None
        }
    }

    fn pivot_and_overlap(&self) -> (Option<usize>, usize) {
        let take = match self.pivot() {
            Pivot::NewRoot => self.existing.len(),
            Pivot::Index(pivot) => pivot,
            Pivot::None => return (None, 0),
        };
        let overlap = self
            .existing
            .iter()
            .take(take)
            .rev()
            .zip(self.incoming.iter())
            .position(|(e, n)| e != n.as_ref())
            .unwrap_or_else(|| take.min(self.incoming.len()));
        (Some(take), overlap)
    }

    #[allow(dead_code)]
    pub fn existing(&self) -> &[A] {
        self.existing.as_ref()
    }

    pub fn incoming(&self) -> &[B] {
        self.incoming.as_ref()
    }
}



================================================
File: crates/holochain/src/conductor/conductor/tests.rs
================================================
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::DnaHashFixturator;
use std::sync::atomic::AtomicU32;
use std::sync::atomic::Ordering;

use super::Conductor;
use super::ConductorState;
use super::*;
use crate::conductor::api::error::ConductorApiError;
use crate::core::ribosome::guest_callback::validate::ValidateResult;
use crate::sweettest::*;
use crate::test_utils::inline_zomes::simple_crud_zome;
use crate::{
    assert_eq_retry_10s, core::ribosome::guest_callback::genesis_self_check::GenesisSelfCheckResult,
};
use ::fixt::prelude::*;
use holochain_conductor_api::AppInfoStatus;
use holochain_conductor_api::CellInfo;
use holochain_keystore::crude_mock_keystore::*;
use holochain_keystore::test_keystore;
use holochain_types::inline_zome::InlineZomeSet;
use holochain_types::test_utils::fake_cell_id;
use holochain_wasm_test_utils::TestWasm;
use holochain_zome_types::op::Op;
use maplit::hashset;
use matches::assert_matches;

// Module with tests for agent key revocation. With or without DPKI, an agent can revoke their key,
// which prevents further modifications of the source chain.
mod agent_key_revocation;
// Module with tests related to an agent's key lineage. Agents can update their key. Both old and new
// key belong to the same key lineage, they belong to the same agent.
#[cfg(feature = "unstable-functions")]
pub mod agent_lineage;
#[cfg(feature = "unstable-dpki")]
mod test_dpki;

#[tokio::test(flavor = "multi_thread")]
async fn can_update_state() {
    let db_dir = test_db_dir();
    let ribosome_store = RibosomeStore::new();
    let keystore = test_keystore();
    let holochain_p2p = holochain_p2p::stub_network().await;
    let (post_commit_sender, _post_commit_receiver) =
        tokio::sync::mpsc::channel(POST_COMMIT_CHANNEL_BOUND);
    let config = ConductorConfig {
        data_root_path: Some(db_dir.path().to_path_buf().into()),
        ..Default::default()
    };
    let (outcome_tx, _outcome_rx) = futures::channel::mpsc::channel(8);
    let spaces = Spaces::new(
        config.clone().into(),
        sodoken::BufRead::new_no_lock(b"passphrase"),
    )
    .await
    .unwrap();
    let conductor = Conductor::new(
        config.into(),
        ribosome_store,
        keystore,
        holochain_p2p,
        spaces,
        post_commit_sender,
        outcome_tx,
    );
    let state = conductor.get_state().await.unwrap();
    let mut expect_state = ConductorState::default();
    expect_state.set_tag(state.tag().clone());
    assert_eq!(state, expect_state);

    let cell_id = fake_cell_id(1);
    let installed_cell = InstalledCell::new(cell_id.clone(), "role_name".to_string());
    let app = InstalledAppCommon::new_legacy("fake app", vec![installed_cell]).unwrap();

    conductor
        .update_state(|mut state| {
            state.add_app(app)?;
            Ok(state)
        })
        .await
        .unwrap();
    let state = conductor.get_state().await.unwrap();
    assert_eq!(
        state.stopped_apps().map(second).collect::<Vec<_>>()[0]
            .all_cells()
            .collect::<Vec<_>>()
            .as_slice(),
        &[cell_id]
    );
}

/// App can't be installed if another app is already installed under the
/// same InstalledAppId
#[tokio::test(flavor = "multi_thread")]
async fn app_ids_are_unique() {
    let db_dir = test_db_dir();
    let ribosome_store = RibosomeStore::new();
    let holochain_p2p = holochain_p2p::stub_network().await;
    let (post_commit_sender, _post_commit_receiver) =
        tokio::sync::mpsc::channel(POST_COMMIT_CHANNEL_BOUND);

    let (outcome_tx, _outcome_rx) = futures::channel::mpsc::channel(8);
    let config = ConductorConfig {
        data_root_path: Some(db_dir.path().to_path_buf().into()),
        ..Default::default()
    };
    let spaces = Spaces::new(
        config.clone().into(),
        sodoken::BufRead::new_no_lock(b"passphrase"),
    )
    .await
    .unwrap();
    let conductor = Conductor::new(
        config.into(),
        ribosome_store,
        test_keystore(),
        holochain_p2p,
        spaces,
        post_commit_sender,
        outcome_tx,
    );

    let cell_id = fake_cell_id(1);

    let installed_cell = InstalledCell::new(cell_id.clone(), "handle".to_string());
    let app = InstalledAppCommon::new_legacy("id".to_string(), vec![installed_cell]).unwrap();

    conductor.add_disabled_app_to_db(app.clone()).await.unwrap();

    assert_matches!(
        conductor.add_disabled_app_to_db(app.clone()).await,
        Err(ConductorError::AppAlreadyInstalled(id))
        if id == *"id"
    );

    //- it doesn't matter whether the app is active or inactive
    let (_, delta) = conductor
        .transition_app_status("id".to_string(), AppStatusTransition::Enable)
        .await
        .unwrap();
    assert_eq!(delta, AppStatusFx::SpinUp);
    assert_matches!(
        conductor.add_disabled_app_to_db(app.clone()).await,
        Err(ConductorError::AppAlreadyInstalled(id))
        if &id == "id"
    );
}

/// App can't be installed if it contains duplicate RoleNames
#[tokio::test(flavor = "multi_thread")]
async fn role_names_are_unique() {
    let agent = fixt!(AgentPubKey);
    let cells = vec![
        InstalledCell::new(CellId::new(fixt!(DnaHash), agent.clone()), "1".into()),
        InstalledCell::new(CellId::new(fixt!(DnaHash), agent.clone()), "1".into()),
        InstalledCell::new(CellId::new(fixt!(DnaHash), agent.clone()), "2".into()),
    ];
    let result = InstalledAppCommon::new_legacy("id", cells.into_iter());
    matches::assert_matches!(
        result,
        Err(AppError::DuplicateRoleNames(_, role_names)) if role_names == vec!["1".to_string()]
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn can_set_fake_state() {
    let db_dir = test_db_dir();
    let expected = ConductorState::default();
    let conductor = ConductorBuilder::new()
        .config(SweetConductorConfig::standard().no_dpki().into())
        .fake_state(expected.clone())
        .with_data_root_path(db_dir.path().to_path_buf().into())
        .test(&[])
        .await
        .unwrap();
    let actual = conductor.get_state_from_handle().await.unwrap();
    assert_eq!(actual, expected);
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "This kind of cell sharing is no longer possible.
            Keeping the test here to highlight the intention,
            though it will have to be removed or totally rewritten some day."]
async fn test_list_running_apps_for_dependent_cell_id() {
    holochain_trace::test_run();

    let mk_dna = |name: &'static str| async move {
        let zome = InlineIntegrityZome::new_unique(Vec::new(), 0);
        SweetDnaFile::unique_from_inline_zomes((name, zome)).await
    };

    // Create three unique DNAs
    let (dna1, _, _) = mk_dna("zome1").await;
    let (dna2, _, _) = mk_dna("zome2").await;
    let (dna3, _, _) = mk_dna("zome3").await;

    // Install two apps on the Conductor:
    // Both share a CellId in common, and also include a distinct CellId each.
    let mut conductor = SweetConductor::from_standard_config().await;
    let app1 = conductor.setup_app("app1", [&dna1, &dna2]).await.unwrap();
    let alice = app1.agent().clone();
    let app2 = conductor
        .setup_app_for_agent("app2", alice, [&dna1, &dna3])
        .await
        .unwrap();

    let (cell1, cell2) = app1.into_tuple();
    let (_, cell3) = app2.into_tuple();

    let list_apps = |conductor: ConductorHandle, cell: SweetCell| async move {
        conductor
            .list_running_apps_for_dependent_cell_id(cell.cell_id())
            .await
            .unwrap()
    };

    // - Ensure that the first CellId is associated with both apps,
    //   and the other two are only associated with one app each.
    assert_eq!(
        list_apps(conductor.clone(), cell1).await,
        hashset!["app1".to_string(), "app2".to_string()]
    );
    assert_eq!(
        list_apps(conductor.clone(), cell2).await,
        hashset!["app1".to_string()]
    );
    assert_eq!(
        list_apps(conductor.clone(), cell3).await,
        hashset!["app2".to_string()]
    );
}

async fn mk_dna(
    zomes: impl Into<InlineZomeSet>,
) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>) {
    SweetDnaFile::unique_from_inline_zomes(zomes.into()).await
}

/// A function that sets up a SweetApp, used in several tests in this module
async fn common_genesis_test_app(
    conductor: &mut SweetConductor,
    custom_zomes: impl Into<InlineZomeSet>,
) -> ConductorApiResult<SweetApp> {
    let hardcoded_zome = InlineIntegrityZome::new_unique(Vec::new(), 0);

    // Create one DNA which always works, and another from a zome that gets passed in
    let (dna_hardcoded, _, _) = mk_dna(("hardcoded", hardcoded_zome)).await;
    let (dna_custom, _, _) = mk_dna(custom_zomes).await;

    // Install both DNAs under the same app:
    conductor
        .setup_app("app", &[dna_hardcoded, dna_custom])
        .await
}

#[tokio::test(flavor = "multi_thread")]
async fn test_uninstall_app() {
    holochain_trace::test_run();
    let (dna, _, _) = mk_dna(simple_crud_zome()).await;
    let mut conductor = SweetConductorConfig::standard().build_conductor().await;

    let app1 = conductor.setup_app("app1", [&dna]).await.unwrap();

    let hash1: ActionHash = conductor
        .call(
            &app1.cells()[0].zome("coordinator"),
            "create_string",
            "1".to_string(),
        )
        .await;

    let app2 = conductor.setup_app("app2", [&dna]).await.unwrap();

    let hash2: ActionHash = conductor
        .call(
            &app2.cells()[0].zome("coordinator"),
            "create_string",
            "1".to_string(),
        )
        .await;

    assert!(conductor
        .call::<_, Option<Record>>(&app1.cells()[0].zome("coordinator"), "read", hash2.clone())
        .await
        .is_some());
    assert!(conductor
        .call::<_, Option<Record>>(&app2.cells()[0].zome("coordinator"), "read", hash1.clone())
        .await
        .is_some());

    // - Ensure that the apps are active
    assert_eq_retry_10s!(
        {
            let state = conductor.get_state_from_handle().await.unwrap();
            (state.running_apps().count(), state.stopped_apps().count())
        },
        (2, 0)
    );

    let db1 = conductor
        .spaces
        .get_or_create_authored_db(dna.dna_hash(), app1.cells()[0].agent_pubkey().clone())
        .unwrap();
    let db2 = conductor
        .spaces
        .get_or_create_authored_db(dna.dna_hash(), app2.cells()[0].agent_pubkey().clone())
        .unwrap();

    // - Check that both authored database files exist
    std::fs::File::open(db1.path()).unwrap();
    std::fs::File::open(db2.path()).unwrap();

    // - Uninstall the first app
    conductor
        .raw_handle()
        .uninstall_app(&"app1".to_string(), false)
        .await
        .unwrap();

    // - Check that the first authored DB file is deleted since the cell was removed.
    #[cfg(not(windows))]
    std::fs::File::open(db1.path()).unwrap_err();
    std::fs::File::open(db2.path()).unwrap();

    // - Ensure that the remaining app can still access both hashes
    assert!(conductor
        .call::<_, Option<Record>>(&app2.cells()[0].zome("coordinator"), "read", hash1.clone())
        .await
        .is_some());
    assert!(conductor
        .call::<_, Option<Record>>(&app2.cells()[0].zome("coordinator"), "read", hash2.clone())
        .await
        .is_some());

    // - Uninstall the remaining app
    conductor
        .raw_handle()
        .uninstall_app(&"app2".to_string(), false)
        .await
        .unwrap();

    // - Check that second authored DB file is deleted since the cell was removed.
    #[cfg(not(windows))]
    std::fs::File::open(db2.path()).unwrap_err();

    // - Ensure that the apps are removed
    assert_eq_retry_10s!(
        {
            let state = conductor.get_state_from_handle().await.unwrap();
            (state.running_apps().count(), state.stopped_apps().count())
        },
        (0, 0)
    );

    // - A new app can't read any of the data from the previous two, because once the last instance
    //   of the cells was destroyed, all data was destroyed as well.
    let app3 = conductor.setup_app("app2", [&dna]).await.unwrap();
    assert!(conductor
        .call::<_, Option<Record>>(&app3.cells()[0].zome("coordinator"), "read", hash1.clone())
        .await
        .is_none());
    assert!(conductor
        .call::<_, Option<Record>>(&app3.cells()[0].zome("coordinator"), "read", hash2.clone())
        .await
        .is_none());
}

#[tokio::test(flavor = "multi_thread")]
async fn test_reconciliation_idempotency() {
    holochain_trace::test_run();
    let zome = InlineIntegrityZome::new_unique(Vec::new(), 0);
    let mut conductor = SweetConductor::from_standard_config().await;
    common_genesis_test_app(&mut conductor, ("custom", zome))
        .await
        .unwrap();

    conductor
        .raw_handle()
        .reconcile_cell_status_with_app_status()
        .await
        .unwrap();
    conductor
        .raw_handle()
        .reconcile_cell_status_with_app_status()
        .await
        .unwrap();

    // - Ensure that the app is active
    assert_eq_retry_10s!(conductor.list_running_apps().await.unwrap().len(), 1);
}

#[tokio::test(flavor = "multi_thread")]
async fn test_signing_error_during_genesis() {
    holochain_trace::test_run();
    let bad_keystore = spawn_crude_mock_keystore(|| "spawn_crude_mock_keystore error".into()).await;

    let db_dir = test_db_dir();
    let config = ConductorConfig {
        data_root_path: Some(db_dir.path().to_path_buf().into()),
        dpki: DpkiConfig::disabled(),
        device_seed_lair_tag: Some("nonexistent-tag".to_string()),
        ..Default::default()
    };
    let mut conductor = SweetConductor::new(
        SweetConductor::handle_from_existing(bad_keystore, &config, &[]).await,
        db_dir.into(),
        config.into(),
        None,
    )
    .await;

    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Sign]).await;

    let result = conductor
        .setup_app_for_agents("app", &[fixt!(AgentPubKey)], [&dna])
        .await;

    // - Assert that we got an error during Genesis. However, this test is
    //   pretty useless. What we really want is to ensure that the system is
    //   resilient when this type of error comes up in a real setting.
    let err = if let Err(err) = result {
        err
    } else {
        panic!("this should have been an error")
    };

    if let ConductorApiError::ConductorError(inner) = err {
        assert_matches!(inner, ConductorError::GenesisFailed { errors } if errors.len() == 1);
    } else {
        panic!("this should have been an error too");
    }
}

// async fn make_signing_call(
//     conductor: &SweetConductor,
//     client: &mut WebsocketSender,
//     keystore_control: &MockLairControl,
//     cell: &SweetCell,
// ) -> AppResponse {
//     let reinstate_mock = keystore_control.using_mock();
//     if reinstate_mock {
//         keystore_control.use_real();
//     }
//     let (nonce, expires_at) = fresh_nonce(Timestamp::now()).unwrap();
//     let request = AppRequest::CallZome(Box::new(
//         ZomeCall::try_from_unsigned_zome_call(
//             conductor.raw_handle().keystore(),
//             ZomeCallUnsigned {
//                 cell_id: cell.cell_id().clone(),
//                 zome_name: "sign".into(),
//                 fn_name: "sign_ephemeral".into(),
//                 payload: ExternIO::encode(()).unwrap(),
//                 cap_secret: None,
//                 provenance: cell.agent_pubkey().clone(),
//                 nonce,
//                 expires_at,
//             },
//         )
//         .await
//         .unwrap(),
//     ));
//     if reinstate_mock {
//         keystore_control.use_mock();
//     }
//     client.request(request).await.unwrap()
// }

// A test which simulates Keystore errors with a test keystore which is designed
// to fail.
//
// This test was written making the assumption that we could swap out the
// MetaLairClient for each Cell at runtime, but given our current concurrency
// model which puts each Cell in an Arc, this is not possible.
// In order to implement this test, we should probably have the "crude mock
// keystore" listen on a channel which toggles its behavior from always-correct
// to always-failing. However, the problem that this test is testing for does
// not seem to be an issue, therefore I'm not putting the effort into fixing it
// right now.
// @todo fix test by using new InstallApp call
// #[tokio::test(flavor = "multi_thread")]
// async fn test_signing_error_during_genesis_doesnt_bork_interfaces() {
//     holochain_trace::test_run();
//     let (keystore, keystore_control) = spawn_real_or_mock_keystore(|_| Err("test error".into()))
//         .await
//         .unwrap();

//     let db_dir = test_db_dir();
//     let config = standard_config();
//     let mut conductor = SweetConductor::new(
//         SweetConductor::handle_from_existing(db_dir.path(), keystore.clone(), &config, &[]).await,
//         db_dir.into(),
//         config,
//     )
//     .await;

//     let (agent1, agent2, agent3) = SweetAgents::three(keystore.clone()).await;

//     let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Sign]).await;

//     let app1 = conductor
//         .setup_app_for_agent("app1", agent1.clone(), &[dna.clone()])
//         .await
//         .unwrap();

//     let app2 = conductor
//         .setup_app_for_agent("app2", agent2.clone(), &[dna.clone()])
//         .await
//         .unwrap();

//     let (cell1,) = app1.into_tuple();
//     let (cell2,) = app2.into_tuple();

//     let app_port = conductor
//         .raw_handle()
//         .add_app_interface(either::Either::Left(0))
//         .await
//         .unwrap();
//     let (mut app_client, _) = websocket_client_by_port(app_port).await.unwrap();
//     let (mut admin_client, _) = conductor.admin_ws_client().await;

//     // Now use the bad keystore to cause a signing error on the next zome call
//     keystore_control.use_mock();

//     let response: AdminResponse = admin_client
//         .request(AdminRequest::InstallApp(Box::new(InstallAppPayload {
//             installed_app_id: "app3".into(),
//             agent_key: agent3.clone(),
//             dnas: vec![InstallAppDnaPayload {
//                 role_name: "whatever".into(),
//                 hash: dna.dna_hash().clone(),
//                 membrane_proof: None,
//             }],
//         })))
//         .await
//         .unwrap();

// assert_matches!(response, AdminResponse::Error(_));
// let response = make_signing_call(&conductor, &mut app_client, &keystore_control, &cell2).await;

//     assert_matches!(response, AppResponse::Error(_));

//     // Go back to the good keystore, see if we can proceed
//     keystore_control.use_real();

// let response = make_signing_call(&conductor, &mut app_client, &keystore_control, &cell2).await;
// assert_matches!(response, AppResponse::ZomeCall(_));

// let response = make_signing_call(&conductor, &mut app_client, &keystore_control, &cell1).await;
// assert_matches!(response, AppResponse::ZomeCall(_));
// }

pub(crate) fn simple_create_entry_zome() -> InlineIntegrityZome {
    let unit_entry_def = EntryDef::default_from_id("unit");
    InlineIntegrityZome::new_unique(vec![unit_entry_def.clone()], 0)
        .function("create", move |api, ()| {
            let entry = Entry::app(().try_into().unwrap()).unwrap();
            let hash = api.create(CreateInput::new(
                InlineZomeSet::get_entry_location(&api, EntryDefIndex(0)),
                EntryVisibility::Public,
                entry,
                ChainTopOrdering::default(),
            ))?;
            Ok(hash)
        })
        .function("get", move |api, hash: ActionHash| {
            let record = api
                .get(vec![GetInput::new(hash.into(), Default::default())])?
                .pop()
                .unwrap();

            Ok(record)
        })
}

#[tokio::test(flavor = "multi_thread")]
async fn test_enable_disable_enable_app() {
    holochain_trace::test_run();
    let zome = simple_create_entry_zome();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = common_genesis_test_app(&mut conductor, ("zome", zome))
        .await
        .unwrap();

    let all_apps = conductor.list_apps(None).await.unwrap();
    assert_eq!(all_apps.len(), 1);

    let inactive_apps = conductor
        .list_apps(Some(AppStatusFilter::Disabled))
        .await
        .unwrap();
    let active_apps = conductor
        .list_apps(Some(AppStatusFilter::Enabled))
        .await
        .unwrap();
    assert_eq!(inactive_apps.len(), 0);
    assert_eq!(active_apps.len(), 1);
    assert_eq!(active_apps[0].cell_info.len(), 2);
    assert_matches!(active_apps[0].status, AppInfoStatus::Running);

    let (_, cell) = app.into_tuple();

    let hash: ActionHash = conductor
        .call_fallible(&cell.zome("zome"), "create", ())
        .await
        .unwrap();

    conductor
        .disable_app("app".to_string(), DisabledAppReason::User)
        .await
        .unwrap();

    let inactive_apps = conductor
        .list_apps(Some(AppStatusFilter::Disabled))
        .await
        .unwrap();
    let active_apps = conductor
        .list_apps(Some(AppStatusFilter::Enabled))
        .await
        .unwrap();
    assert_eq!(active_apps.len(), 0);
    assert_eq!(inactive_apps.len(), 1);
    assert_eq!(inactive_apps[0].cell_info.len(), 2);
    assert_matches!(
        inactive_apps[0].status,
        AppInfoStatus::Disabled {
            reason: DisabledAppReason::User
        }
    );

    // - We can't make a zome call while disabled
    assert!(conductor
        .call_fallible::<_, Option<Record>>(&cell.zome("zome"), "get", hash.clone())
        .await
        .is_err());

    conductor.enable_app("app".to_string()).await.unwrap();

    // - We can still make a zome call after reactivation
    assert!(conductor
        .call_fallible::<_, Option<Record>>(&cell.zome("zome"), "get", hash.clone())
        .await
        .is_ok());

    // - Ensure that the app is active

    assert_eq_retry_10s!(conductor.list_running_apps().await.unwrap().len(), 1);
    let inactive_apps = conductor
        .list_apps(Some(AppStatusFilter::Disabled))
        .await
        .unwrap();
    let active_apps = conductor
        .list_apps(Some(AppStatusFilter::Enabled))
        .await
        .unwrap();
    assert_eq!(active_apps.len(), 1);
    assert_eq!(inactive_apps.len(), 0);
}

#[tokio::test(flavor = "multi_thread")]
async fn test_enable_disable_enable_clone_cell() {
    holochain_trace::test_run();
    let zome = simple_create_entry_zome();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = common_genesis_test_app(&mut conductor, ("zome", zome))
        .await
        .unwrap();
    let app_id = app.installed_app_id().clone();

    let (clone, role_name) = {
        let (_, cell) = app.into_tuple();
        let role_name = cell.cell_id().dna_hash().to_string();

        let clone = conductor
            .create_clone_cell(
                &app_id,
                CreateCloneCellPayload {
                    role_name: role_name.clone(),
                    modifiers: DnaModifiersOpt::default().with_network_seed("new seed".into()),
                    membrane_proof: None,
                    name: None,
                },
            )
            .await
            .unwrap();

        (clone, role_name)
    };
    let zome = SweetZome::new(clone.cell_id.clone(), "zome".into());
    let hash: ActionHash = conductor.call(&zome, "create", ()).await;

    let clone_cell_id = CloneCellId::CloneId(clone.clone_id);
    conductor
        .disable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_cell_id.clone(),
            },
        )
        .await
        .unwrap();

    // - should not be able to call a zome fn on a disabled clone cell
    let result: Result<Option<Record>, _> =
        conductor.call_fallible(&zome, "get", hash.clone()).await;

    assert!(matches!(
        result,
        Err(ConductorApiError::ConductorError(
            ConductorError::CellDisabled(_)
        ))
    ));

    conductor.shutdown().await;
    conductor.startup().await;

    {
        // - cell should still be disabled after restart
        let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
        let cell = unwrap_cell_info_clone(app_info.cell_info.get(&role_name).unwrap()[1].clone());
        assert!(!cell.enabled);
    }
    {
        // - should *still* not be able to call a zome fn on a disabled clone cell after restart
        let result: Result<Option<Record>, _> =
            conductor.call_fallible(&zome, "get", hash.clone()).await;
        assert!(matches!(
            result,
            Err(ConductorApiError::ConductorError(
                ConductorError::CellDisabled(_)
            ))
        ));
    }

    conductor
        .raw_handle()
        .enable_clone_cell(&app_id, &EnableCloneCellPayload { clone_cell_id })
        .await
        .unwrap();

    {
        // - cell should still be enabled now
        let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
        let cell = unwrap_cell_info_clone(app_info.cell_info.get(&role_name).unwrap()[1].clone());
        assert!(cell.enabled);
    }
    {
        // - can call clone again
        let _: Option<Record> = conductor
            .call_fallible(&zome, "get", hash)
            .await
            .expect("can call zome fn now");
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn name_has_no_effect_on_dna_hash() {
    holochain_trace::test_run();
    let mut conductor = SweetConductor::from_standard_config().await;
    let dna = SweetDnaFile::unique_empty().await;
    let apps = conductor.setup_apps("app", 3, [&dna]).await.unwrap();
    let app_id1 = apps[0].installed_app_id().clone();
    let app_id2 = apps[1].installed_app_id().clone();
    let app_id3 = apps[2].installed_app_id().clone();
    let ((cell1,), (cell2,), (cell3,)) = apps.into_tuples();
    let role_name1 = cell1.cell_id().dna_hash().to_string();
    let role_name2 = cell2.cell_id().dna_hash().to_string();
    let role_name3 = cell3.cell_id().dna_hash().to_string();

    let clone1 = conductor
        .create_clone_cell(
            &app_id1,
            CreateCloneCellPayload {
                role_name: role_name1.clone(),
                modifiers: DnaModifiersOpt::default().with_network_seed("new seed".into()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();

    let clone2 = conductor
        .create_clone_cell(
            &app_id2,
            CreateCloneCellPayload {
                role_name: role_name2.clone(),
                modifiers: DnaModifiersOpt::default().with_network_seed("new seed".into()),
                membrane_proof: None,
                name: Some("Rumpelstiltskin".to_string()),
            },
        )
        .await
        .unwrap();

    let clone3 = conductor
        .create_clone_cell(
            &app_id3,
            CreateCloneCellPayload {
                role_name: role_name3.clone(),
                modifiers: DnaModifiersOpt::default().with_network_seed("new seed".into()),
                membrane_proof: None,
                name: Some("Chara".to_string()),
            },
        )
        .await
        .unwrap();

    assert_eq!(clone1.cell_id.dna_hash(), clone2.cell_id.dna_hash());
    assert_eq!(clone2.cell_id.dna_hash(), clone3.cell_id.dna_hash());
}

fn unwrap_cell_info_clone(cell_info: CellInfo) -> holochain_zome_types::clone::ClonedCell {
    match cell_info {
        CellInfo::Cloned(cell) => cell,
        _ => panic!("wrong cell type: {:?}", cell_info),
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn test_installation_fails_if_genesis_self_check_is_invalid() {
    holochain_trace::test_run();
    let bad_zome = InlineZomeSet::new_unique_single("integrity", "custom", Vec::new(), 0).function(
        "integrity",
        "genesis_self_check",
        |_api, _data: GenesisSelfCheckData| {
            Ok(GenesisSelfCheckResult::Invalid(
                "intentional invalid result for testing".into(),
            ))
        },
    );

    let mut conductor = SweetConductor::from_standard_config().await;
    let err = if let Err(err) = common_genesis_test_app(&mut conductor, bad_zome).await {
        err
    } else {
        panic!("this should have been an error")
    };

    if let ConductorApiError::ConductorError(inner) = err {
        assert_matches!(inner, ConductorError::GenesisFailed { errors } if errors.len() == 1);
    } else {
        panic!("this should have been an error too");
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn test_bad_entry_validation_after_genesis_returns_zome_call_error() {
    holochain_trace::test_run();
    let unit_entry_def = EntryDef::default_from_id("unit");
    let bad_zome =
        InlineZomeSet::new_unique_single("integrity", "custom", vec![unit_entry_def.clone()], 0)
            .function("integrity", "validate", |_api, op: Op| match op {
                Op::StoreEntry(StoreEntry { action, .. })
                    if action.hashed.content.app_entry_def().is_some() =>
                {
                    Ok(ValidateResult::Invalid(
                        "intentional invalid result for testing".into(),
                    ))
                }
                _ => Ok(ValidateResult::Valid),
            })
            .function("custom", "create", move |api, ()| {
                let entry = Entry::app(().try_into().unwrap()).unwrap();
                let hash = api.create(CreateInput::new(
                    InlineZomeSet::get_entry_location(&api, EntryDefIndex(0)),
                    EntryVisibility::Public,
                    entry,
                    ChainTopOrdering::default(),
                ))?;
                Ok(hash)
            });

    let mut conductor = SweetConductorConfig::standard()
        .no_dpki()
        .build_conductor()
        .await;
    let app = common_genesis_test_app(&mut conductor, bad_zome)
        .await
        .unwrap();

    let (_, cell_bad) = app.into_tuple();

    let result: ConductorApiResult<ActionHash> = conductor
        .call_fallible(&cell_bad.zome("custom"), "create", ())
        .await;

    // - The failed validation simply causes the zome call to return an error
    assert_matches!(result, Err(_));

    // - The app is not disabled
    assert_eq_retry_10s!(
        {
            let state = conductor.get_state_from_handle().await.unwrap();
            (state.running_apps().count(), state.stopped_apps().count())
        },
        (1, 0)
    );
}

// NB: currently the pre-genesis and post-genesis handling of panics is the same.
//   If we implement [ B-04188 ], then this test will be made more possible.
//   Otherwise, we have to devise a way to discover whether a panic happened
//   during genesis or not.
// NOTE: we need a test with a failure during a validation callback that happens
//       *inline*. It's not enough to have a failing validate for
//       instance, because that failure will be returned by the zome call.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "need to figure out how to write this test, i.e. to make genesis panic"]
async fn test_apps_disable_on_panic_after_genesis() {
    holochain_trace::test_run();
    let unit_entry_def = EntryDef::default_from_id("unit");
    let bad_zome =
        InlineZomeSet::new_unique_single("integrity", "custom", vec![unit_entry_def.clone()], 0)
            // We need a different validation callback that doesn't happen inline
            // so we can cause failure in it. But it must also be after genesis.
            .function("integrity", "validate", |_api, op: Op| {
                match op {
                    Op::StoreEntry(StoreEntry { action, .. })
                        if action.hashed.content.app_entry_def().is_some() =>
                    {
                        // Trigger a deserialization error
                        let _: Entry = SerializedBytes::try_from(())?.try_into()?;
                        Ok(ValidateResult::Valid)
                    }
                    _ => Ok(ValidateResult::Valid),
                }
            })
            .function("custom", "create", move |api, ()| {
                let entry = Entry::app(().try_into().unwrap()).unwrap();
                let hash = api.create(CreateInput::new(
                    InlineZomeSet::get_entry_location(&api, EntryDefIndex(0)),
                    EntryVisibility::Public,
                    entry,
                    ChainTopOrdering::default(),
                ))?;
                Ok(hash)
            });

    let mut conductor = SweetConductor::from_standard_config().await;
    let app = common_genesis_test_app(&mut conductor, bad_zome)
        .await
        .unwrap();

    let (_, cell_bad) = app.into_tuple();

    let _: ConductorApiResult<ActionHash> = conductor
        .call_fallible(&cell_bad.zome("custom"), "create", ())
        .await;

    assert_eq_retry_10s!(
        {
            let state = conductor.get_state_from_handle().await.unwrap();
            (state.running_apps().count(), state.stopped_apps().count())
        },
        (0, 1)
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn test_app_status_states() {
    holochain_trace::test_run();
    let zome = simple_create_entry_zome();
    let mut conductor = SweetConductor::from_standard_config().await;
    common_genesis_test_app(&mut conductor, ("zome", zome))
        .await
        .unwrap();

    let all_apps = conductor.list_apps(None).await.unwrap();
    assert_eq!(all_apps.len(), 1);

    let get_status = || async { conductor.list_apps(None).await.unwrap()[0].status.clone() };

    // RUNNING -pause-> PAUSED

    conductor
        .pause_app("app".to_string(), PausedAppReason::Error("because".into()))
        .await
        .unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Paused { .. });

    // PAUSED  --start->  RUNNING

    conductor.start_app("app".to_string()).await.unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Running);

    // RUNNING  --disable->  DISABLED

    conductor
        .disable_app("app".to_string(), DisabledAppReason::User)
        .await
        .unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Disabled { .. });

    // DISABLED  --start->  DISABLED

    conductor.start_app("app".to_string()).await.unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Disabled { .. });

    // DISABLED  --pause->  DISABLED

    conductor
        .pause_app("app".to_string(), PausedAppReason::Error("because".into()))
        .await
        .unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Disabled { .. });

    // DISABLED  --enable->  ENABLED

    conductor.enable_app("app".to_string()).await.unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Running);

    // RUNNING  --pause->  PAUSED

    conductor
        .pause_app("app".to_string(), PausedAppReason::Error("because".into()))
        .await
        .unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Paused { .. });

    // PAUSED  --enable->  RUNNING

    conductor.enable_app("app".to_string()).await.unwrap();
    assert_matches!(get_status().await, AppInfoStatus::Running);
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "we don't have the ability to share cells across apps yet, but will need a test for that once we do"]
async fn test_app_status_states_multi_app() {
    todo!("write a test similar to the previous one, testing various state transitions, including switching on and off individual Cells");
}

#[tokio::test(flavor = "multi_thread")]
async fn test_cell_and_app_status_reconciliation() {
    holochain_trace::test_run();
    use AppStatusFx::*;
    use AppStatusKind::*;
    let mk_zome = || ("zome", InlineIntegrityZome::new_unique(Vec::new(), 0));
    let dnas = [
        mk_dna(mk_zome()).await.0,
        mk_dna(mk_zome()).await.0,
        mk_dna(mk_zome()).await.0,
    ];
    let app_id = "app".to_string();
    let config = SweetConductorConfig::standard().no_dpki();
    let mut conductor = SweetConductor::from_config(config).await;
    conductor.setup_app(&app_id, &dnas).await.unwrap();

    let cell_ids: Vec<_> = conductor.running_cell_ids().into_iter().collect();
    let cell1 = &cell_ids[0..1];

    let check = || async {
        (
            AppStatusKind::from(AppStatus::from(
                conductor.list_apps(None).await.unwrap()[0].status.clone(),
            )),
            conductor.running_cell_ids().len(),
        )
    };

    assert_eq!(check().await, (Running, 3));

    // - Simulate a cell being removed due to error
    conductor.remove_cells(cell1).await;
    assert_eq!(check().await, (Running, 2));

    // - Again, app state should be reconciled to Paused due to missing cell
    let delta = conductor
        .reconcile_app_status_with_cell_status(None)
        .await
        .unwrap();
    assert_eq!(delta, SpinDown);
    assert_eq!(check().await, (Paused, 2));

    // - Disabling the app causes all cells to be removed
    conductor
        .disable_app(app_id.clone(), DisabledAppReason::User)
        .await
        .unwrap();
    assert_eq!(check().await, (Disabled, 0));

    // - Starting a disabled app does nothing
    conductor.start_app(app_id.clone()).await.unwrap();
    assert_eq!(check().await, (Disabled, 0));

    // - ...but enabling one does
    conductor.enable_app(app_id).await.unwrap();
    assert_eq!(check().await, (Running, 3));
}

#[tokio::test(flavor = "multi_thread")]
async fn test_app_status_filters() {
    holochain_trace::test_run();
    let zome = InlineIntegrityZome::new_unique(Vec::new(), 0);
    let dnas = [mk_dna(("dna", zome)).await.0];

    let mut conductor = SweetConductor::from_standard_config().await;

    conductor.setup_app("running", &dnas).await.unwrap();
    conductor.setup_app("paused", &dnas).await.unwrap();
    conductor.setup_app("disabled", &dnas).await.unwrap();

    // put apps in the proper states for testing

    conductor
        .pause_app(
            "paused".to_string(),
            PausedAppReason::Error("because".into()),
        )
        .await
        .unwrap();

    conductor
        .disable_app("disabled".to_string(), DisabledAppReason::User)
        .await
        .unwrap();

    macro_rules! list_apps {
        ($filter: expr) => {
            conductor.list_apps($filter).await.unwrap()
        };
    }

    // Check the counts returned by each filter
    use AppStatusFilter::*;

    assert_eq!(list_apps!(None).len(), 3);
    assert_eq!(
        (
            list_apps!(Some(Running)).len(),
            list_apps!(Some(Stopped)).len(),
            list_apps!(Some(Enabled)).len(),
            list_apps!(Some(Disabled)).len(),
            list_apps!(Some(Paused)).len(),
        ),
        (1, 2, 2, 1, 1,)
    );

    // check that paused apps move to Running state on conductor restart

    conductor.shutdown().await;
    conductor.startup().await;

    assert_eq!(list_apps!(None).len(), 3);
    assert_eq!(
        (
            list_apps!(Some(Running)).len(),
            list_apps!(Some(Stopped)).len(),
            list_apps!(Some(Enabled)).len(),
            list_apps!(Some(Disabled)).len(),
            list_apps!(Some(Paused)).len(),
        ),
        (2, 1, 2, 1, 0,)
    );
}

/// Check that the init() callback is only ever called once, even under many
/// concurrent initial zome function calls
#[tokio::test(flavor = "multi_thread")]
async fn test_init_concurrency() {
    holochain_trace::test_run();
    let num_inits = Arc::new(AtomicU32::new(0));
    let num_calls = Arc::new(AtomicU32::new(0));
    let num_inits_clone = num_inits.clone();
    let num_calls_clone = num_calls.clone();

    let zome = InlineZomeSet::new_unique_single("integrity", "zome", vec![], 0)
        .function("zome", "init", move |_, ()| {
            num_inits.clone().fetch_add(1, Ordering::SeqCst);
            Ok(InitCallbackResult::Pass)
        })
        .function("zome", "zomefunc", move |_, ()| {
            std::thread::sleep(std::time::Duration::from_millis(5));
            num_calls.clone().fetch_add(1, Ordering::SeqCst);
            Ok(())
        });
    let dnas = [mk_dna(zome).await.0];
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor.setup_app("app", &dnas).await.unwrap();
    let (cell,) = app.into_tuple();
    let conductor = Arc::new(conductor);

    // Perform 100 concurrent zome calls
    let num_iters = Arc::new(AtomicU32::new(0));
    let call_tasks = (0..100_u32).map(|_i| {
        let conductor = conductor.clone();
        let zome = cell.zome("zome");
        let num_iters = num_iters.clone();
        tokio::spawn(async move {
            num_iters.fetch_add(1, Ordering::SeqCst);
            let _: () = conductor.call(&zome, "zomefunc", ()).await;
        })
    });
    let _ = futures::future::join_all(call_tasks).await;

    assert_eq!(num_iters.fetch_add(0, Ordering::SeqCst), 100);
    assert_eq!(num_calls_clone.fetch_add(0, Ordering::SeqCst), 100);
    assert_eq!(num_inits_clone.fetch_add(0, Ordering::SeqCst), 1);
}

/// Check that an app can be installed with deferred memproof provisioning and:
/// - all status checks return correctly while still provisioned,
/// - no zome calls can be made while awaiting memproofs,
/// - cells can be cloned while awaiting memproofs (even though this is unusual),
/// - conductor can be restarted and app still in AwaitingMemproofs state,
/// - app functions normally after memproofs provided
#[tokio::test(flavor = "multi_thread")]
async fn test_deferred_memproof_provisioning() {
    holochain_trace::test_run();
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Foo]).await;
    let mut conductor = SweetConductor::from_standard_config().await;
    let app_id = "app-id".to_string();
    let role_name = "role".to_string();
    let bundle = app_bundle_from_dnas(&[(role_name.clone(), dna)], true, None).await;
    let bundle_bytes = bundle.encode().unwrap();

    //- Install with deferred memproofs
    let app = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            source: AppBundleSource::Bytes(bundle_bytes),
            agent_key: None,
            installed_app_id: Some(app_id.clone()),
            roles_settings: Default::default(),
            network_seed: None,
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();
    assert_eq!(app.role_assignments().len(), 1);

    let cell_id = app.all_cells().next().unwrap().clone();

    //- Status is AwaitingMemproofs and there is 1 cell assignment
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    assert_eq!(app_info.status, AppInfoStatus::AwaitingMemproofs);
    assert_eq!(app_info.cell_info.len(), 1);

    let cell = conductor.get_sweet_cell(cell_id.clone()).unwrap();

    //- Can't make zome calls, error returned is CellDisabled
    //  (which isn't ideal, but gets the message across well enough)
    let result: Result<String, _> = conductor.call_fallible(&cell.zome("foo"), "foo", ()).await;
    assert_matches!(
        result,
        Err(ConductorApiError::ConductorError(
            ConductorError::CellDisabled(_)
        ))
    );

    conductor.shutdown().await;
    conductor.startup().await;

    //- Status is still AwaitingMemproofs after a restart
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    assert_eq!(app_info.status, AppInfoStatus::AwaitingMemproofs);

    //- Status is still AwaitingMemproofs after enabling but before memproofs
    let r = conductor.enable_app(app_id.clone()).await;
    assert_matches!(r, Err(ConductorError::AppStatusError(_)));
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    assert_eq!(app_info.status, AppInfoStatus::AwaitingMemproofs);

    //- Can not create a clone cell until memproofs have been provided
    let error = conductor
        .create_clone_cell(
            &app_id,
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seeeeed".into()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap_err();
    assert_matches!(
        error,
        ConductorError::SourceChainError(SourceChainError::ChainEmpty)
    );

    //- Rotate app agent key a few times just for the heck of it
    // TODO: not yet implemented

    // let agent1 = conductor.rotate_app_agent_key(&app_id).await.unwrap();
    // let agent2 = conductor.rotate_app_agent_key(&app_id).await.unwrap();
    // let agent3 = conductor.rotate_app_agent_key(&app_id).await.unwrap();
    // assert_ne!(agent_key, agent1);
    // assert_ne!(agent1, agent2);
    // assert_ne!(agent2, agent3);

    //- Now provide the memproofs
    conductor
        .clone()
        .provide_memproofs(&app_id, MemproofMap::new())
        .await
        .unwrap();

    //- Status is now Disabled with the special `NotStartedAfterProvidingMemproofs` reason.
    //    It's not tested in this test, but this status allows the app to be enabled
    //    over the app interface.
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    assert_eq!(
        app_info.status,
        AppInfoStatus::Disabled {
            reason: DisabledAppReason::NotStartedAfterProvidingMemproofs
        }
    );

    conductor.enable_app(app_id.clone()).await.unwrap();

    //- Status is now Running and there is 1 cell assignment
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    assert_eq!(app_info.status, AppInfoStatus::Running);

    //- And now we can make a zome call successfully
    let _: String = conductor.call(&cell.zome("foo"), "foo", ()).await;

    //- And create a clone cell
    conductor
        .create_clone_cell(
            &app_id,
            CreateCloneCellPayload {
                role_name,
                modifiers: DnaModifiersOpt::none().with_network_seed("seeeeed".into()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
}

/// Can uninstall an app with deferred memproofs before providing memproofs
#[tokio::test(flavor = "multi_thread")]
async fn test_deferred_memproof_provisioning_uninstall() {
    holochain_trace::test_run();
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Foo]).await;
    let conductor = SweetConductor::from_standard_config().await;
    let app_id = "app-id".to_string();
    let role_name = "role".to_string();
    let bundle = app_bundle_from_dnas(&[(role_name.clone(), dna)], true, None).await;
    let bundle_bytes = bundle.encode().unwrap();

    //- Install with deferred memproofs
    conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            source: AppBundleSource::Bytes(bundle_bytes),
            agent_key: None,
            installed_app_id: Some(app_id.clone()),
            roles_settings: Default::default(),
            network_seed: None,
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();

    assert_eq!(conductor.list_apps(None).await.unwrap().len(), 1);
    conductor
        .clone()
        .uninstall_app(&app_id, false)
        .await
        .unwrap();
    assert_eq!(conductor.list_apps(None).await.unwrap().len(), 0);
}

#[tokio::test(flavor = "multi_thread")]
async fn test_list_apps_sorted_consistently() {
    holochain_trace::test_run();

    // Create a DNA
    let zome = InlineIntegrityZome::new_unique(Vec::new(), 0);
    let (dna1, _, _) = SweetDnaFile::unique_from_inline_zomes(("zome1", zome)).await;

    // Install two apps on the Conductor:
    // Both share a CellId in common, and also include a distinct CellId each.
    let mut conductor = SweetConductor::from_standard_config().await;
    let _ = conductor.setup_app("app1", [&dna1]).await.unwrap();
    let _ = conductor.setup_app("app2", [&dna1]).await.unwrap();
    let _ = conductor.setup_app("app3", [&dna1]).await.unwrap();

    let list_app_ids = |conductor: ConductorHandle| async move {
        conductor
            .list_apps(None)
            .await
            .unwrap()
            .into_iter()
            .map(|app_info| app_info.installed_app_id)
            .collect::<Vec<String>>()
    };

    // Ensure that ordering is sorted by installed_at descending
    assert_eq!(
        list_app_ids(conductor.clone()).await,
        ["app3".to_string(), "app2".to_string(), "app1".to_string()]
    );

    // Ensure that ordering is consistent every time
    assert_eq!(
        list_app_ids(conductor.clone()).await,
        ["app3".to_string(), "app2".to_string(), "app1".to_string()]
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn test_app_info_cells_sorted_consistently() {
    holochain_trace::test_run();

    // Create a DNA
    let zome = InlineIntegrityZome::new_unique(Vec::new(), 0);
    let (dna1, _, _) = SweetDnaFile::unique_from_inline_zomes(("zome1", zome.clone())).await;
    let (dna2, _, _) = SweetDnaFile::unique_from_inline_zomes(("zome1", zome.clone())).await;
    let (dna3, _, _) = SweetDnaFile::unique_from_inline_zomes(("zome1", zome)).await;

    // Install app on the Conductor:
    let mut conductor = SweetConductor::from_standard_config().await;
    let _ = conductor
        .setup_app(
            "app1",
            [
                &("dna1".to_string(), dna1),
                &("dna2".to_string(), dna2),
                &("dna3".to_string(), dna3),
            ],
        )
        .await
        .unwrap();

    let get_app_info = |conductor: ConductorHandle| async move {
        conductor
            .get_app_info(&"app1".to_string())
            .await
            .expect("Failed to get app info")
            .unwrap()
            .cell_info
    };

    // Ensure that ordering is sorted
    assert_eq!(
        get_app_info(conductor.clone())
            .await
            .keys()
            .collect::<Vec<&String>>(),
        vec![
            &"dna1".to_string(),
            &"dna2".to_string(),
            &"dna3".to_string()
        ]
    );

    // Ensure that ordering is consistent every time
    assert_eq!(
        get_app_info(conductor.clone())
            .await
            .keys()
            .collect::<Vec<&String>>(),
        vec![
            &"dna1".to_string(),
            &"dna2".to_string(),
            &"dna3".to_string()
        ]
    );
}



================================================
File: crates/holochain/src/conductor/conductor/zome_call_signature_verification.rs
================================================
use holo_hash::{sha2_512, AgentPubKey};
use holochain_keystore::AgentPubKeyExt;
use holochain_types::prelude::Signature;

use crate::conductor::api::error::ConductorApiResult;

pub(crate) async fn is_valid_signature(
    provenance: &AgentPubKey,
    bytes: &[u8],
    signature: &Signature,
) -> ConductorApiResult<bool> {
    // Signature is verified against the hash of the signed zome call parameter bytes.
    let bytes_hash = sha2_512(bytes);
    Ok(provenance
        .verify_signature_raw(signature, bytes_hash.into())
        .await?)
}

#[cfg(test)]
mod tests {
    use holo_hash::{sha2_512, AgentPubKey};
    use holochain_keystore::{test_keystore, AgentPubKeyExt};
    use holochain_types::prelude::Signature;

    use super::is_valid_signature;

    #[tokio::test(flavor = "multi_thread")]
    async fn valid_signature() {
        let keystore = test_keystore();
        let agent_key = keystore.new_sign_keypair_random().await.unwrap();
        let bytes = vec![0u8];
        let bytes_hash = sha2_512(&bytes);
        let signature = agent_key
            .sign_raw(&keystore, bytes_hash.into())
            .await
            .unwrap();
        let is_valid = is_valid_signature(&agent_key, &bytes, &signature)
            .await
            .unwrap();
        assert!(is_valid);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn invalid_signature() {
        let keystore = test_keystore();
        let agent_key = keystore.new_sign_keypair_random().await.unwrap();
        let bytes = vec![0u8];
        let signature = Signature::from([0u8; 64]);
        let is_valid = is_valid_signature(&agent_key, &bytes, &signature)
            .await
            .unwrap();
        assert!(!is_valid);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn invalid_provenance() {
        let agent_key = AgentPubKey::from_raw_32(vec![0u8; 32]);
        let bytes = vec![0u8];
        let signature = Signature::from([0u8; 64]);
        let is_valid = is_valid_signature(&agent_key, &bytes, &signature)
            .await
            .unwrap();
        assert!(!is_valid);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn valid_signature_but_different_provenance() {
        let keystore = test_keystore();
        let signer_key = keystore.new_sign_keypair_random().await.unwrap();
        let bytes = vec![0u8];
        let bytes_hash = sha2_512(&bytes);
        let signature = signer_key.sign_raw(&keystore, bytes.into()).await.unwrap();
        let provenance = keystore.new_sign_keypair_random().await.unwrap();
        let is_valid = is_valid_signature(&provenance, &bytes_hash, &signature)
            .await
            .unwrap();
        assert!(!is_valid);
    }
}



================================================
File: crates/holochain/src/conductor/conductor/tests/agent_key_revocation.rs
================================================
use holo_hash::{ActionHash, AgentPubKey, DnaHash, EntryHash};
use holochain_p2p::actor::HolochainP2pRefToDna;
use holochain_state::source_chain::{SourceChain, SourceChainError};
use holochain_types::app::{AppError, CreateCloneCellPayload};
use holochain_types::dna::DnaFile;
use holochain_wasm_test_utils::TestWasm;
use holochain_zome_types::action::ActionType;
use holochain_zome_types::cell::CellId;
use holochain_zome_types::dependencies::holochain_integrity_types::DnaModifiersOpt;
use holochain_zome_types::record::Record;

use matches::assert_matches;
use rusqlite::Row;

use crate::conductor::api::error::ConductorApiError;
use crate::conductor::{conductor::ConductorError, CellError};
use crate::core::workflow::WorkflowError;
use crate::core::ValidationOutcome;
use crate::sweettest::{
    await_consistency, SweetConductor, SweetConductorBatch, SweetConductorConfig, SweetDnaFile,
    SweetZome,
};
use holochain_p2p::HolochainP2pDnaT;
#[cfg(feature = "unstable-dpki")]
use {
    crate::core::SysValidationError,
    holochain_conductor_services::{DpkiServiceError, KeyRevocation, KeyState, RevokeKeyInput},
    holochain_keystore::AgentPubKeyExt,
    holochain_types::deepkey_roundtrip_backward,
    holochain_zome_types::dependencies::holochain_integrity_types::Signature,
    holochain_zome_types::timestamp::Timestamp,
    holochain_zome_types::validate::ValidationStatus,
    rusqlite::named_params,
};

use super::SweetApp;

mod single_conductor {

    use super::*;

    #[cfg(feature = "unstable-dpki")]
    #[tokio::test(flavor = "multi_thread")]
    async fn revoke_agent_key_with_dpki() {
        holochain_trace::test_run();
        let TestCase {
            mut conductor,
            dna_file_1,
            dna_file_2,
            role_1,
            role_2,
            app,
            agent_key,
            cell_id_1,
            cell_id_2,
            zome_1,
            zome_2,
            create_fn_name,
            read_fn_name,
        } = TestCase::dpki().await;

        // No agent key provided, so the installed DPKI service will be used to generate an agent key
        let dpki = conductor
            .running_services()
            .dpki
            .expect("dpki must be running");
        let key_state = dpki
            .state()
            .await
            .key_state(agent_key.clone(), Timestamp::now())
            .await
            .unwrap();
        assert_matches!(key_state, KeyState::Valid(_));

        // Deleting a non-existing key should fail
        let non_existing_key = AgentPubKey::from_raw_32(vec![0; 32]);
        let result = conductor
            .clone()
            .revoke_agent_key_for_app(non_existing_key.clone(), app.installed_app_id().clone())
            .await;
        assert_matches!(
            result,
            Err(ConductorError::DpkiError(DpkiServiceError::DpkiAgentMissing(key))) if key == non_existing_key
        );

        // Writing to cells should succeed
        let action_hash_1: ActionHash = conductor.call(&zome_1, &*create_fn_name, ()).await;
        let action_hash_2: ActionHash = conductor.call(&zome_2, &*create_fn_name, ()).await;

        // Deleting the key should succeed
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Ok(()));
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Ok(()));

        // Key should be in invalid in DPKI
        let key_state = dpki
            .state()
            .await
            .key_state(agent_key.clone(), Timestamp::now())
            .await
            .unwrap();
        assert_matches!(key_state, KeyState::Invalid(_));

        // Last source chain action in both cells should be 'Delete' action of the agent key
        assert_delete_agent_key_present_in_source_chain(
            agent_key.clone(),
            &conductor,
            dna_file_1.dna_hash(),
        );
        assert_delete_agent_key_present_in_source_chain(
            agent_key.clone(),
            &conductor,
            dna_file_2.dna_hash(),
        );

        // Deleting same agent key again should succeed, even though the key is not deleted again.
        // The call itself is successful and should contain errors for the individual cells.
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_1);
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_2);

        // Reading an entry should still succeed
        let result: Option<Record> = conductor.call(&zome_1, &*read_fn_name, action_hash_1).await;
        assert!(result.is_some());
        let result: Option<Record> = conductor.call(&zome_2, &*read_fn_name, action_hash_2).await;
        assert!(result.is_some());

        // Creating an entry should fail now for both cells
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_1, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_dpki_agent_key(error, agent_key.clone());
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_2, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_dpki_agent_key(error, agent_key.clone());

        // Cloning cells should fail for both cells
        let mut create_clone_cell_payload = CreateCloneCellPayload {
            role_name: role_1.to_string(),
            membrane_proof: None,
            modifiers: DnaModifiersOpt::none().with_network_seed("network_seed".into()),
            name: None,
        };
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload.clone())
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::DpkiError(DpkiServiceError::DpkiAgentInvalid(invalid_key, _timestamp)) if invalid_key == agent_key);
        create_clone_cell_payload.role_name = role_2.to_string();
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload)
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::DpkiError(DpkiServiceError::DpkiAgentInvalid(invalid_key, _timestamp)) if invalid_key == agent_key);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn revoke_agent_key_without_dpki() {
        let TestCase {
            mut conductor,
            dna_file_1,
            dna_file_2,
            role_1,
            role_2,
            app,
            agent_key,
            cell_id_1,
            cell_id_2,
            zome_1,
            zome_2,
            create_fn_name,
            read_fn_name,
        } = TestCase::no_dpki().await;

        // Deleting a non-existing key should fail
        let non_existing_key = AgentPubKey::from_raw_32(vec![0; 32]);
        let result = conductor
            .clone()
            .revoke_agent_key_for_app(non_existing_key.clone(), app.installed_app_id().clone())
            .await;
        assert_matches!(
            result,
            Err(ConductorError::AppError(AppError::AgentKeyMissing(key, app_id))) if key == non_existing_key && app_id == *app.installed_app_id()
        );

        // Writing to cells should succeed
        let action_hash_1: ActionHash = conductor.call(&zome_1, &*create_fn_name, ()).await;
        let action_hash_2: ActionHash = conductor.call(&zome_2, &*create_fn_name, ()).await;

        // Deleting the key should succeed
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Ok(()));
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Ok(()));

        // Last source chain action in both cells should be 'Delete' action of the agent key
        assert_delete_agent_key_present_in_source_chain(
            agent_key.clone(),
            &conductor,
            dna_file_1.dna_hash(),
        );
        assert_delete_agent_key_present_in_source_chain(
            agent_key.clone(),
            &conductor,
            dna_file_2.dna_hash(),
        );

        // Deleting same agent key again should succeed, even though the key is not deleted again.
        // The call itself is successful and should contain errors for the individual cells.
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_1);
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_2);

        // Reading an entry should still succeed
        let result: Option<Record> = conductor.call(&zome_1, &*read_fn_name, action_hash_1).await;
        assert!(result.is_some());
        let result: Option<Record> = conductor.call(&zome_2, &*read_fn_name, action_hash_2).await;
        assert!(result.is_some());

        // Creating an entry should fail now for both cells
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_1, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_agent_key_in_source_chain(error, agent_key.clone());
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_2, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_agent_key_in_source_chain(error, agent_key.clone());

        // Cloning cells should fail for both cells
        let mut create_clone_cell_payload = CreateCloneCellPayload {
            role_name: role_1.to_string(),
            membrane_proof: None,
            modifiers: DnaModifiersOpt::none().with_network_seed("network_seed".into()),
            name: None,
        };
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload.clone())
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::SourceChainError(SourceChainError::InvalidAgentKey(invalid_key, cell_id)) if invalid_key == agent_key && cell_id == cell_id_1);
        create_clone_cell_payload.role_name = role_2.to_string();
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload)
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::SourceChainError(SourceChainError::InvalidAgentKey(invalid_key, cell_id)) if invalid_key == agent_key && cell_id == cell_id_2);
    }

    #[cfg(feature = "unstable-dpki")]
    #[tokio::test(flavor = "multi_thread")]
    async fn recover_from_partial_revocation_with_dpki() {
        let TestCase {
            mut conductor,
            role_1,
            role_2,
            app,
            agent_key,
            cell_id_1,
            cell_id_2,
            zome_1,
            zome_2,
            create_fn_name,
            read_fn_name,
            ..
        } = TestCase::dpki().await;

        // Writing to cells should succeed
        let action_hash_1: ActionHash = conductor.call(&zome_1, &*create_fn_name, ()).await;
        let action_hash_2: ActionHash = conductor.call(&zome_2, &*create_fn_name, ()).await;

        // Revoke agent key in Dpki
        revoke_agent_key_in_dpki(&conductor, agent_key.clone()).await;

        // Delete agent key of cell 1 of the app
        let source_chain_1 = SourceChain::new(
            conductor
                .get_or_create_authored_db(cell_id_1.dna_hash(), agent_key.clone())
                .unwrap(),
            conductor.get_dht_db(cell_id_1.dna_hash()).unwrap(),
            conductor.get_dht_db_cache(cell_id_1.dna_hash()).unwrap(),
            conductor.keystore().clone(),
            agent_key.clone(),
        )
        .await
        .unwrap();
        source_chain_1.delete_valid_agent_pub_key().await.unwrap();
        let network = conductor
            .holochain_p2p()
            .to_dna(cell_id_1.dna_hash().clone(), conductor.get_chc(&cell_id_1));
        source_chain_1
            .flush(network.storage_arcs().await.unwrap(), network.chc())
            .await
            .unwrap();

        // Check agent key is invalid in cell 1
        let invalid_agent_key_error = source_chain_1
            .valid_create_agent_key_action()
            .await
            .unwrap_err();
        assert_matches!(invalid_agent_key_error, SourceChainError::InvalidAgentKey(invalid_key, cell_id) if invalid_key == agent_key && cell_id == cell_id_1);

        // Reading an entry should still succeed
        let result: Option<Record> = conductor.call(&zome_1, &*read_fn_name, action_hash_1).await;
        assert!(result.is_some());
        let result: Option<Record> = conductor.call(&zome_2, &*read_fn_name, action_hash_2).await;
        assert!(result.is_some());

        // Creating an entry should fail now for both cells as the key is invalid in Dpki.
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_1, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_dpki_agent_key(error, agent_key.clone());
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_2, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_dpki_agent_key(error, agent_key.clone());

        // Cloning cells should fail for both cells
        let mut create_clone_cell_payload = CreateCloneCellPayload {
            role_name: role_1.to_string(),
            membrane_proof: None,
            modifiers: DnaModifiersOpt::none().with_network_seed("network_seed".into()),
            name: None,
        };
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload.clone())
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::DpkiError(DpkiServiceError::DpkiAgentInvalid(invalid_key, _timestamp)) if invalid_key == agent_key);
        create_clone_cell_payload.role_name = role_2.to_string();
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload)
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::DpkiError(DpkiServiceError::DpkiAgentInvalid(invalid_key, _timestamp)) if invalid_key == agent_key);

        // Calling key revocation should succeed and return an error result for cell 1
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_1);
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Ok(()));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn recover_from_partial_revocation_without_dpki() {
        let TestCase {
            mut conductor,
            role_1,
            role_2,
            app,
            agent_key,
            cell_id_1,
            cell_id_2,
            zome_1,
            zome_2,
            create_fn_name,
            read_fn_name,
            ..
        } = TestCase::no_dpki().await;

        // Writing to cells should succeed
        let action_hash_1: ActionHash = conductor.call(&zome_1, &*create_fn_name, ()).await;
        let action_hash_2: ActionHash = conductor.call(&zome_2, &*create_fn_name, ()).await;

        // Delete agent key of cell 1 of the app
        let source_chain_1 = SourceChain::new(
            conductor
                .get_or_create_authored_db(cell_id_1.dna_hash(), agent_key.clone())
                .unwrap(),
            conductor.get_dht_db(cell_id_1.dna_hash()).unwrap(),
            conductor.get_dht_db_cache(cell_id_1.dna_hash()).unwrap(),
            conductor.keystore().clone(),
            agent_key.clone(),
        )
        .await
        .unwrap();
        source_chain_1.delete_valid_agent_pub_key().await.unwrap();
        let network = conductor
            .holochain_p2p()
            .to_dna(cell_id_1.dna_hash().clone(), conductor.get_chc(&cell_id_1));
        source_chain_1
            .flush(network.storage_arcs().await.unwrap(), network.chc())
            .await
            .unwrap();

        // Check agent key is invalid in cell 1
        let invalid_agent_key_error = source_chain_1
            .valid_create_agent_key_action()
            .await
            .unwrap_err();
        assert_matches!(invalid_agent_key_error, SourceChainError::InvalidAgentKey(invalid_key, cell_id) if invalid_key == agent_key && cell_id == cell_id_1);

        // Reading an entry should still succeed
        let result: Option<Record> = conductor.call(&zome_1, &*read_fn_name, action_hash_1).await;
        assert!(result.is_some());
        let result: Option<Record> = conductor.call(&zome_2, &*read_fn_name, action_hash_2).await;
        assert!(result.is_some());

        // Creating an entry should fail for cell 1 as the agent key is invalid.
        let error = conductor
            .call_fallible::<_, ActionHash>(&zome_1, &*create_fn_name, ())
            .await
            .unwrap_err();
        assert_error_due_to_invalid_agent_key_in_source_chain(error, agent_key.clone());
        // Creating an entry should succeed for cell 2 as the agent key is still valid.
        let _ = conductor
            .call_fallible::<_, ActionHash>(&zome_2, &*create_fn_name, ())
            .await
            .unwrap();

        // Cloning cells should fail for cell 1 as the agent key is invalid.
        let mut create_clone_cell_payload = CreateCloneCellPayload {
            role_name: role_1.to_string(),
            membrane_proof: None,
            modifiers: DnaModifiersOpt::none().with_network_seed("network_seed".into()),
            name: None,
        };
        let result = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload.clone())
            .await
            .unwrap_err();
        assert_matches!(result, ConductorError::SourceChainError(SourceChainError::InvalidAgentKey(invalid_key, cell_id)) if invalid_key == agent_key && cell_id == cell_id_1);
        // Cloning cells should succeed for cell 2 as the agent key is still valid.
        create_clone_cell_payload.role_name = role_2.to_string();
        let _ = conductor
            .create_clone_cell(app.installed_app_id(), create_clone_cell_payload)
            .await
            .unwrap();

        // Calling key revocation should succeed and return an error result for cell 1
        let revocation_result_per_cell = conductor
            .clone()
            .revoke_agent_key_for_app(agent_key.clone(), app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(&cell_id_1).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(key, cell_id))) if *key == agent_key && *cell_id == cell_id_1);
        assert_matches!(revocation_result_per_cell.get(&cell_id_2).unwrap(), Ok(()));
    }
}

mod multi_conductor {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn revoke_agent_key_without_dpki() {
        holochain_trace::test_run();
        let no_dpki_conductor_config = SweetConductorConfig::rendezvous(true).no_dpki();
        let mut conductors =
            SweetConductorBatch::from_config_rendezvous(2, no_dpki_conductor_config).await;
        let (dna_file, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let role = "role";
        let apps = conductors
            .setup_app("", [&(role.to_string(), dna_file.clone())])
            .await
            .unwrap();
        let cells = apps.cells_flattened();
        let alice = cells[0].agent_pubkey().clone();

        await_consistency(20, &cells).await.unwrap();

        // Deleting the key should succeed
        let revocation_result_per_cell = conductors[0]
            .clone()
            .revoke_agent_key_for_app(alice.clone(), apps[0].installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(
            revocation_result_per_cell.get(cells[0].cell_id()).unwrap(),
            Ok(())
        );

        await_consistency(20, &cells).await.unwrap();
    }

    #[cfg(feature = "unstable-dpki")]
    #[tokio::test(flavor = "multi_thread")]
    async fn revoke_agent_key_with_dpki() {
        holochain_trace::test_run();
        let mut conductors = SweetConductorBatch::from_standard_config_rendezvous(2).await;
        let (dna_file, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let role = "role";
        let mut apps = conductors
            .setup_app("", [&(role.to_string(), dna_file.clone())])
            .await
            .unwrap()
            .into_inner()
            .into_iter();
        let alice_app = apps.next().unwrap();
        let bob_app = apps.next().unwrap();
        let alice = alice_app.agent().clone();
        let bob = bob_app.agent().clone();
        let alice_cell = alice_app.into_cells().into_iter().next().unwrap();
        let bob_cell = bob_app.into_cells().into_iter().next().unwrap();

        // Await Dpki consistency of Alice's and Bob's conductors.
        await_consistency(
            60,
            [
                &conductors[0].dpki_cell().unwrap(),
                &conductors[1].dpki_cell().unwrap(),
            ],
        )
        .await
        .unwrap();

        // Await app cell consistency
        await_consistency(60, [&alice_cell, &bob_cell])
            .await
            .unwrap();

        // Alice's key should be valid on Alice's conductor.
        assert_key_valid_in_dpki(&conductors[0], alice.clone()).await;
        // Bob's key should be valid on Alice's conductor.
        assert_key_valid_in_dpki(&conductors[0], bob.clone()).await;
        // Alice's key should be valid on Bob's conductor.
        assert_key_valid_in_dpki(&conductors[1], alice.clone()).await;
        // Bob's key should be valid on Bob's conductor.
        assert_key_valid_in_dpki(&conductors[1], bob.clone()).await;

        // Revoke Alice's key
        {
            let dpki = conductors[0].running_services().dpki.unwrap();
            let dpki_state = dpki.state().await;
            let key_meta = dpki_state.query_key_meta(alice.clone()).await.unwrap();
            // Sign revocation request
            let signature = dpki
                .cell_id
                .agent_pubkey()
                .sign_raw(
                    &conductors[0].keystore,
                    key_meta.key_registration_addr.get_raw_39().into(),
                )
                .await
                .unwrap();
            let signature = deepkey_roundtrip_backward!(Signature, &signature);
            // Revoke key in DPKI
            let _revocation = dpki_state
                .revoke_key(RevokeKeyInput {
                    key_revocation: KeyRevocation {
                        prior_key_registration: key_meta.key_registration_addr,
                        revocation_authorization: vec![(0, signature)],
                    },
                })
                .await
                .unwrap();

            // Alice's key should be invalid on Alice's conductor.
            let key_state = dpki_state
                .key_state(alice.clone(), Timestamp::now())
                .await
                .unwrap();
            assert_matches!(key_state, KeyState::Invalid(_));
        }

        // Await Dpki consistency of Alice's and Bob's conductors.
        await_consistency(
            30,
            [
                &conductors[0].dpki_cell().unwrap(),
                &conductors[1].dpki_cell().unwrap(),
            ],
        )
        .await
        .unwrap();

        // Alice's key should be invalid on Bob's conductor.
        {
            let dpki = conductors[1].running_services().dpki.unwrap();
            let dpki_state = dpki.state().await;
            let key_state = dpki_state
                .key_state(alice.clone(), Timestamp::now())
                .await
                .unwrap();
            assert_matches!(key_state, KeyState::Invalid(_));
        }

        // Delete Alice's key on the source chain
        let mut alice_source_chain = conductors[0]
            .get_agent_source_chain(&alice, dna_file.dna_hash())
            .await;
        delete_agent_key_from_source_chain(
            &conductors[0],
            &mut alice_source_chain,
            alice_cell.cell_id(),
        )
        .await;

        // Await app cell consistency.
        await_consistency(60, [&alice_cell, &bob_cell])
            .await
            .unwrap();

        // Check Alice's agent key `Delete` has been accepted by Alice's validation.
        assert_delete_agent_key_accepted_by_validation(&alice, &conductors[0], dna_file.dna_hash());
        // Check Alice's agent key `Delete` has been accepted by Bob's validation.
        assert_delete_agent_key_accepted_by_validation(&alice, &conductors[1], dna_file.dna_hash());
    }

    #[cfg(feature = "unstable-dpki")]
    #[tokio::test(flavor = "multi_thread")]
    async fn recover_from_partial_revocation_with_dpki() {
        holochain_trace::test_run();
        let mut conductors = SweetConductorBatch::from_standard_config_rendezvous(2).await;
        let (dna_file_1, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let (dna_file_2, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let role_1 = "role_1".to_string();
        let role_2 = "role_2".to_string();
        let mut apps = conductors
            .setup_app(
                "",
                [
                    &(role_1.to_string(), dna_file_1.clone()),
                    &(role_2.to_string(), dna_file_2.clone()),
                ],
            )
            .await
            .unwrap()
            .into_iter();
        let alice_app = apps.next().unwrap();
        let alice = alice_app.agent().clone();
        let alice_cell_1 = alice_app.cells().first().unwrap();
        let alice_cell_2 = alice_app.cells().get(1).unwrap();
        let bob_app = apps.next().unwrap();
        let bob_cell_1 = bob_app.cells().first().unwrap();
        let bob_cell_2 = bob_app.cells().get(1).unwrap();

        // Await initial DHT sync of Dpki and both cells
        await_consistency(
            20,
            [
                &conductors[0].dpki_cell().unwrap(),
                &conductors[1].dpki_cell().unwrap(),
            ],
        )
        .await
        .unwrap();
        await_consistency(20, [alice_cell_1, bob_cell_1])
            .await
            .unwrap();
        await_consistency(5, [alice_cell_2, bob_cell_2])
            .await
            .unwrap();

        // Revoke agent key in Dpki
        revoke_agent_key_in_dpki(&conductors[0], alice.clone()).await;

        // Await for revocation to reach bob's Dpki
        await_consistency(
            20,
            [
                &conductors[0].dpki_cell().unwrap(),
                &conductors[1].dpki_cell().unwrap(),
            ],
        )
        .await
        .unwrap();

        // Delete agent key of cell 1 of the app and publish and integrate ops
        let mut alice_source_chain_1 = conductors[0]
            .get_agent_source_chain(&alice, dna_file_1.dna_hash())
            .await;
        delete_agent_key_from_source_chain(
            &conductors[0],
            &mut alice_source_chain_1,
            alice_cell_1.cell_id(),
        )
        .await;

        // Check agent key is invalid in cell 1
        let invalid_agent_key_error = alice_source_chain_1
            .valid_create_agent_key_action()
            .await
            .unwrap_err();
        assert_matches!(invalid_agent_key_error, SourceChainError::InvalidAgentKey(invalid_key, cell_id) if invalid_key == alice && cell_id == *alice_cell_1.cell_id());

        // Wait for key deletion on source chain to sync with bob
        await_consistency(20, [alice_cell_1, bob_cell_1])
            .await
            .unwrap();

        // Check Alice's agent key `Delete` in cell 1 has been accepted by Alice's validation.
        assert_delete_agent_key_accepted_by_validation(
            &alice,
            &conductors[0],
            dna_file_1.dna_hash(),
        );
        // Check Alice's agent key `Delete` in cell 1 has been accepted by Bob's validation.
        assert_delete_agent_key_accepted_by_validation(
            &alice,
            &conductors[1],
            dna_file_1.dna_hash(),
        );

        // Calling key revocation should succeed and return an error result for cell 1
        let revocation_result_per_cell = conductors[0]
            .clone()
            .revoke_agent_key_for_app(alice.clone(), alice_app.installed_app_id().clone())
            .await
            .unwrap();
        assert_matches!(revocation_result_per_cell.get(alice_cell_1.cell_id()).unwrap(), Err(ConductorApiError::SourceChainError(SourceChainError::InvalidAgentKey(invalid_key, cell_id))) if *invalid_key == alice && *cell_id == *alice_cell_1.cell_id());

        // Await consistency of cell 2
        await_consistency(20, [alice_cell_2, bob_cell_2])
            .await
            .unwrap();

        // Check Alice's agent key `Delete` in cell 2 has been accepted by Alice's validation.
        assert_delete_agent_key_accepted_by_validation(
            &alice,
            &conductors[0],
            dna_file_2.dna_hash(),
        );
        // Check Alice's agent key `Delete` in cell 2 has been accepted by Bob's validation.
        assert_delete_agent_key_accepted_by_validation(
            &alice,
            &conductors[1],
            dna_file_2.dna_hash(),
        );
    }
}

struct TestCase {
    conductor: SweetConductor,
    dna_file_1: DnaFile,
    dna_file_2: DnaFile,
    role_1: String,
    role_2: String,
    app: SweetApp,
    agent_key: AgentPubKey,
    cell_id_1: CellId,
    cell_id_2: CellId,
    zome_1: SweetZome,
    zome_2: SweetZome,
    create_fn_name: String,
    read_fn_name: String,
}

impl TestCase {
    async fn new(dpki: bool) -> TestCase {
        let conductor_config = if dpki {
            SweetConductorConfig::standard()
        } else {
            SweetConductorConfig::standard().no_dpki()
        };
        let mut conductor = SweetConductor::from_config(conductor_config).await;
        let (dna_file_1, _, coordinator_zomes_1) =
            SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let (dna_file_2, _, coordinator_zomes_2) =
            SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let role_1 = "role_1".to_string();
        let role_2 = "role_2".to_string();
        let app = conductor
            .setup_app(
                "",
                [
                    &(role_1.to_string(), dna_file_1.clone()),
                    &(role_2.to_string(), dna_file_2.clone()),
                ],
            )
            .await
            .unwrap();
        let agent_key = app.agent().clone();
        let cell_id_1 = app.cells()[0].cell_id().clone();
        let cell_id_2 = app.cells()[1].cell_id().clone();
        let zome_1 = SweetZome::new(cell_id_1.clone(), coordinator_zomes_1[0].name.clone());
        let zome_2 = SweetZome::new(cell_id_2.clone(), coordinator_zomes_2[0].name.clone());
        let create_fn_name = "create_entry".to_string();
        let read_fn_name = "get_post".to_string();
        TestCase {
            conductor,
            dna_file_1,
            dna_file_2,
            role_1,
            role_2,
            app,
            agent_key,
            cell_id_1,
            cell_id_2,
            zome_1,
            zome_2,
            create_fn_name,
            read_fn_name,
        }
    }

    #[cfg(feature = "unstable-dpki")]
    async fn dpki() -> TestCase {
        TestCase::new(true).await
    }

    async fn no_dpki() -> TestCase {
        TestCase::new(false).await
    }
}

fn assert_delete_agent_key_present_in_source_chain(
    agent_key: AgentPubKey,
    conductor: &SweetConductor,
    dna_hash: &DnaHash,
) {
    let sql = "\
        SELECT author, type, deletes_entry_hash
        FROM Action
        ORDER BY seq DESC";
    let row_fn = {
        let agent_key = agent_key.clone();
        move |row: &Row| {
            let author = row.get::<_, AgentPubKey>("author").unwrap();
            let action_type = row.get::<_, String>("type").unwrap();
            let deletes_entry_hash = row.get::<_, EntryHash>("deletes_entry_hash").unwrap();
            assert_eq!(author, agent_key);
            assert_eq!(action_type, ActionType::Delete.to_string());
            assert_eq!(deletes_entry_hash, agent_key.clone().into());
            Ok(())
        }
    };
    conductor
        .get_or_create_authored_db(dna_hash, agent_key.clone())
        .unwrap()
        .test_read(move |txn| txn.query_row(sql, [], row_fn).unwrap());
}

#[cfg(feature = "unstable-dpki")]
fn assert_delete_agent_key_accepted_by_validation(
    agent_key: &AgentPubKey,
    conductor: &SweetConductor,
    dna_hash: &DnaHash,
) {
    let sql = "\
        SELECT Action.author, Action.type, Action.deletes_entry_hash
        FROM Action
        JOIN DhtOp On DhtOp.action_hash = Action.hash
        WHERE DhtOp.validation_status = :valid_status
        AND Action.deletes_entry_hash IS NOT NULL
        ORDER BY seq DESC";
    conductor.get_dht_db(dna_hash).unwrap().test_read({
        let agent_key = agent_key.clone();
        move |txn| {
            let mut stmt = txn.prepare(sql).unwrap();
            let rows: Vec<_> = stmt
                .query_map(
                    named_params! { ":valid_status": ValidationStatus::Valid },
                    |row| {
                        let author = row.get::<_, AgentPubKey>("author").unwrap();
                        let action_type = row.get::<_, String>("type").unwrap();
                        let deletes_entry_hash =
                            row.get::<_, EntryHash>("deletes_entry_hash").unwrap();
                        assert_eq!(author, agent_key.clone());
                        assert_eq!(action_type, ActionType::Delete.to_string());
                        assert_eq!(deletes_entry_hash, agent_key.clone().into());
                        Ok(())
                    },
                )
                .unwrap()
                .collect();
            assert!(!rows.is_empty());
        }
    });
}

#[cfg(feature = "unstable-dpki")]
async fn assert_key_valid_in_dpki(conductor: &SweetConductor, agent_key: AgentPubKey) {
    let dpki = conductor.running_services().dpki.unwrap();
    let dpki_state = dpki.state().await;
    let key_state = dpki_state
        .key_state(agent_key, Timestamp::now())
        .await
        .unwrap();
    assert_matches!(key_state, KeyState::Valid(_));
}

#[cfg(feature = "unstable-dpki")]
fn assert_error_due_to_invalid_dpki_agent_key(error: ConductorApiError, agent_key: AgentPubKey) {
    if let ConductorApiError::CellError(CellError::WorkflowError(workflow_error)) = error {
        assert_matches!(
            *workflow_error,
            WorkflowError::SysValidationError(SysValidationError::ValidationOutcome(ValidationOutcome::DpkiAgentInvalid(invalid_key, _timestamp))) if invalid_key == agent_key.clone()
        );
    } else {
        panic!("different error than expected {error}");
    }
}

fn assert_error_due_to_invalid_agent_key_in_source_chain(
    error: ConductorApiError,
    agent_key: AgentPubKey,
) {
    if let ConductorApiError::CellError(CellError::WorkflowError(workflow_error)) = error {
        assert_matches!(
            *workflow_error,
            WorkflowError::SourceChainError(
                SourceChainError::InvalidCommit(message)
            ) if message == ValidationOutcome::InvalidAgentKey(agent_key.clone()).to_string()
        );
    } else {
        panic!("different error than expected {error}");
    }
}

#[cfg(feature = "unstable-dpki")]
async fn revoke_agent_key_in_dpki(conductor: &SweetConductor, agent_key: AgentPubKey) {
    let dpki_service = conductor
        .running_services()
        .dpki
        .expect("dpki must be installed");
    let dpki_state = dpki_service.state().await;
    let timestamp = Timestamp::now();
    match dpki_state
        .key_state(agent_key.clone(), timestamp)
        .await
        .unwrap()
    {
        KeyState::Valid(_) => {
            // Get action hash of key registration
            let key_meta = dpki_state.query_key_meta(agent_key.clone()).await.unwrap();
            // Sign revocation request
            let signature = dpki_service
                .cell_id
                .agent_pubkey()
                .sign_raw(
                    &conductor.keystore(),
                    key_meta.key_registration_addr.get_raw_39().into(),
                )
                .await
                .unwrap();
            let signature = deepkey_roundtrip_backward!(Signature, &signature);
            // Revoke key in DPKI
            let _revocation = dpki_state
                .revoke_key(RevokeKeyInput {
                    key_revocation: KeyRevocation {
                        prior_key_registration: key_meta.key_registration_addr,
                        revocation_authorization: vec![(0, signature)],
                    },
                })
                .await
                .unwrap();
        }
        _state => panic!("key must be valid but is {_state:?}"),
    }
}

#[cfg(feature = "unstable-dpki")]
async fn delete_agent_key_from_source_chain(
    conductor: &SweetConductor,
    source_chain: &mut SourceChain,
    cell_id: &CellId,
) {
    source_chain.delete_valid_agent_pub_key().await.unwrap();
    let network = conductor
        .holochain_p2p()
        .to_dna(cell_id.dna_hash().clone(), conductor.get_chc(cell_id));
    source_chain
        .flush(network.storage_arcs().await.unwrap(), network.chc())
        .await
        .unwrap();
    conductor
        .get_cell_triggers(cell_id)
        .await
        .unwrap()
        .publish_dht_ops
        .trigger(&"key deletion");
    conductor
        .get_cell_triggers(cell_id)
        .await
        .unwrap()
        .integrate_dht_ops
        .trigger(&"key deletion");
}



================================================
File: crates/holochain/src/conductor/conductor/tests/agent_lineage.rs
================================================
use crate::conductor::CellError;
use crate::core::workflow::WorkflowError;
use crate::{
    conductor::api::error::ConductorApiError,
    sweettest::{SweetConductor, SweetConductorConfig, SweetDnaFile},
};
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::{ActionHash, AgentPubKey};
use holochain_state::prelude::SourceChainError;
use holochain_wasm_test_utils::TestWasm;
use matches::assert_matches;

#[cfg(feature = "unstable-functions")]
#[tokio::test(flavor = "multi_thread")]
async fn is_same_agent_without_dpki() {
    let mut conductor =
        SweetConductor::from_config(SweetConductorConfig::standard().no_dpki()).await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let agent_key = app.agent().clone();
    // Test wasm with a function to create an entry that contains two agent keys. The agent keys are
    // checked for `is_same_agent` during validation.
    // Without DPKI installed, the keys are compared for equality.
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // Creating an entry with identical agent keys should succeed.
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (agent_key.clone(), agent_key.clone()),
        )
        .await;
    assert!(response.is_ok());

    // Creating an entry with two non-existing agent keys should succeed too. As there is no DPKI
    // to check for lineage, it just checks if the keys are identical.
    let fake_agent_key = ::fixt::fixt!(AgentPubKey);
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (fake_agent_key.clone(), fake_agent_key.clone()),
        )
        .await;
    assert!(response.is_ok());

    // Creating an entry with two different agent keys should fail.
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (agent_key.clone(), ::fixt::fixt!(AgentPubKey)),
        )
        .await;
    if let Err(ConductorApiError::CellError(CellError::WorkflowError(workflow_error))) = response {
        assert_matches!(
            *workflow_error,
            WorkflowError::SourceChainError(SourceChainError::InvalidCommit(e)) if e.contains("agent key is not of same lineage")
        );
    } else {
        panic!("expected workflow error");
    }
}

#[cfg(all(feature = "unstable-dpki", feature = "unstable-functions"))]
#[tokio::test(flavor = "multi_thread")]
async fn is_same_agent() {
    let mut conductor = SweetConductor::from_standard_config().await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let agent_key = app.agent().clone();
    // Test wasm with a function to create an entry that contains two agent keys. The agent keys are
    // checked for `is_same_agent` during validation.
    // Two keys of the same lineage will let validation pass.
    // Two keys that are not of the same lineage lets validation fail.
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // Creating an entry with the two identical keys should succeed.
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (agent_key.clone(), agent_key.clone()),
        )
        .await;
    assert!(response.is_ok());

    // Creating an entry with the valid agent key and a fake agent key should fail, because the
    // fake key is not of the agent's key lineage.
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (agent_key.clone(), ::fixt::fixt!(AgentPubKey)),
        )
        .await;
    if let Err(ConductorApiError::CellError(CellError::WorkflowError(workflow_error))) = response {
        assert_matches!(*workflow_error, WorkflowError::SourceChainError(_));
    } else {
        panic!("expected workflow error");
    }

    // Creating an entry with a fake agent key twice should fail, because the
    // fake key is not registered in DPKI.
    let fake_agent_key = ::fixt::fixt!(AgentPubKey);
    let response: Result<ActionHash, _> = conductor
        .call_fallible(
            &zome,
            "create_entry_if_keys_of_same_lineage",
            (fake_agent_key.clone(), fake_agent_key.clone()),
        )
        .await;
    if let Err(ConductorApiError::CellError(CellError::WorkflowError(workflow_error))) = response {
        assert_matches!(*workflow_error, WorkflowError::SourceChainError(_));
    } else {
        panic!("expected workflow error");
    }

    // TODO: When adding a function to update an agent key to DPKI service, append to this test
    // a key update and make sure `create_entry_if_keys_of_same_lineage` succeeds for new agent key.
}

#[cfg(feature = "unstable-functions")]
#[tokio::test(flavor = "multi_thread")]
async fn get_agent_key_lineage_during_init_without_dpki() {
    let mut conductor =
        SweetConductor::from_config(SweetConductorConfig::standard().no_dpki()).await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // Call a no op function that will only trigger init. Init gets key lineage and returns `Pass`
    // if successful and otherwise returns an error.
    let _: () = conductor.call(&zome, "no_op_init", ()).await;
}

#[cfg(all(feature = "unstable-dpki", feature = "unstable-functions"))]
#[tokio::test(flavor = "multi_thread")]
async fn get_agent_key_lineage_during_init() {
    let mut conductor = SweetConductor::from_config(SweetConductorConfig::standard()).await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // TODO: Update key first before calling init to make sure that get lineage call used DPKI and
    // returns two keys.

    // Call a no op function that will only trigger init. Init gets key lineage and returns `Pass`
    // if successful and otherwise returns an error.
    let _: () = conductor.call(&zome, "no_op_init", ()).await;
}

#[cfg(feature = "unstable-functions")]
#[tokio::test(flavor = "multi_thread")]
async fn get_agent_key_lineage_without_dpki() {
    let mut conductor =
        SweetConductor::from_config(SweetConductorConfig::standard().no_dpki()).await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let agent_key = app.agent().clone();
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // Without DPKI, the lineage should just be the one agent key.
    let response: Vec<AgentPubKey> = conductor
        .call(&zome, "get_lineage_of_agent_keys", agent_key.clone())
        .await;
    assert_eq!(response, vec![agent_key.clone()]);
}

#[cfg(all(feature = "unstable-dpki", feature = "unstable-functions"))]
#[tokio::test(flavor = "multi_thread")]
async fn get_agent_key_lineage() {
    let mut conductor = SweetConductor::from_config(SweetConductorConfig::standard()).await;
    let dna_file = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::AgentKeyLineage])
        .await
        .0;
    let app = conductor.setup_app("", &[dna_file]).await.unwrap();
    let agent_key = app.agent().clone();
    let zome = app.cells()[0].zome(TestWasm::AgentKeyLineage.coordinator_zome_name());

    // The lineage should just be the one agent key.
    let response: Vec<AgentPubKey> = conductor
        .call(&zome, "get_lineage_of_agent_keys", agent_key.clone())
        .await;
    assert_eq!(response, vec![agent_key.clone()]);

    // TODO: Update key and call get lineage of keys again.
}



================================================
File: crates/holochain/src/conductor/conductor/tests/test_dpki.rs
================================================
use super::*;
use crate::test_utils::inline_zomes::simple_create_read_zome;
use holo_hash::fixt::ActionHashFixturator;
use holochain_types::{deepkey_roundtrip_backward, deepkey_roundtrip_forward};
use parking_lot::Mutex;

/// Instead of reading KeyState from a Deepkey DNA's chain, we store the KeyState
/// of each agent in a hashmap.
type DpkiKeyState = Arc<Mutex<HashMap<AgentPubKey, KeyState>>>;

fn make_mock_dpki_impl(state: DpkiKeyState) -> DpkiImpl {
    let mut dpki = MockDpkiState::new();

    let fake_register_key_output = (
        fixt!(ActionHash),
        KeyRegistration::Create(KeyGeneration {
            new_key: fixt!(AgentPubKey),
            new_key_signing_of_author: fixt!(Signature),
        }),
        KeyMeta {
            app_binding_addr: fixt!(ActionHash),
            key_index: 200,
            key_registration_addr: fixt!(ActionHash),
            key_anchor_addr: fixt!(ActionHash),
            derivation_seed: None,
            derivation_bytes: None,
        },
    );

    dpki.expect_key_state().returning({
        let state = state.clone();
        move |a, _t| {
            let state = state.lock().get(&a).cloned().unwrap_or(KeyState::NotFound);
            async move { Ok(state) }.boxed()
        }
    });

    dpki.expect_next_derivation_details().returning(move |_| {
        let app_index = AtomicU32::new(0);
        async move {
            Ok(DerivationDetails {
                app_index: app_index.fetch_add(1, Ordering::Relaxed),
                key_index: 0,
            })
        }
        .boxed()
    });

    dpki.expect_register_key().returning({
        move |input| {
            let state = state.clone();
            let fake_register_key_output = fake_register_key_output.clone();
            async move {
                let agent = deepkey_roundtrip_forward!(AgentPubKey, &input.key_generation.new_key);
                let action =
                    deepkey_roundtrip_backward!(SignedActionHashed, &fixt!(SignedActionHashed));
                state.lock().insert(agent, KeyState::Valid(action));
                Ok(fake_register_key_output)
            }
            .boxed()
        }
    });

    // All share same DNA, but different agent
    let cell_id = CellId::new(DnaHash::from_raw_32(vec![0; 32]), fixt!(AgentPubKey));

    Arc::new(DpkiService::new(cell_id, dpki))
}

async fn make_dpki_conductor_builder(
    config: ConductorConfig,
    state: DpkiKeyState,
) -> ConductorBuilder {
    let keystore = test_keystore();

    // Generate DPKI device seed
    keystore
        .lair_client()
        .new_seed("MOCK_DEVICE_SEED".to_string().into(), None, false)
        .await
        .unwrap();

    let dpki = make_mock_dpki_impl(state);
    let mut builder = Conductor::builder()
        .with_keystore(keystore)
        .config(config)
        .no_print_setup();
    builder.dpki = Some(dpki);
    builder
}

async fn get_key_state(conductor: &SweetConductor, agent: &AgentPubKey) -> KeyState {
    conductor
        .running_services()
        .dpki
        .as_ref()
        .unwrap()
        .state()
        .await
        .key_state(agent.clone(), Timestamp::now())
        .await
        .unwrap()
}

/// Check that if a node can't validate another agent's DPKI KeyState due to the state
/// not being present yet, it will retry and eventually successfully validate.
///
/// We actually do a consistency check which is expected to panic here, so there is
/// a bunch of panic output from this test.
#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
async fn mock_dpki_validation_limbo() {
    holochain_trace::test_run();

    let states = std::iter::repeat_with(|| Arc::new(Mutex::new(HashMap::new())))
        .take(2)
        .collect::<Vec<_>>();

    let rendezvous = SweetLocalRendezvous::new().await;

    let config: ConductorConfig = SweetConductorConfig::rendezvous(true)
        .apply_rendezvous(&rendezvous)
        .into();

    let mut conductors = SweetConductorBatch::new(vec![
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[0].clone()).await,
            rendezvous.clone(),
        )
        .await,
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[1].clone()).await,
            rendezvous.clone(),
        )
        .await,
    ]);

    let (app_dna_file, _, _) =
        SweetDnaFile::unique_from_inline_zomes(("simple", simple_create_read_zome())).await;

    let ((alice,), (bob,)) = conductors
        .setup_app("app", [&("role".to_string(), app_dna_file)])
        .await
        .unwrap()
        .into_tuples();

    assert!(matches!(
        get_key_state(&conductors[0], alice.agent_pubkey()).await,
        KeyState::Valid(_)
    ));
    assert!(matches!(
        get_key_state(&conductors[0], bob.agent_pubkey()).await,
        KeyState::NotFound
    ));
    assert!(matches!(
        get_key_state(&conductors[1], alice.agent_pubkey()).await,
        KeyState::NotFound
    ));
    assert!(matches!(
        get_key_state(&conductors[1], bob.agent_pubkey()).await,
        KeyState::Valid(_)
    ));

    let hash: ActionHash = conductors[0]
        .call(&alice.zome("simple"), "create", ())
        .await;

    let alice_clone = alice.clone();
    let bob_clone = bob.clone();

    // Assert that we *can't* reach consistency in 3 seconds
    await_consistency(3, [&alice_clone, &bob_clone])
        .await
        .unwrap_err();

    let record_bob: Option<Record> = conductors[1]
        .call(&bob.zome("simple"), "read", hash.clone())
        .await;

    assert!(record_bob.is_none());

    {
        // lock all state mutexes
        let mut ks: Vec<_> = states.iter().map(|s| s.lock()).collect();

        // exchange all key states
        let pairs = ks
            .iter()
            .flat_map(|h| (*h).clone().into_iter())
            .collect::<Vec<_>>();

        ks[0..=1].iter_mut().for_each(|h| {
            h.extend(pairs.clone());
        });
    }

    assert!(matches!(
        get_key_state(&conductors[0], bob.agent_pubkey()).await,
        KeyState::Valid(_)
    ));
    assert!(matches!(
        get_key_state(&conductors[1], alice.agent_pubkey()).await,
        KeyState::Valid(_)
    ));

    await_consistency(10, [&alice, &bob]).await.unwrap();

    let record_alice: Option<Record> = conductors[0]
        .call(&alice.zome("simple"), "read", hash.clone())
        .await;

    let record_bob: Option<Record> = conductors[1]
        .call(&bob.zome("simple"), "read", hash.clone())
        .await;

    assert!(record_alice.is_some());
    assert!(record_bob.is_some());
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "slow_tests")]
async fn mock_dpki_invalid_key_state() {
    holochain_trace::test_run();

    let states = std::iter::repeat_with(|| Arc::new(Mutex::new(HashMap::new())))
        .take(2)
        .collect::<Vec<_>>();

    let rendezvous = SweetLocalRendezvous::new().await;

    let config: ConductorConfig = SweetConductorConfig::rendezvous(true)
        .apply_rendezvous(&rendezvous)
        .into();

    let mut conductors = SweetConductorBatch::new(vec![
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[0].clone()).await,
            rendezvous.clone(),
        )
        .await,
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[1].clone()).await,
            rendezvous.clone(),
        )
        .await,
    ]);

    let (app_dna_file, _, _) =
        SweetDnaFile::unique_from_inline_zomes(("simple", simple_create_read_zome())).await;

    let ((alice,), (bob,)) = conductors
        .setup_app("app", [&("role".to_string(), app_dna_file)])
        .await
        .unwrap()
        .into_tuples();

    {
        let mut s0 = states[0].lock();
        let mut s1 = states[1].lock();

        let a0 = s0.keys().next().unwrap().clone();
        let a1 = s1.keys().next().unwrap().clone();

        // Alice thinks Bob's DPKI key is invalid
        s0.insert(a1, KeyState::Invalid(None));
        // But Bob thinks Alice is valid
        s1.insert(
            a0,
            KeyState::Valid(deepkey_roundtrip_backward!(
                SignedActionHashed,
                &fixt!(SignedActionHashed)
            )),
        );
    }

    let hash: ActionHash = conductors[1].call(&bob.zome("simple"), "create", ()).await;

    let alice_clone = alice.clone();
    let bob_clone = bob.clone();

    // Assert that we *can't* reach consistency in 3 seconds
    tokio::spawn(async move {
        await_consistency(3, [&alice_clone, &bob_clone])
            .await
            .unwrap()
    })
    .await
    .unwrap_err();

    let record_alice: Option<Details> = conductors[0]
        .call(&alice.zome("simple"), "read_details", hash.clone())
        .await;

    assert_matches!(
        record_alice.unwrap(),
        Details::Record(RecordDetails {
            validation_status: ValidationStatus::Rejected,
            ..
        })
    );
}

/// Crude check that an agent without the same DPKI instance as others can't
/// validate actions, due to preflight check failure.
#[tokio::test(flavor = "multi_thread")]
async fn mock_dpki_preflight_check() {
    holochain_trace::test_run();

    let states = std::iter::repeat_with(|| Arc::new(Mutex::new(HashMap::new())))
        .take(2)
        .collect::<Vec<_>>();

    let rendezvous = SweetLocalRendezvous::new().await;

    let config: ConductorConfig = SweetConductorConfig::rendezvous(true)
        .apply_rendezvous(&rendezvous)
        .into();

    let mut conductors = SweetConductorBatch::new(vec![
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[0].clone()).await,
            rendezvous.clone(),
        )
        .await,
        SweetConductor::from_builder_rendezvous(
            make_dpki_conductor_builder(config.clone(), states[1].clone()).await,
            rendezvous.clone(),
        )
        .await,
        SweetConductor::from_config_rendezvous(config, rendezvous.clone()).await,
    ]);

    let (app_dna_file, _, _) =
        SweetDnaFile::unique_from_inline_zomes(("simple", simple_create_read_zome())).await;

    let ((alice,), (bob,), (carol,)) = conductors
        .setup_app("app", [&("role".to_string(), app_dna_file)])
        .await
        .unwrap()
        .into_tuples();

    assert!(matches!(
        get_key_state(&conductors[0], alice.agent_pubkey()).await,
        KeyState::Valid(_)
    ));
    assert!(matches!(
        get_key_state(&conductors[0], bob.agent_pubkey()).await,
        KeyState::NotFound
    ));
    assert!(matches!(
        get_key_state(&conductors[1], alice.agent_pubkey()).await,
        KeyState::NotFound
    ));
    assert!(matches!(
        get_key_state(&conductors[1], bob.agent_pubkey()).await,
        KeyState::Valid(_)
    ));

    {
        // lock all state mutexes
        let mut ks: Vec<_> = states.iter().map(|s| s.lock()).collect();

        // exchange all key states
        let pairs = ks
            .iter()
            .flat_map(|h| (**h).clone().into_iter())
            .collect::<Vec<_>>();

        ks[0..=1].iter_mut().for_each(|h| {
            h.extend(pairs.clone());
        });
    }

    await_consistency(60, [&alice, &bob]).await.unwrap();

    let hash: ActionHash = conductors[0]
        .call(&alice.zome("simple"), "create", ())
        .await;

    await_consistency(60, [&alice, &bob]).await.unwrap();

    assert!(matches!(
        get_key_state(&conductors[0], bob.agent_pubkey()).await,
        KeyState::Valid(_)
    ));
    assert!(matches!(
        get_key_state(&conductors[1], alice.agent_pubkey()).await,
        KeyState::Valid(_)
    ));

    // Carol is nowhere to be found since she never installed DPKI
    assert!(matches!(
        get_key_state(&conductors[0], carol.agent_pubkey()).await,
        KeyState::NotFound
    ));
    assert!(matches!(
        get_key_state(&conductors[1], carol.agent_pubkey()).await,
        KeyState::NotFound
    ));

    let record_bob: Option<Record> = conductors[1]
        .call(&bob.zome("simple"), "read", hash.clone())
        .await;
    let record_carol: Option<Record> = conductors[2]
        .call(&carol.zome("simple"), "read", hash.clone())
        .await;

    assert!(record_bob.is_some());

    // Carol can't get the record. This doesn't necessarily prove that DPKI
    // is working, but it at least demonstrates something basic about validation.
    // A better test would check the *reason* why the record couldn't be fetched.
    assert!(
        record_carol.is_none(),
        "Carol should not be able to communicate with the other two"
    );
}



================================================
File: crates/holochain/src/conductor/entry_def_store/error.rs
================================================
#![allow(missing_docs)]

use crate::core::ribosome::error::RibosomeError;
use holochain_zome_types::zome::ZomeName;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum EntryDefStoreError {
    #[error(transparent)]
    DnaError(#[from] RibosomeError),
    #[error("Unable to retrieve DNA from the RibosomeStore. DnaHash: {0}")]
    DnaFileMissing(holo_hash::DnaHash),
    #[error(
        "Too many entry definitions in a single zome. Entry definitions are limited to 255 per zome"
    )]
    TooManyEntryDefs,
    #[error("The entry def callback for {0} failed because {1}")]
    CallbackFailed(ZomeName, String),
    #[error("Entry type is missing from the zome types map on the Ribosome")]
    EntryTypeMissing,
    #[error(transparent)]
    JoinError(#[from] tokio::task::JoinError),
}

pub type EntryDefStoreResult<T> = Result<T, EntryDefStoreError>;



================================================
File: crates/holochain/src/conductor/interface/error.rs
================================================
use crate::conductor::error::ConductorError;
use holochain_serialized_bytes::SerializedBytesError;
use holochain_types::signal::Signal;

/// Interface Error Type
#[derive(Debug, thiserror::Error)]
pub enum InterfaceError {
    #[error(transparent)]
    SerializedBytes(#[from] SerializedBytesError),
    #[error(transparent)]
    JoinError(#[from] tokio::task::JoinError),
    #[error("Error while sending a Signal to an Interface: {0:?}")]
    SignalSend(tokio::sync::broadcast::error::SendError<Signal>),
    #[error(transparent)]
    SignalReceive(tokio::sync::broadcast::error::TryRecvError),
    #[error(transparent)]
    RequestHandler(Box<ConductorError>),
    #[error("Got an unexpected message: {0}")]
    UnexpectedMessage(String),
    #[error("Failed to send across interface")]
    SendError,
    #[error("Other error: {0}")]
    Other(String),
    #[error("Interface closed")]
    Closed,
    #[error(transparent)]
    IoError(#[from] std::io::Error),
    #[error(transparent)]
    WebsocketError(#[from] holochain_websocket::WebsocketError),
    #[error("Failed to find free port")]
    PortError,
}

impl From<String> for InterfaceError {
    fn from(o: String) -> Self {
        InterfaceError::Other(o)
    }
}

impl From<futures::channel::mpsc::SendError> for InterfaceError {
    fn from(_: futures::channel::mpsc::SendError) -> Self {
        InterfaceError::SendError
    }
}

/// Interface Result Type
pub type InterfaceResult<T> = Result<T, InterfaceError>;



================================================
File: crates/holochain/src/conductor/interface/websocket.rs
================================================
//! Module for establishing Websocket-based Interfaces,
//! i.e. those configured with `InterfaceDriver::Websocket`

use super::error::InterfaceResult;
use crate::conductor::conductor::app_broadcast::AppBroadcast;
use crate::conductor::manager::TaskManagerClient;
use holochain_serialized_bytes::SerializedBytes;
use holochain_types::signal::Signal;
use holochain_websocket::WebsocketConfig;
use holochain_websocket::WebsocketListener;
use holochain_websocket::WebsocketReceiver;
use holochain_websocket::WebsocketSender;
use holochain_websocket::{ReceiveMessage, WebsocketError};
use std::net::{Ipv4Addr, Ipv6Addr, SocketAddrV4, SocketAddrV6};

use crate::conductor::api::{AdminInterfaceApi, AppAuthentication, AppInterfaceApi};
use holochain_conductor_api::{
    AdminRequest, AdminResponse, AppAuthenticationRequest, AppRequest, AppResponse,
};
use holochain_types::app::InstalledAppId;
use holochain_types::websocket::AllowedOrigins;
use std::sync::Arc;
use tokio::pin;
use tokio::sync::broadcast;
use tokio::task::JoinHandle;
use tracing::*;

/// Concurrency count for websocket message processing.
/// This could represent a significant memory investment for
/// e.g. app installations, but we also need enough buffer
/// to accommodate interdependent operations.
const CONCURRENCY_COUNT: usize = 128;

/// The maximum number of connections allowed to the admin interface
pub const MAX_CONNECTIONS: usize = 400;

/// Create a WebsocketListener to be used in interfaces
pub async fn spawn_websocket_listener(
    port: u16,
    allowed_origins: AllowedOrigins,
) -> InterfaceResult<WebsocketListener> {
    trace!("Initializing Admin interface");

    let mut config = WebsocketConfig::LISTENER_DEFAULT;
    config.allowed_origins = Some(allowed_origins);

    let listener = WebsocketListener::dual_bind(
        Arc::new(config),
        SocketAddrV4::new(Ipv4Addr::LOCALHOST, port),
        SocketAddrV6::new(Ipv6Addr::LOCALHOST, port, 0, 0),
    )
    .await?;
    trace!("LISTENING AT: {:?}", listener.local_addrs()?);
    Ok(listener)
}

type TaskListInner = Arc<parking_lot::Mutex<Vec<JoinHandle<()>>>>;

/// Abort tokio tasks on Drop.
#[derive(Default, Clone)]
struct TaskList(pub TaskListInner);
impl Drop for TaskList {
    fn drop(&mut self) {
        debug!("TaskList Dropped!");
        for h in self.0.lock().iter() {
            h.abort();
        }
    }
}

impl TaskList {
    /// Clean up already closed tokio tasks.
    pub fn prune(&mut self) {
        self.0.lock().retain(|h| !h.is_finished());
    }
}

/// Create an Admin Interface, which only receives AdminRequest messages
/// from the external client
pub fn spawn_admin_interface_tasks(
    tm: TaskManagerClient,
    listener: WebsocketListener,
    api: AdminInterfaceApi,
    port: u16,
) {
    tm.add_conductor_task_ignored(&format!("admin interface, port {}", port), move || {
        async move {
            let mut task_list = TaskList::default();
            // establish a new connection to a client
            loop {
                match listener.accept().await {
                    Ok((_, rx_from_iface)) => {
                        task_list.prune();
                        let conn_count = task_list.0.lock().len();
                        if conn_count >= MAX_CONNECTIONS {
                            warn!("Connection limit reached, dropping newly opened connection. num_connections={}", conn_count);
                            // Max connections so drop this connection
                            // which will close it.
                            continue;
                        };
                        debug!("Accepting new connection with number of existing connections {}", conn_count);
                        task_list.0.lock().push(tokio::task::spawn(recv_incoming_admin_msgs(
                            api.clone(),
                            rx_from_iface,
                        )));
                    }
                    Err(err) => {
                        warn!("Admin socket connection failed: {}", err);
                    }
                }
            }
        }
    });
}

/// Create an App Interface, which includes the ability to receive signals
/// from Cells via a broadcast channel
pub async fn spawn_app_interface_task(
    tm: TaskManagerClient,
    port: u16,
    allowed_origins: AllowedOrigins,
    installed_app_id: Option<InstalledAppId>,
    api: AppInterfaceApi,
    app_broadcast: AppBroadcast,
) -> InterfaceResult<u16> {
    trace!("Initializing App interface");

    let mut config = WebsocketConfig::LISTENER_DEFAULT;
    config.allowed_origins = Some(allowed_origins);

    let listener = WebsocketListener::dual_bind(
        Arc::new(config),
        SocketAddrV4::new(Ipv4Addr::LOCALHOST, port),
        SocketAddrV6::new(Ipv6Addr::LOCALHOST, port, 0, 0),
    )
    .await?;
    let addrs = listener.local_addrs()?;
    trace!("LISTENING AT: {:?}", addrs);
    let port = addrs[0].port();

    tm.add_conductor_task_ignored("app interface new connection handler", move || {
        async move {
            let task_list = TaskList::default();
            // establish a new connection to a client
            loop {
                match listener.accept().await {
                    Ok((tx_to_iface, rx_from_iface)) => {
                        authenticate_incoming_app_connection(
                            task_list.0.clone(),
                            api.clone(),
                            rx_from_iface,
                            app_broadcast.clone(),
                            tx_to_iface,
                            installed_app_id.clone(),
                            port,
                        );
                    }
                    Err(err) => {
                        warn!("App socket connection failed: {}", err);
                    }
                }
            }
        }
    });
    Ok(port)
}

/// Polls for messages coming in from the external client.
/// Used by Admin interface.
async fn recv_incoming_admin_msgs(api: AdminInterfaceApi, rx_from_iface: WebsocketReceiver) {
    use futures::stream::StreamExt;

    let rx_from_iface =
        futures::stream::unfold(rx_from_iface, move |mut rx_from_iface| async move {
            loop {
                match rx_from_iface.recv().await {
                    Ok(r) => return Some((r, rx_from_iface)),
                    Err(err) => {
                        match err {
                            WebsocketError::Deserialize(_) => {
                                // No need to log here because `holochain_websocket` logs errors
                                continue;
                            }
                            _ => {
                                info!(?err);
                                return None;
                            }
                        }
                    }
                }
            }
        });

    // TODO - metrics to indicate if we're getting overloaded here.
    rx_from_iface
        .for_each_concurrent(CONCURRENCY_COUNT, move |msg| {
            let api = api.clone();
            async move {
                if let Err(e) = handle_incoming_admin_message(msg, api.clone()).await {
                    error!(error = &e as &dyn std::error::Error)
                }
            }
        })
        .await;

    info!("Admin listener finished");
}

/// Takes an open connection and waits for an authentication message to complete the connection
/// registration.
/// If the connection is not authenticated within 10s or any other content is sent, then the
/// connection is dropped.
/// If the authentication succeeds, then message handling tasks are spawned to handle normal
/// communication with the client.
fn authenticate_incoming_app_connection(
    task_list: TaskListInner,
    api: AppInterfaceApi,
    mut rx_from_iface: WebsocketReceiver,
    app_broadcast: AppBroadcast,
    tx_to_iface: WebsocketSender,
    installed_app_id: Option<InstalledAppId>,
    port: u16,
) {
    let join_handle = tokio::task::spawn({
        let task_list = task_list.clone();
        async move {
            let auth_payload_result = tokio::time::timeout(std::time::Duration::from_secs(10), async {
                if let Ok(msg) = rx_from_iface.recv::<AppRequest>().await {
                    return match msg {
                        ReceiveMessage::Authenticate(auth_payload) => {
                            Ok(auth_payload)
                        }
                        _ => {
                            warn!("Connection to Holochain app port {port} tried to send a message before authenticating. Dropping connection.");
                            Err(())
                        }
                    }
                }

                warn!("Could not receive authentication message, the client either disconnected or sent a message that didn't decode to an authentication request. Dropping connection.");
                Err(())
            }).await;

            match auth_payload_result {
                Err(_) => {
                    warn!("Connection to Holochain app port {port} timed out while awaiting authentication. Dropping connection.");
                }
                Ok(Err(_)) => {
                    // Already logged, continue to drop connection
                }
                Ok(Ok(auth_payload)) => {
                    let payload: AppAuthenticationRequest = match SerializedBytes::from(
                        holochain_serialized_bytes::UnsafeBytes::from(auth_payload),
                    )
                    .try_into()
                    {
                        Ok(payload) => payload,
                        Err(e) => {
                            warn!("Holochain app port {port} received a payload that failed to decode into an authentication payload: {e}. Dropping connection.");
                            return;
                        }
                    };

                    match api
                        .auth(AppAuthentication {
                            token: payload.token,
                            installed_app_id,
                        })
                        .await
                    {
                        Ok(installed_app_id) => {
                            // Once authentication passes we know which app this connection is for,
                            // so we can subscribe to app signals now.
                            let rx_from_cell = app_broadcast.subscribe(installed_app_id.clone());

                            spawn_app_signals_handler(
                                task_list.clone(),
                                rx_from_cell,
                                tx_to_iface.clone(),
                                port,
                                installed_app_id.clone(),
                            );
                            spawn_recv_incoming_app_msgs(
                                task_list,
                                api,
                                rx_from_iface,
                                installed_app_id,
                            );
                        }
                        Err(e) => {
                            warn!("Connection to Holochain app port {port} failed to authenticate: {e}. Dropping connection.");
                        }
                    }
                }
            }
        }
    });

    let mut task_list_lock = task_list.lock();
    task_list_lock.push(join_handle);
}

/// Starts a task that listens for signals coming from apps with `rx_from_cell` and sends them to
/// the connected client via `tx_to_iface`.
fn spawn_app_signals_handler(
    task_list: TaskListInner,
    rx_from_cell: broadcast::Receiver<Signal>,
    tx_to_iface: WebsocketSender,
    port: u16,
    installed_app_id: InstalledAppId,
) {
    use futures::stream::StreamExt;

    let rx_from_cell = futures::stream::unfold(rx_from_cell, move |mut rx_from_cell| {
        let installed_app_id = installed_app_id.clone();
        async move {
            loop {
                match rx_from_cell.recv().await {
                    // We missed some signals, but the channel is still open
                    Err(tokio::sync::broadcast::error::RecvError::Lagged(dropped)) => {
                        warn!("Holochain app port {port} dropped {dropped} signals. The app '{installed_app_id}' is emitting signals too fast.");
                        continue;
                    }
                    Ok(item) => return Some((item, rx_from_cell)),
                    _ => {
                        debug!("SignalChannelClosed");
                        return None;
                    }
                }
            }
        }
    });

    task_list.lock().push(tokio::task::spawn(async move {
        pin!(rx_from_cell);
        loop {
            if let Some(signal) = rx_from_cell.next().await {
                trace!(msg = "Sending signal!", ?signal);
                if let Err(err) = tx_to_iface.signal(signal).await {
                    if let WebsocketError::Close(_) = err {
                        info!(
                            "Client has closed their websocket connection, closing signal handler"
                        );
                    } else {
                        error!(?err, "failed to emit signal, closing emitter");
                    }
                    break;
                }
            } else {
                trace!("No more signals from this cell, closing signal handler");
                break;
            }
        }
    }));
}

/// Starts a task that listens for messages coming from the external client on `rx_from_iface`
/// and calls the provided `api` to handle them. Responses from the `api` are sent back to the
/// client via `tx_to_iface`.
fn spawn_recv_incoming_app_msgs(
    task_list: TaskListInner,
    api: AppInterfaceApi,
    rx_from_iface: WebsocketReceiver,
    installed_app_id: InstalledAppId,
) {
    use futures::stream::StreamExt;

    trace!("CONNECTION: {}", rx_from_iface.peer_addr());

    let rx_from_iface =
        futures::stream::unfold(rx_from_iface, move |mut rx_from_iface| async move {
            loop {
                match rx_from_iface.recv().await {
                    Ok(r) => return Some((r, rx_from_iface)),
                    Err(err) => {
                        match err {
                            WebsocketError::Deserialize(_) => {
                                // No need to log here because `holochain_websocket` logs errors
                                continue;
                            }
                            _ => {
                                info!(?err);
                                return None;
                            }
                        }
                    }
                }
            }
        });

    // TODO - metrics to indicate if we're getting overloaded here.
    task_list
        .lock()
        .push(tokio::task::spawn(rx_from_iface.for_each_concurrent(
            CONCURRENCY_COUNT,
            move |msg| {
                let installed_app_id = installed_app_id.clone();
                let api = api.clone();
                async move {
                    if let Err(err) = handle_incoming_app_message(msg, installed_app_id, api).await
                    {
                        error!(?err, "error handling app websocket message");
                    }
                }
            },
        )));
}

/// Handles messages on admin interfaces
async fn handle_incoming_admin_message(
    ws_msg: ReceiveMessage<AdminRequest>,
    api: AdminInterfaceApi,
) -> InterfaceResult<()> {
    match ws_msg {
        ReceiveMessage::Signal(_) => {
            warn!("Unexpected Signal From client");
            Ok(())
        }
        ReceiveMessage::Authenticate(_) => {
            warn!("Unexpected Authenticate from client on an admin interface");
            Ok(())
        }
        ReceiveMessage::Request(data, respond) => {
            use holochain_serialized_bytes::SerializedBytesError;
            let result: AdminResponse = api.handle_request(Ok(data)).await?;
            // Have to jump through some hoops, because our response type
            // only implements try_into, but the responder needs try_from.
            let result = result.try_into();
            #[derive(Debug)]
            struct Cnv(Result<SerializedBytes, SerializedBytesError>);
            impl std::convert::TryFrom<Cnv> for SerializedBytes {
                type Error = SerializedBytesError;
                fn try_from(b: Cnv) -> Result<SerializedBytes, Self::Error> {
                    b.0
                }
            }
            let result = Cnv(result);
            respond.respond(result).await?;
            Ok(())
        }
    }
}

/// Handles messages on app interfaces
async fn handle_incoming_app_message(
    ws_msg: ReceiveMessage<AppRequest>,
    installed_app_id: InstalledAppId,
    api: AppInterfaceApi,
) -> InterfaceResult<()> {
    match ws_msg {
        ReceiveMessage::Signal(_) => {
            warn!("Unexpected Signal from client");
            Ok(())
        }
        ReceiveMessage::Authenticate(_) => {
            warn!("Unexpected Authenticate from client");
            Ok(())
        }
        ReceiveMessage::Request(data, respond) => {
            use holochain_serialized_bytes::SerializedBytesError;
            let result: AppResponse = api.handle_request(installed_app_id, Ok(data)).await?;
            // Have to jump through some hoops, because our response type
            // only implements try_into, but the responder needs try_from.
            let result = result.try_into();
            #[derive(Debug)]
            struct Cnv(Result<SerializedBytes, SerializedBytesError>);
            impl std::convert::TryFrom<Cnv> for SerializedBytes {
                type Error = SerializedBytesError;
                fn try_from(b: Cnv) -> Result<SerializedBytes, Self::Error> {
                    b.0
                }
            }
            let result = Cnv(result);
            respond.respond(result).await?;
            Ok(())
        }
    }
}

/// Test items needed by other crates
#[cfg(any(test, feature = "test_utils"))]
pub use crate::test_utils::setup_app_in_new_conductor;

#[cfg(test)]
mod test {
    use super::*;
    use crate::conductor::api::error::ExternalApiWireError;
    use crate::conductor::api::AdminInterfaceApi;
    use crate::conductor::api::AdminRequest;
    use crate::conductor::api::AdminResponse;
    use crate::conductor::api::AppInterfaceApi;
    use crate::conductor::conductor::ConductorBuilder;
    use crate::conductor::state::AppInterfaceId;
    use crate::conductor::state::ConductorState;
    use crate::conductor::Conductor;
    use crate::conductor::ConductorHandle;
    use crate::fixt::RealRibosomeFixturator;
    use crate::sweettest::websocket_client_by_port;
    use crate::sweettest::SweetConductorConfig;
    use crate::sweettest::SweetDnaFile;
    use crate::sweettest::WsPollRecv;
    use crate::sweettest::{app_bundle_from_dnas, authenticate_app_ws_client};
    use crate::test_utils::install_app_in_conductor;
    use ::fixt::prelude::*;
    use holo_hash::fixt::AgentPubKeyFixturator;
    use holochain_conductor_api::conductor::ConductorConfig;
    use holochain_conductor_api::conductor::DpkiConfig;
    use holochain_conductor_api::*;
    use holochain_keystore::test_keystore;
    use holochain_p2p::{AgentPubKeyExt, DnaHashExt};
    use holochain_serialized_bytes::prelude::*;
    use holochain_state::prelude::*;
    use holochain_trace;
    use holochain_types::test_utils::fake_agent_pubkey_1;
    use holochain_types::test_utils::fake_dna_zomes;
    use holochain_wasm_test_utils::TestWasm;
    use holochain_wasm_test_utils::TestZomes;
    use holochain_zome_types::test_utils::fake_agent_pubkey_2;
    use kitsune_p2p::agent_store::AgentInfoSigned;
    use kitsune_p2p::dependencies::kitsune_p2p_types::fetch_pool::FetchPoolInfo;
    use kitsune_p2p::{KitsuneAgent, KitsuneSpace};
    use kitsune_p2p_types::fixt::*;
    use matches::assert_matches;
    use pretty_assertions::assert_eq;
    use std::collections::HashSet;
    use tempfile::TempDir;
    use uuid::Uuid;

    async fn test_handle_incoming_admin_message(
        msg: AdminRequest,
        respond: impl FnOnce(AdminResponse) + 'static + Send,
        api: AdminInterfaceApi,
    ) -> InterfaceResult<()> {
        let result: AdminResponse = api.handle_request(Ok(msg)).await?;
        respond(result);
        Ok(())
    }

    async fn test_handle_incoming_app_message(
        installed_app_id: InstalledAppId,
        msg: AppRequest,
        respond: impl FnOnce(AppResponse) + 'static + Send,
        api: AppInterfaceApi,
    ) -> InterfaceResult<()> {
        let result: AppResponse = api.handle_request(installed_app_id, Ok(msg)).await?;
        respond(result);
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn signal_in_post_commit() {
        holochain_trace::test_run();
        let db_dir = test_db_dir();
        let conductor_handle = ConductorBuilder::new()
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();

        let admin_port = conductor_handle
            .clone()
            .add_admin_interfaces(vec![AdminInterfaceConfig {
                driver: InterfaceDriver::Websocket {
                    port: 0,
                    allowed_origins: AllowedOrigins::Any,
                },
            }])
            .await
            .unwrap()[0];

        let (admin_tx, rx) = websocket_client_by_port(admin_port).await.unwrap();
        let _rx = WsPollRecv::new::<AdminResponse>(rx);

        let (dna_file, _, _) =
            SweetDnaFile::unique_from_test_wasms(vec![TestWasm::PostCommitSignal]).await;
        let app_bundle = app_bundle_from_dnas(&[dna_file.clone()], false, None)
            .await
            .encode()
            .expect("failed to encode app bundle as bytes");
        let request = AdminRequest::InstallApp(Box::new(InstallAppPayload {
            source: AppBundleSource::Bytes(app_bundle),
            agent_key: None,
            installed_app_id: None,
            roles_settings: Default::default(),
            network_seed: None,
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        }));
        let response: AdminResponse = admin_tx.request(request).await.unwrap();
        let app_info = match response {
            AdminResponse::AppInstalled(app_info) => app_info,
            _ => panic!("didn't install app"),
        };
        let cell_id = match &app_info
            .cell_info
            .get(&dna_file.dna_hash().to_string())
            .unwrap()[0]
        {
            CellInfo::Provisioned(cell) => cell.cell_id.clone(),
            _ => panic!("emit_signal cell not available"),
        };
        let agent_key = cell_id.agent_pubkey().clone();

        // Activate cells
        let request = AdminRequest::EnableApp {
            installed_app_id: app_info.installed_app_id.clone(),
        };
        let response: AdminResponse = admin_tx.request(request).await.unwrap();
        assert_matches!(response, AdminResponse::AppEnabled { .. });

        // Attach App Interface
        let request = AdminRequest::AttachAppInterface {
            port: None,
            allowed_origins: AllowedOrigins::Any,
            installed_app_id: None,
        };
        let response: AdminResponse = admin_tx.request(request).await.unwrap();
        let app_port = match response {
            AdminResponse::AppInterfaceAttached { port } => port,
            _ => panic!("app interface couldn't be attached"),
        };

        let (app_tx, mut rx) = websocket_client_by_port(app_port).await.unwrap();
        let (s_send, mut s_recv) = tokio::sync::mpsc::unbounded_channel();
        let app_rx_task = tokio::task::spawn(async move {
            while let Ok(ReceiveMessage::Signal(s)) = rx.recv::<AppResponse>().await {
                s_send.send(s).unwrap();
            }
        });
        authenticate_app_ws_client(
            app_tx.clone(),
            conductor_handle
                .get_arbitrary_admin_websocket_port()
                .expect("No admin port on this conductor"),
            app_info.installed_app_id,
        )
        .await;

        // Call Zome
        let (nonce, expires_at) = holochain_nonce::fresh_nonce(Timestamp::now()).unwrap();
        let request = AppRequest::CallZome(Box::new(
            ZomeCallParamsSigned::try_from_params(
                conductor_handle.keystore(),
                ZomeCallParams {
                    provenance: agent_key.clone(),
                    cell_id: cell_id.clone(),
                    zome_name: TestWasm::EmitSignal.coordinator_zome_name(),
                    fn_name: "commit_entry_and_emit_signal_post_commit".into(),
                    cap_secret: None,
                    payload: ExternIO::encode(()).unwrap(),
                    nonce,
                    expires_at,
                },
            )
            .await
            .unwrap(),
        ));
        let _: AppResponse = app_tx.request(request).await.unwrap();

        #[derive(Serialize, Deserialize, SerializedBytes, Debug)]
        #[serde(tag = "type")]
        pub enum TestSignal {
            Tested,
        }

        // ensure that the signal is received and is decodable
        match Signal::try_from_vec(s_recv.recv().await.unwrap()).unwrap() {
            Signal::App { signal, .. } => {
                let expected = AppSignal::new(ExternIO::encode(TestSignal::Tested).unwrap());
                assert_eq!(expected, signal);
            }
            oth => panic!("unexpected: {oth:?}"),
        }

        app_rx_task.abort();
    }

    async fn setup_admin() -> (Arc<TempDir>, ConductorHandle) {
        let db_dir = test_db_dir();
        let conductor_handle = Conductor::builder()
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();
        (Arc::new(db_dir), conductor_handle)
    }

    async fn setup_admin_fake_cells(
        agent: AgentPubKey,
        dnas_with_proofs: Vec<(DnaFile, Option<MembraneProof>)>,
    ) -> (Arc<TempDir>, ConductorHandle) {
        let db_dir = test_db_dir();
        let config = holochain::sweettest::SweetConductorConfig::standard()
            .no_dpki()
            .into();
        let conductor_handle = ConductorBuilder::new()
            .config(config)
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();

        for (dna, _) in dnas_with_proofs.iter() {
            conductor_handle.register_dna(dna.clone()).await.unwrap();
        }

        conductor_handle
            .clone()
            .install_app_minimal("test app".to_string(), Some(agent), &dnas_with_proofs, None)
            .await
            .unwrap();

        (Arc::new(db_dir), conductor_handle)
    }

    async fn activate(conductor_handle: ConductorHandle) -> ConductorHandle {
        conductor_handle
            .clone()
            .enable_app("test app".to_string())
            .await
            .unwrap();

        let errors = conductor_handle
            .clone()
            .reconcile_cell_status_with_app_status()
            .await
            .unwrap();

        assert!(errors.is_empty());

        conductor_handle
    }

    async fn call_zome<R: FnOnce(AppResponse) + 'static + Send>(
        conductor_handle: ConductorHandle,
        cell_id: CellId,
        zome_name: ZomeName,
        function_name: String,
        respond: R,
    ) {
        // Now make sure we can call a zome once again
        let zome_call_params = ZomeCallParams {
            provenance: fixt!(AgentPubKey, Predictable, 0),
            cell_id,
            zome_name,
            fn_name: function_name.into(),
            cap_secret: None,
            payload: ExternIO::encode(()).unwrap(),
            nonce: Nonce256Bits::from(ThirtyTwoBytesFixturator::new(Unpredictable).next().unwrap()),
            expires_at: (Timestamp::now() + std::time::Duration::from_secs(30)).unwrap(),
        };
        let zome_call_signed =
            ZomeCallParamsSigned::try_from_params(&test_keystore(), zome_call_params)
                .await
                .unwrap();

        let msg = AppRequest::CallZome(Box::new(zome_call_signed));
        test_handle_incoming_app_message(
            "".to_string(),
            msg,
            respond,
            AppInterfaceApi::new(conductor_handle.clone()),
        )
        .await
        .unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore]
    #[allow(unreachable_code, unused_variables, clippy::diverging_sub_expression)]
    async fn invalid_request() {
        holochain_trace::test_run();
        let (_tmpdir, conductor_handle) = setup_admin().await;
        let admin_api = AdminInterfaceApi::new(conductor_handle.clone());
        let dna_payload = InstallAppDnaPayload::hash_only(fake_dna_hash(1), "".to_string());
        let agent_key = fake_agent_pubkey_1();
        let payload = todo!("Use new payload struct");
        // let payload = InstallAppPayload {
        //     dnas: vec![dna_payload],
        //     installed_app_id: "test app".to_string(),
        //     agent_key,
        // };
        let msg = AdminRequest::InstallApp(Box::new(payload));
        let respond = |response: AdminResponse| {
            assert_matches!(
                response,
                AdminResponse::Error(ExternalApiWireError::DnaReadError(_))
            );
        };
        test_handle_incoming_admin_message(msg, respond, admin_api)
            .await
            .unwrap();
        conductor_handle.shutdown();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn websocket_call_zome_function() {
        holochain_trace::test_run();
        let uuid = Uuid::new_v4();
        let dna = fake_dna_zomes(
            &uuid.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );

        // warm the zome
        let _ = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![TestWasm::Foo]))
            .next()
            .unwrap();

        let dna_hash = dna.dna_hash().clone();

        let (_tmpdir, _, handle, agent_key) =
            setup_app_in_new_conductor("test app".to_string(), None, vec![(dna, None)]).await;
        let cell_id = CellId::from((dna_hash.clone(), agent_key));

        call_zome(
            handle.clone(),
            cell_id.clone(),
            TestWasm::Foo.coordinator_zome_name(),
            "foo".into(),
            |response: AppResponse| {
                assert_matches!(response, AppResponse::ZomeCalled { .. });
            },
        )
        .await;

        // the time here should be almost the same (about +0.1ms) vs. the raw real_ribosome call
        // the overhead of a websocket request locally is small

        handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn gossip_info_request() {
        holochain_trace::test_run();
        let uuid = Uuid::new_v4();
        let dna = fake_dna_zomes(
            &uuid.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );

        // warm the zome
        let _ = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![TestWasm::Foo]))
            .next()
            .unwrap();

        let dna_hash = dna.dna_hash().clone();

        let (_tmpdir, app_api, handle, agent_pub_key) =
            setup_app_in_new_conductor("test app".to_string(), None, vec![(dna, None)]).await;
        let request = NetworkInfoRequestPayload {
            agent_pub_key,
            dnas: vec![dna_hash],
            last_time_queried: None,
        };

        let msg = AppRequest::NetworkInfo(Box::new(request));
        let respond = |response: AppResponse| match response {
            AppResponse::NetworkInfo(info) => {
                assert_eq!(
                    info,
                    vec![NetworkInfo {
                        fetch_pool_info: FetchPoolInfo::default(),
                        current_number_of_peers: 1,
                        arc_size: 1.0,
                        total_network_peers: 1,
                        bytes_since_last_time_queried: 1838,
                        completed_rounds_since_last_time_queried: 0,
                    }]
                )
            }
            other => panic!("unexpected response {:?}", other),
        };
        test_handle_incoming_app_message("test app".to_string(), msg, respond, app_api)
            .await
            .unwrap();
        // the time here should be almost the same (about +0.1ms) vs. the raw real_ribosome call
        // the overhead of a websocket request locally is small

        handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn storage_info() {
        holochain_trace::test_run();
        let uuid_1 = Uuid::new_v4();
        let dna_1 = fake_dna_zomes(
            &uuid_1.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );
        let uuid_2 = Uuid::new_v4();
        let dna_2 = fake_dna_zomes(
            &uuid_2.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );

        // warm the zome
        let _ = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![TestWasm::Foo]))
            .next()
            .unwrap();

        let cell_id_1 = CellId::from((dna_1.dna_hash().clone(), fake_agent_pubkey_1()));

        let cell_id_2 = CellId::from((dna_2.dna_hash().clone(), fake_agent_pubkey_1()));

        // Run the same DNA in cell 3 to check that grouping works correctly
        let cell_id_3 = CellId::from((dna_2.dna_hash().clone(), fake_agent_pubkey_2()));

        let db_dir = test_db_dir();

        let handle = ConductorBuilder::new()
            .config(ConductorConfig {
                dpki: DpkiConfig::disabled(),
                ..Default::default()
            })
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();

        install_app_in_conductor(
            handle.clone(),
            "test app 1".to_string(),
            Some(cell_id_1.agent_pubkey().clone()),
            &[(dna_1, None)],
        )
        .await;

        install_app_in_conductor(
            handle.clone(),
            "test app 2".to_string(),
            Some(cell_id_2.agent_pubkey().clone()),
            &[(dna_2.clone(), None)],
        )
        .await;

        install_app_in_conductor(
            handle.clone(),
            "test app 3".to_string(),
            Some(cell_id_3.agent_pubkey().clone()),
            &[(dna_2, None)],
        )
        .await;

        let msg = AdminRequest::StorageInfo;
        let respond = move |response: AdminResponse| match response {
            AdminResponse::StorageInfo(info) => {
                assert_eq!(info.blobs.len(), 2);

                let blob_one: &DnaStorageInfo =
                    get_app_data_storage_info(&info, "test app 1".to_string());
                dbg!(&blob_one);

                assert_eq!(blob_one.used_by, vec!["test app 1".to_string()]);
                assert!(blob_one.authored_data_size > 12_000);
                assert!(blob_one.authored_data_size_on_disk > 110_000);
                assert!(blob_one.dht_data_size > 12_000);
                assert!(blob_one.dht_data_size_on_disk > 110_000);
                assert!(blob_one.cache_data_size > 7_000);
                assert!(blob_one.cache_data_size_on_disk > 110_000);

                let blob_two: &DnaStorageInfo =
                    get_app_data_storage_info(&info, "test app 2".to_string());
                dbg!(&blob_two);

                let mut used_by_two = blob_two.used_by.clone();
                used_by_two.sort();
                assert_eq!(
                    used_by_two,
                    vec!["test app 2".to_string(), "test app 3".to_string()]
                );
                assert!(blob_two.authored_data_size > 17_000);
                assert!(blob_two.authored_data_size_on_disk > 110_000);
                assert!(blob_two.dht_data_size > 17_000);
                assert!(blob_two.dht_data_size_on_disk > 110_000);
                assert!(blob_two.cache_data_size > 7_000);
                assert!(blob_two.cache_data_size_on_disk > 110_000);
            }
            other => panic!("unexpected response {:?}", other),
        };
        test_handle_incoming_admin_message(msg, respond, AdminInterfaceApi::new(handle.clone()))
            .await
            .unwrap();

        handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn enable_disable_enable_app() {
        holochain_trace::test_run();
        let agent_key = fake_agent_pubkey_1();
        let mut dnas = Vec::new();
        for _i in 0..2_u32 {
            let integrity_zomes = vec![TestWasm::Link.into()];
            let coordinator_zomes = vec![TestWasm::Link.into()];
            let def = DnaDef::unique_from_zomes(integrity_zomes, coordinator_zomes);
            dnas.push(DnaFile::new(def, Vec::<DnaWasm>::from(TestWasm::Link)).await);
        }
        let dna_hashes = dnas.iter().map(|d| d.dna_hash()).collect::<Vec<_>>();
        let dnas_with_proofs = dnas.iter().cloned().map(|d| (d, None)).collect::<Vec<_>>();
        let cell_id_0 = CellId::new(
            dnas_with_proofs
                .first()
                .cloned()
                .unwrap()
                .0
                .dna_hash()
                .clone(),
            agent_key.clone(),
        );

        let (_tmpdir, conductor_handle) =
            setup_admin_fake_cells(agent_key.clone(), dnas_with_proofs).await;

        let app_id = "test app".to_string();

        // Enable the app
        println!("### ENABLE ###");

        let msg = AdminRequest::EnableApp {
            installed_app_id: app_id.clone(),
        };
        let respond = |response: AdminResponse| {
            assert_matches!(response, AdminResponse::AppEnabled { .. });
        };

        test_handle_incoming_admin_message(
            msg,
            respond,
            AdminInterfaceApi::new(conductor_handle.clone()),
        )
        .await
        .unwrap();

        // Get the state
        let initial_state: ConductorState = conductor_handle.get_state_from_handle().await.unwrap();

        // Now make sure we can call a zome
        println!("### CALL ZOME ###");

        call_zome(
            conductor_handle.clone(),
            cell_id_0.clone(),
            TestWasm::Link.coordinator_zome_name(),
            "get_links".into(),
            |response: AppResponse| {
                assert_matches!(response, AppResponse::ZomeCalled { .. });
            },
        )
        .await;

        // State should match
        let state = conductor_handle.get_state_from_handle().await.unwrap();
        assert_eq!(initial_state, state);

        // Check it is running, and get all cells
        let cell_ids: HashSet<CellId> = state
            .get_app(&app_id)
            .inspect(|app| {
                assert_eq!(*app.status(), AppStatus::Running);
            })
            .unwrap()
            .all_cells()
            .collect();

        // Collect the expected result
        let expected = dna_hashes
            .into_iter()
            .map(|hash| CellId::from((hash.clone(), agent_key.clone())))
            .collect::<HashSet<_>>();

        assert_eq!(expected, cell_ids);

        // Check that it is returned in get_app_info as running
        let maybe_info = conductor_handle.get_app_info(&app_id).await.unwrap();
        if let Some(info) = maybe_info {
            assert_eq!(info.installed_app_id, app_id);
            assert_matches!(info.status, AppInfoStatus::Running);
        }

        // Now deactivate app
        println!("### DISABLE ###");

        let msg = AdminRequest::DisableApp {
            installed_app_id: app_id.clone(),
        };
        let respond = |response: AdminResponse| {
            assert_matches!(response, AdminResponse::AppDisabled);
        };

        test_handle_incoming_admin_message(
            msg,
            respond,
            AdminInterfaceApi::new(conductor_handle.clone()),
        )
        .await
        .unwrap();

        // Get the state
        let state = conductor_handle.get_state_from_handle().await.unwrap();

        // Check it's deactivated, and get all cells
        let cell_ids: HashSet<CellId> = state
            .get_app(&app_id)
            .inspect(|app| {
                assert_matches!(*app.status(), AppStatus::Disabled(_));
            })
            .unwrap()
            .all_cells()
            .collect();

        assert_eq!(expected, cell_ids);

        // Check that it is returned in get_app_info as deactivated
        let maybe_info = conductor_handle.get_app_info(&app_id).await.unwrap();
        if let Some(info) = maybe_info {
            assert_eq!(info.installed_app_id, app_id);
            assert_matches!(info.status, AppInfoStatus::Disabled { .. });
        }

        // Enable the app one more time
        println!("### ENABLE ###");

        let msg = AdminRequest::EnableApp {
            installed_app_id: app_id.clone(),
        };
        let respond = |response: AdminResponse| {
            assert_matches!(response, AdminResponse::AppEnabled { .. });
        };

        test_handle_incoming_admin_message(
            msg,
            respond,
            AdminInterfaceApi::new(conductor_handle.clone()),
        )
        .await
        .unwrap();

        // Get the state again after reenabling, make sure it's identical to the initial state.
        let state: ConductorState = conductor_handle.get_state_from_handle().await.unwrap();
        assert_eq!(initial_state, state);

        // Now make sure we can call a zome once again
        println!("### CALL ZOME ###");

        call_zome(
            conductor_handle.clone(),
            cell_id_0.clone(),
            TestWasm::Link.coordinator_zome_name(),
            "get_links".into(),
            |response: AppResponse| {
                assert_matches!(response, AppResponse::ZomeCalled { .. });
            },
        )
        .await;

        conductor_handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn attach_app_interface() {
        holochain_trace::test_run();
        let (_tmpdir, conductor_handle) = setup_admin().await;
        let admin_api = AdminInterfaceApi::new(conductor_handle.clone());
        let msg = AdminRequest::AttachAppInterface {
            port: None,
            allowed_origins: AllowedOrigins::Any,
            installed_app_id: None,
        };
        let respond = |response: AdminResponse| {
            assert_matches!(response, AdminResponse::AppInterfaceAttached { .. });
        };
        test_handle_incoming_admin_message(msg, respond, admin_api)
            .await
            .unwrap();
        conductor_handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn dump_state() {
        holochain_trace::test_run();
        let uuid = Uuid::new_v4();
        let dna = fake_dna_zomes(
            &uuid.to_string(),
            vec![("zomey".into(), TestWasm::Foo.into())],
        );
        let agent_pubkey = fake_agent_pubkey_1();
        let cell_id = CellId::from((dna.dna_hash().clone(), agent_pubkey.clone()));

        let (_tmpdir, conductor_handle) =
            setup_admin_fake_cells(agent_pubkey, vec![(dna, None)]).await;
        let conductor_handle = activate(conductor_handle).await;

        // Allow agents time to join
        tokio::time::sleep(std::time::Duration::from_secs(2)).await;

        // Get state
        let expected = conductor_handle.dump_cell_state(&cell_id).await.unwrap();

        let admin_api = AdminInterfaceApi::new(conductor_handle.clone());
        let msg = AdminRequest::DumpState {
            cell_id: Box::new(cell_id),
        };
        let respond = move |response: AdminResponse| {
            assert_matches!(response, AdminResponse::StateDumped(s) if s == expected);
        };
        test_handle_incoming_admin_message(msg, respond, admin_api)
            .await
            .unwrap();
        conductor_handle.shutdown().await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn dump_conductor_state() {
        holochain_trace::test_run();
        let uuid = Uuid::new_v4();
        let dna = fake_dna_zomes(
            &uuid.to_string(),
            vec![("zomey".into(), TestWasm::Foo.into())],
        );
        let agent_pubkey = fake_agent_pubkey_1();

        let _ = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![TestWasm::Foo]))
            .next()
            .unwrap();
        let (_tmpdir, _, conductor_handle, _agent_key) = setup_app_in_new_conductor(
            "test app".to_string(),
            Some(agent_pubkey.clone()),
            vec![(dna.clone(), None)],
        )
        .await;
        let dna_hash = dna.dna_hash();

        conductor_handle
            .clone()
            .add_app_interface(
                either::Either::Left(12345),
                AllowedOrigins::Any,
                Some("test app".into()),
            )
            .await
            .unwrap();

        // Construct expected response
        #[derive(Serialize, Debug)]
        pub struct ConductorSerialized {
            running_cells: Vec<(DnaHashB64, AgentPubKeyB64)>,
            shutting_down: bool,
            admin_websocket_ports: Vec<u16>,
            app_interfaces: Vec<AppInterfaceId>,
        }

        #[derive(Serialize, Debug)]
        struct ConductorDump {
            conductor: ConductorSerialized,
            state: ConductorState,
        }

        let dpki_cell_id = conductor_handle
            .running_services()
            .dpki
            .map(|dpki| dpki.cell_id.clone());
        let mut running_cells = vec![];
        if let Some(cell_id) = dpki_cell_id {
            running_cells.push((
                cell_id.dna_hash().clone().into(),
                cell_id.agent_pubkey().clone().into(),
            ));
        }
        running_cells.push((dna_hash.clone().into(), agent_pubkey.clone().into()));

        let expected = ConductorDump {
            conductor: ConductorSerialized {
                running_cells,
                shutting_down: false,
                admin_websocket_ports: vec![],
                app_interfaces: vec![AppInterfaceId::new(12345)],
            },
            state: conductor_handle.get_state_from_handle().await.unwrap(),
        };
        let expected_json = serde_json::to_string_pretty(&expected).unwrap();

        // Get state
        let admin_api = AdminInterfaceApi::new(conductor_handle.clone());
        let respond = move |response: AdminResponse| {
            assert_matches!(response, AdminResponse::ConductorStateDumped(s) if s == expected_json);
        };
        test_handle_incoming_admin_message(AdminRequest::DumpConductorState, respond, admin_api)
            .await
            .unwrap();
        conductor_handle.shutdown().await.unwrap().unwrap();
    }

    async fn make_dna(network_seed: &str, zomes: Vec<TestWasm>) -> DnaFile {
        DnaFile::new(
            DnaDef {
                name: "conductor_test".to_string(),
                modifiers: DnaModifiers {
                    network_seed: network_seed.to_string(),
                    properties: SerializedBytes::try_from(()).unwrap(),
                    origin_time: Timestamp::HOLOCHAIN_EPOCH,
                    quantum_time: holochain_p2p::dht::spacetime::STANDARD_QUANTUM_TIME,
                },
                integrity_zomes: zomes
                    .clone()
                    .into_iter()
                    .map(TestZomes::from)
                    .map(|z| z.integrity.into_inner())
                    .collect(),
                coordinator_zomes: zomes
                    .clone()
                    .into_iter()
                    .map(TestZomes::from)
                    .map(|z| z.coordinator.into_inner())
                    .collect(),
                lineage: Default::default(),
            },
            zomes.into_iter().flat_map(Vec::<DnaWasm>::from),
        )
        .await
    }

    /// Check that we can add and get agent info for a conductor
    /// across the admin websocket.
    #[tokio::test(flavor = "multi_thread")]
    async fn add_agent_info_via_admin() {
        holochain_trace::test_run();
        let dnas = vec![
            make_dna("1", vec![TestWasm::Anchor]).await,
            make_dna("2", vec![TestWasm::Anchor]).await,
        ];

        let mut conductor = SweetConductorConfig::standard()
            .no_dpki()
            .build_conductor()
            .await;

        let agent = conductor.setup_app("app", &dnas).await.unwrap().cells()[0]
            .agent_pubkey()
            .clone();

        let handle = conductor.raw_handle();
        let spaces = handle.get_spaces();
        let dnas = dnas
            .into_iter()
            .map(|d| d.dna_hash().clone())
            .collect::<Vec<_>>();

        // - Give time for the agents to join the network.
        crate::assert_eq_retry_10s!(
            {
                let mut count = 0;
                for env in spaces.get_from_spaces(|s| s.p2p_agents_db.clone()) {
                    let space = env.kind().0.clone();
                    count += env.test_read(move |txn| txn.p2p_list_agents(space).unwrap().len())
                }
                count
            },
            2
        );

        // - Get agents and space
        let agent_infos = AgentInfoSignedFixturator::new(Unpredictable)
            .take(5)
            .collect::<Vec<_>>();

        let mut expect = to_key(agent_infos.clone());
        let k0 = (dnas[0].to_kitsune(), agent.to_kitsune());
        let k1 = (dnas[1].to_kitsune(), agent.to_kitsune());
        expect.push(k0.clone());
        expect.push(k1.clone());
        expect.sort();

        let admin_api = AdminInterfaceApi::new(handle.clone());

        // - Add the agent infos
        let req = AdminRequest::AddAgentInfo { agent_infos };
        let r = make_req(admin_api.clone(), req).await.await.unwrap();
        assert_matches!(r, AdminResponse::AgentInfoAdded);

        // - Request all the infos
        let req = AdminRequest::AgentInfo { cell_id: None };
        let r = make_req(admin_api.clone(), req).await.await.unwrap();
        let results = to_key(unwrap_to::unwrap_to!(r => AdminResponse::AgentInfo).clone());
        assert_eq!(expect, results);

        // - Request the first cell
        let req = AdminRequest::AgentInfo {
            cell_id: Some(CellId::new(dnas[0].clone(), agent.clone())),
        };
        let r = make_req(admin_api.clone(), req).await.await.unwrap();
        let results = to_key(unwrap_to::unwrap_to!(r => AdminResponse::AgentInfo).clone());

        assert_eq!(vec![k0], results);

        // - Request the second cell
        let req = AdminRequest::AgentInfo {
            cell_id: Some(CellId::new(dnas[1].clone(), agent.clone())),
        };
        let r = make_req(admin_api.clone(), req).await.await.unwrap();
        let results = to_key(unwrap_to::unwrap_to!(r => AdminResponse::AgentInfo).clone());

        assert_eq!(vec![k1], results);
    }

    async fn make_req(
        admin_api: AdminInterfaceApi,
        req: AdminRequest,
    ) -> tokio::sync::oneshot::Receiver<AdminResponse> {
        let (tx, rx) = tokio::sync::oneshot::channel();

        let respond = move |response: AdminResponse| {
            tx.send(response).unwrap();
        };

        test_handle_incoming_admin_message(req, respond, admin_api)
            .await
            .unwrap();
        rx
    }

    fn to_key(r: Vec<AgentInfoSigned>) -> Vec<(Arc<KitsuneSpace>, Arc<KitsuneAgent>)> {
        let mut results = r
            .into_iter()
            .map(|a| (a.space.clone(), a.agent.clone()))
            .collect::<Vec<_>>();
        results.sort();
        results
    }

    fn get_app_data_storage_info(
        info: &StorageInfo,
        match_app_id: InstalledAppId,
    ) -> &DnaStorageInfo {
        info.blobs
            .iter()
            .filter_map(|blob| match blob {
                StorageBlob::Dna(app_data) => {
                    if app_data.used_by.contains(&match_app_id) {
                        Some(app_data)
                    } else {
                        None
                    }
                }
            })
            .last()
            .unwrap()
    }
}



================================================
File: crates/holochain/src/conductor/kitsune_host_impl/query_region_op_hashes.rs
================================================
use holo_hash::DhtOpHash;
use holochain_p2p::{dht::prelude::*, DhtOpHashExt};
use holochain_sqlite::prelude::*;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::OpHashSized;
use rusqlite::named_params;

use crate::conductor::error::ConductorResult;

/// Get all op hashes within a region
#[allow(clippy::let_and_return)] // required to drop temporary
pub async fn query_region_op_hashes(
    db: DbWrite<DbKindDht>,
    bounds: RegionBounds,
) -> ConductorResult<Vec<OpHashSized>> {
    Ok(db
        .read_async(move |txn| {
            let sql = holochain_sqlite::sql::sql_cell::FETCH_REGION_OP_HASHES;
            let mut stmt = txn.prepare_cached(sql).map_err(DatabaseError::from)?;
            let (x0, x1) = bounds.x;
            let (t0, t1) = bounds.t;
            let hashes = stmt
                .query_map(
                    named_params! {
                        ":storage_start_loc": x0,
                        ":storage_end_loc": x1,
                        ":timestamp_min": t0,
                        ":timestamp_max": t1,
                    },
                    |row| {
                        let hash: DhtOpHash = row.get("hash")?;
                        let action_size: usize = row.get("action_size")?;
                        // will be NULL if the op has no associated entry
                        let entry_size: Option<usize> = row.get("entry_size")?;
                        let op_size = (action_size + entry_size.unwrap_or(0)).into();
                        Ok(OpHashSized::new(hash.to_kitsune(), Some(op_size)))
                    },
                )?
                .collect::<Result<Vec<_>, _>>()
                .map_err(DatabaseError::from);
            hashes
        })
        .await?)
}



================================================
File: crates/holochain/src/conductor/kitsune_host_impl/query_region_set.rs
================================================
use std::sync::Arc;

use holochain_p2p::dht::prelude::*;
use holochain_sqlite::prelude::*;
use rusqlite::named_params;

use crate::conductor::error::ConductorResult;

/// The network module needs info about various groupings ("regions") of ops.
///
/// Note that this always includes all ops regardless of integration status.
/// This is to avoid the degenerate case of freshly joining a network, and
/// having several new peers gossiping with you at once about the same regions.
/// If we calculate our region hash only by integrated ops, we will experience
/// mismatches for a large number of ops repeatedly until we have integrated
/// those ops. Note that when *sending* ops we filter out ops in limbo.
pub async fn query_region_set(
    db: DbWrite<DbKindDht>,
    topology: Topology,
    _strat: &ArqStrat,
    arq_set: Arc<ArqSet>,
) -> ConductorResult<RegionSetLtcs> {
    let times = TelescopingTimes::historical(&topology);
    let coords = RegionCoordSetLtcs::new(times, (*arq_set).clone());

    let region_set = db
        .read_async(move |txn| {
            let sql = holochain_sqlite::sql::sql_cell::FETCH_OP_REGION;
            let mut stmt = txn.prepare_cached(sql).map_err(DatabaseError::from)?;
            let regions = coords
                .into_region_set(|(_, coords)| query_region_data(&mut stmt, &topology, coords))?;
            DatabaseResult::Ok(regions)
        })
        .await?;

    Ok(region_set)
}

pub(super) fn query_region_data(
    stmt: &mut rusqlite::CachedStatement,
    topology: &Topology,
    coords: RegionCoords,
) -> Result<RegionData, DatabaseError> {
    let bounds = coords.to_bounds(topology);
    let (x0, x1) = bounds.x;
    let (t0, t1) = bounds.t;
    stmt.query_row(
        named_params! {
            ":storage_start_loc": x0,
            ":storage_end_loc": x1,
            ":timestamp_min": t0,
            ":timestamp_max": t1,
        },
        |row| {
            let total_action_size: f64 = row.get("total_action_size")?;
            let total_entry_size: f64 = row.get("total_entry_size")?;
            let size = total_action_size + total_entry_size;
            Ok(RegionData {
                hash: RegionHash::from_vec(row.get("xor_hash")?)
                    .expect("region hash must be 32 bytes"),
                size: size.min(u32::MAX as f64) as u32,
                count: row.get("count")?,
            })
        },
    )
    .map_err(DatabaseError::from)
}

#[cfg(test)]
mod tests {
    use std::time::Duration;

    use super::*;
    use holochain_serialized_bytes::UnsafeBytes;
    use holochain_state::{prelude::*, test_utils::test_dht_db};

    /// Ensure that the size reported by RegionData is "close enough" to the actual size of
    /// ops that get transferred over the wire.
    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "flaky: somehow in CI, the DB thread acquisition consistently times out"]
    async fn query_region_set_diff_size() {
        let db = test_dht_db();
        let topo = Topology::standard(kitsune_p2p_types::Timestamp::now(), Duration::ZERO);
        let strat = ArqStrat::default();
        let arq_set = Arc::new(ArqSet::full_std());

        let regions_empty = query_region_set(db.to_db(), topo.clone(), &strat, arq_set.clone())
            .await
            .unwrap();
        {
            let sum: RegionData = regions_empty.regions().map(|r| r.data).sum();
            assert_eq!(sum.count, 0);
            assert_eq!(sum.size, 0);
        }

        let mk_op = |i: u8| {
            let entry = Entry::App(AppEntryBytes(
                UnsafeBytes::from(vec![i % 10; 10_000_000]).into(),
            ));
            let sig = ::fixt::fixt!(Signature);
            let mut create = ::fixt::fixt!(Create);
            create.timestamp = holochain_timestamp::Timestamp::now();
            let action = NewEntryAction::Create(create);
            DhtOpHashed::from_content_sync(ChainOp::StoreEntry(sig, action, entry))
        };
        let num = 100;

        let ops: Vec<_> = (0..num).map(mk_op).collect();
        let wire_bytes: usize = ops
            .clone()
            .into_iter()
            .map(|op| {
                holochain_p2p::WireDhtOpData {
                    op_data: op.into_content(),
                }
                .encode()
                .unwrap()
                .len()
            })
            .sum();

        db.test_write(move |txn| {
            for op in ops.iter() {
                insert_op_dht(txn, op, None).unwrap()
            }
        });

        let regions = query_region_set(db.to_db(), topo, &strat, arq_set)
            .await
            .unwrap();

        let diff = regions.diff(regions_empty).unwrap();
        {
            let sum: RegionData = diff.into_iter().map(|r| r.data).sum();
            assert_eq!(sum.count, num as u32);
            // 32 bytes is "close enough"
            assert!(wire_bytes as u32 - sum.size < 32 * num as u32);
        }
    }
}



================================================
File: crates/holochain/src/conductor/kitsune_host_impl/query_size_limited_regions.rs
================================================
use holochain_p2p::dht::prelude::*;
use holochain_sqlite::prelude::*;
use holochain_types::prelude::first_ref;

use crate::conductor::error::ConductorResult;

use super::query_region_set::query_region_data;

/// Given a set of Regions, return an equivalent set of Regions (which covers the same
/// area of the DHT) such that no region is larger than `size_limit`.
/// Regions larger than size_limit will be quadrisected, and the size of each subregion
/// will be fetched from the database. The quadrisecting is recursive until either all
/// regions are either small enough, or cannot be further subdivided.
#[cfg_attr(feature = "instrument", tracing::instrument(skip(db, topology)))]
pub async fn query_size_limited_regions(
    db: DbWrite<DbKindDht>,
    topology: Topology,
    regions: Vec<Region>,
    size_limit: u32,
) -> ConductorResult<Vec<Region>> {
    Ok(db
        .read_async(move |txn| {
            let sql = holochain_sqlite::sql::sql_cell::FETCH_OP_REGION;
            let mut stmt = txn.prepare_cached(sql).map_err(DatabaseError::from)?;

            // The regions whose size has not been checked. A `true` boolean specifies that
            // the region could not be subdivided, and should be returned as-is.
            let mut unchecked: Vec<(Region, bool)> =
                regions.into_iter().map(|r| (r, false)).collect();
            let mut checked: Vec<Region> = vec![];
            while !unchecked.is_empty() {
                // partition the set into regions which need to be split, and those which don't
                let (smalls, bigs): (Vec<_>, Vec<_>) = unchecked
                    .iter()
                    .partition(|(r, quantum)| *quantum || r.data.size <= size_limit);
                // add the unsplittables to the final set to be returned
                checked.extend(smalls.into_iter().map(first_ref).cloned());

                // split up the splittables and check the new sizes, using this list
                // as the starting point of the next iteration
                unchecked = bigs
                    .into_iter()
                    .flat_map(|(r, _)| {
                        r.coords
                            .quadrisect()
                            .map(|rs| rs.into_iter().map(|r| (r, false)).collect())
                            .unwrap_or_else(|| vec![(r.coords, true)])
                    })
                    .map(|(c, q)| {
                        let data = query_region_data(&mut stmt, &topology, c)?;
                        DatabaseResult::Ok((Region::new(c, data), q))
                    })
                    .collect::<Result<Vec<(Region, bool)>, _>>()?;
            }
            DatabaseResult::Ok(checked)
        })
        .await?)
}



================================================
File: crates/holochain/src/conductor/manager/error.rs
================================================
#![allow(missing_docs)]

use crate::conductor::error::ConductorError;
use thiserror::Error;

/// An error that is thrown from within the Task Manager itself.
/// An unrecoverable ManagedTaskError can be bubbled up into a TaskManagerError.
#[derive(Error, Debug)]
pub enum TaskManagerError {
    #[error("Conductor has exited due to an unrecoverable error in a managed task {0}")]
    Unrecoverable(Box<ManagedTaskError>),

    #[error("Task manager failed to start")]
    TaskManagerFailedToStart,

    #[error(transparent)]
    Join(#[from] tokio::task::JoinError),

    #[error("Task manager encountered an internal error: {0}")]
    Internal(Box<dyn std::error::Error + Send + Sync>),
}

impl TaskManagerError {
    pub fn internal<E>(err: E) -> Self
    where
        E: std::error::Error + Send + Sync + 'static,
    {
        Self::Internal(Box::new(err))
    }
}

pub type TaskManagerResult = Result<(), TaskManagerError>;

/// An error that is thrown from within a managed task
#[derive(Error, Debug)]
pub enum ManagedTaskError {
    #[error(transparent)]
    Conductor(#[from] Box<ConductorError>),

    #[error(transparent)]
    Io(#[from] std::io::Error),

    #[error(transparent)]
    Join(#[from] tokio::task::JoinError),

    #[error(transparent)]
    Recv(#[from] tokio::sync::broadcast::error::RecvError),
}

pub type ManagedTaskResult = Result<(), ManagedTaskError>;

impl ManagedTaskError {
    pub fn is_recoverable(&self) -> bool {
        use ConductorError as C;
        use ManagedTaskError::*;
        #[allow(clippy::match_like_matches_macro)]
        match self {
            Io(_) | Join(_) | Recv(_) => false,
            Conductor(err) => match **err {
                C::ShuttingDown => true,
                // TODO: identify all recoverable cases
                _ => false,
            },
        }
    }
}



================================================
File: crates/holochain/src/conductor/manager/mod.rs
================================================
//! Holochain Task Manager
//!
//! The TaskManager is used to manage long running tasks that are critical to the
//! operation of the conductor.
//!
//! Tasks added to the manager can be in one of three groups:
//!
//! - Conductor: Tasks which are associated with the conductor as a whole
//! - Dna: Tasks which are associated with a particular DNA
//! - Cell: Tasks which are associated with a particular Cell
//!
//! The outcome of a task in a group can affect the other tasks in its group.
//! Tasks which are critical to the operation of its group level will cause
//! the other tasks in that group to be stopped.
//!
//! For instance, the tasks which run the workflows for a cell are critical
//! to the cell's functioning, so if any of these tasks fail, then the cell
//! is no longer able to function. Task failure is a signal that the cell
//! needs to be shut down, so the task manager takes the steps necessary to
//! accomplish that:
//!
//! 1. Stop all other tasks related to the cell, so they don't continue in the background.
//! 2. Pause or disable any apps which depend on the cell, because the app cannot
//!     function without the proper functioning of that cell.

mod error;
pub use error::*;

use futures::Future;
use futures::FutureExt;
use holochain_types::prelude::*;
use parking_lot::Mutex;
use std::sync::Arc;
use task_motel::StopListener;
use tokio::task::JoinHandle;
use tokio_stream::StreamExt;
use tracing::*;

use super::ConductorHandle;

/// The main interface for interacting with a task manager.
/// Contains functions for adding tasks to groups, stopping task groups,
/// and shutting down the task manager.
#[derive(Clone)]
pub struct TaskManagerClient {
    tm: Arc<Mutex<Option<task_motel::TaskManager<TaskGroup, TaskOutcome>>>>,
}

impl TaskManagerClient {
    /// Construct the TaskManager and the outcome channel receiver
    pub fn new(tx: OutcomeSender, scope: String) -> Self {
        let span = tracing::info_span!("managed task", scope = scope);
        let tm = task_motel::TaskManager::new_instrumented(span, tx, |g| match g {
            TaskGroup::Conductor => None,
            TaskGroup::Dna(_) => Some(TaskGroup::Conductor),
            TaskGroup::Cell(cell_id) => Some(TaskGroup::Dna(Arc::new(cell_id.dna_hash().clone()))),
        });
        Self {
            tm: Arc::new(Mutex::new(Some(tm))),
        }
    }

    /// Stop all managed tasks and await their completion.
    pub fn stop_all_tasks(&self) -> ShutdownHandle {
        if let Some(tm) = self.tm.lock().as_mut() {
            tokio::spawn(tm.stop_group(&TaskGroup::Conductor))
        } else {
            tracing::warn!("Tried to shutdown task manager while it's already shutting down");
            tokio::spawn(async move {})
        }
    }

    /// Stop all tasks associated with a Cell and await their completion.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub fn stop_cell_tasks(&self, cell_id: CellId) -> ShutdownHandle {
        if let Some(tm) = self.tm.lock().as_mut() {
            tokio::spawn(tm.stop_group(&TaskGroup::Cell(cell_id)).in_current_span())
        } else {
            tracing::warn!("Tried to shutdown cell's tasks while they're already shutting down");
            tokio::spawn(async move {})
        }
    }

    /// Stop all tasks and prevent any new tasks from being added to the manager.
    /// Returns a future to await completion of all tasks.
    pub fn shutdown(&mut self) -> ShutdownHandle {
        if let Some(mut tm) = self.tm.lock().take() {
            tokio::spawn(tm.stop_group(&TaskGroup::Conductor))
        } else {
            // already shutting down
            tokio::spawn(async move {})
        }
    }

    /// Add a conductor-level task whose outcome is ignored.
    pub fn add_conductor_task_ignored<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        f: impl FnOnce() -> Fut + Send + 'static,
    ) {
        self.add_conductor_task(name, TaskKind::Ignore, move |stop| async move {
            tokio::select! {
                _ = stop => (),
                _ = f() => (),
            }
            Ok(())
        })
    }

    /// Add a conductor-level task which will cause the conductor to shut down if it fails
    pub fn add_conductor_task_unrecoverable<
        Fut: Future<Output = ManagedTaskResult> + Send + 'static,
    >(
        &self,
        name: &str,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        self.add_conductor_task(name, TaskKind::Unrecoverable, f)
    }

    /// Add a DNA-level task which will cause all cells under that DNA to be disabled if
    /// the task fails
    pub fn add_dna_task_critical<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        dna_hash: Arc<DnaHash>,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        self.add_dna_task(name, TaskKind::DnaCritical(dna_hash.clone()), dna_hash, f)
    }

    /// Add a Cell-level task whose outcome is ignored
    pub fn add_cell_task_ignored<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        cell_id: CellId,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        self.add_cell_task(name, TaskKind::Ignore, cell_id, f)
    }

    /// Add a Cell-level task which will cause that to be disabled if the task fails
    pub fn add_cell_task_critical<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        cell_id: CellId,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        self.add_cell_task(name, TaskKind::CellCritical(cell_id.clone()), cell_id, f)
    }

    fn add_conductor_task<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        task_kind: TaskKind,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        let name = name.to_string();
        let f = move |stop| f(stop).map(move |t| produce_task_outcome(&task_kind, t, name));
        self.add_task(TaskGroup::Conductor, f)
    }

    fn add_dna_task<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        task_kind: TaskKind,
        dna_hash: Arc<DnaHash>,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        let name = name.to_string();
        let f = move |stop| f(stop).map(move |t| produce_task_outcome(&task_kind, t, name));
        self.add_task(TaskGroup::Dna(dna_hash), f)
    }

    fn add_cell_task<Fut: Future<Output = ManagedTaskResult> + Send + 'static>(
        &self,
        name: &str,
        task_kind: TaskKind,
        cell_id: CellId,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        let name = name.to_string();
        let f = move |stop| f(stop).map(move |t| produce_task_outcome(&task_kind, t, name));
        self.add_task(TaskGroup::Cell(cell_id), f)
    }

    fn add_task<Fut: Future<Output = TaskOutcome> + Send + 'static>(
        &self,
        group: TaskGroup,
        f: impl FnOnce(StopListener) -> Fut + Send + 'static,
    ) {
        if let Some(tm) = self.tm.lock().as_mut() {
            tm.add_task(group, f)
        } else {
            tracing::warn!("Tried to add task while task manager is shutting down.");
        }
    }
}

/// The "kind" of a managed task determines how the Result from the task's
/// completion will be handled.
pub enum TaskKind {
    /// Log an error if there is one, but otherwise do nothing.
    Ignore,
    /// If the task returns an error, shut down the conductor.
    Unrecoverable,
    /// If the task returns an error, "freeze" the cell which caused the error,
    /// but continue running the rest of the conductor and other managed tasks.
    CellCritical(CellId),
    /// If the task returns an error, "freeze" all cells with this dna hash,
    /// but continue running the rest of the conductor and other managed tasks.
    DnaCritical(Arc<DnaHash>),
}

/// The outcome of a task that has finished.
pub enum TaskOutcome {
    /// Log an info trace and take no other action.
    LogInfo(String),
    /// Log an error and take no other action.
    MinorError(Box<ManagedTaskError>, String),
    /// Close the conductor down because this is an unrecoverable error.
    ShutdownConductor(Box<ManagedTaskError>, String),
    /// Either pause or disable all apps which contain the problematic Cell,
    /// depending upon the specific error.
    StopApps(CellId, Box<ManagedTaskError>, String),
    /// Either pause or disable all apps which contain the problematic Dna,
    /// depending upon the specific error.
    StopAppsWithDna(Arc<DnaHash>, Box<ManagedTaskError>, String),
}

/// Spawn a task which performs some action after each task has completed,
/// as recieved by the outcome channel produced by the task manager.
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub(crate) fn spawn_task_outcome_handler(
    conductor: ConductorHandle,
    mut outcomes: OutcomeReceiver,
) -> JoinHandle<TaskManagerResult> {
    let span = tracing::info_span!(
        "spawn_task_outcome_handler",
        scope = conductor.get_config().tracing_scope()
    );
    tokio::spawn(async move {
        while let Some((_group, result)) = outcomes.next().await {
            match result {
                // TaskOutcome::Noop => (),
                TaskOutcome::LogInfo(context) => {
                    debug!("Managed task completed: {}", context)
                }
                TaskOutcome::MinorError(error, context) => {
                    error!(
                        "Minor error during managed task: {:?}\nContext: {}",
                        error, context
                    )
                }
                TaskOutcome::ShutdownConductor(error, context) => {
                    let error = match *error {
                        ManagedTaskError::Join(error) => {
                            match error.try_into_panic() {
                                Ok(reason) => {
                                    // Resume the panic on the main task
                                    std::panic::resume_unwind(reason);
                                }
                                Err(error) => ManagedTaskError::Join(error),
                            }
                        }
                        error => error,
                    };
                    error!(
                        "Shutting down conductor due to unrecoverable error: {:?}\nContext: {}",
                        error, context
                    );
                    return Err(TaskManagerError::Unrecoverable(Box::new(error)));
                }
                TaskOutcome::StopApps(cell_id, error, context) => {
                    tracing::error!("About to automatically stop apps");
                    let app_ids = conductor
                        .list_running_apps_for_dependent_cell_id(&cell_id)
                        .await
                        .map_err(TaskManagerError::internal)?;
                    if error.is_recoverable() {
                        conductor.remove_cells(&[cell_id]).await;

                        // The following message assumes that only the app_ids calculated will be paused, but other apps
                        // may have been paused as well.
                        tracing::error!(
                            "PAUSING the following apps due to a recoverable error: {:?}\nError: {:?}\nContext: {}",
                            app_ids,
                            error,
                            context
                        );

                        // MAYBE: it could be helpful to modify this function so that when providing Some(app_ids),
                        //   you can also pass in a PausedAppReason override, so that the reason for the apps being paused
                        //   can be set to the specific error message encountered here, rather than having to read it from
                        //   the logs.
                        let delta = conductor
                            .reconcile_app_status_with_cell_status(None)
                            .await
                            .map_err(TaskManagerError::internal)?;
                        tracing::debug!(delta = ?delta);

                        tracing::error!("Apps paused.");
                    } else {
                        // Since the error is unrecoverable, we don't expect to be able to use this Cell anymore.
                        // Therefore, we disable every app which requires that cell.
                        tracing::error!(
                            "DISABLING the following apps due to an unrecoverable error: {:?}\nError: {:?}\nContext: {}",
                            app_ids,
                            error,
                            context
                        );
                        for app_id in app_ids.iter() {
                            conductor
                                .clone()
                                .disable_app(
                                    app_id.to_string(),
                                    DisabledAppReason::Error(error.to_string()),
