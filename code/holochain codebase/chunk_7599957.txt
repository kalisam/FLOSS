    max_processing_time: std::time::Duration,
    count: u32,
}

impl Stats {
    /// Reset the stats.
    fn reset() -> Self {
        Stats {
            start: Instant::now(),
            last: None,
            avg_processing_time: std::time::Duration::default(),
            max_processing_time: std::time::Duration::default(),
            count: 0,
        }
    }
}

impl ShardedGossip {
    /// Constructor
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        config: Arc<KitsuneP2pConfig>,
        space: Arc<KitsuneSpace>,
        ep_hnd: MetaNet,
        host_api: HostApiLegacy,
        gossip_type: GossipType,
        bandwidth: Arc<BandwidthThrottle>,
        metrics: MetricsSync,
        fetch_pool: FetchPool,
        #[cfg(test)] enable_history: bool,
    ) -> Arc<Self> {
        #[cfg(test)]
        let state = if enable_history {
            ShardedGossipState::with_history()
        } else {
            Default::default()
        };

        #[cfg(not(test))]
        let state = Default::default();

        let tuning_params = config.tuning_params.clone();

        let this = Arc::new(Self {
            ep_hnd,
            state: Share::new(state),
            gossip: ShardedGossipLocal {
                tuning_params,
                space,
                host_api,
                inner: Share::new(ShardedGossipLocalState::new(metrics)),
                gossip_type,
                closing: AtomicBool::new(false),
                fetch_pool,
            },
            bandwidth,
        });

        let mut refresh_agent_list_timer = std::time::Instant::now();

        metric_task_instrumented(config.tracing_scope.clone(), {
            let this = this.clone();

            async move {
                let mut agent_info_session = this.create_agent_info_session().await?;

                let mut stats = Stats::reset();
                while !this
                    .gossip
                    .closing
                    .load(std::sync::atomic::Ordering::Relaxed)
                {
                    if refresh_agent_list_timer.elapsed() > AGENT_LIST_FETCH_INTERVAL {
                        agent_info_session = this.create_agent_info_session().await?;
                        refresh_agent_list_timer = std::time::Instant::now();
                    }

                    this.run_one_iteration(&mut agent_info_session).await;
                    this.stats(&mut stats);

                    tokio::time::sleep(GOSSIP_LOOP_INTERVAL).await;
                }
                KitsuneResult::Ok(())
            }
        });
        this
    }

    async fn create_agent_info_session(&self) -> KitsuneResult<AgentInfoSession> {
        let all_agents =
            match store::all_agent_info(&self.gossip.host_api, &self.gossip.space).await {
                Ok(a) => a,
                Err(e) => {
                    tracing::error!("Failed to query for all agents - {:?}", e);
                    vec![]
                }
            };

        // Find local agents by filtering the complete list of known agents against the agents which have joined Kitsune.
        let local_agents = self.gossip.inner.share_ref(|s| {
            Ok(all_agents
                .iter()
                .filter(|a| s.local_agents.contains(&a.agent))
                .cloned()
                .collect())
        })?;

        Ok(AgentInfoSession::new(local_agents, all_agents))
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn process_outgoing(&self, outgoing: Outgoing) -> KitsuneResult<()> {
        let (cert, how, gossip) = outgoing;
        match self.gossip.gossip_type {
            GossipType::Recent => {
                let s = tracing::trace_span!("process_outgoing_recent", cert = ?cert, agents = ?self.gossip.show_local_agents());
                s.in_scope(|| tracing::trace!(?gossip));
            }
            GossipType::Historical => {
                let s = tracing::trace_span!("process_outgoing_historical", cert = ?cert, agents = ?self.gossip.show_local_agents());
                match &gossip {
                    ShardedGossipWire::MissingOpHashes(MissingOpHashes { ops, finished }) => {
                        s.in_scope(|| {
                            tracing::trace!(
                                num_ops = %ops.len(),
                                total_bytes = ops.iter().map(|op| op.0.len()).sum::<usize>(),
                                ?finished
                            )
                        });
                    }
                    _ => {
                        s.in_scope(|| tracing::trace!(?gossip));
                    }
                }
            }
        };

        let encoded = gossip.encode_vec().map_err(KitsuneError::other)?;
        let bytes = encoded.len();
        let wire = wire::Wire::gossip(
            self.gossip.space.clone(),
            encoded.into(),
            self.gossip.gossip_type.into(),
        );

        let timeout = self.gossip.tuning_params.implicit_timeout();

        self.bandwidth.outgoing_bytes(bytes).await;

        let con = match how.clone() {
            HowToConnect::Con(con, remote_url) => {
                if con.is_closed() {
                    self.ep_hnd.get_connection(remote_url, timeout).await?
                } else {
                    con
                }
            }
            HowToConnect::Url(url) => self.ep_hnd.get_connection(url, timeout).await?,
        };

        // Wait for enough available outgoing bandwidth here before
        // actually sending the gossip.
        con.notify(&wire, timeout).await?;

        if let ShardedGossipWire::MissingOpHashes(MissingOpHashes { ops, finished: _ }) = gossip {
            for hash in ops.iter() {
                self.gossip.host_api.handle_op_hash_transmitted(
                    &self.gossip.space,
                    hash,
                    TransferMethod::Gossip(self.gossip.gossip_type),
                );
            }
        }

        Ok(())
    }

    async fn process_incoming_outgoing(
        &self,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<()> {
        let (incoming, outgoing) = self.pop_queues()?;
        let gossip_type_char = match self.gossip.gossip_type {
            GossipType::Recent => 'R',
            GossipType::Historical => 'H',
        };

        if let Some(msg) = outgoing.as_ref() {
            let remote_url = match &msg.1 {
                HowToConnect::Con(_, url) => url.clone(),
                HowToConnect::Url(url) => url.clone(),
            };
            tracing::trace!(
                target: "NETAUDIT",
                "OUTGOING GOSSIP [{}]  => {:17} ({:10}) : this -> {:?} [{}]",
                gossip_type_char,
                msg.2
                    .variant_type()
                    .to_string()
                    .replace("ShardedGossipWire::", ""),
                msg.2.encode_vec().expect("can't encode msg").len(),
                remote_url,
                self.gossip
                    .inner
                    .share_mut(|s, _| Ok(s.round_map.current_rounds().len()))
                    .unwrap(),
            );
        }

        if let Some((con, remote_url, msg, bytes)) = incoming {
            self.bandwidth.incoming_bytes(bytes).await;
            let variant_type = msg
                .variant_type()
                .to_string()
                .replace("ShardedGossipWire::", "");
            let len = msg.encode_vec().expect("can't encode msg").len();
            let outgoing = match self
                .gossip
                .process_incoming(con.peer_id(), msg, agent_info_session)
                .await
            {
                Ok(r) => {
                    tracing::trace!(
                        target: "NETAUDIT",
                        "INCOMING GOSSIP [{}] <=  {:17} ({:10}) : {:?} -> this [{}]",
                        gossip_type_char,
                        variant_type,
                        len,
                        remote_url,
                        self.gossip
                            .inner
                            .share_mut(|s, _| Ok(s.round_map.current_rounds().len()))
                            .unwrap(),
                    );
                    r
                }
                Err(e) => {
                    tracing::error!("FAILED to process incoming gossip {:?}", e);
                    self.gossip.remove_state(&con.peer_id(), true)?;
                    vec![ShardedGossipWire::error(e.to_string())]
                }
            };
            self.state.share_mut(|i, _| {
                i.push_outgoing(outgoing.into_iter().map(|msg| {
                    (
                        con.peer_id(),
                        HowToConnect::Con(con.clone(), remote_url.to_string()),
                        msg,
                    )
                }));
                Ok(())
            })?;
        }
        if let Some(outgoing) = outgoing {
            let cert = outgoing.0.clone();
            if let Err(err) = self.process_outgoing(outgoing).await {
                self.gossip.remove_state(&cert, true)?;

                // TODO: track all connection attempts, if all of them fail within a certain period of time, then log as error
                tracing::warn!(
                    "Gossip failed to send outgoing message because of: {:?}",
                    err
                );
            }
        }

        Ok(())
    }

    async fn run_one_iteration(&self, agent_info_session: &mut AgentInfoSession) {
        match self.gossip.try_initiate(agent_info_session).await {
            Ok(Some(outgoing)) => {
                if let Err(err) = self.state.share_mut(|i, _| {
                    i.push_outgoing([outgoing]);
                    Ok(())
                }) {
                    tracing::error!(
                        "Gossip failed to get share mut when trying to initiate with {:?}",
                        err
                    );
                }
            }
            Ok(None) => (),
            Err(err) => tracing::error!("Gossip failed when trying to initiate with {:?}", err),
        }
        if let Err(err) = self.process_incoming_outgoing(agent_info_session).await {
            tracing::error!("Gossip failed to process a message because of: {:?}", err);
        }
        self.gossip.record_timeouts();
    }

    fn pop_queues(&self) -> KitsuneResult<(Option<Incoming>, Option<Outgoing>)> {
        self.state.share_mut(move |inner, _| Ok(inner.pop()))
    }

    /// Log the statistics for the gossip loop.
    fn stats(&self, stats: &mut Stats) {
        if let Some(last) = stats.last {
            let elapsed = last.elapsed();
            stats.avg_processing_time += elapsed;
            stats.max_processing_time = std::cmp::max(stats.max_processing_time, elapsed);
        }
        stats.last = Some(tokio::time::Instant::now());
        stats.count += 1;
        let elapsed = stats.start.elapsed();
        if elapsed.as_secs() > 5 {
            stats.avg_processing_time = stats
                .avg_processing_time
                .checked_div(stats.count)
                .unwrap_or_default();
            let lens = self
                .state
                .share_mut(|i, _| Ok((i.incoming.len(), i.outgoing.len())))
                .map(|(i, o)| format!("Queues: Incoming: {}, Outgoing {}", i, o))
                .unwrap_or_else(|_| "Queues empty".to_string());
            let _ = self.gossip.inner.share_mut(|i, _| {
                    let s = tracing::trace_span!("gossip_metrics", gossip_type = %self.gossip.gossip_type);
                    s.in_scope(|| tracing::trace!(
                        "{}\nStats over last 5s:\n\tAverage processing time {:?}\n\tIteration count: {}\n\tMax gossip processing time: {:?}\n\t{}",
                        i.metrics,
                        stats.avg_processing_time,
                        stats.count,
                        stats.max_processing_time,
                        lens
                    ));
                    Ok(())
                });
            *stats = Stats::reset();
        }
    }
}

/// The parts of sharded gossip which are concerned only with the gossiping node:
/// - managing local state
/// - making requests to the local backend
/// - processing incoming messages to produce outgoing messages (which actually)
///     get sent by the enclosing `ShardedGossip`
pub struct ShardedGossipLocal {
    gossip_type: GossipType,
    tuning_params: KitsuneP2pTuningParams,
    space: Arc<KitsuneSpace>,
    host_api: HostApiLegacy,
    inner: Share<ShardedGossipLocalState>,
    closing: AtomicBool,
    fetch_pool: FetchPool,
}

/// Incoming gossip.
type Incoming = (MetaNetCon, String, ShardedGossipWire, usize);
/// Outgoing gossip.
type Outgoing = (NodeCert, HowToConnect, ShardedGossipWire);

/// A peer (from the perspective of any other node) is uniquely identified by its Cert
pub type NodeId = NodeCert;

/// Info associated with an outgoing gossip target
#[derive(Debug)]
pub(crate) struct ShardedGossipTarget {
    pub(crate) remote_agent_list: Vec<AgentInfoSigned>,
    pub(crate) cert: NodeCert,
    pub(crate) tie_break: u32,
    pub(crate) when_initiated: Option<tokio::time::Instant>,
    #[allow(dead_code)]
    pub(crate) url: TxUrl,
}

/// The internal mutable state for [`ShardedGossipLocal`]
#[derive(Default)]
pub struct ShardedGossipLocalState {
    /// The list of agents on this node
    local_agents: HashSet<Arc<KitsuneAgent>>,
    /// If Some, we are in the process of trying to initiate gossip with this target.
    initiate_tgt: Option<ShardedGossipTarget>,
    round_map: RoundStateMap,
    /// Metrics that track remote node states and help guide
    /// the next node to gossip with.
    metrics: MetricsSync,
}

impl ShardedGossipLocalState {
    fn new(metrics: MetricsSync) -> Self {
        Self {
            metrics,
            ..Default::default()
        }
    }

    fn remove_state(
        &mut self,
        state_key: &NodeCert,
        gossip_type: GossipType,
        error: bool,
    ) -> Option<RoundState> {
        // Check if the round to be removed matches the current initiate_tgt
        let init_tgt = self
            .initiate_tgt
            .as_ref()
            .map(|tgt| &tgt.cert == state_key)
            .unwrap_or(false);
        let remote_agent_list = if init_tgt {
            let initiate_tgt = self.initiate_tgt.take().unwrap();
            initiate_tgt.remote_agent_list
        } else {
            vec![]
        };
        let r = self.round_map.remove(state_key);
        let mut metrics = self.metrics.write();
        if let Some(r) = &r {
            if error {
                metrics.record_error(&r.remote_agent_list, gossip_type.into());
            } else {
                metrics.record_success(&r.remote_agent_list, gossip_type.into());
            }
        } else if init_tgt && error {
            metrics.record_error(&remote_agent_list, gossip_type.into());
        }

        metrics.complete_current_round(state_key, error);
        r
    }

    fn check_tgt_expired(&mut self, gossip_type: GossipType, round_timeout: Duration) {
        if let Some((remote_agent_list, cert, when_initiated)) = self
            .initiate_tgt
            .as_ref()
            .map(|tgt| (&tgt.remote_agent_list, tgt.cert.clone(), tgt.when_initiated))
        {
            // Check if no current round exists and we've timed out the initiate.
            let no_current_round_exist = !self.round_map.round_exists(&cert);
            match when_initiated {
                Some(when_initiated)
                    if no_current_round_exist && when_initiated.elapsed() > round_timeout =>
                {
                    tracing::warn!(
                        "Peer node timed out its gossip round. Cert: {:?}, Local agents: {:?}, Remote agents: {:?}",
                        cert,
                        self.local_agents,
                        remote_agent_list
                            .iter()
                            .map(|i| i.agent())
                            .collect::<Vec<_>>()
                    );
                    {
                        let mut metrics = self.metrics.write();
                        metrics.complete_current_round(&cert, true);
                        metrics.record_error(remote_agent_list, gossip_type.into());
                    }
                    self.initiate_tgt = None;
                }
                None if no_current_round_exist => {
                    {
                        let mut metrics = self.metrics.write();
                        metrics.complete_current_round(&cert, true);
                    }
                    self.initiate_tgt = None;
                }
                _ => (),
            }
        }
    }

    fn new_integrated_data(&mut self) -> KitsuneResult<()> {
        let s = tracing::trace_span!("gossip_trigger", agents = ?self.show_local_agents());
        s.in_scope(|| self.log_state());
        self.metrics.write().record_force_initiate();
        Ok(())
    }

    fn show_local_agents(&self) -> &HashSet<Arc<KitsuneAgent>> {
        &self.local_agents
    }

    pub(crate) fn log_state(&self) {
        tracing::trace!(
            ?self.round_map,
            ?self.initiate_tgt,
        )
    }
}

/// The incoming and outgoing queues for [`ShardedGossip`]
#[derive(Default, Clone, Debug)]
pub struct ShardedGossipQueues {
    incoming: VecDeque<Incoming>,
    outgoing: VecDeque<Outgoing>,
}

/// The internal mutable state for [`ShardedGossip`]
#[derive(Default, derive_more::Deref)]
pub(crate) struct ShardedGossipState {
    /// The incoming and outgoing queues
    #[deref]
    queues: ShardedGossipQueues,
    /// If Some, these queues are never cleared, and contain every message
    /// ever sent and received, for diagnostics and debugging.
    history: Option<ShardedGossipQueues>,
}

impl ShardedGossipState {
    /// Construct state with history queues
    #[cfg(feature = "test_utils")]
    #[allow(dead_code)]
    pub fn with_history() -> Self {
        Self {
            queues: Default::default(),
            history: Some(Default::default()),
        }
    }

    #[cfg(feature = "test_utils")]
    #[allow(dead_code)]
    pub fn get_history(&self) -> Option<ShardedGossipQueues> {
        self.history.clone()
    }

    pub fn push_incoming<I: Clone + IntoIterator<Item = Incoming>>(&mut self, incoming: I) {
        if let Some(history) = &mut self.history {
            history.incoming.extend(incoming.clone());
        }
        self.queues.incoming.extend(incoming);
    }

    pub fn push_outgoing<I: Clone + IntoIterator<Item = Outgoing>>(&mut self, outgoing: I) {
        if let Some(history) = &mut self.history {
            history.outgoing.extend(outgoing.clone());
        }
        self.queues.outgoing.extend(outgoing);
    }

    pub fn pop(&mut self) -> (Option<Incoming>, Option<Outgoing>) {
        (
            self.queues.incoming.pop_front(),
            self.queues.outgoing.pop_front(),
        )
    }
}

/// The state representing a single active ongoing "round" of gossip with a
/// remote node
#[derive(Debug, Clone)]
pub struct RoundState {
    /// The remote agents hosted by the remote node, used for metrics tracking
    pub(crate) remote_agent_list: Vec<AgentInfoSigned>,
    /// The common ground with our gossip partner for the purposes of this round
    common_arq_set: Arc<ArqSet>,
    /// We've received the last op bloom filter from our partner
    /// (the one with `finished` == true)
    received_all_incoming_op_blooms: bool,
    /// If historic gossip, we calculated and queued our region diff (will be true for Recent)
    regions_are_queued: bool,
    /// Number of ops blooms we have sent for this round, which is also the
    /// number of MissingOpHashes sets we expect in response
    num_expected_op_blooms: u16,
    /// Received all responses to OpRegions, which is the batched set of Op data
    /// in the diff of regions
    has_pending_historical_op_data: bool,
    /// There are still op blooms to send because the previous
    /// batch was too big to send in a single gossip iteration.
    bloom_batch_cursor: Option<Timestamp>,
    /// Missing op hashes that have been batched for
    /// future processing.
    ops_batch_queue: OpsBatchQueue,
    /// Last moment we had any contact for this round.
    last_touch: Instant,
    /// Amount of time before a round is considered expired.
    round_timeout: std::time::Duration,
    /// The RegionSet we will send to our gossip partner during Historical
    /// gossip (will be None for Recent).
    region_set_sent: Option<Arc<RegionSetLtcs>>,
    /// Region diffs, if doing Historical gossip
    pub(crate) region_diffs: RegionDiffs,
    /// Unique string ID for this round
    pub(crate) id: String,
}

/// Our region diff and their region diff
pub type RegionDiffs = Option<(Vec<Region>, Vec<Region>)>;

impl RoundState {
    /// Constructor
    pub fn new(
        remote_agent_list: Vec<AgentInfoSigned>,
        common_arq_set: Arc<ArqSet>,
        region_set_sent: Option<Arc<RegionSetLtcs<RegionData>>>,
        round_timeout: Duration,
    ) -> Self {
        RoundState {
            remote_agent_list,
            common_arq_set,
            received_all_incoming_op_blooms: false,
            has_pending_historical_op_data: false,
            regions_are_queued: false,
            bloom_batch_cursor: None,
            num_expected_op_blooms: 0,
            ops_batch_queue: OpsBatchQueue::new(),
            id: nanoid::nanoid!(),
            last_touch: Instant::now(),
            round_timeout,
            region_set_sent,
            region_diffs: Default::default(),
        }
    }

    /// Get the common arcs as continuous arcs
    pub fn common_arc_set(&self) -> Arc<DhtArcSet> {
        Arc::new(self.common_arq_set.to_dht_arc_set_std())
    }
}

/// Incoming and outgoing throughput
#[derive(
    Debug, Clone, Default, PartialEq, Eq, derive_more::Add, serde::Serialize, serde::Deserialize,
)]
pub struct InOut {
    /// Incoming throughput
    pub incoming: u32,
    /// Outgoing throughput
    pub outgoing: u32,
}

impl ShardedGossipLocal {
    const TGT_FP: f64 = 0.01;
    /// This should give us just under 1.6MB for the bloom filter.
    /// Based on a compression of 75%.
    const UPPER_HASHES_BOUND: usize = 20_000;

    /// The number of bloom filters we want to send in a single gossip iteration.
    const UPPER_BLOOM_BOUND: usize = 10;

    /// Calculate the time range for a gossip round.
    fn calculate_time_range(&self) -> TimeWindow {
        const NOW: Duration = Duration::from_secs(0);
        let threshold = Duration::from_secs(self.tuning_params.danger_gossip_recent_threshold_secs);
        match self.gossip_type {
            GossipType::Recent => time_range(threshold, NOW),
            GossipType::Historical => {
                let one_hour_ago = std::time::UNIX_EPOCH
                    .elapsed()
                    .expect("Your clock is set before unix epoch")
                    - threshold;
                Timestamp::from_micros(0)
                    ..Timestamp::from_micros(
                        one_hour_ago
                            .as_micros()
                            .try_into()
                            .expect("Epoch micro seconds has overflowed"),
                    )
            }
        }
    }

    fn new_state(
        &self,
        remote_agent_list: Vec<AgentInfoSigned>,
        common_arq_set: Arc<ArqSet>,
        region_set_sent: Option<RegionSetLtcs>,
        round_timeout: Duration,
    ) -> KitsuneResult<RoundState> {
        Ok(RoundState::new(
            remote_agent_list,
            common_arq_set,
            region_set_sent.map(Arc::new),
            round_timeout,
        ))
    }

    fn get_state(&self, id: &NodeCert) -> KitsuneResult<Option<RoundState>> {
        self.inner
            .share_mut(|i, _| Ok(i.round_map.get(id).cloned()))
    }

    fn remove_state(&self, id: &NodeCert, error: bool) -> KitsuneResult<Option<RoundState>> {
        self.inner
            .share_mut(|i, _| Ok(i.remove_state(id, self.gossip_type, error)))
    }

    fn remove_target(&self, id: &NodeCert, error: bool) -> KitsuneResult<()> {
        self.inner.share_mut(|i, _| {
            if i.initiate_tgt
                .as_ref()
                .map(|tgt| &tgt.cert == id)
                .unwrap_or(false)
            {
                let initiate_tgt = i.initiate_tgt.take().unwrap();
                if error {
                    i.metrics
                        .write()
                        .record_error(&initiate_tgt.remote_agent_list, self.gossip_type.into());
                }
            }
            Ok(())
        })
    }

    /// If the round is still active then update the state.
    fn update_state_if_active(&self, key: NodeCert, state: RoundState) -> KitsuneResult<()> {
        self.inner.share_mut(|i, _| {
            if i.round_map.round_exists(&key) {
                if state.is_finished() {
                    i.remove_state(&key, self.gossip_type, false);
                } else {
                    i.round_map.insert(key, state);
                }
            }
            Ok(())
        })
    }

    fn incoming_op_blooms_finished(
        &self,
        state_id: &NodeCert,
    ) -> KitsuneResult<Option<RoundState>> {
        self.inner.share_mut(|i, _| {
            let finished = i
                .round_map
                .get_mut(state_id)
                .map(|state| {
                    state.received_all_incoming_op_blooms = true;
                    state.is_finished()
                })
                .unwrap_or(true);
            if finished {
                Ok(i.remove_state(state_id, self.gossip_type, false))
            } else {
                Ok(i.round_map.get(state_id).cloned())
            }
        })
    }

    fn decrement_op_blooms(&self, state_id: &NodeCert) -> KitsuneResult<Option<RoundState>> {
        self.inner.share_mut(|i, _| {
            let remove_state = |state: &mut RoundState| {
                let num_op_blooms = state.num_expected_op_blooms.saturating_sub(1);
                state.num_expected_op_blooms = num_op_blooms;
                // NOTE: there is only ever one "batch" of OpRegions
                state.has_pending_historical_op_data = false;
                state.is_finished()
            };
            if i.round_map
                .get_mut(state_id)
                .map(remove_state)
                .unwrap_or(true)
            {
                Ok(i.remove_state(state_id, self.gossip_type, false))
            } else {
                Ok(i.round_map.get(state_id).cloned())
            }
        })
    }

    async fn process_incoming(
        &self,
        peer_cert: NodeCert,
        msg: ShardedGossipWire,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        let s = match self.gossip_type {
            GossipType::Recent => {
                let s = tracing::trace_span!("process_incoming_recent", ?peer_cert, agents = ?self.show_local_agents(), ?msg);
                s.in_scope(|| self.log_state());
                s
            }
            GossipType::Historical => match &msg {
                ShardedGossipWire::MissingOpHashes(MissingOpHashes { ops, finished }) => {
                    let s = tracing::trace_span!("process_incoming_historical", ?peer_cert, agents = ?self.show_local_agents(), msg = %"MissingOpHashes", num_ops = %ops.len(), ?finished);
                    s.in_scope(|| self.log_state());
                    s
                }
                _ => {
                    let s = tracing::trace_span!("process_incoming_historical", ?peer_cert, agents = ?self.show_local_agents(), ?msg);
                    s.in_scope(|| self.log_state());
                    s
                }
            },
        };

        // If we don't have the state for a message then the other node will need to timeout.
        let r = match msg {
            ShardedGossipWire::Initiate(Initiate {
                intervals,
                id,
                agent_list,
            }) => {
                self.incoming_initiate(peer_cert, intervals, id, agent_list, agent_info_session)
                    .await?
            }
            ShardedGossipWire::Accept(Accept {
                intervals,
                agent_list,
            }) => {
                self.incoming_accept(peer_cert, intervals, agent_list, agent_info_session)
                    .await?
            }
            ShardedGossipWire::Agents(Agents { filter }) => {
                if let Some(state) = self.get_state(&peer_cert)? {
                    let filter = decode_bloom_filter(&filter);
                    self.incoming_agents(state, filter, agent_info_session)
                        .await?
                } else {
                    Vec::with_capacity(0)
                }
            }
            ShardedGossipWire::MissingAgents(MissingAgents { agents }) => {
                if self.get_state(&peer_cert)?.is_some() {
                    self.incoming_missing_agents(agents.as_slice()).await?;
                }
                Vec::with_capacity(0)
            }
            ShardedGossipWire::OpBloom(OpBloom {
                missing_hashes,
                finished,
            }) => {
                let state = if finished {
                    self.incoming_op_blooms_finished(&peer_cert)?
                } else {
                    self.get_state(&peer_cert)?
                };
                match state {
                    Some(state) => match missing_hashes {
                        EncodedTimedBloomFilter::NoOverlap => Vec::with_capacity(0),
                        EncodedTimedBloomFilter::MissingAllHashes { time_window } => {
                            let filter = TimedBloomFilter {
                                bloom: None,
                                time: time_window,
                            };
                            self.incoming_op_bloom(state, filter, None).await?
                        }
                        EncodedTimedBloomFilter::HaveHashes {
                            filter,
                            time_window,
                        } => {
                            let filter = TimedBloomFilter {
                                bloom: Some(decode_bloom_filter(&filter)),
                                time: time_window,
                            };
                            self.incoming_op_bloom(state, filter, None).await?
                        }
                    },
                    None => Vec::with_capacity(0),
                }
            }
            ShardedGossipWire::OpRegions(OpRegions { region_set }) => {
                if let Some(state) = self.incoming_op_blooms_finished(&peer_cert)? {
                    self.queue_incoming_regions(&peer_cert, state, region_set)
                        .await?
                } else {
                    vec![]
                }
            }
            ShardedGossipWire::MissingOpHashes(MissingOpHashes { ops, finished }) => {
                let mut gossip = Vec::with_capacity(0);
                let finished = MissingOpsStatus::try_from(finished)?;

                let state = match finished {
                    // This is a single chunk of ops. No need to reply.
                    MissingOpsStatus::ChunkComplete => self.get_state(&peer_cert)?,
                    // This is the last chunk in the batch. Reply with [`OpBatchReceived`]
                    // to get the next batch of missing ops.
                    MissingOpsStatus::BatchComplete => {
                        gossip = vec![ShardedGossipWire::op_batch_received()];
                        self.get_state(&peer_cert)?
                    }
                    // All the batches of missing ops for the bloom this node sent
                    // to the remote node have been sent back to this node.
                    MissingOpsStatus::AllComplete => {
                        // This node can decrement the number of outstanding ops bloom replies
                        // it is waiting for.
                        let mut state = self.decrement_op_blooms(&peer_cert)?;

                        // If there are more blooms to send because this node had to batch the blooms
                        // and all the outstanding blooms have been received then this node will send
                        // the next batch of ops blooms starting from the saved cursor.
                        if let Some(state) = state.as_mut().filter(|s| {
                            s.bloom_batch_cursor.is_some() && s.num_expected_op_blooms == 0
                        }) {
                            // We will be producing some gossip so we need to allocate.
                            gossip = Vec::new();
                            // Generate the next ops blooms batch.
                            *state = self.next_bloom_batch(state.clone(), &mut gossip).await?;
                            // Update the state.
                            self.update_state_if_active(peer_cert.clone(), state.clone())?;
                        }
                        state
                    }
                };

                // TODO: come back to this later after implementing batching for
                //      region gossip, for now I just don't care about the state,
                //      and just want to handle the incoming ops.
                if (self.gossip_type == GossipType::Historical || state.is_some())
                    && !ops.is_empty()
                {
                    if let Some(state) = state.as_ref() {
                        // NOTE: we could probably make a better choice than "any arbitrary remote agent".
                        //       ostensibly, only a subset of remote agents are holding the data we're asking for,
                        //       and the "source" should at least be one of those. Though, the notion of a
                        //       particular agent being the source is hazy at best, and maybe this is one of those
                        //       cases where any agent on a remote node is a valid "reference" to that node.
                        if let Some(agent) = state.remote_agent_list.first() {
                            // there is at least 1 agent
                            let agent = agent.agent.clone();
                            let source = FetchSource::Agent(agent);
                            self.incoming_missing_op_hashes(
                                source,
                                ops,
                                TransferMethod::Gossip(self.gossip_type),
                            )
                            .await?;
                        } else {
                            tracing::warn!(
                                "Op hashes were received for a round with no remote agent(s). {} ops dropped!",
                                ops.len()
                            );
                        }
                    } else {
                        tracing::warn!(
                            "Op hashes were received after a round was dropped. {} ops dropped!",
                            ops.len()
                        );
                    }
                }
                gossip
            }
            ShardedGossipWire::OpBatchReceived(_) => match self.get_state(&peer_cert)? {
                Some(state) => {
                    // The last ops batch has been received by the
                    // remote node so now send the next batch.
                    let r = self.next_missing_ops_batch(state.clone()).await?;
                    if state.is_finished() {
                        self.remove_state(&peer_cert, false)?;
                    }
                    r
                }
                None => Vec::with_capacity(0),
            },
            ShardedGossipWire::NoAgents(_) => {
                tracing::warn!("No agents to gossip with on the node {:?}", peer_cert);
                self.remove_state(&peer_cert, true)?;
                Vec::with_capacity(0)
            }
            ShardedGossipWire::AlreadyInProgress(_) => {
                self.remove_target(&peer_cert, false)?;
                Vec::with_capacity(0)
            }
            ShardedGossipWire::Busy(_) => {
                tracing::warn!("The node {:?} is busy", peer_cert);
                self.remove_target(&peer_cert, true)?;
                Vec::with_capacity(0)
            }
            ShardedGossipWire::Error(Error { message }) => {
                tracing::warn!("gossiping with: {:?} and got error: {}", peer_cert, message);
                self.remove_state(&peer_cert, true)?;
                Vec::with_capacity(0)
            }
        };
        s.in_scope(|| {
            let ops_s = r
                .iter()
                .map(|g| match &g {
                    ShardedGossipWire::MissingOpHashes(MissingOpHashes { ops, finished }) => {
                        format!("num_ops = {}, finished = {}", ops.len(), finished)
                    }
                    _ => {
                        format!("{:?}", g)
                    }
                })
                .collect::<String>();
            tracing::trace!(%ops_s);
            self.log_state()
        });
        Ok(r)
    }

    /// Record all timed out rounds into metrics
    fn record_timeouts(&self) {
        self.inner
            .share_mut(|i, _| {
                for (cert, ref r) in i.round_map.take_timed_out_rounds() {
                    tracing::warn!("The node {:?} has timed out its gossip round", cert);
                    let mut metrics = i.metrics.write();
                    metrics.record_error(&r.remote_agent_list, self.gossip_type.into());
                    metrics.complete_current_round(&cert, true);
                }
                Ok(())
            })
            .ok();
    }

    fn show_local_agents(&self) -> HashSet<Arc<KitsuneAgent>> {
        self.inner
            .share_mut(|i, _| Ok(i.local_agents.clone()))
            .unwrap_or_default()
    }

    fn log_state(&self) {
        self.inner
            .share_mut(|i, _| {
                i.log_state();
                Ok(())
            })
            .ok();
    }
}

impl RoundState {
    fn increment_expected_op_blooms(&mut self) -> u16 {
        self.num_expected_op_blooms += 1;
        self.num_expected_op_blooms
    }

    /// A round is finished if:
    /// - There are no blooms sent to the remote node that are awaiting responses.
    /// - This node has received all the ops blooms from the remote node.
    /// - This node has no saved ops bloom batch cursor.
    /// - This node has no queued missing ops to send to the remote node.
    /// - If running historical gossip, the number of ops sent/received matches expectations
    fn is_finished(&self) -> bool {
        self.num_expected_op_blooms == 0
            && !self.has_pending_historical_op_data
            && self.received_all_incoming_op_blooms
            && self.regions_are_queued
            && self.bloom_batch_cursor.is_none()
            && self.ops_batch_queue.is_empty()
    }
}

/// Time range from now into the past.
/// Start must be < end.
fn time_range(start: Duration, end: Duration) -> TimeWindow {
    // TODO: write in terms of chrono::now()
    let now = SystemTime::now();
    let start = now
        .checked_sub(start)
        .and_then(|t| t.duration_since(SystemTime::UNIX_EPOCH).ok())
        .map(|t| Timestamp::from_micros(t.as_micros() as i64))
        .unwrap_or(Timestamp::MIN);

    let end = now
        .checked_sub(end)
        .and_then(|t| t.duration_since(SystemTime::UNIX_EPOCH).ok())
        .map(|t| Timestamp::from_micros(t.as_micros() as i64))
        .unwrap_or(Timestamp::MAX);

    start..end
}

/// An encoded timed bloom filter of missing op hashes.
#[derive(serde::Serialize, serde::Deserialize, Debug, Clone, PartialEq, Eq)]
pub enum EncodedTimedBloomFilter {
    /// I have no overlap with your agents
    /// Please don't send any ops.
    NoOverlap,
    /// I have overlap and I have no hashes.
    /// Please send all your ops.
    MissingAllHashes {
        /// The time window that we are missing hashes for.
        time_window: TimeWindow,
    },
    /// I have overlap and I have some hashes.
    /// Please send any missing ops.
    HaveHashes {
        /// The encoded bloom filter.
        filter: PoolBuf,
        /// The time window these hashes are for.
        time_window: TimeWindow,
    },
}

impl EncodedTimedBloomFilter {
    /// Get the size in bytes of the bloom filter, if one exists
    pub fn size(&self) -> usize {
        match self {
            Self::HaveHashes { filter, .. } => filter.len(),
            _ => 0,
        }
    }
}

#[derive(Debug, Clone, Copy)]
/// The possible states when receiving missing ops.
/// Note this is not sent over the wire and is instead
/// converted to a u8 to save bandwidth.
pub enum MissingOpsStatus {
    /// There are more chunks in this batch to come. No reply is needed.
    ChunkComplete = 0,
    /// This chunk is done but there are more batches
    /// to come and you should reply with [`OpBatchReceived`]
    /// when you are ready to get the next batch.
    BatchComplete = 1,
    /// This is the final batch of missing ops and there
    /// are no more ops to come. No reply is needed.
    AllComplete = 2,
}

kitsune_p2p_types::write_codec_enum! {
    /// ShardedGossip Wire Protocol Codec
    codec ShardedGossipWire {
        /// Initiate a round of gossip with a remote node
        Initiate(0x10) {
            /// The list of arc intervals (equivalent to a [`DhtArcSet`])
            /// for all local agents
            intervals.0: Vec<ArqBounds>,
            /// A random number to resolve concurrent initiates.
            id.1: u32,
            /// List of active local agents represented by this node.
            agent_list.2: Vec<AgentInfoSigned>,
        },

        /// Accept an incoming round of gossip from a remote node
        Accept(0x20) {
            /// The list of arc intervals (equivalent to a [`DhtArcSet`])
            /// for all local agents
            intervals.0: Vec<ArqBounds>,
            /// List of active local agents represented by this node.
            agent_list.1: Vec<AgentInfoSigned>,
        },

        /// Send Agent Info Bloom
        Agents(0x30) {
            /// The bloom filter for agent data
            filter.0: PoolBuf,
        },

        /// Any agents that were missing from the remote bloom.
        MissingAgents(0x40) {
            /// The missing agents
            agents.0: Vec<Arc<AgentInfoSigned>>,
        },

        /// Send Op Bloom filter
        OpBloom(0x50) {
            /// The bloom filter for op data
            missing_hashes.0: EncodedTimedBloomFilter,
            /// Is this the last bloom to be sent?
            finished.1: bool,
        },

        /// Send Op region hashes
        OpRegions(0x51) {
            /// The region hashes for all common ops
            region_set.0: RegionSetLtcs,
        },

        /// Any ops that were missing from the remote bloom.
        MissingOpHashes(0x60) {
            /// The missing op hashes
            ops.0: Vec<OpHashSized>,
            /// Ops that are missing from a bloom that you have sent.
            /// These will be chunked into a maximum size of about 16MB.
            /// If the amount of missing ops is larger then the
            /// [`ShardedGossipLocal::UPPER_BATCH_BOUND`] then the set of
            /// missing ops chunks will be sent in batches.
            /// Each batch will require a reply message of [`OpBatchReceived`]
            /// in order to get the next batch.
            /// This is to prevent overloading the receiver with too much
            /// incoming data.
            ///
            /// 0: There is more chunks in this batch to come. No reply is needed.
            /// 1: This chunk is done but there is more batches
            /// to come and you should reply with [`OpBatchReceived`]
            /// when you are ready to get the next batch.
            /// 2: This is the final missing ops and there
            /// are no more ops to come. No reply is needed.
            ///
            /// See [`MissingOpsStatus`]
            finished.1: u8,
        },

        /// I have received a complete batch of
        /// missing ops and I am ready to receive the
        /// next batch.
        OpBatchReceived(0x61) {
        },


        /// The node you are gossiping with has hit an error condition
        /// and failed to respond to a request.
        Error(0xa0) {
            /// The error message.
            message.0: String,
        },

        /// The node currently is gossiping with too many
        /// other nodes and is too busy to accept your initiate.
        /// Please try again later.
        Busy(0xa1) {
        },

        /// The node you are trying to gossip with has no agents anymore.
        NoAgents(0xa2) {
        },

        /// You have sent a stale initiate to a node
        /// that already has an active round with you.
        AlreadyInProgress(0xa3) {
        },
    }
}

impl AsGossipModule for ShardedGossip {
    fn incoming_gossip(
        &self,
        con: MetaNetCon,
        remote_url: String,
        gossip_data: Box<[u8]>,
    ) -> KitsuneResult<()> {
        use kitsune_p2p_types::codec::*;
        let (bytes, gossip) =
            ShardedGossipWire::decode_ref(&gossip_data).map_err(KitsuneError::other)?;
        let new_initiate = matches!(gossip, ShardedGossipWire::Initiate(_));
        self.state.share_mut(move |i, _| {
            let overloaded = i.incoming.len() > 20;
            if overloaded {
                tracing::warn!(
                    "Overloaded with incoming gossip.. {} messages",
                    i.incoming.len()
                );
            }
            // If we are overloaded then return busy to any new initiates.
            if overloaded && new_initiate {
                i.push_outgoing([(
                    con.peer_id(),
                    HowToConnect::Con(con, remote_url),
                    ShardedGossipWire::busy(),
                )]);
            } else {
                i.push_incoming([(con, remote_url, gossip, bytes as usize)]);
            }
            Ok(())
        })
    }

    fn local_agent_join(&self, a: Arc<KitsuneAgent>) {
        let _ = self.gossip.inner.share_mut(move |i, _| {
            i.new_integrated_data()?;
            i.local_agents.insert(a);
            let s = tracing::trace_span!("gossip_trigger", agents = ?i.show_local_agents(), msg = "New agent joining");
            s.in_scope(|| i.log_state());
            Ok(())
        });
    }

    fn local_agent_leave(&self, a: Arc<KitsuneAgent>) {
        let _ = self.gossip.inner.share_mut(move |i, _| {
            i.local_agents.remove(&a);
            Ok(())
        });
    }

    fn close(&self) {
        self.gossip
            .closing
            .store(true, std::sync::atomic::Ordering::Relaxed);
    }

    fn new_integrated_data(&self) {
        let _ = self.gossip.inner.share_mut(move |i, _| {
            i.new_integrated_data()?;
            let s = tracing::trace_span!("gossip_trigger", agents = ?i.show_local_agents(), msg = "New integrated data");
            s.in_scope(|| i.log_state());
            Ok(())
        });
    }
}

struct ShardedRecentGossipFactory {
    bandwidth: Arc<BandwidthThrottle>,
}

impl ShardedRecentGossipFactory {
    fn new(bandwidth: Arc<BandwidthThrottle>) -> Self {
        Self { bandwidth }
    }
}

impl AsGossipModuleFactory for ShardedRecentGossipFactory {
    fn spawn_gossip_task(
        &self,
        config: Arc<KitsuneP2pConfig>,
        space: Arc<KitsuneSpace>,
        ep_hnd: MetaNet,
        host: HostApiLegacy,
        metrics: MetricsSync,
        fetch_pool: FetchPool,
    ) -> GossipModule {
        GossipModule(ShardedGossip::new(
            config,
            space,
            ep_hnd,
            host,
            GossipType::Recent,
            self.bandwidth.clone(),
            metrics,
            fetch_pool,
            #[cfg(test)]
            false,
        ))
    }
}

struct ShardedHistoricalGossipFactory {
    bandwidth: Arc<BandwidthThrottle>,
}

impl ShardedHistoricalGossipFactory {
    fn new(bandwidth: Arc<BandwidthThrottle>) -> Self {
        Self { bandwidth }
    }
}

impl AsGossipModuleFactory for ShardedHistoricalGossipFactory {
    fn spawn_gossip_task(
        &self,
        config: Arc<KitsuneP2pConfig>,
        space: Arc<KitsuneSpace>,
        ep_hnd: MetaNet,
        host: HostApiLegacy,
        metrics: MetricsSync,
        fetch_pool: FetchPool,
    ) -> GossipModule {
        GossipModule(ShardedGossip::new(
            config,
            space,
            ep_hnd,
            host,
            GossipType::Historical,
            self.bandwidth.clone(),
            metrics,
            fetch_pool,
            #[cfg(test)]
            false,
        ))
    }
}

/// Create a recent `GossipModuleFactory`
pub fn recent_factory(bandwidth: Arc<BandwidthThrottle>) -> GossipModuleFactory {
    GossipModuleFactory(Arc::new(ShardedRecentGossipFactory::new(bandwidth)))
}

/// Create a historical `GossipModuleFactory`
pub fn historical_factory(bandwidth: Arc<BandwidthThrottle>) -> GossipModuleFactory {
    GossipModuleFactory(Arc::new(ShardedHistoricalGossipFactory::new(bandwidth)))
}

#[allow(dead_code)]
fn clamp64(u: u64) -> i64 {
    if u > i64::MAX as u64 {
        i64::MAX
    } else {
        u as i64
    }
}

impl From<GossipType> for GossipModuleType {
    fn from(g: GossipType) -> Self {
        match g {
            GossipType::Recent => GossipModuleType::ShardedRecent,
            GossipType::Historical => GossipModuleType::ShardedHistorical,
        }
    }
}

impl From<GossipModuleType> for GossipType {
    fn from(g: GossipModuleType) -> Self {
        match g {
            GossipModuleType::ShardedRecent => GossipType::Recent,
            GossipModuleType::ShardedHistorical => GossipType::Historical,
        }
    }
}

impl TryFrom<u8> for MissingOpsStatus {
    type Error = KitsuneError;

    fn try_from(value: u8) -> Result<Self, Self::Error> {
        let r = match value {
            0 => Self::ChunkComplete,
            1 => Self::BatchComplete,
            2 => Self::AllComplete,
            _ => return Err("Failed to parse u8 as MissingOpsStatus".into()),
        };
        debug_assert_eq!(value, r as u8);
        Ok(r)
    }
}

/// Data and handlers for diagnostic info, to be used by the host.
#[derive(Clone, Debug)]
pub struct KitsuneDiagnostics {
    /// Access to metrics info
    pub metrics: MetricsSync,
    /// Access to FetchPool,
    pub fetch_pool: FetchPoolReader,
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/accept.rs
================================================
use kitsune_p2p_types::dht::{spacetime::SpaceDimension, ArqBounds};

use super::*;

impl ShardedGossipLocal {
    /// Incoming accept gossip round message.
    /// - Send back the agent bloom and ops bloom gossip messages.
    /// - Only send the agent bloom if this is a recent gossip type.
    pub(super) async fn incoming_accept(
        &self,
        peer_cert: NodeCert,
        remote_arq_set: Vec<ArqBounds>,
        remote_agent_list: Vec<AgentInfoSigned>,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        let (local_agents, when_initiated, accept_is_from_target) =
            self.inner.share_mut(|i, _| {
                let accept_is_from_target = i
                    .initiate_tgt
                    .as_ref()
                    .map(|tgt| tgt.cert == peer_cert)
                    .unwrap_or(false);
                let when_initiated = i.initiate_tgt.as_ref().and_then(|i| i.when_initiated);
                Ok((
                    i.local_agents.clone(),
                    when_initiated,
                    accept_is_from_target,
                ))
            })?;

        if let Some(when_initiated) = when_initiated {
            let _ = self.inner.share_ref(|i| {
                i.metrics
                    .write()
                    .record_latency_micros(when_initiated.elapsed().as_micros(), &local_agents);
                Ok(())
            });
        }

        // This accept is not from our current target so ignore.
        if !accept_is_from_target {
            // The other node will have to timeout on this but nodes should
            // not be sending accepts to nodes that aren't targeting them.
            return Ok(vec![]);
        }

        // If we don't have a local agent then there's nothing to do.
        if local_agents.is_empty() {
            return Ok(vec![ShardedGossipWire::no_agents()]);
        }

        // Get the local intervals.
        let local_agent_arqs: Vec<_> = agent_info_session
            .local_agent_arqs()
            .into_iter()
            .filter_map(|(agent, arq)| {
                if local_agents.contains(&agent) {
                    Some(arq.to_bounds(SpaceDimension::standard()))
                } else {
                    None
                }
            })
            .collect();

        let mut gossip = Vec::new();

        // Generate the bloom filters and new state.
        let state = self
            .generate_blooms_or_regions(
                remote_agent_list.clone(),
                local_agent_arqs,
                remote_arq_set,
                &mut gossip,
                agent_info_session,
            )
            .await?;

        self.inner.share_mut(|inner, _| {
            // TODO: What happen if we are in the middle of a new outgoing and
            // a stale accept comes in for the same peer cert?
            // Maybe we need to check timestamps on messages or have unique round ids?

            let mut metrics = inner.metrics.write();
            metrics.update_current_round(&peer_cert, self.gossip_type.into(), &state);
            metrics.record_initiate(&remote_agent_list, self.gossip_type.into());

            inner.round_map.insert(peer_cert.clone(), state);
            Ok(())
        })?;
        Ok(gossip)
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/agents.rs
================================================
use super::*;

impl ShardedGossipLocal {
    /// Incoming agents bloom filter.
    /// - Check for any missing agents and send them back.
    pub(super) async fn incoming_agents(
        &self,
        state: RoundState,
        remote_bloom: BloomFilter,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        // Unpack this rounds state.
        let RoundState { common_arq_set, .. } = state;

        // Get all agents within common arc and filter out
        // the ones in the remote bloom.
        let missing: Vec<_> = agent_info_session
            .agent_info_within_arc_set(&self.host_api, &self.space, (*common_arq_set).clone())
            .await?
            .into_iter()
            .filter(|info| {
                // Check them against the bloom
                !remote_bloom.check(&MetaOpKey::Agent(info.agent.clone(), info.signed_at_ms))
            })
            .map(Arc::new)
            .collect();

        // Send any missing.
        Ok(if !missing.is_empty() {
            vec![ShardedGossipWire::missing_agents(missing)]
        } else {
            // It's ok if we don't respond to agent blooms because
            // rounds are ended by ops not agents.
            vec![]
        })
    }

    /// Incoming missing agents.
    /// - Add these agents to the peer store
    ///   for this space for agents that contain the
    ///   incoming agents within their arcs.
    pub(super) async fn incoming_missing_agents(
        &self,
        agents: &[Arc<AgentInfoSigned>],
    ) -> KitsuneResult<()> {
        // Add the agents to the stores.
        store::put_agent_info(&self.host_api, agents).await?;
        Ok(())
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/bandwidth.rs
================================================
use std::{
    num::NonZeroU32,
    sync::atomic::{AtomicU64, AtomicUsize},
};

use governor::{clock::Clock, Quota};

use super::*;

#[derive(Clone)]
/// Set of bandwidth throttles for all gossip loops.
pub struct BandwidthThrottles {
    recent: Arc<BandwidthThrottle>,
    historic: Arc<BandwidthThrottle>,
}

impl BandwidthThrottles {
    /// Create a new set of throttles from the configuration.
    pub fn new(tuning_params: &KitsuneP2pTuningParams) -> Self {
        let recent = BandwidthThrottle::new(
            tuning_params.gossip_inbound_target_mbps,
            tuning_params.gossip_outbound_target_mbps,
            tuning_params.gossip_burst_ratio,
        );
        let historic = BandwidthThrottle::new(
            tuning_params.gossip_historic_inbound_target_mbps,
            tuning_params.gossip_historic_outbound_target_mbps,
            tuning_params.gossip_burst_ratio,
        );
        Self {
            recent: Arc::new(recent),
            historic: Arc::new(historic),
        }
    }

    /// Get the throttle for the recent loop.
    pub fn recent(&self) -> Arc<BandwidthThrottle> {
        self.recent.clone()
    }

    /// Get the throttle for the historical loop.
    pub fn historical(&self) -> Arc<BandwidthThrottle> {
        self.historic.clone()
    }
}

/// Manages incoming and outgoing bandwidth by providing methods which
/// asynchronously wait for enough bandwidth to become available before
/// processing a chunk of bytes
pub struct BandwidthThrottle<C = DefaultClock>
where
    C: Clock,
{
    clock: C,
    inbound: Option<RateLimiter<NotKeyed, InMemoryState, C>>,
    outbound: Option<RateLimiter<NotKeyed, InMemoryState, C>>,
    start_time: Instant,
    bits_inbound: AtomicUsize,
    peak_inbound: AtomicUsize,
    bits_outbound: AtomicUsize,
    peak_outbound: AtomicUsize,
    last_inbound_time: AtomicU64,
    last_outbound_time: AtomicU64,
}

impl BandwidthThrottle {
    /// Set the inbound and outbound bandwidth limits in megabits per second.
    pub fn new(inbound_mbps: f64, outbound_mbps: f64, burst_ratio: f64) -> Self {
        Self::new_inner(
            inbound_mbps,
            outbound_mbps,
            burst_ratio,
            governor::clock::DefaultClock::default(),
        )
    }
}

#[cfg(test)]
impl BandwidthThrottle<governor::clock::FakeRelativeClock> {
    fn test(
        inbound_mbps: f64,
        outbound_mbps: f64,
        burst_ratio: f64,
        clock: governor::clock::FakeRelativeClock,
    ) -> Self {
        Self::new_inner(inbound_mbps, outbound_mbps, burst_ratio, clock)
    }
}

impl<C> BandwidthThrottle<C>
where
    C: Clock,
{
    fn new_inner(inbound_mbps: f64, outbound_mbps: f64, burst_ratio: f64, clock: C) -> Self {
        // Convert to bits per second.
        let inbound_bps = inbound_mbps * 1000.0 * 1000.0;
        let outbound_bps = outbound_mbps * 1000.0 * 1000.0;

        let inbound = NonZeroU32::new(inbound_bps as u32).map(|bps| {
            let burst = NonZeroU32::new((inbound_bps * burst_ratio) as u32)
                .expect("burst_ratio cannot be 0");
            RateLimiter::direct_with_clock(Quota::per_second(bps).allow_burst(burst), &clock)
        });

        let outbound = NonZeroU32::new(outbound_bps as u32).map(|bps| {
            let burst = NonZeroU32::new((outbound_bps * burst_ratio) as u32)
                .expect("burst_ratio cannot be 0");
            RateLimiter::direct_with_clock(Quota::per_second(bps).allow_burst(burst), &clock)
        });
        Self {
            clock,
            inbound,
            outbound,
            start_time: Instant::now(),
            bits_inbound: AtomicUsize::new(0),
            peak_inbound: AtomicUsize::new(0),
            bits_outbound: AtomicUsize::new(0),
            peak_outbound: AtomicUsize::new(0),
            last_inbound_time: AtomicU64::new(0),
            last_outbound_time: AtomicU64::new(0),
        }
    }

    async fn try_throttle(
        &self,
        verb: &str,
        throttle: &RateLimiter<NotKeyed, InMemoryState, C>,
        bytes: usize,
        bits: NonZeroU32,
    ) {
        while let Err(e) = throttle.check_n(bits) {
            match e {
                governor::NegativeMultiDecision::BatchNonConforming(_, n) => {
                    let dur = n.wait_time_from(governor::clock::Clock::now(&self.clock));
                    if dur.as_secs() > 1 {
                        tracing::info!(
                            "Waiting {:?} to {} {} bits, {} bytes",
                            dur,
                            verb,
                            bits,
                            bytes
                        );
                    }
                    tokio::time::sleep(dur).await;
                }
                governor::NegativeMultiDecision::InsufficientCapacity(mut cap) => {
                    tracing::error!(
                        "Tried to {} {} bits, which is larger than the maximum possible of {} bits. Allowing this large message through anyway!", 
                        verb, bits, cap
                    );
                    // TODO: rather than allowing this message through, we should bubble this error up so that the sender can split
                    // the message into smaller chunks. We don't easily have that capacity right now, so, better to violate rate
                    // limiting than to go into an infinite loop...

                    // Drain the rate limiter's capacity completely, to be as accurate as possible.
                    // (ideally we would just drain the capacity completely in one fell swoop, but `governor`'s API does not allow this.)
                    while cap > 1 {
                        throttle
                            .check_n(unsafe { NonZeroU32::new_unchecked(cap) })
                            .ok();
                        cap /= 2;
                    }
                    break;
                }
            }
        }
    }

    /// Wait until there's enough bandwidth to send this many bytes.
    pub async fn outgoing_bytes(&self, bytes: usize) {
        if let Some(bits) = NonZeroU32::new(bytes as u32 * 8) {
            if let Some(outbound) = &self.outbound {
                self.try_throttle("send", outbound, bytes, bits).await;
            }
            let el = self.start_time.elapsed();
            let last_s = self
                .last_outbound_time
                .swap(el.as_secs(), std::sync::atomic::Ordering::Relaxed);
            let total_bits = self
                .bits_outbound
                .fetch_add(bits.get() as usize, std::sync::atomic::Ordering::Relaxed)
                + bits.get() as usize;
            let bps = total_bits
                .checked_div(el.as_secs() as usize)
                .unwrap_or_default();
            let current_bps = (bits.get() as u64).checked_div(last_s).unwrap_or_default();
            let max_bps = self
                .peak_outbound
                .fetch_max(bps, std::sync::atomic::Ordering::Relaxed)
                .max(bps);
            let s = tracing::trace_span!("bandwidth");
            s.in_scope(|| {
                tracing::trace!(
                    "Outbound current: {}bps {:.2}mbps, average: {}bps {:.2}mbps, max: {}bps {:.2}mbps",
                    current_bps,
                    current_bps as f64 / 1_048_576.0,
                    bps,
                    bps as f64 / 1_048_576.0,
                    max_bps,
                    max_bps as f64 / 1_048_576.0
                )
            })
        }
    }

    /// Wait until there's enough bandwidth to receive this many bytes.
    pub async fn incoming_bytes(&self, bytes: usize) {
        if let Some(bits) = NonZeroU32::new(bytes as u32 * 8) {
            if let Some(inbound) = &self.inbound {
                self.try_throttle("receive", inbound, bytes, bits).await;
            }
            let el = self.start_time.elapsed();
            let last_s = self
                .last_inbound_time
                .swap(el.as_secs(), std::sync::atomic::Ordering::Relaxed);
            let total_bits = self
                .bits_inbound
                .fetch_add(bits.get() as usize, std::sync::atomic::Ordering::Relaxed)
                + bits.get() as usize;
            let bps = total_bits
                .checked_div(el.as_secs() as usize)
                .unwrap_or_default();
            let current_bps = (bits.get() as u64).checked_div(last_s).unwrap_or_default();
            let max_bps = self
                .peak_inbound
                .fetch_max(bps, std::sync::atomic::Ordering::Relaxed)
                .max(bps);
            let s = tracing::trace_span!("bandwidth");
            s.in_scope(|| {
                tracing::trace!(
                    "Inbound current: {}bps {:.2}mbps, average: {}bps {:.2}mbps, max: {}bps {:.2}mbps",
                    current_bps,
                    current_bps as f64 / 1_000_000.0,
                    bps,
                    bps as f64 / 1_000_000.0,
                    max_bps,
                    max_bps as f64 / 1_000_000.0
                )
            })
        }
    }
}

#[cfg(test)]
mod tests {
    use std::time::Duration;

    use super::*;

    #[tokio::test(flavor = "current_thread", start_paused = true)]
    async fn test_limiter() {
        holochain_trace::test_run();
        let clock = governor::clock::FakeRelativeClock::default();
        // max * 2 * 8 = 0.1 * 1_000_000 * burst_ratio => burst_ratio = max * 2 * 8 / 0.1 / 1_000_000
        let burst_ratio = MAX_SEND_BUF_BYTES as f64 * 2.0 * 8.0 / 1_000_000.0 / 0.1;
        assert_eq!(burst_ratio, 2560.0);
        let bandwidth = BandwidthThrottle::test(0.1, 0.1, burst_ratio, clock.clone());
        let bytes = MAX_SEND_BUF_BYTES;
        // Hit the burst limit.
        bandwidth.outgoing_bytes(MAX_SEND_BUF_BYTES).await;
        bandwidth.outgoing_bytes(MAX_SEND_BUF_BYTES).await;
        let mut count = 0;

        // Now we will be limited to 0.1 mbps.
        let mut seconds = 0;
        for _ in 0..5 {
            let megabits = (bytes * 8) as f64 / 1_000_000.0;
            let time = megabits / 0.1;
            let advance_by = Duration::from_secs(time as u64 - 1);
            seconds += advance_by.as_nanos();
            clock.advance(advance_by);
            let r = tokio::time::timeout(Duration::from_secs(10), bandwidth.outgoing_bytes(bytes))
                .await;
            // When we advance the clock 1 second less than the required time
            // the outgoing bytes times out because the clock is set to just before
            // enough time to send the bytes
            assert!(r.is_err());

            let advance_by = Duration::from_secs(1);
            seconds += advance_by.as_nanos();
            clock.advance(advance_by);
            let n = tokio::time::Instant::now();
            bandwidth.outgoing_bytes(bytes).await;
            // Now we advance the clock and the function returns
            // immediately.
            assert!(n.elapsed().is_zero());
            count += bytes;
        }
        let megabits = (count * 8) as f64 / 1_000_000.0;
        let mbps = megabits / seconds as f64;
        // Allow for small rounding error.
        assert!(mbps < 0.11);
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/bloom.rs
================================================
use crate::gossip::sharded_gossip::store::TimeChunk;

use super::*;

impl ShardedGossipLocal {
    /// Generate a bloom filter of all agents.
    /// - Agents are only included if they are within the common arc set.
    /// - The bloom is `KitsuneAgent` + `signed_at_ms`. So multiple agent infos could
    ///   be in the same filter.
    /// - Only create the filter if there are any agents matching the above criteria.
    ///
    /// No empty bloom filters.
    /// - Bloom has a 1% chance of false positive (which will lead to agents not being sent back).
    /// - Expect this function to complete in an average of 10 ms and worst case 100 ms.
    pub(super) async fn generate_agent_bloom(
        &self,
        state: RoundState,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Option<BloomFilter>> {
        let RoundState { common_arq_set, .. } = state;

        // Get the time range for this gossip.
        // Get all the agent info that is within the common arc set.
        let agents_within_arc: Vec<_> = agent_info_session
            .agent_info_within_arc_set(&self.host_api, &self.space, (*common_arq_set).clone())
            .await?;

        // There was no agents so we don't create a bloom.
        if agents_within_arc.is_empty() {
            return Ok(None);
        }

        // Create a new bloom with the correct size.
        let mut bloom = bloomfilter::Bloom::new_for_fp_rate(agents_within_arc.len(), Self::TGT_FP);

        for info in agents_within_arc {
            let signed_at_ms = info.signed_at_ms;
            // The key is the agent hash + the signed at.
            let key = MetaOpKey::Agent(info.0.agent.clone(), signed_at_ms);
            bloom.set(&key);
        }
        Ok(Some(bloom))
    }

    /// Generate a bloom filter of all ops.
    /// - Ops are only included if they are within the common arc set.
    /// - The bloom is `KitsuneOpHah`.
    /// - Ops are only included from local agents that are within the common arc set.
    /// - Only create the filter if there are any ops matching the above criteria.
    ///
    /// No empty bloom filters.
    /// - Bloom has a 1% chance of false positive (which will lead to agents not being sent back).
    /// - Expect this function to complete in an average of 10 ms and worst case 100 ms.
    pub(super) async fn generate_op_blooms_for_time_window(
        &self,
        common_arqs: &ArqSet,
        search_time_window: TimeWindow,
    ) -> KitsuneResult<Batch<TimedBloomFilter>> {
        use futures::TryStreamExt;

        let common_arc_set = common_arqs.to_dht_arc_set_std();

        // If the common arc set is empty there's no
        // blooms to generate.
        if common_arc_set.is_empty() {
            return Ok(Batch::Complete(Vec::with_capacity(0)));
        }

        let mut total_blooms = 0;
        let search_end = search_time_window.end;

        let stream = store::hash_chunks_query(
            self.host_api.clone(),
            self.space.clone(),
            common_arc_set.clone(),
            search_time_window.clone(),
            true,
        );
        let batch = stream
            // Take more chunks while there is less then
            // the upper limit for number of blooms.
            .try_take_while(|_| {
                total_blooms += 1;
                futures::future::ready(Ok(total_blooms <= Self::UPPER_BLOOM_BOUND))
            })
            // Fold the chunks into a batch of [`TimedBloomFilter`].
            .try_fold(
                // Start with a partial batch where the cursor is
                // set to the end of the time window.
                Batch::Partial {
                    cursor: search_time_window.end,
                    data: Vec::new(),
                },
                |batch,
                 TimeChunk {
                     window,
                     cursor,
                     hashes,
                 }| {
                    async move {
                        // If the window for this time chunk matches
                        // the end of our search window then this is
                        // the final result.
                        let complete = search_end == window.end;

                        // If there were no hashes found then create an
                        // empty bloom filter for this time window.

                        let bloom = if hashes.is_empty() {
                            TimedBloomFilter {
                                bloom: None,
                                time: window,
                            }
                        } else {
                            // Otherwise create the bloom filter from the hashes.
                            let mut bloom =
                                bloomfilter::Bloom::new_for_fp_rate(hashes.len(), Self::TGT_FP);

                            let mut iter = hashes.into_iter().peekable();

                            while iter.peek().is_some() {
                                for hash in iter.by_ref().take(100) {
                                    bloom.set(&MetaOpKey::Op(hash));
                                }
                                // Yield to the conductor every 100 hashes. Because tasks have
                                // polling budgets this gives the runtime a chance to schedule other
                                // tasks so they don't starve.
                                tokio::task::yield_now().await;
                            }
                            TimedBloomFilter {
                                bloom: Some(bloom),
                                time: window,
                            }
                        };
                        match batch {
                            Batch::Partial { mut data, .. } | Batch::Complete(mut data) => {
                                // Add this bloom to the batch and set it to complete
                                // if this is the final bloom.
                                data.push(bloom);
                                if complete {
                                    Ok(Batch::Complete(data))
                                } else {
                                    Ok(Batch::Partial { data, cursor })
                                }
                            }
                        }
                    }
                },
            )
            .await?;

        match batch {
            Batch::Complete(data) => Ok(Batch::Complete(data)),
            Batch::Partial { cursor, data } => {
                // If the take while limit was reached then this is a
                // partial batch, otherwise is must be complete.
                if data.len() == Self::UPPER_BLOOM_BOUND {
                    Ok(Batch::Partial { cursor, data })
                } else {
                    Ok(Batch::Complete(data))
                }
            }
        }
    }

    /// Check a bloom filter for missing ops.
    /// - For each local agent that is within the common arc set,
    ///   get all ops that are within the common arc set and missing from the filter.
    /// - There is a 1% chance of false positives.
    /// - The performance of this function is dependent on the number of ops that fit the
    ///   above criteria and the number of local agents.
    /// - The worst case is maximum amount of ops that could be created for the time period.
    /// - The expected performance per op is average 10ms and worst 100 ms.
    pub(super) async fn check_op_bloom(
        &self,
        common_arc_set: DhtArcSet,
        remote_bloom: &TimedBloomFilter,
    ) -> KitsuneResult<Batch<Arc<KitsuneOpHash>>> {
        use futures::TryStreamExt;
        let TimedBloomFilter {
            bloom: remote_bloom,
            time,
        } = remote_bloom;
        let end = time.end;
        let mut stream = store::hash_chunks_query(
            self.host_api.clone(),
            self.space.clone(),
            common_arc_set,
            time.clone(),
            false,
        );
        // Take a single chunk of hashes for this time window.
        let chunk = stream.try_next().await?;

        match chunk {
            Some(TimeChunk {
                window,
                cursor,
                hashes,
            }) => {
                // A chunk was found so check the bloom.
                let missing_hashes = match remote_bloom {
                    Some(remote_bloom) => {
                        let mut iter = hashes.into_iter().peekable();
                        let mut missing_hashes = Vec::new();

                        while iter.peek().is_some() {
                            for hash in iter.by_ref().take(100) {
                                if !remote_bloom.check(&MetaOpKey::Op(hash.clone())) {
                                    missing_hashes.push(hash);
                                }
                            }
                            // Yield to avoid starving the runtime.
                            tokio::task::yield_now().await;
                        }
                        missing_hashes
                    }
                    // No remote bloom so they are missing everything.
                    None => hashes,
                };

                // If the found time window is the same as the blooms window
                // then this batch of missing hashes is complete.
                if window.end == end {
                    Ok(Batch::Complete(missing_hashes))
                } else {
                    // Otherwise save the cursor and return
                    // a partial batch.
                    Ok(Batch::Partial {
                        cursor,
                        data: missing_hashes,
                    })
                }
            }
            None => Ok(Batch::Complete(Vec::with_capacity(0))),
        }
    }
}

#[derive(Debug)]
/// A batch of data which is either complete
/// or has the cursor for the timestamp the partial
/// batch got to.
pub(super) enum Batch<T> {
    Complete(Vec<T>),
    Partial { cursor: Timestamp, data: Vec<T> },
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/initiate.rs
================================================
use super::*;
use crate::metrics::{GENERATE_OP_BLOOMS_TIME, GENERATE_OP_REGION_SET_TIME};
use kitsune_p2p_types::dht::{arq::ArqSet, ArqBounds};
use rand::Rng;

impl ShardedGossipLocal {
    /// Try to initiate gossip if we don't currently
    /// have an outgoing gossip.
    pub(super) async fn try_initiate(
        &self,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Option<Outgoing>> {
        // Get local agents
        let (has_target, local_agents) = self.inner.share_mut(|i, _| {
            i.check_tgt_expired(self.gossip_type, self.tuning_params.gossip_round_timeout());
            let has_target = i.initiate_tgt.is_some();
            // Clear any expired rounds.
            i.round_map.current_rounds();
            Ok((has_target, i.local_agents.clone()))
        })?;
        // There's already a target so there's nothing to do.
        if has_target {
            return Ok(None);
        }

        // If we don't have a local agent then there's nothing to do.
        if local_agents.is_empty() {
            // No local agents so there's no one to initiate gossip from.
            return Ok(None);
        }

        // Get the local agents intervals.
        let intervals: Vec<ArqBounds> = agent_info_session
            .local_arqs()
            .into_iter()
            .map(|a| a.to_bounds_std())
            .collect();

        // Choose a remote agent to gossip with.
        let remote_agent = self
            .find_remote_agent_within_arcset(ArqSet::new(intervals.clone()), agent_info_session)
            .await?;

        let maybe_gossip = if let Some(next_target::Node {
            agent_info_list,
            cert,
            url,
        }) = remote_agent
        {
            let id = rand::thread_rng().gen();

            // TODO Why send both the agents and the intervals? The agents contain their arcs
            let gossip = ShardedGossipWire::initiate(
                intervals,
                id,
                agent_info_session.get_local_agents().to_vec(),
            );

            let tgt = ShardedGossipTarget {
                remote_agent_list: agent_info_list,
                cert: cert.clone(),
                tie_break: id,
                when_initiated: Some(Instant::now()),
                url: url.clone(),
            };

            self.inner.share_mut(|inner, _| {
                inner.initiate_tgt = Some(tgt);
                Ok(())
            })?;
            Some((cert, HowToConnect::Url(url.to_string()), gossip))
        } else {
            None
        };
        Ok(maybe_gossip)
    }

    /// Receiving an incoming initiate.
    /// - Send back the accept, agent bloom and ops bloom gossip messages.
    /// - Only send the agent bloom if this is a recent gossip type.
    pub(super) async fn incoming_initiate(
        &self,
        peer_cert: NodeCert,
        remote_arqs: Vec<ArqBounds>,
        remote_id: u32,
        remote_agent_list: Vec<AgentInfoSigned>,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        let (local_agents, same_as_target, already_in_progress) =
            self.inner.share_mut(|i, _| {
                let already_in_progress = i.round_map.round_exists(&peer_cert);
                let same_as_target = i
                    .initiate_tgt
                    .as_ref()
                    .filter(|tgt| tgt.cert == peer_cert)
                    .map(|tgt| tgt.tie_break);
                Ok((i.local_agents.clone(), same_as_target, already_in_progress))
            })?;

        // The round is already in progress from our side.
        // The remote side should not be initiating.
        if already_in_progress {
            // This means one side has already started a round but
            // a stale initiate was received.
            return Ok(vec![ShardedGossipWire::already_in_progress()]);
        }

        // If this is the same connection as our current target then we need to decide who proceeds.
        if let Some(our_id) = same_as_target {
            // If we have a lower id then we proceed
            // and the remote will exit.
            // If we have a higher id than the remote
            // then we exit and the remote will proceed.
            // If we tie then we both exit (This will be very rare).
            if our_id >= remote_id {
                return Ok(Vec::with_capacity(0));
            } else {
                self.inner.share_mut(|i, _| {
                    i.initiate_tgt = None;
                    Ok(())
                })?;
            }
        }

        // If we don't have a local agent then there's nothing to do.
        if local_agents.is_empty() {
            // No local agents so there's no one to initiate gossip from.
            return Ok(vec![ShardedGossipWire::no_agents()]);
        }

        // Get the local intervals.
        let local_arqs: Vec<ArqBounds> = agent_info_session
            .local_arqs()
            .into_iter()
            .map(|arc| arc.to_bounds_std())
            .collect();

        let agent_list = agent_info_session.get_local_agents().to_vec();

        // Send the intervals back as the accept message.
        let mut gossip = vec![ShardedGossipWire::accept(local_arqs.clone(), agent_list)];

        // Generate the bloom filters and new state.
        let state = self
            .generate_blooms_or_regions(
                remote_agent_list.clone(),
                local_arqs,
                remote_arqs,
                &mut gossip,
                agent_info_session,
            )
            .await?;

        self.inner.share_mut(|inner, _| {
            // If this is not the target we are accepting
            // then record it as a remote round.
            if inner
                .initiate_tgt
                .as_ref()
                .map_or(true, |tgt| tgt.cert != peer_cert)
            {
                let mut metrics = inner.metrics.write();

                metrics.update_current_round(&peer_cert, self.gossip_type.into(), &state);
                metrics.record_accept(&remote_agent_list, self.gossip_type.into());
            }

            inner.round_map.insert(peer_cert.clone(), state);

            // If this is the target then we should clear the when initiated timeout.
            if let Some(tgt) = inner.initiate_tgt.as_mut() {
                if tgt.cert == peer_cert {
                    tgt.when_initiated = None;
                    // we also want to update the agent list
                    // with that reported by the remote end
                    tgt.remote_agent_list = remote_agent_list;
                }
            }
            Ok(())
        })?;
        Ok(gossip)
    }

    /// Fetch a current list of agents to initiate gossip with.
    #[cfg(test)]
    pub(super) async fn query_agents_by_local_agents(&self) -> KitsuneResult<Vec<AgentInfoSigned>> {
        let local_agents = self.inner.share_mut(|i, _| Ok(i.local_agents.clone()))?;

        Ok(store::all_agent_info(&self.host_api, &self.space)
            .await?
            .into_iter()
            .filter(|a| local_agents.contains(&a.agent))
            .collect())
    }

    /// Generate the bloom filters and generate a new state.
    /// - Agent bloom is only generated if this is a `Recent` gossip type.
    /// - Empty blooms are not created.
    /// - A new state is created for this round.
    pub(super) async fn generate_blooms_or_regions(
        &self,
        remote_agent_list: Vec<AgentInfoSigned>,
        local_arqs: Vec<ArqBounds>,
        remote_arqs: Vec<ArqBounds>,
        gossip: &mut Vec<ShardedGossipWire>,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<RoundState> {
        let topo = self
            .host_api
            .get_topology(self.space.clone())
            .await
            .map_err(KitsuneError::other)?;

        // Create the common arc set from the remote and local arcs.
        let local_arqs = ArqSet::new(local_arqs);
        let remote_arqs = ArqSet::new(remote_arqs);
        let common_arqs = Arc::new(local_arqs.intersection(&topo, &remote_arqs));

        let region_set = if let GossipType::Historical = self.gossip_type {
            let start = Instant::now();
            let region_set = store::query_region_set(
                self.host_api.clone().api,
                self.space.clone(),
                (*common_arqs).clone(),
            )
            .await?;
            GENERATE_OP_REGION_SET_TIME.record(
                start.elapsed().as_secs_f64(),
                &[opentelemetry_api::KeyValue::new(
                    "space",
                    format!("{:?}", self.space),
                )],
            );
            gossip.push(ShardedGossipWire::op_regions(region_set.clone()));
            Some(region_set)
        } else {
            None
        };

        // Generate the new state.
        let mut state = self.new_state(
            remote_agent_list,
            common_arqs,
            region_set,
            self.tuning_params.gossip_round_timeout(),
        )?;

        // Generate the agent bloom.
        if let GossipType::Recent = self.gossip_type {
            let bloom = self
                .generate_agent_bloom(state.clone(), agent_info_session)
                .await?;
            if let Some(bloom) = bloom {
                let bloom = encode_bloom_filter(&bloom);
                gossip.push(ShardedGossipWire::agents(bloom));
            }

            // we consider recent gossip to have "sent its region"
            // for purposes of determining the round is complete
            state.regions_are_queued = true;

            self.next_bloom_batch(state, gossip).await
        } else {
            // Everything has already been taken care of for Historical
            // gossip already. Just mark this true so that the state will not
            // be considered "finished" until all op data is received.
            state.has_pending_historical_op_data = true;
            state.regions_are_queued = false;
            Ok(state)
        }
    }

    /// Generate the next batch of blooms from this state.
    /// If there is a saved cursor from a previous partial
    /// batch then this will pick up from there.
    /// Otherwise s batch of blooms for the entire search window
    /// will be attempted (if this is too many hashes then it will
    /// create a new partial batch of blooms.)
    pub(super) async fn next_bloom_batch(
        &self,
        mut state: RoundState,
        gossip: &mut Vec<ShardedGossipWire>,
    ) -> KitsuneResult<RoundState> {
        // Get the default window for this gossip loop.
        let mut window = self.calculate_time_range();

        // If there is a previously saved cursor then start from there.
        if let Some(cursor) = state.bloom_batch_cursor.take() {
            window.start = cursor;
        }

        let start = Instant::now();
        let blooms = self
            .generate_op_blooms_for_time_window(&state.common_arq_set, window)
            .await?;
        GENERATE_OP_BLOOMS_TIME.record(
            start.elapsed().as_secs_f64(),
            &[
                opentelemetry_api::KeyValue::new("space", format!("{:?}", self.space)),
                opentelemetry_api::KeyValue::new(
                    "batch_size",
                    match &blooms {
                        bloom::Batch::Complete(blooms) => blooms.len() as i64,
                        bloom::Batch::Partial { data, .. } => data.len() as i64,
                    },
                ),
            ],
        );

        let blooms = match blooms {
            bloom::Batch::Complete(blooms) => blooms,
            bloom::Batch::Partial { cursor, data } => {
                // This batch of blooms is partial so save the cursor in this rounds state.
                state.bloom_batch_cursor = Some(cursor);
                data
            }
        };

        // If no blooms were found for this time window then return a no overlap.
        if blooms.is_empty() {
            // Check if this is the final time window.
            gossip.push(ShardedGossipWire::op_bloom(
                EncodedTimedBloomFilter::NoOverlap,
                true,
            ));
        }

        let len = blooms.len();

        // Encode each bloom found for this time window.
        for (i, bloom) in blooms.into_iter().enumerate() {
            let time_window = bloom.time;
            let bloom = match bloom.bloom {
                // We have some hashes so request all missing from the bloom.
                Some(bloom) => {
                    let bytes = encode_bloom_filter(&bloom);
                    EncodedTimedBloomFilter::HaveHashes {
                        filter: bytes,
                        time_window,
                    }
                }
                // We have no hashes for this time window but we do have agents
                // that hold the arc so request all the ops the remote holds.
                None => EncodedTimedBloomFilter::MissingAllHashes { time_window },
            };
            state.increment_expected_op_blooms();

            // Check if this is the final time window and the final bloom for this window.
            if i == len - 1 && state.bloom_batch_cursor.is_none() {
                gossip.push(ShardedGossipWire::op_bloom(bloom, true));
            } else {
                gossip.push(ShardedGossipWire::op_bloom(bloom, false));
            }
        }

        Ok(state)
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/next_target.rs
================================================
use std::cmp::Ordering;

use super::*;
use crate::metrics::*;

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
/// A remote node we can connect to.
/// Note that a node can contain many agents.
pub(crate) struct Node {
    pub(crate) agent_info_list: Vec<AgentInfoSigned>,
    pub(crate) cert: NodeCert,
    pub(crate) url: TxUrl,
}

impl ShardedGossipLocal {
    /// Find a remote endpoint from agents within arc set.
    pub(super) async fn find_remote_agent_within_arcset(
        &self,
        arc_set: ArqSet,
        agent_info_session: &mut AgentInfoSession,
    ) -> KitsuneResult<Option<Node>> {
        let mut remote_nodes: HashMap<NodeCert, Node> = HashMap::new();

        let local_agents = agent_info_session.get_local_kitsune_agents();

        // Get all the remote nodes in this arc set.
        let remote_agents_within_arc_set: HashSet<_> = agent_info_session
            .agent_info_within_arc_set(&self.host_api, &self.space, arc_set.clone())
            .await?
            .into_iter()
            .filter_map(|a| {
                if !local_agents.contains(&a.agent) {
                    Some(a.agent.clone())
                } else {
                    None
                }
            })
            .collect();

        // Get all the agent info for these remote nodes.
        for info in agent_info_session
            .get_agents()
            .iter()
            .filter(|a| {
                std::time::Duration::from_millis(a.expires_at_ms)
                    > std::time::UNIX_EPOCH
                        .elapsed()
                        .expect("Your system clock is set before UNIX epoch")
            })
            .filter(|a| remote_agents_within_arc_set.contains(&a.agent))
            .filter(|a| !a.storage_arc().is_empty())
        {
            // Get an address if there is one.
            let info = info
                .url_list
                .iter()
                .filter_map(|url| {
                    kitsune_p2p_types::tx_utils::ProxyUrl::from_full(url.as_str())
                        .map_err(|e| tracing::error!("Failed to parse url {:?}", e))
                        .ok()
                        .map(|purl| {
                            KitsuneResult::Ok((
                                info.clone(),
                                purl.digest()?.0.into(),
                                url.to_string(),
                            ))
                        })
                })
                .next()
                .transpose()?;

            // If we found a remote address add this agent to the node
            // or create the node if it doesn't exist.
            if let Some((info, cert, url)) = info {
                match remote_nodes.get_mut::<NodeCert>(&cert) {
                    // Add the agent to the node.
                    Some(node) => node.agent_info_list.push(info),
                    None => {
                        // This is a new node.
                        remote_nodes.insert(
                            cert.clone(),
                            Node {
                                agent_info_list: vec![info],
                                cert,
                                url: url.try_into()?,
                            },
                        );
                    }
                }
            }
        }

        let remote_nodes = remote_nodes.into_values().collect();
        let tuning_params = self.tuning_params.clone();
        // We could clone the metrics store out of the lock here but I don't think
        // the next_remote_node will be that slow so we can just choose the next node inline.
        self.inner.share_mut(|i, _| {
            let node = next_remote_node(remote_nodes, &i.metrics, tuning_params);
            Ok(node)
        })
    }
}

/// Find the next remote node to sync with.
fn next_remote_node(
    mut remote_nodes: Vec<Node>,
    metrics: &MetricsSync,
    tuning_params: KitsuneP2pTuningParams,
) -> Option<Node> {
    // Sort the nodes by longest time since we last successfully gossiped with them.
    // Note the smaller an Instant the longer it is in the past.
    remote_nodes.sort_unstable_by(|a, b| {
        match (
            metrics.read().last_success(&a.agent_info_list),
            metrics.read().last_success(&b.agent_info_list),
        ) {
            // Choose the smallest (oldest) Instant.
            (Some(a), Some(b)) => a.cmp(b),
            // Put a behind b that hasn't been gossiped with.
            (Some(_), None) => Ordering::Greater,
            // Put b behind a that hasn't been gossiped with.
            (None, Some(_)) => Ordering::Less,
            // Randomly break ties.
            (None, None) => Ordering::Equal,
        }
    });

    let forced_initiate = metrics.read().forced_initiate();

    remote_nodes
        .into_iter()
        // Don't initiate with nodes we are currently gossiping with.
        .filter(|n| !metrics.read().is_current_round(&n.agent_info_list))
        .find(|n| {
            match metrics.read().last_outcome(&n.agent_info_list) {
                Some(RoundOutcome::Success(when)) => {
                    // If we should force initiate then we don't need to wait for the delay.
                    forced_initiate
                        || when.elapsed().as_millis() as u32
                            >= tuning_params.gossip_peer_on_success_next_gossip_delay_ms
                }
                Some(RoundOutcome::Error(when)) => {
                    when.elapsed().as_millis() as u32
                        >= tuning_params.gossip_peer_on_error_next_gossip_delay_ms
                }
                _ => true,
            }
        })
}

#[cfg(test)]
mod tests {
    use ::fixt::prelude::*;
    use kitsune_p2p_types::dht::arq::ArqSize;
    use rand::distributions::Alphanumeric;
    use test_case::test_case;

    use super::*;

    /// Generate a random valid proxy url.
    fn random_url(rng: &mut ThreadRng) -> url2::Url2 {
        let cert_string: String = rng
            .sample_iter(&Alphanumeric)
            .take(39)
            .map(char::from)
            .collect();
        let port = rng.gen_range(5000..6000);

        url2::url2!(
            "kitsune-proxy://{}mqcw/kitsune-quic/h/localhost/p/{}/-",
            cert_string,
            port
        )
    }

    /// Generate a random pseudo-valid signed agent info
    fn random_agent_info(rng: &mut ThreadRng) -> AgentInfoSigned {
        let space = Arc::new(KitsuneSpace(vec![0x01; 36]));
        let mut agent = vec![0x00; 36];
        rng.fill(&mut agent[..]);
        let agent = Arc::new(KitsuneAgent(agent));

        futures::executor::block_on(AgentInfoSigned::sign(
            space,
            agent,
            ArqSize::from_half_len(42),
            vec![random_url(rng).into()],
            42,
            69,
            |_| async move { Ok(Arc::new(vec![0x03; 64].into())) },
        ))
        .unwrap()
    }

    /// Tuning params with no delay on recently gossiped to nodes.
    fn tuning_params_no_delay() -> KitsuneP2pTuningParams {
        let mut t = tuning_params_struct::KitsuneP2pTuningParams::default();
        t.gossip_peer_on_success_next_gossip_delay_ms = 0;
        t.gossip_peer_on_error_next_gossip_delay_ms = 0;
        Arc::new(t)
    }

    /// Tuning params with a delay on recently gossiped to nodes.
    fn tuning_params_delay(success: u32, error: u32) -> KitsuneP2pTuningParams {
        let mut t = tuning_params_struct::KitsuneP2pTuningParams::default();
        t.gossip_peer_on_success_next_gossip_delay_ms = success;
        t.gossip_peer_on_error_next_gossip_delay_ms = error;
        Arc::new(t)
    }

    fn create_remote_nodes(n: usize) -> Vec<Node> {
        let mut rng = thread_rng();
        (0..n)
            .map(|_| {
                let info = random_agent_info(&mut rng);
                let url = info.url_list.first().unwrap().clone();
                let purl = kitsune_p2p_types::tx_utils::ProxyUrl::from_full(url.as_str()).unwrap();
                Node {
                    agent_info_list: vec![info],
                    cert: NodeCert::from(purl.digest().unwrap().0),
                    url,
                }
            })
            .collect()
    }

    #[test]
    /// Test that we can find a remote node to sync with
    /// when there is only one to choose from.
    fn next_remote_node_sanity() {
        // - Create one remote node.
        let remote_nodes = create_remote_nodes(1);

        let r = next_remote_node(
            remote_nodes.clone(),
            &Default::default(),
            tuning_params_no_delay(),
        );

        // - That node is chosen.
        assert_eq!(r, remote_nodes.first().cloned());
    }

    /// Test that given N remote nodes we choose the one
    /// we talked to the least recently.
    #[test_case(1)]
    #[test_case(2)]
    #[test_case(10)]
    #[test_case(100)]
    fn next_remote_node_least_recently(n: usize) {
        // - Create N remote nodes.
        let mut remote_nodes = create_remote_nodes(n);

        let metrics = MetricsSync::default();

        // - Pop the last node off the list.
        let last = remote_nodes.pop().unwrap();

        // - Record a successful initiate round for the last node at the earliest time.
        metrics
            .write()
            .record_initiate(&last.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&last.agent_info_list, GossipModuleType::ShardedRecent);

        // - Record successful initiate rounds for the rest of the nodes at later times.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
            metrics
                .write()
                .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        // - Push the last node back into the remote nodes.
        remote_nodes.push(last);

        let r = next_remote_node(remote_nodes.clone(), &metrics, tuning_params_no_delay());

        // - Expect the last node to be chosen because it was the least recently gossiped with.
        assert_eq!(r, remote_nodes.last().cloned());
    }

    /// Test that given N remote nodes we choose the one
    /// we've never talked to before over all others.
    #[test_case(1)]
    #[test_case(2)]
    #[test_case(10)]
    #[test_case(100)]
    fn next_remote_node_never_talked_to(n: usize) {
        // - Create N remote nodes.
        let mut remote_nodes = create_remote_nodes(n);

        let metrics = MetricsSync::default();

        // - Pop the last node off the list.
        let last = remote_nodes.pop().unwrap();

        // - Record successful initiate rounds for the rest of the nodes.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
            metrics
                .write()
                .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        // - Push the last node back into the remote nodes.
        remote_nodes.push(last);

        let r = next_remote_node(remote_nodes.clone(), &metrics, tuning_params_no_delay());

        // - Expect the last node to be chosen because it was never gossiped with.
        assert_eq!(r, remote_nodes.last().cloned());
    }

    /// Test that given N remote nodes we never choose a current round.
    #[test_case(1)]
    #[test_case(2)]
    #[test_case(10)]
    #[test_case(100)]
    fn dont_choose_current_rounds(n: usize) {
        // - Create N remote nodes.
        let mut remote_nodes = create_remote_nodes(n);

        let metrics = MetricsSync::default();

        // - Pop the last node off the list.
        let last = remote_nodes.pop().unwrap();

        // - Record remote rounds for the rest of the nodes
        // but don't record any successes.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_accept(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        let r = next_remote_node(remote_nodes.clone(), &metrics, tuning_params_no_delay());

        // - Without the last node we expect no nodes to be chosen.
        assert!(r.is_none());

        // - Record the last node as a successful round and push it into the list.
        metrics
            .write()
            .record_initiate(&last.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&last.agent_info_list, GossipModuleType::ShardedRecent);
        remote_nodes.push(last);

        let r = next_remote_node(remote_nodes.clone(), &metrics, tuning_params_no_delay());

        // - Now we expect the last node to be chosen.
        // (because we're using "no delay" for the tuning params)
        assert_eq!(r, remote_nodes.last().cloned());
    }

    #[test]
    /// Test we don't choose nodes we've seen too recently.
    fn dont_choose_very_recent_rounds() {
        // - Create 100 remote nodes.
        let remote_nodes = create_remote_nodes(100);

        let metrics = MetricsSync::default();

        // - Record successful initiate rounds for the all of the nodes.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
            metrics
                .write()
                .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Expect no nodes to be chosen.
        assert!(r.is_none());

        // - Use up 10 ms.
        std::thread::sleep(Duration::from_millis(10));

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Still no result.
        assert!(r.is_none());

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a 9 ms after the successful round.
            tuning_params_delay(9, 0),
        );

        // - Now we should get a result.
        assert!(r.is_some());

        // - Record error outcomes for every node.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
            metrics
                .write()
                .record_error(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(0, 1000 * 60),
        );

        // - Expect no nodes to be chosen.
        assert!(r.is_none());

        // - Use up 10ms.
        std::thread::sleep(Duration::from_millis(10));

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(0, 1000 * 60),
        );

        // - Still no result.
        assert!(r.is_none());

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a 9 ms after an error round.
            tuning_params_delay(1000 * 60, 9),
        );

        // - Now we should get a result.
        assert!(r.is_some());
    }

    /// Test that given N remote nodes and a force initiate trigger
    /// we will choose the least recent node even if it's too recent.
    #[test_case(1)]
    #[test_case(2)]
    #[test_case(10)]
    #[test_case(100)]
    fn force_initiate(n: usize) {
        // - Create N remote nodes.
        let mut remote_nodes = create_remote_nodes(n);

        let metrics = MetricsSync::default();

        // - Pop the last node off the list.
        let last = remote_nodes.pop().unwrap();

        // - Record a successful initiate round for the last node before the other nodes.
        metrics
            .write()
            .record_initiate(&last.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&last.agent_info_list, GossipModuleType::ShardedRecent);

        // - Record successful initiate rounds for the rest of the nodes.
        for node in remote_nodes.iter() {
            metrics
                .write()
                .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
            metrics
                .write()
                .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);
        }

        // - Push the last node back on the list.
        remote_nodes.push(last);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Expect no nodes to be chosen.
        assert!(r.is_none());

        // - First force initiate.
        metrics.write().record_force_initiate();

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Expect the last node to be chosen because it was successfully gossiped
        // with the lest recently and we are force initiating.
        assert_eq!(r, remote_nodes.last().cloned());

        // - Record this successful initiate round.
        let last = remote_nodes.last().unwrap();
        metrics
            .write()
            .record_initiate(&last.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&last.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now the first node is the least recently gossiped with.
        assert_eq!(r, remote_nodes.first().cloned());

        // - Record this successful initiate round.
        let first = remote_nodes.first().unwrap();
        metrics
            .write()
            .record_initiate(&first.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&first.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );
        // - Force initiate only forces 2 nodes so now we expect no nodes
        // to be chosen because they are all more recent then the tuning params delay.
        assert!(r.is_none());

        // - Second force initiate.
        metrics.write().record_force_initiate();

        // Helper function to get the next expected node.
        let expected_node = |i| {
            match n {
                // - Only one node so the first will always be chosen.
                1 => remote_nodes.first(),
                // Two nodes so it will alternate between the first and last.
                2 => {
                    if i % 2 == 0 {
                        remote_nodes.first()
                    } else {
                        remote_nodes.last()
                    }
                }
                // All other tests will climb in the order they recorded success.
                _ => remote_nodes.get(i),
            }
        };

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now we expect node 1 to be chosen (unless there is only one node).
        assert_eq!(r, expected_node(1).cloned());

        // - Record the successful initiate round for this node.
        let node = expected_node(1).unwrap();
        metrics
            .write()
            .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now we expect node 2 to be chosen (unless there is only one node).
        assert_eq!(r, expected_node(2).cloned());

        // - Record the successful initiate round for this node.
        let node = expected_node(2).unwrap();
        metrics
            .write()
            .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - We expect no nodes to be chosen because the forced initiate has run out.
        assert!(r.is_none());

        // - Third force initiate.
        metrics.write().record_force_initiate();

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now we expect node 3 to be chosen (unless there is only one or two nodes).
        assert_eq!(r, expected_node(3).cloned());

        // - Record the successful initiate round for this node.
        let node = expected_node(3).unwrap();
        metrics
            .write()
            .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);

        // - Forth force initiate overlaps with third so it resets.
        metrics.write().record_force_initiate();

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now we expect node 4 to be chosen (unless there is only one or two nodes).
        assert_eq!(r, expected_node(4).cloned());

        // - Record the successful initiate round for this node.
        let node = expected_node(4).unwrap();
        metrics
            .write()
            .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - We expect the 5 node to be chosen because the forced initiate was reset.
        assert_eq!(r, expected_node(5).cloned());

        // - Record the successful initiate round for this node.
        let node = expected_node(5).unwrap();
        metrics
            .write()
            .record_initiate(&node.agent_info_list, GossipModuleType::ShardedRecent);
        metrics
            .write()
            .record_success(&node.agent_info_list, GossipModuleType::ShardedRecent);

        let r = next_remote_node(
            remote_nodes.clone(),
            &metrics,
            // - Set the tuning params to a delay in the future.
            tuning_params_delay(1000 * 60, 0),
        );

        // - Now the reset has run out we get no nodes.
        assert!(r.is_none());
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/ops.rs
================================================
use kitsune_p2p_fetch::{FetchKey, FetchPoolPush, OpHashSized, TransferMethod};
use kitsune_p2p_types::{combinators::second, dht::region::Region};

use super::*;

#[derive(Clone, derive_more::Deref)]
/// A queue of missing op hashes that have been batched
/// for future processing.
pub struct OpsBatchQueue(Share<OpsBatchQueueInner>);

/// Each queue is associated with a bloom filter that
/// this node received from the remote node and given an unique id.
pub struct OpsBatchQueueInner {
    /// A simple always increasing usize
    /// is used to give the queues unique ids.
    next_id: usize,
    queues: HashMap<usize, VecDeque<QueuedOps>>,
    region_queue: VecDeque<Region>,
}

/// Identify the next items to process from the region queue.
/// Always returns at least one item if the queue is not empty, regardless of size constraints.
/// The total size of regions returned will be less than the batch size, unless the first item
/// on its own is larger than the batch size.
pub fn get_region_queue_batch(queue: &mut VecDeque<Region>, batch_size: u32) -> Vec<Region> {
    let mut size = 0;
    let mut to_fetch = vec![];
    let mut first = true;
    while let Some(region) = queue.front() {
        // Only op hashes are gossiped now, so we just count the 36 bytes for each hash.
        size += region.data.count * 36;
        if first || size <= batch_size {
            to_fetch.push(queue.pop_front().unwrap());
            if size > batch_size {
                // TODO: we should split this Region up into smaller chunks
                tracing::warn!(
                    "Including a region of size {}, which is larger than the batch size of {}",
                    size,
                    batch_size
                );
            }
        }
        first = false;
        if size > batch_size {
            break;
        }
    }
    to_fetch
}

/// Queued MissingOpHashes hashes can either
/// be saved as the remaining hashes or if this
/// is too large the bloom filter is saved so the
/// remaining hashes can be generated in the future.
enum QueuedOps {
    /// Hashes that need to be fetched and returned
    /// as MissingOpHashes to a remote node.
    Hashes(Vec<Arc<KitsuneOpHash>>),
    /// A remote nodes bloom filter that has been adjusted
    /// to the remaining time window to fetch the remaining hashes.
    Bloom(TimedBloomFilter),
}

impl ShardedGossipLocal {
    /// Incoming ops bloom.
    /// - Send back chunks of missing ops.
    /// - Don't send a chunk larger then MAX_SEND_BUF_SIZE.
    pub(super) async fn incoming_op_bloom(
        &self,
        state: RoundState,
        mut remote_bloom: TimedBloomFilter,
        mut queue_id: Option<usize>,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        // Check which ops are missing.
        let missing_hashes = self
            .check_op_bloom((*state.common_arc_set()).clone(), &remote_bloom)
            .await?;

        let missing_hashes = match missing_hashes {
            bloom::Batch::Complete(hashes) => hashes,
            bloom::Batch::Partial { cursor, data } => {
                // If a partial batch of hashes was found for this bloom then adjust
                // the remote blooms time window to the cursor and queue it for future processing.
                remote_bloom.time.start = cursor;

                // Queue this bloom using the unique id if there is one.
                let id = state.ops_batch_queue.0.share_mut(|queue, _| {
                    Ok(queue.push_back(queue_id, QueuedOps::Bloom(remote_bloom)))
                })?;

                // If there was no id then a new one is created from the push_back call.
                queue_id = Some(id);

                data
            }
        };

        self.batch_missing_ops_from_bloom(state, missing_hashes, queue_id)
            .await
    }

    pub(super) async fn queue_incoming_regions(
        &self,
        peer_cert: &NodeCert,
        state: RoundState,
        region_set: RegionSetLtcs,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        if let Some(sent) = state.region_set_sent.as_ref().map(|r| (**r).clone()) {
            // because of the order of arguments, the diff regions will contain the data
            // from *our* side, not our partner's.
            let our_region_diff = sent
                .clone()
                .diff(region_set.clone())
                .map_err(KitsuneError::other)?;
            let their_region_diff = region_set.clone().diff(sent).map_err(KitsuneError::other)?;

            self.inner.share_mut(|i, _| {
                if let Some(round) = i.round_map.get_mut(peer_cert) {
                    round.region_diffs = Some((our_region_diff.clone(), their_region_diff));
                    round.regions_are_queued = true;
                    i.metrics.write().update_current_round(
                        peer_cert,
                        GossipModuleType::ShardedHistorical,
                        round,
                    );
                } else {
                    tracing::warn!(
                        "attempting to queue_incoming_regions for round with no cert: {:?}",
                        peer_cert
                    );
                }
                Ok(())
            })?;

            // This is a good place to see all the region data go by.
            // Note, this is a LOT of output!
            // tracing::info!("region diffs ({}): {:?}", diff_regions.len(), diff_regions);

            state.ops_batch_queue.0.share_mut(|queue, _| {
                for region in our_region_diff {
                    queue.region_queue.push_back(region)
                }
                Ok(())
            })?;

            self.process_next_region_batch(state).await
        } else {
            Err(KitsuneError::other("We received OpRegions gossip without sending any ourselves. This can only happen if Recent gossip somehow sends an OpRegions message."))
        }
    }

    pub(super) async fn process_next_region_batch(
        &self,
        state: RoundState,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        let (to_fetch, finished) = state.ops_batch_queue.share_mut(|queues, _| {
            let items = get_region_queue_batch(
                &mut queues.region_queue,
                self.tuning_params.gossip_max_batch_size,
            );
            Ok((items, queues.region_queue.is_empty()))
        })?;

        let queries = to_fetch.into_iter().map(|region| {
            self.host_api
                .query_op_hashes_by_region(self.space.clone(), region.coords)
        });

        let ops: Vec<OpHashSized> = futures::future::join_all(queries)
            .await
            .into_iter()
            .collect::<Result<Vec<_>, _>>()
            .map_err(KitsuneError::other)?
            .into_iter()
            .flatten()
            .collect();

        // TODO: make region set diffing more robust to different times (arc power differences are already handled)

        let finished_val = if finished { 2 } else { 1 };
        Ok(vec![ShardedGossipWire::missing_op_hashes(
            ops,
            finished_val,
        )])
    }

    /// Generate the next batch of missing ops.
    pub(super) async fn next_missing_ops_batch(
        &self,
        state: RoundState,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        match self.gossip_type {
            GossipType::Historical => self.process_next_region_batch(state).await,
            GossipType::Recent => {
                // Pop the next queued batch.
                let next_batch = state
                    .ops_batch_queue
                    .0
                    .share_mut(|queue, _| Ok(queue.pop_front()))?;

                match next_batch {
                    // The next batch is hashes, batch them into ops using the queue id.
                    Some((queue_id, QueuedOps::Hashes(missing_hashes))) => {
                        self.batch_missing_ops_from_bloom(state, missing_hashes, Some(queue_id))
                            .await
                    }
                    // The next batch is a bloom so the hashes need to be fetched before
                    // fetching the hashes.
                    Some((queue_id, QueuedOps::Bloom(remote_bloom))) => {
                        self.incoming_op_bloom(state, remote_bloom, Some(queue_id))
                            .await
                    }
                    // Nothing is queued so this node is done.
                    None => Ok(vec![ShardedGossipWire::missing_op_hashes(
                        Vec::with_capacity(0),
                        MissingOpsStatus::AllComplete as u8,
                    )]),
                }
            }
        }
    }

    /// Fetch missing ops into the appropriate size chunks of
    /// and batch for future processing if there is too much data.
    async fn batch_missing_ops_from_bloom(
        &self,
        state: RoundState,
        mut missing_hashes: Vec<Arc<KitsuneOpHash>>,
        mut queue_id: Option<usize>,
    ) -> KitsuneResult<Vec<ShardedGossipWire>> {
        let num_missing = missing_hashes.len();
        let mut gossip = Vec::new();

        // Fetch the missing ops if there is any.
        let missing_op_hashes = if missing_hashes.is_empty() {
            Vec::with_capacity(0)
        } else {
            self.host_api
                .legacy
                .fetch_op_data(FetchOpDataEvt {
                    space: self.space.clone(),
                    query: FetchOpDataEvtQuery::Hashes {
                        op_hash_list: missing_hashes.clone(),
                        include_limbo: false,
                    },
                })
                .await
                .map_err(KitsuneError::other)?
                .into_iter()
                .map(second)
                .collect()
        };

        let got_len = missing_op_hashes.len();

        // If there is less ops then missing hashes the call was batched.
        let is_batched = got_len < num_missing;

        if is_batched {
            // Queue the remaining hashes for future processing.
            let id = state.ops_batch_queue.0.share_mut(|queue, _| {
                Ok(queue.push_back(
                    queue_id,
                    QueuedOps::Hashes(missing_hashes.drain(got_len..).collect()),
                ))
            })?;
            queue_id = Some(id);
        }

        // If this call is part of a queue and then queue
        // is not empty then the final chunk is set to [`BatchComplete`]
        // otherwise this is the final batch for this remote bloom
        // and the final chunk is set to [`AllComplete`].
        let complete = match queue_id {
            Some(queue_id) => {
                if state
                    .ops_batch_queue
                    .0
                    .share_ref(|queue| Ok(queue.is_empty(&queue_id)))?
                {
                    MissingOpsStatus::AllComplete as u8
                } else {
                    MissingOpsStatus::BatchComplete as u8
                }
            }
            None => MissingOpsStatus::AllComplete as u8,
        };

        // Chunk the ops into multiple gossip messages if needed.
        into_chunks(&mut gossip, missing_hashes, complete);

        Ok(gossip)
    }

    /// Incoming ops that were missing from this nodes bloom filter.
    pub(super) async fn incoming_missing_op_hashes(
        &self,
        source: FetchSource,
        ops: Vec<OpHashSized>,
        transfer_method: TransferMethod,
    ) -> KitsuneResult<()> {
        for op_hash in ops {
            let (hash, size) = op_hash.into_inner();
            let request = FetchPoolPush {
                key: FetchKey::Op(hash),
                context: None,
                space: self.space.clone(),
                source: source.clone(),
                size,
                transfer_method,
            };
            self.fetch_pool.push(request);
        }
        Ok(())
    }
}

/// Separate gossip into chunks to keep messages under the max size.
// pair(maackle, freesig): can use this for chunking, see above fn for use
fn into_chunks(gossip: &mut Vec<ShardedGossipWire>, hashes: Vec<KOpHash>, complete: u8) {
    let mut chunk = Vec::with_capacity(hashes.len());
    let mut size = 0;

    // If there are no ops missing we send back an empty final chunk
    // so the other side knows we're done.
    if hashes.is_empty() {
        gossip.push(ShardedGossipWire::missing_op_hashes(vec![], complete));
    }

    for op in hashes {
        // Bytes for this op.
        let bytes = op.0.len();

        // Check if this op will fit without going over the max.
        if size + bytes <= MAX_SEND_BUF_BYTES {
            // Op will fit so add it to the chunk and update the size.
            chunk.push(OpHashSized::new(op, None));
            size += bytes;
        } else {
            // Op won't fit so flush the chunk.
            // There will be at least one more chunk so this isn't the final.
            gossip.push(ShardedGossipWire::missing_op_hashes(
                std::mem::take(&mut chunk),
                MissingOpsStatus::ChunkComplete as u8,
            ));
            // Reset the size to this ops size.
            size = bytes;
            // Push this op onto the next chunk.
            chunk.push(OpHashSized::new(op, None));
        }
    }
    // If there is a final chunk to write then add it and set it to final.
    if !chunk.is_empty() {
        gossip.push(ShardedGossipWire::missing_op_hashes(chunk, complete));
    }
}

impl OpsBatchQueue {
    /// Create a new set of queues.
    pub fn new() -> Self {
        Self(Share::new(OpsBatchQueueInner::new()))
    }

    /// Check if all queues are empty.
    pub fn is_empty(&self) -> bool {
        self.0
            .share_mut(|i, _| {
                i.queues.retain(|_, q| !q.is_empty());
                Ok(i.queues.is_empty() && i.region_queue.is_empty())
            })
            .unwrap_or(true)
    }
}

impl OpsBatchQueueInner {
    fn new() -> Self {
        Self {
            next_id: 0,
            queues: HashMap::new(),
            region_queue: VecDeque::new(),
        }
    }

    fn new_id(&mut self) -> usize {
        let id = self.next_id;
        self.next_id += 1;
        id
    }

    /// Push some queued missing ops hashes onto the back of a queue.
    /// If a unique id is provided then that queue is used otherwise
    /// a new id is generated.
    fn push_back(&mut self, id: Option<usize>, queued: QueuedOps) -> usize {
        let id = id.unwrap_or_else(|| self.new_id());
        {
            let queue = self.queues.entry(id).or_default();
            queue.push_back(queued);
        }
        self.queues.retain(|_, q| !q.is_empty());
        id
    }

    /// Pop some queue missing op hashes of any queue.
    fn pop_front(&mut self) -> Option<(usize, QueuedOps)> {
        self.queues.retain(|_, q| !q.is_empty());
        let (id, queue) = self.queues.iter_mut().next()?;
        Some((*id, queue.pop_front()?))
    }

    // Check if a particular queue is empty.
    fn is_empty(&self, id: &usize) -> bool {
        self.queues.get(id).map_or(true, |q| q.is_empty())
    }
}

impl std::fmt::Debug for OpsBatchQueue {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("OpsBatchQueue").finish()?;
        let mut map = f.debug_map();
        let _ = self.0.share_ref(|q| {
            let sizes = q.queues.iter().map(|(id, q)| {
                let h = q
                    .iter()
                    .filter(|b| matches!(b, QueuedOps::Hashes(_)))
                    .count();
                let b = q
                    .iter()
                    .filter(|b| matches!(b, QueuedOps::Bloom(_)))
                    .count();
                (id, (h, b))
            });
            map.entries(sizes);
            Ok(())
        });
        map.finish()
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/state_map.rs
================================================
use super::*;

/// Map of gossip round state that checks for timed out rounds on gets.
#[derive(Default, Debug)]
pub(super) struct RoundStateMap {
    map: HashMap<NodeCert, RoundState>,
    timed_out: Vec<(NodeCert, RoundState)>,
}

impl RoundStateMap {
    /// Check if round has timed out and remove it if it has.
    pub(super) fn check_timeout(&mut self, key: &NodeCert) -> bool {
        let mut timed_out = false;
        let mut finished = false;
        if let Some(state) = self.map.get(key) {
            if state.last_touch.elapsed() > state.round_timeout {
                if let Some(v) = self.map.remove(key) {
                    self.timed_out.push((key.clone(), v));
                }
                timed_out = true;
            } else if state.is_finished() {
                finished = true;
            }
        }
        // MD: I added this just to be safe. It made a difference.
        if finished {
            self.map.remove(key);
        }
        timed_out
    }

    /// Get the state if it hasn't timed out.
    pub(super) fn get(&mut self, key: &NodeCert) -> Option<&RoundState> {
        self.touch(key);
        self.check_timeout(key);
        self.map.get(key)
    }

    /// Get the mutable state if it hasn't timed out.
    pub(super) fn get_mut(&mut self, key: &NodeCert) -> Option<&mut RoundState> {
        self.touch(key);
        self.check_timeout(key);
        self.map.get_mut(key)
    }

    /// Remove the state.
    pub(super) fn remove(&mut self, key: &NodeCert) -> Option<RoundState> {
        self.map.remove(key)
    }

    /// Insert new state and return the old state if there was any.
    pub(super) fn insert(&mut self, key: NodeCert, round_state: RoundState) -> Option<RoundState> {
        self.map.insert(key, round_state)
    }

    /// Get the set of current rounds and remove any expired rounds.
    pub(super) fn current_rounds(&mut self) -> HashSet<NodeCert> {
        for (k, v) in std::mem::take(&mut self.map) {
            if v.last_touch.elapsed() < v.round_timeout {
                self.map.insert(k, v);
            } else {
                self.timed_out.push((k, v));
            }
        }
        self.map.keys().cloned().collect::<HashSet<_>>()
    }

    /// Check if a non-expired round exists.
    pub(super) fn round_exists(&mut self, key: &NodeCert) -> bool {
        self.check_timeout(key);
        self.map.contains_key(key)
    }

    /// Get all timed out rounds.
    pub(super) fn take_timed_out_rounds(&mut self) -> Vec<(NodeCert, RoundState)> {
        std::mem::take(&mut self.timed_out)
    }

    /// Touch a round to reset its timeout.
    fn touch(&mut self, key: &NodeCert) {
        if let Some(state) = self.map.get_mut(key) {
            state.last_touch = Instant::now();
        }
    }
}

impl From<HashMap<NodeCert, RoundState>> for RoundStateMap {
    fn from(map: HashMap<NodeCert, RoundState>) -> Self {
        Self {
            map,
            ..Default::default()
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::gossip::sharded_gossip::state_map::RoundStateMap;
    use crate::gossip::sharded_gossip::{NodeCert, RoundState};
    use kitsune_p2p_types::dht::arq::ArqSet;
    use rand::Rng;
    use std::collections::HashSet;
    use std::sync::Arc;
    use std::time::Duration;

    #[test]
    fn hold_round_state() {
        let (mut state_map, key) = test_round_state_map_with_single_key();
        assert!(state_map.round_exists(&key));

        let state = state_map.get(&key);
        assert!(state.is_some());
        assert_eq!(Duration::from_millis(5), state.unwrap().round_timeout);
    }

    #[test]
    fn remove_round_state() {
        let (mut state_map, key) = test_round_state_map_with_single_key();
        assert!(state_map.round_exists(&key));

        let removed = state_map.remove(&key);
        assert!(removed.is_some());

        assert!(!state_map.round_exists(&key));
    }

    #[test]
    fn modify_round_state() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.id = "test-change-state".to_string();
        }

        assert_eq!(
            "test-change-state".to_string(),
            state_map.get(&key).unwrap().id
        );
    }

    #[test]
    fn round_state_times_out_after_round_timeout() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            // We must use a zero timeout here, unlike the other tests which just set the last_touch in the past,
            // because `get` also performs a touch, which would undo that change.
            let state = state_map.get_mut(&key).unwrap();
            state.round_timeout = Duration::ZERO;
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        let state = state_map.get(&key);
        assert!(state.is_none());
    }

    #[test]
    fn round_state_mut_times_out_after_round_timeout() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            // We must use a zero timeout here, unlike the other tests which just set the last_touch in the past,
            // because `get` also performs a touch, which would undo that change.
            let state = state_map.get_mut(&key).unwrap();
            state.round_timeout = Duration::ZERO;
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        let state = state_map.get_mut(&key);
        assert!(state.is_none());
    }

    #[test]
    fn round_state_does_not_time_out_if_fetched() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.last_touch -= Duration::from_secs(10);
            // Should be marked as timed out on next `round_exists`
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        // Reset the last_touch
        state_map.get(&key);

        // Just reset by getting, so this will not remove
        let exists = state_map.round_exists(&key);
        assert!(exists);

        let state = state_map.get(&key);
        assert!(state.is_some())
    }

    #[test]
    fn round_state_mut_does_not_time_out_if_fetched() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.last_touch -= Duration::from_secs(10);
            // Should be marked as timed out on next `round_exists`
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        // Reset the last_touch
        state_map.get_mut(&key);

        // Just reset by getting, so this will not remove
        let exists = state_map.round_exists(&key);
        assert!(exists);

        let state = state_map.get_mut(&key);
        assert!(state.is_some())
    }

    #[test]
    fn get_current_rounds_from_round_state() {
        let (mut state_map, key_1) = test_round_state_map_with_single_key();

        let key_2 = insert_new_state(&mut state_map);
        let key_3 = insert_new_state(&mut state_map);

        assert_eq!(3, state_map.current_rounds().len());

        // Mark the state for key_2 as timed out
        {
            let state = state_map.get_mut(&key_2).unwrap();
            state.last_touch -= Duration::from_secs(10);
            // Should be marked as timed out on next `round_exists`
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        let mut expected_current = HashSet::new();
        expected_current.insert(key_1);
        expected_current.insert(key_3);

        assert_eq!(expected_current, state_map.current_rounds());
        assert_eq!(1, state_map.take_timed_out_rounds().len());
    }

    #[test]
    fn expired_rounds_can_only_be_fetched_from_round_state_once() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.last_touch -= Duration::from_secs(10);
            // Should be marked as timed out on next `round_exists`
            assert!(state.last_touch.elapsed() > state.round_timeout);
        }

        assert!(!state_map.round_exists(&key));
        assert_eq!(1, state_map.take_timed_out_rounds().len());
        assert_eq!(0, state_map.take_timed_out_rounds().len());
    }

    #[test]
    fn round_state_removed_if_finished_on_get_mut() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.received_all_incoming_op_blooms = true;
            state.regions_are_queued = true;
            assert!(state.is_finished());
        }

        let state = state_map.get_mut(&key);
        assert!(state.is_none());
    }

    #[test]
    fn round_state_removed_if_finished_on_get() {
        let (mut state_map, key) = test_round_state_map_with_single_key();

        {
            let state = state_map.get_mut(&key).unwrap();
            state.received_all_incoming_op_blooms = true;
            state.regions_are_queued = true;
            assert!(state.is_finished());
        }

        let state = state_map.get(&key);
        assert!(state.is_none());
    }

    fn test_round_state_map_with_single_key() -> (RoundStateMap, NodeCert) {
        let mut state_map = RoundStateMap::default();
        let key = insert_new_state(&mut state_map);

        (state_map, key)
    }

    fn test_round_state() -> RoundState {
        RoundState::new(
            vec![],
            Arc::new(ArqSet::empty()),
            None,
            Duration::from_millis(5),
        )
    }

    fn insert_new_state(state_map: &mut RoundStateMap) -> NodeCert {
        let mut cert = vec![0; 32];
        rand::thread_rng().fill(&mut cert[..]);
        let key = NodeCert::from(Arc::new(cert.try_into().unwrap()));
        state_map.insert(key.clone(), test_round_state());

        key
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/store.rs
================================================
//! This module is the ideal interface we would have for the conductor (or other store that kitsune uses).
//! We should update the conductor to match this interface.

use std::collections::{HashMap, HashSet};
use std::ops::ControlFlow;
use std::sync::Arc;

use crate::event::{
    PutAgentInfoSignedEvt, QueryAgentsEvt, QueryOpHashesEvt, TimeWindow, TimeWindowInclusive,
};
use crate::types::event::KitsuneP2pEventSender;
use crate::{HostApi, HostApiLegacy};
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::dht::arq::ArqSet;
use kitsune_p2p_types::dht::region_set::RegionSetLtcs;
use kitsune_p2p_types::dht::Arq;
use kitsune_p2p_types::{
    agent_info::AgentInfoSigned,
    bin_types::{KitsuneAgent, KitsuneOpHash, KitsuneSpace},
    dht_arc::DhtArcSet,
    KitsuneError, KitsuneResult,
};

use super::ShardedGossipLocal;

/// A short-lived session for agent info. Local agents from Kitsune are combined with the list of agents from the host
/// to allow gossip to access agent info as needed. The session should be regularly refreshed from these sources.
#[derive(Default)]
pub(super) struct AgentInfoSession {
    /// The local agents that have joined a Kitsune space, converted to agent infos by calling the host.
    local_agents: Vec<AgentInfoSigned>,

    /// All the agents for this space.
    ///
    /// This includes both local and remote agents but note that it's possible for local agents to exist in this list but not in the `local_agents` list if the agents
    /// are in the host store but haven't yet joined the Kitsune space.
    all_agents: Vec<AgentInfoSigned>,

    /// Cache of agents whose storage arc is contained in an arc set.
    /// Finding these agents requires a host query so we cache the results because they are used frequently.
    agents_by_arc_set_cache: HashMap<ArqSet, Vec<AgentInfoSigned>>,
}

impl AgentInfoSession {
    pub(super) fn new(
        local_agents: Vec<AgentInfoSigned>,
        all_agents: Vec<AgentInfoSigned>,
    ) -> Self {
        Self {
            local_agents,
            all_agents,
            agents_by_arc_set_cache: HashMap::new(),
        }
    }

    pub(super) fn get_agents(&self) -> &[AgentInfoSigned] {
        &self.all_agents
    }

    pub(super) fn get_local_agents(&self) -> &[AgentInfoSigned] {
        &self.local_agents
    }

    pub(super) fn get_local_kitsune_agents(&self) -> HashSet<Arc<KitsuneAgent>> {
        self.local_agents
            .iter()
            .map(|info| info.agent.clone())
            .collect()
    }

    pub(super) fn local_agent_arqs(&self) -> Vec<(Arc<KitsuneAgent>, Arq)> {
        self.local_agents
            .iter()
            .map(|info| (info.agent.clone(), info.storage_arq))
            .collect()
    }

    // Get the arc intervals for locally joined agents.
    pub(super) fn local_arqs(&self) -> Vec<Arq> {
        self.local_agents
            .iter()
            .map(|info| info.storage_arq)
            .collect()
    }

    pub(super) async fn agent_info_within_arc_set(
        &mut self,
        host_api: &HostApiLegacy,
        space: &Arc<KitsuneSpace>,
        arc_set: ArqSet,
    ) -> KitsuneResult<Vec<AgentInfoSigned>> {
        match self.agents_by_arc_set_cache.entry(arc_set.clone()) {
            std::collections::hash_map::Entry::Occupied(o) => Ok(o.get().clone()),
            std::collections::hash_map::Entry::Vacant(v) => {
                let agents = host_api
                    .legacy
                    .query_agents(QueryAgentsEvt::new(space.clone()).by_arq_set(arc_set))
                    .await
                    .map_err(KitsuneError::other)?;
                v.insert(agents.clone());
                Ok(agents)
            }
        }
    }
}

/// Get all agent info signed for a space.
pub(super) async fn all_agent_info(
    host_api: &HostApiLegacy,
    space: &Arc<KitsuneSpace>,
) -> KitsuneResult<Vec<AgentInfoSigned>> {
    host_api
        .legacy
        .query_agents(QueryAgentsEvt::new(space.clone()))
        .await
        .map_err(KitsuneError::other)
}

/// Get all ops for all agents that fall within the specified arcset.
pub(super) async fn all_op_hashes_within_arcset(
    host_api: &HostApiLegacy,
    space: &Arc<KitsuneSpace>,
    common_arc_set: DhtArcSet,
    window: TimeWindow,
    max_ops: usize,
    include_limbo: bool,
) -> KitsuneResult<Option<(Vec<Arc<KitsuneOpHash>>, TimeWindowInclusive)>> {
    host_api
        .legacy
        .query_op_hashes(QueryOpHashesEvt {
            space: space.clone(),
            arc_set: common_arc_set,
            window,
            max_ops,
            include_limbo,
        })
        .await
        .map_err(KitsuneError::other)
}

/// A chunk of hashes.
pub struct TimeChunk {
    /// The time window they were found in.
    pub window: TimeWindow,
    /// The final hashes position.
    /// Note this is not the same as the window.end
    /// as the window is an exclusive range and
    /// the cursor is purposely set the last
    /// hashes position because the next hash could
    /// have the same timestamp.
    pub cursor: Timestamp,
    /// The hashes found in this chunk.
    pub hashes: Vec<Arc<KitsuneOpHash>>,
}

/// This query returns a stream of hashes chunked
/// by time window.
///
/// If all the hashes found in the search time window
/// fit into a single chunk then this will return one chunk.
///
/// Otherwise a chunk will be returned with the window for the hashes
/// that fit into a single chunk and the following chunk will attempt to
/// be produced from the remaining time window.
///
/// This process will continue until the time window is small enough that
/// all the hashes fit into the final chunk.
/// The final chunk will always have a time window with an end that matches
/// the end of the search time window.
///
/// If there are no hashes found for a time window then the remaining
/// time window is returned with an empty hashes vector.
/// Due to this fact this stream always returns at least one value because
/// even if there are no hashes the full time window will return with an empty
/// hashes vector.
///
/// This stream is very useful for pulling hash chunks until some limit is reached
/// where the cursor can be saved an a new hash query can be started in the future
/// where the search time window starts from the previous queries cursor.
pub(super) fn hash_chunks_query(
    host_api: HostApiLegacy,
    space: Arc<KitsuneSpace>,
    common_arc_set: DhtArcSet,
    search_time_window: TimeWindow,
    include_limbo: bool,
) -> impl futures::stream::TryStream<Ok = TimeChunk, Error = KitsuneError> + Unpin {
    let f = futures::stream::try_unfold(
        // The stream starts with the full time window and control flow is set to continue.
        (search_time_window, ControlFlow::Continue(())),
        move |(mut search_time_window, control_flow)| {
            let host_api = host_api.clone();
            let space = space.clone();
            let common_arc_set = common_arc_set.clone();
            async move {
                if let ControlFlow::Break(_) = control_flow {
                    // The previous iteration has decided to break the stream.
                    return Ok(None);
                }

                // Run the hash query for the current search time window up to the hashes limit.
                let result = all_op_hashes_within_arcset(
                    &host_api,
                    &space,
                    common_arc_set.clone(),
                    search_time_window.clone(),
                    ShardedGossipLocal::UPPER_HASHES_BOUND,
                    include_limbo,
                )
                .await?;

                let (hashes, found_time_window) = match result {
                    Some(r) => r,
                    None => {
                        // If no hashes were found then return the final time chunk with
                        // an empty hashes vector and break the stream.
                        let chunk = TimeChunk {
                            window: search_time_window.clone(),
                            cursor: search_time_window.end,
                            hashes: Vec::with_capacity(0),
                        };
                        return Ok(Some((chunk, (search_time_window, ControlFlow::Break(())))));
                    }
                };

                let num_found = hashes.len();

                // The found time window is inclusive and the end is the timestamp
                // of the final hash. If this is the final chunk the consumer wants
                // then this is the cursor they should start from in the future.
                let cursor = *found_time_window.end();

                // If we found the upper hashes bound then we are not done.
                if num_found >= ShardedGossipLocal::UPPER_HASHES_BOUND {
                    // The time window is the searches start to the found windows
                    // end.
                    // Because this window needs to be exclusive a micro second (the smallest
                    // unit in our timestamps) is added.
                    let window = search_time_window.start
                        ..found_time_window
                            .end()
                            .saturating_add(&std::time::Duration::from_micros(1));

                    // The search window for the next call is reduced to the timestamp of the final
                    // hash from this call (because multiple hashes can share the same timestamp) to
                    // the end of the search time window.
                    search_time_window = *found_time_window.end()..search_time_window.end;

                    let chunk = TimeChunk {
                        window,
                        cursor,
                        hashes,
                    };

                    // Return this chunk and continue the stream.
                    Ok(Some((
                        chunk,
                        (search_time_window, ControlFlow::Continue(())),
                    )))
                } else {
                    // The remaining hashes fit into this chunk so
                    // this is the final chunk and has a time window equal to
                    // this iterations search window.
                    let chunk = TimeChunk {
                        window: search_time_window.clone(),
                        cursor,
                        hashes,
                    };

                    // Return the final chunk and break the stream.
                    Ok(Some((chunk, (search_time_window, ControlFlow::Break(())))))
                }
            }
        },
    );
    Box::pin(f)
}

pub(super) async fn query_region_set<'a>(
    host_api: HostApi,
    space: Arc<KitsuneSpace>,
    common_arq_set: ArqSet,
) -> KitsuneResult<RegionSetLtcs> {
    host_api
        .query_region_set(space, common_arq_set)
        .await
        .map_err(KitsuneError::other)
}

/// Add new agent info to the p2p store.
pub(super) async fn put_agent_info(
    host_api: &HostApiLegacy,
    agents: &[Arc<AgentInfoSigned>],
) -> KitsuneResult<()> {
    let peer_data: Vec<_> = agents.iter().map(|i| (**i).clone()).collect();
    host_api
        .legacy
        .put_agent_info_signed(PutAgentInfoSignedEvt { peer_data })
        .await
        .map_err(KitsuneError::other)?;
    Ok(())
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/test_utils.rs
================================================
use std::sync::Arc;

use kitsune_p2p_types::{
    agent_info::AgentInfoSigned,
    bin_types::{KitsuneAgent, KitsuneBinType, KitsuneOpHash},
    tx_utils::PoolBuf,
};

use crate::gossip::{decode_bloom_filter, encode_bloom_filter, MetaOpKey};

use super::*;

/// Create an agent bloom for testing.
pub fn create_agent_bloom<'a>(
    agents: impl Iterator<Item = &'a AgentInfoSigned>,
    filter: Option<&AgentInfoSigned>,
) -> Option<PoolBuf> {
    let agents: Vec<_> = match filter {
        Some(filter) => agents
            .filter(|a| filter.storage_arc().contains(a.agent.get_loc()))
            .collect(),
        None => agents.collect(),
    };
    let mut bloom = bloomfilter::Bloom::new_for_fp_rate(agents.len(), 0.01);
    let empty = agents.is_empty();
    for info in agents {
        let signed_at_ms = info.signed_at_ms;
        // The key is the agent hash + the signed at.
        let key = MetaOpKey::Agent(info.0.agent.clone(), signed_at_ms);
        bloom.set(&key);
    }
    if empty {
        None
    } else {
        Some(encode_bloom_filter(&bloom))
    }
}

/// Create an ops bloom for testing.
pub fn create_op_bloom(ops: Vec<Arc<KitsuneOpHash>>) -> PoolBuf {
    let len = ops.len();
    let bloom = ops.into_iter().fold(
        bloomfilter::Bloom::new_for_fp_rate(len, 0.01),
        |mut bloom, op| {
            let key = MetaOpKey::Op(op);
            bloom.set(&key);
            bloom
        },
    );

    encode_bloom_filter(&bloom)
}

/// Check an ops bloom for testing.
pub fn check_ops_bloom<'a>(
    ops: impl Iterator<Item = (kitsune_p2p_timestamp::Timestamp, &'a Arc<KitsuneOpHash>)>,
    bloom: EncodedTimedBloomFilter,
) -> Vec<&'a Arc<KitsuneOpHash>> {
    match bloom {
        EncodedTimedBloomFilter::NoOverlap => vec![],
        EncodedTimedBloomFilter::MissingAllHashes { time_window } => ops
            .filter(|(t, _)| time_window.contains(t))
            .map(|(_, h)| h)
            .collect(),
        EncodedTimedBloomFilter::HaveHashes {
            filter,
            time_window,
        } => {
            let filter = decode_bloom_filter(&filter);
            ops.filter(|(t, _)| time_window.contains(t))
                .map(|(_, h)| h)
                .filter(|op| !filter.check(&MetaOpKey::Op((**op).clone())))
                .collect()
        }
    }
}

/// Check an ops bloom for testing.
pub fn check_agent_boom<'a>(
    agents: impl Iterator<Item = (&'a Arc<KitsuneAgent>, &'a AgentInfoSigned)>,
    bloom: &[u8],
) -> Vec<&'a Arc<KitsuneAgent>> {
    let filter = decode_bloom_filter(bloom);
    agents
        .filter(|(agent, info)| {
            !filter.check(&MetaOpKey::Agent((*agent).clone(), info.signed_at_ms))
        })
        .map(|(a, _)| a)
        .collect()
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/tests.rs
================================================
use super::*;
use crate::spawn::MockKitsuneP2pEventHandler;
use futures::FutureExt;
use rand::Rng;

mod bloom;
mod common;
mod ops;
mod test_two_nodes;

impl ShardedGossipLocal {
    /// Create an instance suitable for testing
    pub fn test(
        gossip_type: GossipType,
        host: HostApiLegacy,
        inner: ShardedGossipLocalState,
    ) -> Self {
        let mut space = vec![0; 36];
        rand::thread_rng().fill(&mut space[..]);
        let space = KitsuneSpace::new(space);
        let space = Arc::new(space);
        let fetch_pool = FetchPool::new_bitwise_or();

        Self {
            gossip_type,
            tuning_params: Default::default(),
            space,
            host_api: host,
            inner: Share::new(inner),
            closing: std::sync::atomic::AtomicBool::new(false),
            fetch_pool,
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/tests/bloom.rs
================================================
use std::collections::BTreeMap;
use std::slice::SliceIndex;

use super::common::*;
use super::*;
use crate::gossip::sharded_gossip::bloom::Batch;
use crate::HostStub;

#[tokio::test(flavor = "multi_thread")]
async fn bloom_windows() {
    let expected_time = time_range(Duration::from_secs(20), Duration::from_secs(2));
    let search_window = time_range(
        std::time::UNIX_EPOCH.elapsed().unwrap(),
        Duration::from_secs(0),
    );

    let r = make_node(1, expected_time.clone())
        .await
        .generate_op_blooms_for_time_window(&ArqSet::full_std(), search_window.clone())
        .await
        .unwrap();

    match r {
        Batch::Complete(v) => {
            assert_eq!(v.len(), 1);
            let r = v.first().unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_some());
            assert_eq!(*time, search_window);
        }
        _ => unreachable!(),
    }

    let r = make_empty_node()
        .await
        .generate_op_blooms_for_time_window(&ArqSet::full_std(), search_window.clone())
        .await
        .unwrap();

    match r {
        Batch::Complete(v) => {
            assert_eq!(v.len(), 1);
            let r = v.first().unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_none());
            assert_eq!(*time, search_window);
        }
        _ => unreachable!(),
    }

    let r = make_node(
        ShardedGossipLocal::UPPER_HASHES_BOUND - 1,
        expected_time.clone(),
    )
    .await
    .generate_op_blooms_for_time_window(&ArqSet::full_std(), search_window.clone())
    .await
    .unwrap();

    match r {
        Batch::Complete(v) => {
            assert_eq!(v.len(), 1);
            let r = v.first().unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_some());
            assert_eq!(*time, search_window);
        }
        _ => unreachable!(),
    }

    let r = make_node(
        ShardedGossipLocal::UPPER_HASHES_BOUND,
        expected_time.clone(),
    )
    .await
    .generate_op_blooms_for_time_window(&ArqSet::full_std(), search_window.clone())
    .await
    .unwrap();

    match r {
        Batch::Complete(v) => {
            assert_eq!(v.len(), 2);
            let r = v.first().unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_some());
            assert_eq!(
                *time,
                search_window.start
                    ..get_time_bounds(
                        ShardedGossipLocal::UPPER_HASHES_BOUND as u32,
                        expected_time.clone(),
                        ..ShardedGossipLocal::UPPER_HASHES_BOUND
                    )
                    .end
            );
            let r = v.get(1).unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_some());
            assert_eq!(
                *time,
                get_time_bounds(
                    ShardedGossipLocal::UPPER_HASHES_BOUND as u32,
                    expected_time.clone(),
                    (ShardedGossipLocal::UPPER_HASHES_BOUND - 1)
                        ..ShardedGossipLocal::UPPER_HASHES_BOUND
                )
                .start..search_window.end
            );
        }
        _ => unreachable!(),
    }

    let r = make_node(
        ShardedGossipLocal::UPPER_HASHES_BOUND * ShardedGossipLocal::UPPER_BLOOM_BOUND,
        expected_time.clone(),
    )
    .await
    .generate_op_blooms_for_time_window(&ArqSet::full_std(), search_window.clone())
    .await
    .unwrap();

    let last_cursor;
    match r {
        Batch::Partial { data: v, cursor } => {
            assert_eq!(v.len(), ShardedGossipLocal::UPPER_BLOOM_BOUND);
            let total =
                ShardedGossipLocal::UPPER_HASHES_BOUND * ShardedGossipLocal::UPPER_BLOOM_BOUND;
            // We use the same timestamp from the end of the last bloom as the beginning of the
            // next bloom incase there are multiple hashes with the same timestamp.
            // So we expect the cursor to land on the time for the last hash - 1 * UPPER_BLOOM_BOUND.
            let end_of_blooms_time = (get_time_bounds(
                total as u32,
                expected_time.clone(),
                ..=(total - ShardedGossipLocal::UPPER_BLOOM_BOUND),
            )
            // Take off the micro second that is added to make the bounds exclusive
            // because the cursor is the actual time of the last hash seen.
            .end - Duration::from_micros(1))
            .unwrap();
            assert_eq!(cursor, end_of_blooms_time);
            last_cursor = cursor;

            let mut expected_window = search_window.clone();
            for (i, TimedBloomFilter { bloom, time }) in v.into_iter().enumerate() {
                assert!(bloom.is_some());

                expected_window.end = get_time_bounds(
                    total as u32,
                    expected_time.clone(),
                    ..ShardedGossipLocal::UPPER_HASHES_BOUND
                        + i * ShardedGossipLocal::UPPER_HASHES_BOUND
                        - i,
                )
                .end;
                dbg!(i);
                eprintln!("{:?} -> {:?}", time, expected_window);
                assert_eq!(time, expected_window);

                // The next bloom starts at the actual last blooms last time (not the exclusive 1us)
                expected_window.start = (expected_window.end - Duration::from_micros(1)).unwrap();
            }
        }
        _ => unreachable!(),
    }

    let r = make_node(
        ShardedGossipLocal::UPPER_HASHES_BOUND * ShardedGossipLocal::UPPER_BLOOM_BOUND,
        expected_time.clone(),
    )
    .await
    .generate_op_blooms_for_time_window(&ArqSet::full_std(), last_cursor..search_window.end)
    .await
    .unwrap();

    match r {
        Batch::Complete(v) => {
            assert_eq!(v.len(), 1);
            let r = v.first().unwrap();
            let TimedBloomFilter { bloom, time } = r;
            assert!(bloom.is_some());
            assert_eq!(*time, last_cursor..search_window.end);
        }
        _ => unreachable!(),
    }
}

async fn make_node(num: usize, window: TimeWindow) -> ShardedGossipLocal {
    make_node_inner(Some((num, window))).await
}

async fn make_empty_node() -> ShardedGossipLocal {
    make_node_inner(None).await
}

async fn make_node_inner(data: Option<(usize, TimeWindow)>) -> ShardedGossipLocal {
    let mut evt_handler = MockKitsuneP2pEventHandler::new();
    let data = data.map(|(n, time)| {
        let len = time.end - time.start;
        let step = dbg!(len.unwrap().to_std().unwrap()) / dbg!(n as u32);
        dbg!(step);
        (0..n)
            .map(|_| Arc::new(KitsuneOpHash(vec![0; 36])))
            .enumerate()
            .map(|(i, data)| ((time.start + step * i as u32).unwrap().as_micros(), data))
            .collect::<BTreeMap<_, _>>()
    });
    evt_handler
        .expect_handle_query_op_hashes()
        .returning(move |input| {
            let data = data.clone();
            let data = data.and_then(|data| {
                let start = data
                    .range(input.window.start.as_micros()..input.window.end.as_micros())
                    .next()?
                    .0;
                let end = data
                    .range(input.window.start.as_micros()..input.window.end.as_micros())
                    .take(input.max_ops)
                    .last()?
                    .0;
                eprintln!(
                    "{} -> {}",
                    input.window.start.as_micros(),
                    input.window.end.as_micros()
                );
                eprintln!("{} -> {}", start, end);
                Some((
                    data.range(input.window.start.as_micros()..input.window.end.as_micros())
                        .map(|(_, d)| d.clone())
                        .take(input.max_ops)
                        .collect(),
                    Timestamp::from_micros(*start)..=Timestamp::from_micros(*end),
                ))
            });
            Ok(async move { Ok(data) }.boxed().into())
        });
    let (evt_sender, _) = spawn_handler(evt_handler).await;
    let host = HostStub::new();

    ShardedGossipLocal::test(
        GossipType::Historical,
        HostApiLegacy::new(host, evt_sender),
        ShardedGossipLocalState::default(),
    )
}

fn get_time_bounds(
    n: u32,
    window: TimeWindow,
    records: impl SliceIndex<[Timestamp], Output = [Timestamp]>,
) -> TimeWindow {
    let len = window.end - window.start;
    let step = len.unwrap().to_std().unwrap() / n;
    let times = (0..n)
        .map(|i| (window.start + step * i).unwrap())
        .collect::<Vec<_>>();

    let mut iter = times[records].iter();
    let start = iter.next().unwrap();
    let mut end = *iter.last().unwrap_or(start);
    end = (end + Duration::from_micros(1)).unwrap();
    Timestamp::from_micros(start.as_micros())..Timestamp::from_micros(end.as_micros())
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/tests/common.rs
================================================
use super::*;
use crate::test_util::hash_op_data;
pub use crate::test_util::spawn_handler;
use crate::{HostApi, KitsuneHost};
use ::fixt::prelude::*;
use kitsune_p2p_bin_data::fixt::*;
use kitsune_p2p_fetch::FetchPoolConfig;
use kitsune_p2p_types::box_fut;
use kitsune_p2p_types::dht::arq::ArqSize;
use kitsune_p2p_types::dht::prelude::{ArqSet, RegionCoordSetLtcs, RegionData};
use kitsune_p2p_types::dht::spacetime::{TelescopingTimes, Topology};
use kitsune_p2p_types::dht::ArqStrat;
use kitsune_p2p_types::dht_arc::MAX_HALF_LENGTH;
use num_traits::Zero;

#[derive(Debug)]
pub struct StandardResponsesHostApi {
    infos: Vec<AgentInfoSigned>,
    topology: Topology,
    _strat: ArqStrat,
    with_data: bool,
}

impl FetchPoolConfig for StandardResponsesHostApi {
    fn merge_fetch_contexts(&self, _a: u32, _b: u32) -> u32 {
        unimplemented!()
    }
}

impl KitsuneHost for StandardResponsesHostApi {
    fn block(&self, _: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn unblock(&self, _: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn is_blocked(
        &self,
        _: kitsune_p2p_block::BlockTargetId,
        _: Timestamp,
    ) -> crate::KitsuneHostResult<bool> {
        box_fut(Ok(false))
    }

    fn get_agent_info_signed(
        &self,
        input: GetAgentInfoSignedEvt,
    ) -> crate::KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>> {
        let agent = self
            .infos
            .clone()
            .into_iter()
            .find(|a| a.agent == input.agent)
            .unwrap();
        box_fut(Ok(Some(agent)))
    }

    fn remove_agent_info_signed(
        &self,
        _input: GetAgentInfoSignedEvt,
    ) -> crate::KitsuneHostResult<bool> {
        // unimplemented
        box_fut(Ok(false))
    }

    fn peer_extrapolated_coverage(
        &self,
        _space: Arc<KitsuneSpace>,
        _dht_arc_set: DhtArcSet,
    ) -> crate::KitsuneHostResult<Vec<f64>> {
        todo!()
    }

    fn query_size_limited_regions(
        &self,
        _space: Arc<KitsuneSpace>,
        _size_limit: u32,
        regions: Vec<dht::region::Region>,
    ) -> crate::KitsuneHostResult<Vec<dht::region::Region>> {
        // This false implementation will work fine as long as we're not trying
        // to test situations with regions with a large byte count getting broken up
        box_fut(Ok(regions))
    }

    fn query_region_set(
        &self,
        _space: Arc<KitsuneSpace>,
        arq_set: ArqSet,
    ) -> crate::KitsuneHostResult<RegionSetLtcs> {
        async move {
            let coords = RegionCoordSetLtcs::new(TelescopingTimes::new(1.into()), arq_set);
            let region_set = if self.with_data {
                // XXX: this is very fake, and completely wrong!
                //      in order to properly match the fake data returned in other methods,
                //      there should really only be one nonzero region.
                let data = RegionData {
                    hash: [1; 32].into(),
                    size: 1,
                    count: 1,
                };
                coords.into_region_set_infallible(|_| data.clone())
            } else {
                coords.into_region_set_infallible(|_| RegionData::zero())
            };
            Ok(region_set)
        }
        .boxed()
        .into()
    }

    fn record_metrics(
        &self,
        _space: Arc<KitsuneSpace>,
        _records: Vec<MetricRecord>,
    ) -> crate::KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn get_topology(
        &self,
        _space: Arc<KitsuneSpace>,
    ) -> crate::KitsuneHostResult<dht::spacetime::Topology> {
        box_fut(Ok(self.topology.clone()))
    }

    fn op_hash(&self, _op_data: KOpData) -> crate::KitsuneHostResult<KOpHash> {
        todo!()
    }

    fn query_op_hashes_by_region(
        &self,
        _space: Arc<KitsuneSpace>,
        _region: dht::region::RegionCoords,
    ) -> crate::KitsuneHostResult<Vec<OpHashSized>> {
        todo!()
    }
}

// TODO: integrate with `HandlerBuilder`
async fn standard_responses(
    agents: Vec<(Arc<KitsuneAgent>, AgentInfoSigned)>,
    with_data: bool,
) -> (MockKitsuneP2pEventHandler, HostApi) {
    let mut evt_handler = MockKitsuneP2pEventHandler::new();
    let infos = agents.iter().map(|(_, i)| i.clone()).collect::<Vec<_>>();
    let host_api = StandardResponsesHostApi {
        infos: infos.clone(),
        topology: Topology::standard_epoch_full(),
        _strat: ArqStrat::default(),
        with_data,
    };
    // Note that this mock is not realistic, query by agents should filter by input agents
    evt_handler.expect_handle_query_agents().returning({
        move |_| {
            let infos = infos.clone();
            Ok(async move { Ok(infos.clone()) }.boxed().into())
        }
    });

    if with_data {
        let fake_data = KitsuneOpData::new(vec![0]);
        let fake_hash = hash_op_data(&fake_data.0);
        let fake_hash_2 = fake_hash.clone();
        evt_handler
            .expect_handle_query_op_hashes()
            .returning(move |_| {
                let hash = fake_hash_2.clone();
                Ok(
                    async move { Ok(Some((vec![hash], full_time_window_inclusive()))) }
                        .boxed()
                        .into(),
                )
            });
        evt_handler
            .expect_handle_fetch_op_data()
            .returning(move |_| {
                let hash = fake_hash.clone();
                let data = fake_data.clone();
                Ok(async move { Ok(vec![(hash, data)]) }.boxed().into())
            });
    } else {
        evt_handler
            .expect_handle_query_op_hashes()
            .returning(|_| Ok(async { Ok(None) }.boxed().into()));
        evt_handler
            .expect_handle_fetch_op_data()
            .returning(|_| Ok(async { Ok(vec![]) }.boxed().into()));
    }
    evt_handler
        .expect_handle_receive_ops()
        .returning(|_, _, _| Ok(async { Ok(()) }.boxed().into()));

    (evt_handler, Arc::new(host_api))
}

pub async fn setup_player(
    state: ShardedGossipLocalState,
    agents: Vec<(Arc<KitsuneAgent>, AgentInfoSigned)>,
    with_data: bool,
) -> ShardedGossipLocal {
    let (evt_handler, host_api) = standard_responses(agents, with_data).await;
    let (evt_sender, _) = spawn_handler(evt_handler).await;
    ShardedGossipLocal::test(
        GossipType::Historical,
        HostApiLegacy::new(host_api, evt_sender),
        state,
    )
}

pub async fn setup_standard_player(
    state: ShardedGossipLocalState,
    agents: Vec<(Arc<KitsuneAgent>, AgentInfoSigned)>,
) -> ShardedGossipLocal {
    setup_player(state, agents, true).await
}

pub async fn setup_empty_player(
    state: ShardedGossipLocalState,
    agents: Vec<(Arc<KitsuneAgent>, AgentInfoSigned)>,
) -> ShardedGossipLocal {
    let (evt_handler, host_api) = standard_responses(agents, false).await;
    let (evt_sender, _) = spawn_handler(evt_handler).await;
    ShardedGossipLocal::test(
        GossipType::Historical,
        HostApiLegacy::new(host_api, evt_sender),
        state,
    )
}

pub async fn agents_with_infos(num_agents: usize) -> Vec<(Arc<KitsuneAgent>, AgentInfoSigned)> {
    let mut out = Vec::with_capacity(num_agents);
    for agent in std::iter::repeat_with(|| Arc::new(fixt!(KitsuneAgent))).take(num_agents) {
        let info = agent_info(agent.clone()).await;
        out.push((agent, info));
    }
    out
}

pub async fn agent_info(agent: Arc<KitsuneAgent>) -> AgentInfoSigned {
    let rand_string: String = thread_rng()
        .sample_iter(&rand::distributions::Alphanumeric)
        .take(10)
        .map(char::from)
        .collect();
    AgentInfoSigned::sign(
        Arc::new(fixt!(KitsuneSpace)),
        agent,
        ArqSize::from_half_len(MAX_HALF_LENGTH),
        vec![url2::url2!(
            "kitsune-proxy://CIW6PxKxs{}cKwUpaMSmB7kLD8xyyj4mqcw/kitsune-quic/h/localhost/p/5778/-",
            rand_string
        )
        .into()],
        std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64,
        (std::time::UNIX_EPOCH.elapsed().unwrap() + std::time::Duration::from_secs(60 * 60))
            .as_millis() as u64,
        |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Predictable))) },
    )
    .await
    .unwrap()
}

/// Get an agents cert from their agent info
pub fn cert_from_info(info: AgentInfoSigned) -> NodeCert {
    let digest = kitsune_p2p_types::tx_utils::ProxyUrl::from_full(info.url_list[0].as_str())
        .unwrap()
        .digest()
        .unwrap();
    NodeCert::from(digest.0)
}

pub fn empty_bloom() -> EncodedTimedBloomFilter {
    EncodedTimedBloomFilter::MissingAllHashes {
        time_window: full_time_window(),
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/tests/ops.rs
================================================
use std::collections::VecDeque;

use kitsune_p2p_types::dht::{
    prelude::Segment,
    region::{Region, RegionCoords, RegionData},
};

use crate::gossip::sharded_gossip::ops::get_region_queue_batch;

fn fake_region(count: u32, size: u32) -> Region {
    Region {
        coords: RegionCoords {
            space: Segment::new(0, 0),
            time: Segment::new(0, 0),
        },
        data: RegionData {
            hash: [0; 32].into(),
            count,
            size,
        },
    }
}

#[test]
fn test_region_queue() {
    fn run(queue: &mut VecDeque<Region>, batch_size: u32) -> Vec<u32> {
        get_region_queue_batch(queue, batch_size)
            .into_iter()
            .map(|r| r.data.size)
            .collect()
    }

    // 5 hashes per batch
    const BATCH_SIZE: u32 = 36 * 5;

    let mut queue: VecDeque<_> = vec![
        fake_region(1, 1000),
        fake_region(2, 2000),
        fake_region(3, 3000),
        fake_region(5, 5000),
        fake_region(8, 8000),
        fake_region(101, 1000),
        fake_region(102, 2000),
        fake_region(103, 3000),
    ]
    .into();
    let initial_len = queue.len();

    assert_eq!(queue.len(), initial_len);

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 2);
    assert_eq!(r, (vec![1000, 2000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 3);
    assert_eq!(r, (vec![3000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 4);
    assert_eq!(r, (vec![5000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 5);
    assert_eq!(r, (vec![8000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 6);
    assert_eq!(r, (vec![1000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), initial_len - 7);
    assert_eq!(r, (vec![2000]));

    let r = run(&mut queue, BATCH_SIZE);
    assert_eq!(queue.len(), 0);
    assert_eq!(r, (vec![3000]));
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/gossip/sharded_gossip/tests/test_two_nodes.rs
================================================
use super::common::*;
use super::*;
use itertools::Itertools;

#[tokio::test(flavor = "multi_thread")]
/// Runs through a happy path gossip round between two agents.
async fn sharded_sanity_test() {
    // - Setup players and data.
    let bob_cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    let agents = agents_with_infos(2).await;
    let all_agents: Vec<AgentInfoSigned> = agents.iter().map(|x| x.1.clone()).collect();
    let mut iter = agents.clone().into_iter();
    let alice_agent = iter.next().unwrap().0;
    let bob_agent = iter.next().unwrap().0;

    let alice = setup_standard_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset! { alice_agent.clone() },
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    let bob = setup_standard_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset! { bob_agent.clone() },
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    let mut agent_info_session = AgentInfoSession::new(
        bob.query_agents_by_local_agents().await.unwrap(),
        all_agents.clone(),
    );

    // - Bob tries to initiate.
    let (_, _, bob_outgoing) = bob
        .try_initiate(&mut agent_info_session)
        .await
        .unwrap()
        .unwrap();
    let alices_cert = bob
        .inner
        .share_ref(|i| Ok(i.initiate_tgt.as_ref().unwrap().cert.clone()))
        .unwrap();

    // - Send initiate to alice.
    let alice_outgoing = alice
        .process_incoming(bob_cert.clone(), bob_outgoing, &mut agent_info_session)
        .await
        .unwrap();

    // - Alice responds to the initiate with 1 accept and 1 blooms.
    assert_eq!(alice_outgoing.len(), 2);
    alice
        .inner
        .share_mut(|i, _| {
            // - Check alice has one current round.
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();

    let mut bob_outgoing = Vec::new();

    // - Send the above to bob.
    for incoming in alice_outgoing {
        let outgoing = bob
            .process_incoming(alices_cert.clone(), incoming, &mut agent_info_session)
            .await
            .unwrap();
        bob_outgoing.extend(outgoing);
    }

    // - Bob responds with 1 blooms and 1 responses to alice's blooms.
    assert_eq!(bob_outgoing.len(), 2);
    bob.inner
        .share_mut(|i, _| {
            // - Check bob has one current round.
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();

    let mut alice_outgoing = Vec::new();

    // - Send the above to alice.
    for incoming in bob_outgoing {
        let outgoing = alice
            .process_incoming(bob_cert.clone(), incoming, &mut agent_info_session)
            .await
            .unwrap();
        alice_outgoing.extend(outgoing);
    }
    // - Alice responds with 1 responses to bob's blooms.
    assert_eq!(alice_outgoing.len(), 1);

    alice
        .inner
        .share_mut(|i, _| {
            // Assert alice has no initiate target.
            assert!(i.initiate_tgt.is_none());
            // Assert alice has no current rounds as alice
            // has now finished this round of gossip.
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();

    let mut bob_outgoing = Vec::new();
    // - Send alice's missing ops messages to bob.
    for incoming in alice_outgoing {
        let outgoing = bob
            .process_incoming(alices_cert.clone(), incoming, &mut agent_info_session)
            .await
            .unwrap();
        bob_outgoing.extend(outgoing);
    }
    // - Bob should have no responses.
    assert_eq!(bob_outgoing.len(), 0);

    bob.inner
        .share_mut(|i, _| {
            // Assert bob has no initiate target.
            assert!(i.initiate_tgt.is_none());
            // Assert bob has no current rounds as alice
            // has now finished this round of gossip.
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This tests that sending missing ops that isn't
/// marked as finished does not finish the round.
async fn partial_missing_doesnt_finish() {
    let cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    // - Set bob up with a current round that expects one
    // response to a sent bloom.
    let bob = setup_standard_player(
        ShardedGossipLocalState {
            round_map: maplit::hashmap! {
                cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 1,
                    received_all_incoming_op_blooms: true,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        vec![],
    )
    .await;

    // - Send a missing ops message that isn't marked as finished.
    let incoming = ShardedGossipWire::MissingOpHashes(MissingOpHashes {
        ops: vec![],
        finished: MissingOpsStatus::ChunkComplete as u8,
    });

    let mut agent_info_session =
        AgentInfoSession::new(bob.query_agents_by_local_agents().await.unwrap(), vec![]);

    let outgoing = bob
        .process_incoming(cert.clone(), incoming, &mut agent_info_session)
        .await
        .unwrap();
    assert_eq!(outgoing.len(), 0);

    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            // - Check bob still has a current round.
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that a missing ops message that is
/// marked as finished does finish the round.
async fn missing_ops_finishes() {
    let cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    // - Set bob up the same as the test above.
    let bob = setup_standard_player(
        ShardedGossipLocalState {
            round_map: maplit::hashmap! {
                cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 1,
                    received_all_incoming_op_blooms: true,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        vec![],
    )
    .await;

    // Send a message marked as finished.
    let incoming = ShardedGossipWire::MissingOpHashes(MissingOpHashes {
        ops: vec![],
        finished: MissingOpsStatus::AllComplete as u8,
    });

    let mut agent_info_session =
        AgentInfoSession::new(bob.query_agents_by_local_agents().await.unwrap(), vec![]);

    let outgoing = bob
        .process_incoming(cert.clone(), incoming, &mut agent_info_session)
        .await
        .unwrap();
    assert_eq!(outgoing.len(), 0);

    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            // - Bob now has no current rounds.
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that a missing ops message that is
/// marked as finished doesn't finish the round when
/// the player is still awaiting incoming blooms.
async fn missing_ops_doesnt_finish_awaiting_bloom_responses() {
    let cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    // - Set bob up awaiting incoming blooms and one response.
    let bob = setup_standard_player(
        ShardedGossipLocalState {
            round_map: maplit::hashmap! {
                cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 1,
                    received_all_incoming_op_blooms: false,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        vec![],
    )
    .await;

    // - Send a message marked as finished.
    let incoming = ShardedGossipWire::MissingOpHashes(MissingOpHashes {
        ops: vec![],
        finished: MissingOpsStatus::AllComplete as u8,
    });

    let mut agent_info_session =
        AgentInfoSession::new(bob.query_agents_by_local_agents().await.unwrap(), vec![]);

    let outgoing = bob
        .process_incoming(cert.clone(), incoming, &mut agent_info_session)
        .await
        .unwrap();
    assert_eq!(outgoing.len(), 0);

    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            // - Bob still has a current round.
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that a ops bloom message does
/// finish the round when there are no outstanding response.
async fn bloom_response_finishes() {
    let cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    // - Set bob up with a current round that expects no responses
    // and has not received all blooms.
    let bob = setup_standard_player(
        ShardedGossipLocalState {
            round_map: maplit::hashmap! {
                cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 0,
                    received_all_incoming_op_blooms: false,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        vec![],
    )
    .await;

    // - Send the final ops bloom message.
    let incoming = ShardedGossipWire::OpBloom(OpBloom {
        missing_hashes: empty_bloom(),
        finished: true,
    });

    let mut agent_info_session =
        AgentInfoSession::new(bob.query_agents_by_local_agents().await.unwrap(), vec![]);

    let outgoing = bob
        .process_incoming(cert.clone(), incoming, &mut agent_info_session)
        .await
        .unwrap();
    assert_eq!(outgoing.len(), 1);

    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            // - Bob now has no current rounds.
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that an ops bloom message doesn't
/// finish the round when their are outstanding responses.
async fn bloom_response_doesnt_finish_outstanding_incoming() {
    let cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));

    // - Set bob up with a current round that expects one response
    // and has not received all blooms.
    let bob = setup_standard_player(
        ShardedGossipLocalState {
            round_map: maplit::hashmap! {
                cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 1,
                    received_all_incoming_op_blooms: false,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        vec![],
    )
    .await;

    // - Send the final ops bloom message.
    let incoming = ShardedGossipWire::OpBloom(OpBloom {
        missing_hashes: empty_bloom(),
        finished: true,
    });

    let mut agent_info_session =
        AgentInfoSession::new(bob.query_agents_by_local_agents().await.unwrap(), vec![]);

    let outgoing = bob
        .process_incoming(cert.clone(), incoming, &mut agent_info_session)
        .await
        .unwrap();
    assert_eq!(outgoing.len(), 1);

    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            // - Bob still has a current round.
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that a round with no data can
/// still finish.
async fn no_data_still_finishes() {
    // - Set up two players with no data.
    let alice_cert = NodeCert::from(Arc::new(vec![1; 32].try_into().unwrap()));
    let bob_cert = NodeCert::from(Arc::new(vec![2; 32].try_into().unwrap()));

    let agents = agents_with_infos(2).await;
    let all_agents = agents.iter().map(|(_, info)| info.clone()).collect_vec();
    // - Alice is expecting no responses and is expecting blooms.
    let alice = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[0].0.clone()),
            round_map: maplit::hashmap! {
                bob_cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 0,
                    received_all_incoming_op_blooms: false,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    // - Bob is expecting one responses and is expecting no blooms.
    let bob = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[1].0.clone()),
            round_map: maplit::hashmap! {
                alice_cert.clone() => RoundState {
                    remote_agent_list: vec![],
                    common_arq_set: Arc::new(ArqSet::full_std()),
                    num_expected_op_blooms: 1,
                    received_all_incoming_op_blooms: true,
                    has_pending_historical_op_data: false,
                    regions_are_queued: true,
                    id: nanoid::nanoid!(),
                    last_touch: Instant::now(),
                    round_timeout: std::time::Duration::MAX,
                    bloom_batch_cursor: None,
                    ops_batch_queue: OpsBatchQueue::new(),
                    region_set_sent: None,
                    region_diffs: Default::default(),
                }
            }
            .into(),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    // - Send the final ops bloom message to alice.
    let incoming = ShardedGossipWire::OpBloom(OpBloom {
        missing_hashes: empty_bloom(),
        finished: true,
    });

    let outgoing = alice
        .process_incoming(
            bob_cert.clone(),
            incoming,
            &mut AgentInfoSession::new(
                alice.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();

    // - Alice responds with an empty missing ops.
    assert_eq!(outgoing.len(), 1);

    // - Send this to bob.
    let outgoing = bob
        .process_incoming(
            alice_cert.clone(),
            outgoing.into_iter().next().unwrap(),
            &mut AgentInfoSession::new(
                bob.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();

    // - Bob has no response.
    assert_eq!(outgoing.len(), 0);

    // - Both players have no current rounds.
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();
    bob.inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_none());
            assert_eq!(i.round_map.current_rounds().len(), 0);
            Ok(())
        })
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that when two players simultaneously
/// initiate a round it is handled correctly.
async fn double_initiate_is_handled() {
    let agents = agents_with_infos(2).await;
    let all_agents: Vec<AgentInfoSigned> = agents.iter().map(|x| x.1.clone()).collect();
    // - Set up two players with themselves as local agents.
    let alice = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[0].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    let bob = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[1].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    // - Both players try to initiate and only have the other as a remote agent.
    let (bob_cert, _, alice_initiate) = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap()
        .unwrap();
    let (alice_cert, _, bob_initiate) = bob
        .try_initiate(&mut AgentInfoSession::new(
            bob.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap()
        .unwrap();

    // - Both players process the initiate.
    let alice_outgoing = alice
        .process_incoming(
            bob_cert,
            bob_initiate,
            &mut AgentInfoSession::new(
                alice.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();
    let bob_outgoing = bob
        .process_incoming(
            alice_cert,
