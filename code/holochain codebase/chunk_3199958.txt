            .call(&bob, "must_get_entry", action.entry_hash())
            .await;
        assert_eq!(Entry::from(must_get_entry), entry);

        // Must get action returns the action if it exists regardless of the
        // validation status.
        let must_get_action: SignedActionHashed = conductor
            .call(&bob, "must_get_action", action_hash.clone())
            .await;
        assert_eq!(must_get_action.action(), &action,);

        // Must get VALID record ONLY returns the record if it is valid.
        let must_get_valid_record: Result<Record, _> = conductor
            .call_fallible(&bob, "must_get_valid_record", action_hash)
            .await;
        assert!(must_get_valid_record.is_err());

        let bad_entry_hash = EntryHash::from_raw_32(vec![1; 32]);
        let bad_must_get_entry: Result<EntryHashed, _> = conductor
            .call_fallible(&bob, "must_get_entry", bad_entry_hash)
            .await;
        assert!(bad_must_get_entry.is_err());

        let bad_action_hash = ActionHash::from_raw_32(vec![2; 32]);
        let bad_must_get_action: Result<SignedActionHashed, _> = conductor
            .call_fallible(&bob, "must_get_action", bad_action_hash)
            .await;
        assert!(bad_must_get_action.is_err());
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/must_get_valid_record.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostContext;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_cascade::CascadeImpl;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(_ribosome, call_context))
)]
pub fn must_get_valid_record(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: MustGetValidRecordInput,
) -> Result<Record, RuntimeError> {
    tracing::debug!("begin must_get_valid_record");
    let ret = match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            read_workspace_deterministic: Permission::Allow,
            ..
        } => {
            let action_hash = input.into_inner();

            // timeouts must be handled by the network
            tokio_helper::block_forever_on(async move {
                let workspace = call_context.host_context.workspace();
                use crate::core::ribosome::ValidateHostAccess;
                let (cascade, opt) = match call_context.host_context {
                    HostContext::Validate(ValidateHostAccess { is_inline, .. }) => {
                        if is_inline {
                            (
                                CascadeImpl::from_workspace_and_network(
                                    &workspace,
                                    call_context.host_context.network().clone(),
                                ),
                                GetOptions::network(),
                            )
                        } else {
                            (
                                CascadeImpl::from_workspace_stores(workspace.stores(), None),
                                GetOptions::local(),
                            )
                        }
                    }
                    _ => (
                        CascadeImpl::from_workspace_and_network(
                            &workspace,
                            call_context.host_context.network().clone(),
                        ),
                        GetOptions::local(),
                    ),
                };
                match cascade
                    .get_record_details(action_hash.clone(), opt)
                    .await
                    .map_err(|cascade_error| -> RuntimeError {
                        wasm_error!(WasmErrorInner::Host(cascade_error.to_string())).into()
                    })? {
                    Some(RecordDetails {
                        record,
                        validation_status: ValidationStatus::Valid,
                        ..
                    }) => Ok(record),
                    _ => match call_context.host_context {
                        HostContext::EntryDefs(_)
                        | HostContext::GenesisSelfCheckV1(_)
                        | HostContext::GenesisSelfCheckV2(_)
                        | HostContext::PostCommit(_)
                        | HostContext::ZomeCall(_) => Err(wasm_error!(WasmErrorInner::Host(
                            format!("Failed to get Record {}", action_hash)
                        ))
                        .into()),
                        HostContext::Init(_) => Err(wasm_error!(WasmErrorInner::HostShortCircuit(
                            holochain_serialized_bytes::encode(
                                &ExternIO::encode(InitCallbackResult::UnresolvedDependencies(
                                    UnresolvedDependencies::Hashes(vec![action_hash.into()],)
                                ))
                                .map_err(|e| -> RuntimeError { wasm_error!(e).into() })?,
                            )
                            .map_err(|e| -> RuntimeError { wasm_error!(e).into() })?
                        ))
                        .into()),
                        HostContext::Validate(_) => {
                            Err(wasm_error!(WasmErrorInner::HostShortCircuit(
                                holochain_serialized_bytes::encode(
                                    &ExternIO::encode(
                                        ValidateCallbackResult::UnresolvedDependencies(
                                            UnresolvedDependencies::Hashes(
                                                vec![action_hash.into()],
                                            )
                                        )
                                    )
                                    .map_err(|e| -> RuntimeError { wasm_error!(e).into() })?,
                                )
                                .map_err(|e| -> RuntimeError { wasm_error!(e).into() })?
                            ))
                            .into())
                        }
                    },
                }
            })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "must_get_valid_record".into(),
            )
            .to_string(),
        ))
        .into()),
    };
    tracing::debug!(?ret);
    ret
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/open_chain.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_wasmer_host::prelude::*;

use holochain_types::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn open_chain(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: OpenChainInput,
) -> Result<ActionHash, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            write_workspace: Permission::Allow,
            ..
        } => {
            // Construct the open chain action
            let action_builder = builder::OpenChain::new(input.prev_target, input.close_hash);

            let action_hash = tokio_helper::block_forever_on(tokio::task::spawn(async move {
                // push the action into the source chain
                let action_hash = call_context
                    .host_context
                    .workspace_write()
                    .source_chain()
                    .as_ref()
                    .expect("Must have source chain if write_workspace access is given")
                    .put_weightless(action_builder, None, ChainTopOrdering::Strict)
                    .await?;
                Ok::<ActionHash, RibosomeError>(action_hash)
            }))
            .map_err(|join_error| -> RuntimeError {
                wasm_error!(WasmErrorInner::Host(join_error.to_string())).into()
            })?
            .map_err(|ribosome_error| -> RuntimeError {
                wasm_error!(WasmErrorInner::Host(ribosome_error.to_string())).into()
            })?;

            // Return the hash of the chain open
            Ok(action_hash)
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "open_chain".into()
            )
            .to_string()
        ))
        .into()),
    }
}

#[cfg(test)]
mod tests {
    use holo_hash::fixt::ActionHashFixturator;
use super::open_chain;
    use crate::fixt::ZomeCallHostAccessFixturator;
    use crate::fixt::{CallContextFixturator, RealRibosomeFixturator};
    use ::fixt::prelude::*;
    use holochain_util::tokio_helper;
    use holochain_wasm_test_utils::{TestWasm, TestWasmPair};
    use holochain_zome_types::prelude::*;
    use std::sync::Arc;

    #[tokio::test(flavor = "multi_thread")]
    async fn call_open_chain() {
        // Note that any zome will do here, we're not calling its functions!
        let ribosome =
            RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![TestWasm::MigrateNew]))
                .next()
                .unwrap();
        let mut call_context = CallContextFixturator::new(Unpredictable).next().unwrap();
        call_context.zome =
            TestWasmPair::<IntegrityZome, CoordinatorZome>::from(TestWasm::MigrateNew)
                .coordinator
                .erase_type();
        let host_access = fixt!(ZomeCallHostAccess, Predictable);
        let host_access_2 = host_access.clone();
        call_context.host_context = host_access.into();
        let input = OpenChainInput {
            prev_target: fixt!(MigrationTarget),
            close_hash: fixt!(ActionHash),
        };

        let output = open_chain(Arc::new(ribosome), Arc::new(call_context), input).unwrap();

        // the chain head should be the committed chain open action
        let chain_head = tokio_helper::block_forever_on(async move {
            host_access_2
                .workspace
                .source_chain()
                .as_ref()
                .unwrap()
                .chain_head()
                .unwrap()
                .unwrap()
                .action
        });

        assert_eq!(chain_head, output);
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/query.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn query(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: ChainQueryFilter,
) -> Result<Vec<Record>, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            read_workspace: Permission::Allow,
            ..
        } => tokio_helper::block_forever_on(async move {
            let records: Vec<Record> = call_context
                .host_context
                .workspace()
                .source_chain()
                .as_ref()
                .expect("Must have source chain to query the source chain")
                .query(input)
                .await
                .map_err(|source_chain_error| -> RuntimeError {
                    wasm_error!(WasmErrorInner::Host(source_chain_error.to_string())).into()
                })?;
            Ok(records)
        }),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "query".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod slow_tests {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    async fn query_smoke_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::Query).await;

        let _hash_a: EntryHash = conductor.call(&alice, "add_path", "a".to_string()).await;
        let _hash_b: EntryHash = conductor.call(&alice, "add_path", "b".to_string()).await;

        let records: Vec<Record> = conductor
            .call(&alice, "query", ChainQueryFilter::default())
            .await;

        assert_eq!(records.len(), 6);
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/random_bytes.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

/// return n crypto secure random bytes from the standard holochain crypto lib
pub fn random_bytes(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: u32,
) -> Result<holochain_types::prelude::Bytes, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            non_determinism: Permission::Allow,
            ..
        } => {
            let mut bytes = vec![0; input as _];
            getrandom::getrandom(&mut bytes)
                .map_err(|error| -> RuntimeError {
                    wasm_error!(WasmErrorInner::Host(error.to_string())).into()
                })?;

            Ok(holochain_types::prelude::Bytes::from(bytes))
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "random_bytes".into()
            )
            .to_string()
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use crate::core::ribosome::host_fn::random_bytes::random_bytes;

    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use crate::core::ribosome::HostContext;
    use crate::fixt::CallContextFixturator;
    use crate::fixt::RealRibosomeFixturator;
    use crate::fixt::ZomeCallHostAccessFixturator;
    use ::fixt::prelude::*;
    use holochain_wasm_test_utils::TestWasm;
    use std::sync::Arc;

    #[tokio::test(flavor = "multi_thread")]
    /// we can get some random data out of the fn directly
    async fn random_bytes_test() {
        let ribosome = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![]))
            .next()
            .unwrap();
        let mut call_context = CallContextFixturator::new(::fixt::Unpredictable)
            .next()
            .unwrap();
        call_context.host_context = HostContext::ZomeCall(fixt!(ZomeCallHostAccess));
        const LEN: u32 = 10;

        let output: holochain_zome_types::prelude::Bytes =
            random_bytes(Arc::new(ribosome), Arc::new(call_context), LEN).unwrap();

        println!("{:?}", output);

        assert_ne!(&[0; LEN as usize], output.as_ref(),);
    }

    #[tokio::test(flavor = "multi_thread")]
    /// we can get some random data out of the fn via. a wasm call
    async fn ribosome_random_bytes_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::RandomBytes).await;
        const LEN: u32 = 5;
        let output: hdk::prelude::Bytes = conductor.call(&alice, "random_bytes", LEN).await;

        assert_ne!(&vec![0; LEN as usize], &output.to_vec());
    }

    #[tokio::test(flavor = "multi_thread")]
    /// we can get some random data out of the fn via. a wasm call
    async fn ribosome_rand_random_bytes_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::RandomBytes).await;
        const LEN: u32 = 5;
        let output: hdk::prelude::Bytes = conductor.call(&alice, "rand_random_bytes", LEN).await;

        assert_ne!(&vec![0; LEN as usize], &output.to_vec());
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/schedule.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn schedule(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: String,
) -> Result<(), RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            write_workspace: Permission::Allow,
            ..
        } => {
            call_context
                .host_context()
                .workspace_write()
                .source_chain()
                .as_ref()
                .expect("Must have source chain if write_workspace access is given")
                .scratch()
                .apply(|scratch| {
                    scratch.add_scheduled_fn(ScheduledFn::new(
                        call_context.zome.zome_name().clone(),
                        input.into(),
                    ));
                })
                .map_err(|e| wasm_error!(WasmErrorInner::Host(e.to_string())))?;
            Ok(())
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "schedule".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
mod tests {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_state::prelude::schedule_fn;
    use holochain_state::prelude::*;
    use holochain_state::schedule::fn_is_scheduled;
    use holochain_state::schedule::live_scheduled_fns;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    #[cfg(feature = "test_utils")]
    async fn schedule_test_low_level() -> anyhow::Result<()> {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            alice_pubkey,
            alice_host_fn_caller,
            ..
        } = RibosomeTestFixture::new(TestWasm::Schedule).await;

        alice_host_fn_caller
            .authored_db
            .write_async(move |txn| {
                let now = Timestamp::now();
                let the_past = (now - std::time::Duration::from_millis(1)).unwrap();
                let the_future = (now + std::time::Duration::from_millis(1000)).unwrap();
                let the_distant_future = (now + std::time::Duration::from_millis(2000)).unwrap();

                let ephemeral_scheduled_fn = ScheduledFn::new("foo".into(), "bar".into());
                let persisted_scheduled_fn = ScheduledFn::new("1".into(), "2".into());
                let persisted_schedule = Schedule::Persisted("* * * * * * *".into());

                schedule_fn(
                    txn,
                    &alice_pubkey,
                    persisted_scheduled_fn.clone(),
                    Some(persisted_schedule.clone()),
                    now,
                )
                .unwrap();
                schedule_fn(
                    txn,
                    &alice_pubkey,
                    ephemeral_scheduled_fn.clone(),
                    None,
                    now,
                )
                .unwrap();

                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey).unwrap()
                );

                // Deleting live ephemeral scheduled fns from now should delete.
                delete_live_ephemeral_scheduled_fns(txn, now, &alice_pubkey).unwrap();
                assert!(
                    !fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );

                schedule_fn(
                    txn,
                    &alice_pubkey,
                    ephemeral_scheduled_fn.clone(),
                    None,
                    now,
                )
                .unwrap();
                assert!(
                    fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );

                // Deleting live ephemeral fns from a past time should do nothing.
                delete_live_ephemeral_scheduled_fns(txn, the_past, &alice_pubkey).unwrap();
                assert!(
                    fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );

                // Deleting live ephemeral fns from the future should delete.
                delete_live_ephemeral_scheduled_fns(txn, the_future, &alice_pubkey).unwrap();
                assert!(
                    !fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );

                // Deleting all ephemeral fns should delete.
                schedule_fn(
                    txn,
                    &alice_pubkey,
                    ephemeral_scheduled_fn.clone(),
                    None,
                    now,
                )
                .unwrap();
                assert!(
                    fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                delete_all_ephemeral_scheduled_fns(txn).unwrap();
                assert!(
                    !fn_is_scheduled(txn, ephemeral_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );
                assert!(
                    fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey,).unwrap()
                );

                let ephemeral_future_schedule =
                    Schedule::Ephemeral(std::time::Duration::from_millis(1001));
                schedule_fn(
                    txn,
                    &alice_pubkey,
                    ephemeral_scheduled_fn.clone(),
                    Some(ephemeral_future_schedule.clone()),
                    now,
                )
                .unwrap();
                assert_eq!(
                    vec![(
                        persisted_scheduled_fn.clone(),
                        Some(persisted_schedule.clone())
                    )],
                    live_scheduled_fns(txn, the_future, &alice_pubkey,).unwrap(),
                );
                assert_eq!(
                    vec![
                        (persisted_scheduled_fn, Some(persisted_schedule)),
                        (ephemeral_scheduled_fn, Some(ephemeral_future_schedule)),
                    ],
                    live_scheduled_fns(txn, the_distant_future, &alice_pubkey,).unwrap(),
                );

                Result::<(), DatabaseError>::Ok(())
            })
            .await
            .unwrap();
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    #[cfg(feature = "test_utils")]
    async fn schedule_test_wasm() -> anyhow::Result<()> {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor,
            alice,
            alice_pubkey,
            alice_host_fn_caller,
            bob,
            bob_pubkey,
            bob_host_fn_caller,
            ..
        } = RibosomeTestFixture::new(TestWasm::Schedule).await;

        // We don't want the scheduler running and messing with our calculations.
        conductor
            .raw_handle()
            .start_scheduler(std::time::Duration::from_millis(1_000_000_000))
            .await?;

        // At first nothing has happened because init won't run until some zome
        // call runs.
        let query_tick: Vec<Record> = conductor.call(&alice, "query_tick_init", ()).await;
        assert!(query_tick.is_empty());

        // Wait to make sure we've init, but it should have happened for sure.
        while {
            !alice_host_fn_caller
                .authored_db
                .write_async({
                    let alice_pubkey = alice_pubkey.clone();
                    move |txn| {
                        let persisted_scheduled_fn = ScheduledFn::new(
                            TestWasm::Schedule.into(),
                            "cron_scheduled_fn_init".into(),
                        );

                        Result::<bool, DatabaseError>::Ok(
                            fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &alice_pubkey)
                                .unwrap(),
                        )
                    }
                })
                .await
                .unwrap()
        } {
            tokio::time::sleep(std::time::Duration::from_millis(1)).await;
        }

        // Round up to the next second so we don't trigger two tocks in quick
        // succession.
        let mut now =
            Timestamp::from_micros((Timestamp::now().as_micros() / 1_000_000 + 1) * 1_000_000 + 1);

        // The ephemeral function will dispatch each millisecond.
        // The tock will dispatch once and wait a second.
        let mut i: usize = 0;
        while i < 10 {
            conductor.raw_handle().dispatch_scheduled_fns(now).await;
            now = (now + std::time::Duration::from_millis(2))?;
            i += 1;
        }
        loop {
            let query_tick_init: Vec<Record> = conductor.call(&alice, "query_tick_init", ()).await;
            let query_tock_init: Vec<Record> = conductor.call(&alice, "query_tock_init", ()).await;
            if query_tick_init.len() == 5 && query_tock_init.len() == 1 {
                break;
            }
        }

        // after a second the tock will run again.
        now = (now + std::time::Duration::from_millis(1000))?;
        conductor.raw_handle().dispatch_scheduled_fns(now).await;
        loop {
            let query_tick_init: Vec<Record> = conductor.call(&alice, "query_tick_init", ()).await;
            let query_tock_init: Vec<Record> = conductor.call(&alice, "query_tock_init", ()).await;
            if query_tick_init.len() == 5 && query_tock_init.len() == 2 {
                break;
            }
        }

        // alice can schedule things outside of init.
        let query_tock: Vec<Record> = conductor.call(&alice, "query_tock", ()).await;
        assert!(query_tock.is_empty());

        let _schedule: () = conductor.call(&alice, "schedule", ()).await;

        // Round up to the next second so we don't trigger two tocks in quick
        // succession.
        now =
            Timestamp::from_micros((Timestamp::now().as_micros() / 1_000_000 + 1) * 1_000_000 + 1);

        let mut i: usize = 0;
        while i < 10 {
            conductor.raw_handle().dispatch_scheduled_fns(now).await;
            now = (now + std::time::Duration::from_millis(2))?;
            i += 1;
        }
        loop {
            tokio::time::sleep(std::time::Duration::from_millis(1)).await;
            let query_tick: Vec<Record> = conductor.call(&alice, "query_tick", ()).await;
            let query_tock: Vec<Record> = conductor.call(&alice, "query_tock", ()).await;
            if query_tick.len() == 5 && query_tock.len() == 1 {
                break;
            }
        }

        // after a second the tock will run again.
        now = (now + std::time::Duration::from_millis(1000))?;
        conductor.raw_handle().dispatch_scheduled_fns(now).await;
        loop {
            tokio::time::sleep(std::time::Duration::from_millis(1)).await;
            let query_tick: Vec<Record> = conductor.call(&alice, "query_tick", ()).await;
            let query_tock: Vec<Record> = conductor.call(&alice, "query_tock", ()).await;
            if query_tick.len() == 5 && query_tock.len() == 2 {
                break;
            }
        }

        // Starting the scheduler should flush ephemeral.
        let _schedule: () = conductor.call(&bob, "schedule", ()).await;

        assert!(bob_host_fn_caller
            .authored_db
            .write_async({
                let bob_pubkey = bob_pubkey.clone();
                move |txn| {
                    let persisted_scheduled_fn =
                        ScheduledFn::new(TestWasm::Schedule.into(), "scheduled_fn".into());

                    Result::<bool, DatabaseError>::Ok(
                        fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &bob_pubkey).unwrap(),
                    )
                }
            })
            .await
            .unwrap());

        conductor
            .raw_handle()
            .start_scheduler(std::time::Duration::from_millis(1_000_000_000))
            .await?;

        assert!(!bob_host_fn_caller
            .authored_db
            .write_async({
                let bob_pubkey = bob_pubkey.clone();
                move |txn| {
                    let persisted_scheduled_fn =
                        ScheduledFn::new(TestWasm::Schedule.into(), "scheduled_fn".into());

                    Result::<bool, DatabaseError>::Ok(
                        fn_is_scheduled(txn, persisted_scheduled_fn.clone(), &bob_pubkey).unwrap(),
                    )
                }
            })
            .await
            .unwrap());

        Ok(())
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/send_remote_signal.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_keystore::AgentPubKeyExt;
use holochain_nonce::fresh_nonce;
use holochain_types::access::Permission;
use holochain_types::prelude::CellId;
use holochain_types::prelude::ExternIO;
use holochain_wasmer_host::prelude::*;
use holochain_zome_types::prelude::Timestamp;
use holochain_zome_types::signal::RemoteSignal;
use holochain_zome_types::zome::FunctionName;
use holochain_zome_types::zome_io::ZomeCallParams;
use std::sync::Arc;
use tracing::Instrument;
use wasmer::RuntimeError;

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(_ribosome, call_context, input))
)]
pub fn send_remote_signal(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: RemoteSignal,
) -> Result<(), RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            write_network: Permission::Allow,
            agent_info: Permission::Allow,
            ..
        } => {
            const FN_NAME: &str = "recv_remote_signal";
            let from_agent = super::agent_info::agent_info(_ribosome, call_context.clone(), ())?
                .agent_latest_pubkey;
            // Timeouts and errors are ignored,
            // this is a send and forget operation.
            let network = call_context.host_context().network().clone();
            let RemoteSignal { agents, signal } = input;
            let zome_name = call_context.zome().zome_name().clone();
            let fn_name: FunctionName = FN_NAME.into();

            tokio::task::spawn(
                async move {
                    let mut to_agent_list = Vec::new();

                    let (nonce, expires_at) = match fresh_nonce(Timestamp::now()) {
                        Ok(nonce) => nonce,
                        Err(e) => {
                            tracing::info!("Failed to get a fresh nonce because of {:?}", e);
                            return;
                        }
                    };

                    for agent in agents {
                        let zome_call_params = ZomeCallParams {
                            provenance: from_agent.clone(),
                            cell_id: CellId::new(network.dna_hash(), agent.clone()),
                            zome_name: zome_name.clone(),
                            fn_name: fn_name.clone(),
                            cap_secret: None,
                            payload: signal.clone(),
                            nonce,
                            expires_at,
                        };
                        let (bytes, bytes_hash) = match zome_call_params.serialize_and_hash() {
                            Ok(bytes_and_hash) => bytes_and_hash,
                            Err(e) => {
                                tracing::info!(
                                    "Failed to serialize zome call for signal because of {:?}",
                                    e
                                );
                                return;
                            }
                        };

                        match from_agent
                            .sign_raw(call_context.host_context.keystore(), bytes_hash.into())
                            .await
                        {
                            Ok(signature) => {
                                to_agent_list.push((agent, ExternIO(bytes), signature))
                            }
                            Err(e) => {
                                tracing::info!(
                                    "Failed to sign and send remote signals because of {:?}",
                                    e
                                );
                                return;
                            }
                        };
                    }

                    if let Err(e) = network.send_remote_signal(to_agent_list).await {
                        tracing::info!("Failed to send remote signals because of {:?}", e);
                    }
                }
                .in_current_span(),
            );
            Ok(())
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "send_remote_signal".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
mod tests {
    use std::sync::atomic::AtomicUsize;
    use std::sync::atomic::Ordering;

    use super::*;
    use crate::sweettest::*;
    use futures::future;
    use hdk::prelude::*;

    fn test_zome(agents: Vec<AgentPubKey>, num_signals: Arc<AtomicUsize>) -> InlineIntegrityZome {
        let entry_def = EntryDef::default_from_id("entrydef");

        InlineIntegrityZome::new_unique(vec![entry_def.clone()], 0)
            .function("signal_others", move |api, ()| {
                let signal = ExternIO::encode("Hey").unwrap();
                let signal = RemoteSignal {
                    agents: agents.clone(),
                    signal,
                };
                tracing::debug!("sending remote signal to {:?}", agents);
                api.send_remote_signal(signal)?;
                Ok(())
            })
            .function("recv_remote_signal", move |api, signal: ExternIO| {
                tracing::debug!("remote signal");
                num_signals.fetch_add(1, Ordering::SeqCst);
                api.emit_signal(AppSignal::new(signal)).map_err(Into::into)
            })
            .function("init", move |api, ()| {
                let mut fns = BTreeSet::new();
                fns.insert((api.zome_info(()).unwrap().name, "recv_remote_signal".into()));
                let functions = GrantedFunctions::Listed(fns);
                let cap_grant_entry = CapGrantEntry {
                    tag: "".into(),
                    // empty access converts to unrestricted
                    access: ().into(),
                    functions,
                };
                api.create(CreateInput::new(
                    EntryDefLocation::CapGrant,
                    EntryVisibility::Private,
                    Entry::CapGrant(cap_grant_entry),
                    ChainTopOrdering::default(),
                ))
                .unwrap();

                Ok(InitCallbackResult::Pass)
            })
    }

    #[tokio::test(flavor = "multi_thread")]
    #[cfg(feature = "test_utils")]
    async fn remote_signal_test() -> anyhow::Result<()> {
        holochain_trace::test_run();
        const NUM_CONDUCTORS: usize = 5;

        let num_signals = Arc::new(AtomicUsize::new(0));

        let config = SweetConductorConfig::standard().no_dpki();
        let mut conductors = SweetConductorBatch::from_config(NUM_CONDUCTORS, config).await;

        let agents =
            future::join_all(conductors.iter().map(|c| SweetAgents::one(c.keystore()))).await;

        let zome = test_zome(agents.clone(), num_signals.clone());
        let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(("zome", zome)).await;

        let apps = conductors
            .setup_app_for_zipped_agents("app", &agents, &[dna_file.clone()])
            .await
            .unwrap();

        conductors.exchange_peer_info().await;

        let cells: Vec<_> = apps.cells_flattened();

        let mut signals = Vec::new();
        for h in conductors.iter() {
            signals.push(h.subscribe_to_app_signals("app".to_string()))
        }

        let _: () = conductors[0]
            .call(&cells[0].zome("zome"), "signal_others", ())
            .await;

        crate::assert_eq_retry_10s!(num_signals.load(Ordering::SeqCst), NUM_CONDUCTORS);

        for mut signal in signals {
            signal.recv().await.expect("Failed to recv signal");
        }

        Ok(())
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/sign.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

#[cfg_attr(feature = "instrument", tracing::instrument(skip(_ribosome, call_context)))]
pub fn sign(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: Sign,
) -> Result<Signature, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => tokio_helper::block_forever_on(async move {
            call_context
                .host_context
                .keystore()
                .sign(input.key, input.data.into_vec().into())
                .await
        })
        .map_err(|keystore_error| -> RuntimeError {
            wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into()
        }),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "sign".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    async fn ribosome_sign_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor,
            alice,
            alice_pubkey,
            bob_pubkey,
            ..
        } = RibosomeTestFixture::new(TestWasm::Sign).await;

        for (key, data) in [
            (alice_pubkey.clone(), vec![100_u8, 200_u8, 50_u8]),
            (bob_pubkey.clone(), vec![100_u8, 200_u8, 50_u8]),
            (alice_pubkey, vec![1_u8, 2_u8, 3_u8]),
            (bob_pubkey, vec![1_u8, 2_u8, 3_u8]),
        ] {
            let mut sigs = HashSet::new();
            for _ in 0..2 {
                let signature: Signature = conductor
                    .call(&alice, "sign", Sign::new_raw(key.clone(), data.clone()))
                    .await;

                sigs.insert(signature.clone());

                let valid: bool = conductor
                    .call(
                        &alice,
                        "verify_signature_raw",
                        VerifySignature {
                            key: key.clone(),
                            signature,
                            data: data.clone(),
                        },
                    )
                    .await;

                assert!(valid);
            }
            assert_eq!(sigs.len(), 1);
        }
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/sign_ephemeral.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn sign_ephemeral(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: SignEphemeral,
) -> Result<EphemeralSignatures, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => tokio_helper::block_forever_on(async move {
            let pk = sodoken::BufWriteSized::new_no_lock();
            let sk = sodoken::BufWriteSized::new_mem_locked()?;
            sodoken::sign::keypair(pk.clone(), sk.clone()).await?;
            let pk = pk.read_lock_sized().to_vec();
            let sk = sk.to_read_sized();

            let mut signatures = Vec::new();

            let sig = sodoken::BufWriteSized::new_no_lock();
            for data in input.into_inner().into_iter() {
                sodoken::sign::detached(sig.clone(), data.to_vec(), sk.clone()).await?;
                signatures.push((*sig.read_lock_sized()).into());
            }

            sodoken::SodokenResult::Ok(EphemeralSignatures {
                signatures,
                key: AgentPubKey::from_raw_32(pk),
            })
        })
        .map_err(|error| -> RuntimeError {
            wasm_error!(WasmErrorInner::Host(error.to_string())).into()
        }),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "sign_ephemeral".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_keystore::AgentPubKeyExt;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    async fn ribosome_sign_ephemeral_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::Sign).await;

        let output: Vec<EphemeralSignatures> = conductor.call(&alice, "sign_ephemeral", ()).await;

        #[derive(Serialize, Deserialize, Debug)]
        struct One([u8; 2]);
        #[derive(Serialize, Deserialize, Debug)]
        struct Two([u8; 2]);

        assert!(output[0]
            .key
            .verify_signature_raw(
                &output[0].signatures[0],
                holochain_serialized_bytes::encode(&One([1, 2]))
                    .unwrap()
                    .into()
            )
            .await
            .unwrap());

        assert!(output[0]
            .key
            .verify_signature_raw(
                &output[0].signatures[1],
                holochain_serialized_bytes::encode(&One([3, 4]))
                    .unwrap()
                    .into()
            )
            .await
            .unwrap());

        assert!(output[1]
            .key
            .verify_signature_raw(
                &output[1].signatures[0],
                holochain_serialized_bytes::encode(&One([1, 2]))
                    .unwrap()
                    .into()
            )
            .await
            .unwrap());

        assert!(output[1]
            .key
            .verify_signature_raw(
                &output[1].signatures[1],
                holochain_serialized_bytes::encode(&Two([2, 3]))
                    .unwrap()
                    .into()
            )
            .await
            .unwrap());
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/sleep.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn sleep(
    _ribosome: Arc<impl RibosomeT>,
    _call_context: Arc<CallContext>,
    _input: core::time::Duration,
) -> Result<(), RuntimeError> {
    unimplemented!()
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/sys_time.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::access::Permission;
use holochain_wasmer_host::prelude::*;
use holochain_zome_types::prelude::Timestamp;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn sys_time(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    _input: (),
) -> Result<Timestamp, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            non_determinism: Permission::Allow,
            ..
        } => Ok(holochain_zome_types::prelude::Timestamp::now()),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "sys_time".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use holochain_wasm_test_utils::TestWasm;
    use holochain_zome_types::prelude::Timestamp;

    #[tokio::test(flavor = "multi_thread")]
    async fn invoke_import_sys_time_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::SysTime).await;
        let _: Timestamp = conductor.call(&alice, "sys_time", ()).await;
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/trace.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use once_cell::unsync::Lazy;
use std::sync::Arc;
use wasmer::RuntimeError;

#[cfg(test)]
use once_cell::sync::Lazy as SyncLazy;
#[cfg(test)]
use std::sync::atomic::AtomicBool;

#[cfg(test)]
static CAPTURE: AtomicBool = AtomicBool::new(false);
#[cfg(test)]
static CAPTURED: SyncLazy<Arc<std::sync::Mutex<Vec<TraceMsg>>>> =
    SyncLazy::new(|| Arc::new(std::sync::Mutex::new(Vec::new())));

#[cfg_attr(feature = "instrument", tracing::instrument(skip(input)))]
pub fn wasm_trace(input: TraceMsg) {
    match input.level {
        holochain_types::prelude::Level::TRACE => tracing::trace!("{}", input.msg),
        holochain_types::prelude::Level::DEBUG => tracing::debug!("{}", input.msg),
        holochain_types::prelude::Level::INFO => tracing::info!("{}", input.msg),
        holochain_types::prelude::Level::WARN => tracing::warn!("{}", input.msg),
        holochain_types::prelude::Level::ERROR => tracing::error!("{}", input.msg),
    }
}

pub fn trace(
    _ribosome: Arc<impl RibosomeT>,
    _call_context: Arc<CallContext>,
    input: TraceMsg,
) -> Result<(), RuntimeError> {
    // Avoid dialing out to the environment on every trace.
    let wasm_log = Lazy::new(|| {
        std::env::var("WASM_LOG").unwrap_or_else(|_| "[wasm_trace]=debug".to_string())
    });
    let collector = tracing_subscriber::fmt()
        .with_env_filter(tracing_subscriber::EnvFilter::new((*wasm_log).clone()))
        .with_target(false)
        .finish();

    #[cfg(test)]
    if CAPTURE.load(std::sync::atomic::Ordering::Relaxed) {
        CAPTURED.lock().unwrap().push(input.clone());
    }

    tracing::subscriber::with_default(collector, || wasm_trace(input));
    Ok(())
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use super::*;

    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use crate::fixt::CallContextFixturator;
    use crate::fixt::RealRibosomeFixturator;
    use holochain_wasm_test_utils::TestWasm;
    use std::sync::Arc;

    /// we can get an entry hash out of the fn directly
    #[tokio::test(flavor = "multi_thread")]
    async fn trace_test() {
        let ribosome = RealRibosomeFixturator::new(crate::fixt::curve::Zomes(vec![]))
            .next()
            .unwrap();
        let call_context = CallContextFixturator::new(::fixt::Unpredictable)
            .next()
            .unwrap();
        let input = TraceMsg {
            level: holochain_types::prelude::Level::DEBUG,
            msg: "ribosome trace works".to_string(),
        };

        let _: () = trace(Arc::new(ribosome), Arc::new(call_context), input).unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "Doesn't work concurrently"]
    async fn wasm_trace_test() {
        use holochain_types::prelude::Level::*;
        CAPTURE.store(true, std::sync::atomic::Ordering::SeqCst);
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::Debug).await;

        let _: () = conductor.call(&alice, "debug", ()).await;
        let r: Vec<_> = CAPTURED.lock().unwrap().clone();
        let expect = vec![
            // two traces from the two validations of genesis entries
            TraceMsg {
                msg:
                    "integrity_test_wasm_debug:debug/src/integrity.rs:5 tracing in validation works"
                        .to_string(),
                level: INFO,
            },
            TraceMsg {
                msg:
                    "integrity_test_wasm_debug:debug/src/integrity.rs:5 tracing in validation works"
                        .to_string(),
                level: INFO,
            },
            // followed by the zome call traces
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:5 tracing works!".to_string(),
                level: TRACE,
            },
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:6 debug works".to_string(),
                level: DEBUG,
            },
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:7 info works".to_string(),
                level: INFO,
            },
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:8 warn works".to_string(),
                level: WARN,
            },
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:9 error works".to_string(),
                level: ERROR,
            },
            TraceMsg {
                msg: "test_wasm_debug:debug/src/lib.rs:10 foo = \"fields\"; bar = \"work\"; too"
                    .to_string(),
                level: DEBUG,
            },
        ];
        assert_eq!(r, expect);
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/unblock_agent.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use holochain_wasmer_host::prelude::*;
use holochain_zome_types::block::Block;
use holochain_zome_types::block::BlockTarget;
use holochain_zome_types::block::CellBlockReason;
use holochain_types::prelude::*;
use wasmer::RuntimeError;

pub fn unblock_agent(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: holochain_zome_types::block::BlockAgentInput,
) -> Result<(), RuntimeError> {
    tokio_helper::block_forever_on(async move {
        call_context.host_context().call_zome_handle().unblock(Block::new(
            BlockTarget::Cell(CellId::new(call_context
                .host_context()
                .call_zome_handle()
                .cell_id()
                .dna_hash()
                .clone(), input.target), CellBlockReason::App(input.reason)),
                input.interval
            )).await.map_err(|e| -> RuntimeError {
            wasm_error!(e.to_string()).into()
        })
    })
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/update.rs
================================================
use super::delete::get_original_entry_data;
use crate::core::ribosome::weigh_placeholder;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_wasmer_host::prelude::*;
use wasmer::RuntimeError;

use holochain_types::prelude::*;
use std::sync::Arc;

pub fn update(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: UpdateInput,
) -> Result<ActionHash, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            write_workspace: Permission::Allow,
            ..
        } => {
            // destructure the args out into an app type def id and entry
            let UpdateInput {
                original_action_address,
                entry,
                chain_top_ordering,
            } = input;

            let (original_entry_address, entry_type) =
                get_original_entry_data(call_context.clone(), original_action_address.clone())?;

            let weight = weigh_placeholder();

            // Countersigned entries have different action handling.
            match entry {
                Entry::CounterSign(_, _) => tokio_helper::block_forever_on(async move {
                    call_context
                        .host_context
                        .workspace_write()
                        .source_chain()
                        .as_ref()
                        .expect("Must have source chain if write_workspace access is given")
                        .put_countersigned(entry, chain_top_ordering, weight)
                        .await
                        .map_err(|source_chain_error| -> RuntimeError {
                            wasm_error!(WasmErrorInner::Host(source_chain_error.to_string())).into()
                        })
                }),
                _ => {
                    // build the entry hash
                    let entry_hash = EntryHash::with_data_sync(&entry);

                    // build an action for the entry being updated
                    let action_builder = builder::Update {
                        original_entry_address,
                        original_action_address,
                        entry_type,
                        entry_hash,
                    };
                    let workspace = call_context.host_context.workspace_write();

                    // return the hash of the updated entry
                    // note that validation is handled by the workflow
                    // if the validation fails this update will be rolled back by virtue of the DB transaction
                    // being atomic
                    tokio_helper::block_forever_on(async move {
                        let source_chain = workspace
                            .source_chain()
                            .as_ref()
                            .expect("Must have source chain if write_workspace access is given");
                        // push the action and the entry into the source chain
                        let action_hash = source_chain
                            .put_weightless(action_builder, Some(entry), chain_top_ordering)
                            .await
                            .map_err(|source_chain_error| -> RuntimeError {
                                wasm_error!(WasmErrorInner::Host(source_chain_error.to_string()))
                                    .into()
                            })?;
                        Ok(action_hash)
                    })
                }
            }
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "update".into()
            )
            .to_string()
        ))
        .into()),
    }
}

// relying on tests for get_details



================================================
File: crates/holochain/src/core/ribosome/host_fn/verify_signature.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_keystore::AgentPubKeyExt;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn verify_signature(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: VerifySignature,
) -> Result<bool, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore_deterministic: Permission::Allow,
            ..
        } => tokio_helper::block_forever_on(async move {
            let VerifySignature {
                key,
                signature,
                data,
            } = input;
            key.verify_signature_raw(&signature, data.into())
                .await
                .map_err(|e| wasmer::RuntimeError::user(Box::new(e)))
        }),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "verify_signature".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {
    use crate::{
        core::ribosome::wasm_test::RibosomeTestFixture,
        sweettest::{SweetConductor, SweetZome},
    };
    use hdk::prelude::*;
    use holochain_wasm_test_utils::TestWasm;

    macro_rules! twice {
        ($e:expr) => {
            $e;
            $e;
        };
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn ribosome_verify_signature_raw_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor,
            alice,
            alice_pubkey,
            bob,
            bob_pubkey,
            ..
        } = RibosomeTestFixture::new(TestWasm::Sign).await;

        // signatures should not change for a given pubkey
        let data = std::sync::Arc::new([1, 2, 3]);

        let alice_sig = conductor
            .keystore()
            .sign(alice_pubkey.clone(), data.clone())
            .await
            .unwrap();
        let bob_sig = conductor
            .keystore()
            .sign(bob_pubkey.clone(), data.clone())
            .await
            .unwrap();
        let mut alice_sig_bad = alice_sig.clone();
        let mut bob_sig_bad = bob_sig.clone();
        alice_sig_bad.0[0] = alice_sig_bad.0[0].wrapping_add(1);
        bob_sig_bad.0[63] = bob_sig_bad.0[63].wrapping_sub(1);

        async fn check_sig(
            conductor: &SweetConductor,
            data: &std::sync::Arc<[u8; 3]>,
            zome: &SweetZome,
            agent: &AgentPubKey,
            sig: &Signature,
        ) -> bool {
            conductor
                .call(
                    zome,
                    "verify_signature_raw",
                    VerifySignature::new_raw(agent.clone(), sig.clone(), data.clone().to_vec()),
                )
                .await
        }

        twice!(assert!(
            check_sig(&conductor, &data, &alice, &alice_pubkey, &alice_sig).await
        ));
        twice!(assert!(
            !check_sig(&conductor, &data, &alice, &alice_pubkey, &alice_sig_bad).await
        ));
        twice!(assert!(
            check_sig(&conductor, &data, &bob, &bob_pubkey, &bob_sig).await
        ));
        twice!(assert!(
            !check_sig(&conductor, &data, &bob, &bob_pubkey, &bob_sig_bad).await
        ));

        twice!(assert!(
            !check_sig(&conductor, &data, &bob, &bob_pubkey, &alice_sig).await
        ));
        twice!(assert!(
            !check_sig(&conductor, &data, &alice, &alice_pubkey, &bob_sig).await
        ));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn ribosome_verify_signature_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor,
            alice,
            alice_pubkey,
            ..
        } = RibosomeTestFixture::new(TestWasm::Sign).await;

        let _nothing: () = conductor
            .call(&alice, "verify_signature", alice_pubkey)
            .await;
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/version.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeT;
use holochain_zome_types::version::ZomeApiVersion;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn version(
    _ribosome: Arc<impl RibosomeT>,
    _call_context: Arc<CallContext>,
    _input: (),
) -> Result<ZomeApiVersion, RuntimeError> {
    unreachable!();
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_25519_x_salsa20_poly1305_decrypt.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_25519_x_salsa20_poly1305_decrypt(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: X25519XSalsa20Poly1305Decrypt,
) -> Result<Option<XSalsa20Poly1305Data>, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore_deterministic: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                // zome_types too restrictive,
                // causing us to have to clone everything because there's
                // no access to the actual internal data (*$%&^#(*$&^
                let mut s_pk: [u8; 32] = [0; 32];
                s_pk.copy_from_slice(input.as_sender_ref().as_ref());
                let mut r_pk: [u8; 32] = [0; 32];
                r_pk.copy_from_slice(input.as_recipient_ref().as_ref());

                let edata = input.as_encrypted_data_ref();
                let mut nonce: [u8; 24] = [0; 24];
                nonce.copy_from_slice(edata.as_nonce_ref().as_ref());
                let data = edata.as_encrypted_data_ref().to_vec();

                let res = call_context
                    .host_context
                    .keystore()
                    .crypto_box_xsalsa_open(s_pk.into(), r_pk.into(), nonce, data.into())
                    .await?;

                // why is this an Option #&*(*#@&*&????????
                holochain_keystore::LairResult::Ok(Some(res.to_vec().into()))
            })
            .map_err(|keystore_error| -> RuntimeError { wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into() })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_25519_x_salsa20_poly1305_decrypt".into()
            )
            .to_string()
        ))
        .into()),
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_25519_x_salsa20_poly1305_encrypt.rs
================================================
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_25519_x_salsa20_poly1305_encrypt(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: X25519XSalsa20Poly1305Encrypt,
) -> Result<XSalsa20Poly1305EncryptedData, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                // zome_types too restrictive,
                // causing us to have to clone everything because there's
                // no access to the actual internal data (*$%&^#(*$&^
                let mut s_pk: [u8; 32] = [0; 32];
                s_pk.copy_from_slice(input.as_sender_ref().as_ref());
                let mut r_pk: [u8; 32] = [0; 32];
                r_pk.copy_from_slice(input.as_recipient_ref().as_ref());
                let data = input.as_data_ref().as_ref().to_vec();

                let (nonce, cipher) = call_context
                    .host_context
                    .keystore()
                    .crypto_box_xsalsa(s_pk.into(), r_pk.into(), data.into())
                    .await?;

                holochain_keystore::LairResult::Ok(XSalsa20Poly1305EncryptedData::new(
                    nonce.into(),
                    cipher.to_vec(),
                ))
            })
            .map_err(|keystore_error| -> RuntimeError {
                wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into()
            })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_25519_x_salsa20_poly1305_encrypt".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {

    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    async fn invoke_import_x_25519_x_salsa20_poly1305_encrypt_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::XSalsa20Poly1305).await;
        let alice_x25519: X25519PubKey = conductor.call(&alice, "create_x25519_keypair", ()).await;
        let bob_x25519: X25519PubKey = conductor.call(&alice, "create_x25519_keypair", ()).await;
        let carol_x25519: X25519PubKey = conductor.call(&alice, "create_x25519_keypair", ()).await;

        let data = XSalsa20Poly1305Data::from(vec![1, 2, 3, 4]);

        let encrypt_input = X25519XSalsa20Poly1305Encrypt::new(
            alice_x25519,
            bob_x25519,
            data.clone(),
        );

        let encrypt_output: XSalsa20Poly1305EncryptedData = conductor
            .call(&alice, "x_25519_x_salsa20_poly1305_encrypt", encrypt_input)
            .await;

        let decrypt_input =
            holochain_zome_types::x_salsa20_poly1305::X25519XSalsa20Poly1305Decrypt::new(
                bob_x25519,
                alice_x25519,
                encrypt_output.clone(),
            );

        let decrypt_output: Option<XSalsa20Poly1305Data> = conductor
            .call(&alice, "x_25519_x_salsa20_poly1305_decrypt", decrypt_input)
            .await;

        assert_eq!(decrypt_output, Some(data.clone()),);

        let bad_decrypt_input =
            holochain_zome_types::x_salsa20_poly1305::X25519XSalsa20Poly1305Decrypt::new(
                carol_x25519,
                alice_x25519,
                encrypt_output,
            );
        let bad_decrypt_output: Result<Option<XSalsa20Poly1305Data>, _> = conductor
            .call_fallible(
                &alice,
                "x_25519_x_salsa20_poly1305_decrypt",
                bad_decrypt_input,
            )
            .await;

        assert!(bad_decrypt_output.is_err());
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_salsa20_poly1305_decrypt.rs
================================================
use super::*;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use wasmer::RuntimeError;

pub fn x_salsa20_poly1305_decrypt(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: XSalsa20Poly1305Decrypt,
) -> Result<Option<XSalsa20Poly1305Data>, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess{
            keystore_deterministic: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                let key_ref = input.as_key_ref_ref().clone();
                let tag = key_ref.to_tag();

                let edata = input.as_encrypted_data_ref();
                let mut nonce: [u8; 24] = [0; 24];
                nonce.copy_from_slice(edata.as_nonce_ref().as_ref());
                let data = edata.as_encrypted_data_ref().to_vec();

                // for some reason, the hdk api expects us to translate
                // errors into None here
                let res = match call_context
                    .host_context
                    .keystore()
                    .shared_secret_decrypt(tag, nonce, data.into())
                    .await
                {
                    Err(_) => None,
                    Ok(res) => Some(res.to_vec().into()),
                };
                holochain_keystore::LairResult::Ok(res)
            })
            .map_err(|keystore_error| -> RuntimeError { wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into() })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(RibosomeError::HostFnPermissions(
            call_context.zome.zome_name().clone(),
            call_context.function_name().clone(),
            "x_salsa20_poly1305_decrypt".into()
        ).to_string())).into())
    }
}

// Tests for the shared secret round trip are in xsalsa20_poly1305_encrypt.



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_salsa20_poly1305_encrypt.rs
================================================
use super::*;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_salsa20_poly1305_encrypt(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: XSalsa20Poly1305Encrypt,
) -> Result<XSalsa20Poly1305EncryptedData, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                let key_ref = input.as_key_ref_ref().clone();
                let tag = key_ref.to_tag();

                let data = input.as_data_ref().as_ref().to_vec();

                let (nonce, cipher) = call_context
                    .host_context
                    .keystore()
                    .shared_secret_encrypt(tag, data.into())
                    .await?;

                holochain_keystore::LairResult::Ok(XSalsa20Poly1305EncryptedData::new(
                    nonce.into(),
                    cipher.to_vec(),
                ))
            })
            .map_err(|keystore_error| -> RuntimeError {
                wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into()
            })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_salsa20_poly1305_encrypt".into(),
            )
            .to_string(),
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod wasm_test {

    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use hdk::prelude::*;
    use holochain_wasm_test_utils::TestWasm;

    #[tokio::test(flavor = "multi_thread")]
    #[cfg(feature = "test_utils")]
    async fn xsalsa20_poly1305_shared_secret_round_trip() {
        holochain_trace::test_run();

        // we need two conductors and two x25519 pub keys to do a round trip

        // conductor1 / pubkey 1
        let RibosomeTestFixture {
            conductor: conductor1, alice: alice1, ..
        } = RibosomeTestFixture::new(TestWasm::XSalsa20Poly1305).await;
        let alice1_x25519: X25519PubKey = conductor1.call(&alice1, "create_x25519_keypair", ()).await;

        // conductor2 / pubkey 2
        let RibosomeTestFixture {
            conductor: conductor2, alice: alice2, ..
        } = RibosomeTestFixture::new(TestWasm::XSalsa20Poly1305).await;
        let alice2_x25519: X25519PubKey = conductor2.call(&alice2, "create_x25519_keypair", ()).await;

        // create a new random shared key
        let key_ref: XSalsa20Poly1305KeyRef = conductor1
            .call(
                &alice1,
                "x_salsa20_poly1305_shared_secret_create_random",
                <Option<XSalsa20Poly1305KeyRef>>::None,
            )
            .await;

        // encrypt some data with that shared key (identified by key_ref)
        let data = XSalsa20Poly1305Data::from(vec![1, 2, 3, 4]);
        let enc_input = holochain_zome_types::x_salsa20_poly1305::XSalsa20Poly1305Encrypt::new(
            key_ref.clone(),
            data.clone(),
        );
        let cipher: XSalsa20Poly1305EncryptedData = conductor1
            .call(&alice1, "x_salsa20_poly1305_encrypt", enc_input)
            .await;

        // export the shared key to send to conductor2
        let exp_input = holochain_zome_types::x_salsa20_poly1305::XSalsa20Poly1305SharedSecretExport::new(
            alice1_x25519, // sender
            alice2_x25519, // recipient
            key_ref.clone(),
        );
        let secret_exp: XSalsa20Poly1305EncryptedData = conductor1
            .call(&alice1, "x_salsa20_poly1305_shared_secret_export", exp_input)
            .await;

        // ingest the shared key on conductor2
        let ing_input = holochain_zome_types::x_salsa20_poly1305::XSalsa20Poly1305SharedSecretIngest::new(
            alice2_x25519, // recipient
            alice1_x25519, // sender
            secret_exp,
            Some(key_ref.clone()),
        );
        let key_ref2: XSalsa20Poly1305KeyRef = conductor2
            .call(&alice2, "x_salsa20_poly1305_shared_secret_ingest", ing_input)
            .await;
        assert_eq!(key_ref, key_ref2);

        // now decrypt the message on conductor2
        let dec_input = holochain_zome_types::x_salsa20_poly1305::XSalsa20Poly1305Decrypt::new(
            key_ref,
            cipher.clone(),
        );
        let output: Option<XSalsa20Poly1305Data> = conductor2
            .call(&alice2, "x_salsa20_poly1305_decrypt", dec_input)
            .await;
        assert_eq!(&output, &Some(data));

        // -- now make sure not every key_ref can decrypt it -- //

        // create a new random shared key
        let key_ref_bad: XSalsa20Poly1305KeyRef = conductor2
            .call(
                &alice2,
                "x_salsa20_poly1305_shared_secret_create_random",
                <Option<XSalsa20Poly1305KeyRef>>::None,
            )
            .await;

        // try decrypting with key_ref_bad
        let dec_input_bad = holochain_zome_types::x_salsa20_poly1305::XSalsa20Poly1305Decrypt::new(
            key_ref_bad,
            cipher,
        );
        let output: Option<XSalsa20Poly1305Data> = conductor2
            .call(&alice2, "x_salsa20_poly1305_decrypt", dec_input_bad)
            .await;
        assert_eq!(&output, &None);
    }
}



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_salsa20_poly1305_shared_secret_create_random.rs
================================================
use super::*;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_salsa20_poly1305_shared_secret_create_random(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: Option<XSalsa20Poly1305KeyRef>,
) -> Result<XSalsa20Poly1305KeyRef, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => tokio_helper::block_forever_on(async move {
            let key_ref = match input {
                Some(key_ref) => key_ref,
                None => rand_utf8::rand_utf8(
                    &mut rand::thread_rng(),
                    DEF_REF_SIZE,
                ).as_bytes().to_vec().into(),
            };

            let tag = key_ref.to_tag();

            call_context
                .host_context
                .keystore()
                .new_shared_secret(tag)
                .await?;

            holochain_keystore::LairResult::Ok(key_ref)
        })
        .map_err(|keystore_error| wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into()),
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_salsa20_poly1305_shared_secret_create_random".into(),
            )
            .to_string(),
        )).into()),
    }
}

// Tests for the shared secret round trip are in xsalsa20_poly1305_encrypt.



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_salsa20_poly1305_shared_secret_export.rs
================================================
use super::*;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_salsa20_poly1305_shared_secret_export(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: XSalsa20Poly1305SharedSecretExport,
) -> Result<XSalsa20Poly1305EncryptedData, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                let tag = input.as_key_ref_ref().to_tag();

                let mut s_pk: [u8; 32] = [0; 32];
                s_pk.copy_from_slice(input.as_sender_ref().as_ref());
                let mut r_pk: [u8; 32] = [0; 32];
                r_pk.copy_from_slice(input.as_recipient_ref().as_ref());

                let (nonce, cipher) = call_context
                    .host_context
                    .keystore()
                    .shared_secret_export(tag, s_pk.into(), r_pk.into())
                    .await?;

                holochain_keystore::LairResult::Ok(XSalsa20Poly1305EncryptedData::new(
                    nonce.into(),
                    cipher.to_vec(),
                ))
            })
            .map_err(|keystore_error| wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into())
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_salsa20_poly1305_shared_secret_export".into(),
            )
            .to_string(),
        )).into()),
    }
}

// Tests for the shared secret round trip are in xsalsa20_poly1305_encrypt.



================================================
File: crates/holochain/src/core/ribosome/host_fn/x_salsa20_poly1305_shared_secret_ingest.rs
================================================
use super::*;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeError;
use crate::core::ribosome::RibosomeT;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn x_salsa20_poly1305_shared_secret_ingest(
    _ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    input: XSalsa20Poly1305SharedSecretIngest,
) -> Result<XSalsa20Poly1305KeyRef, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            keystore: Permission::Allow,
            ..
        } => {
            tokio_helper::block_forever_on(async move {
                let key_ref = match input.as_key_ref_ref() {
                    Some(key_ref) => key_ref.clone(),
                    None => rand_utf8::rand_utf8(
                        &mut rand::thread_rng(),
                        DEF_REF_SIZE,
                    ).as_bytes().to_vec().into(),
                };

                let tag = key_ref.to_tag();

                let mut s_pk: [u8; 32] = [0; 32];
                s_pk.copy_from_slice(input.as_sender_ref().as_ref());
                let mut r_pk: [u8; 32] = [0; 32];
                r_pk.copy_from_slice(input.as_recipient_ref().as_ref());

                let edata = input.as_encrypted_data_ref();
                let mut nonce: [u8; 24] = [0; 24];
                nonce.copy_from_slice(edata.as_nonce_ref().as_ref());
                let data = edata.as_encrypted_data_ref().to_vec();

                call_context
                    .host_context
                    .keystore()
                    .shared_secret_import(
                        s_pk.into(),
                        r_pk.into(),
                        nonce,
                        data.into(),
                        tag,
                    )
                    .await?;

                holochain_keystore::LairResult::Ok(key_ref)
            })
            .map_err(|keystore_error| wasm_error!(WasmErrorInner::Host(keystore_error.to_string())).into())
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "x_salsa20_poly1305_shared_secret_ingest".into(),
            )
            .to_string(),
        )).into()),
    }
}

// Tests for the shared secret round trip are in xsalsa20_poly1305_encrypt.



================================================
File: crates/holochain/src/core/ribosome/host_fn/zome_info.rs
================================================
use crate::core::ribosome::error::RibosomeError;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostFnAccess;
use crate::core::ribosome::RibosomeT;
use holochain_types::prelude::*;
use holochain_wasmer_host::prelude::*;
use std::sync::Arc;
use wasmer::RuntimeError;

pub fn zome_info(
    ribosome: Arc<impl RibosomeT>,
    call_context: Arc<CallContext>,
    _input: (),
) -> Result<ZomeInfo, RuntimeError> {
    match HostFnAccess::from(&call_context.host_context()) {
        HostFnAccess {
            bindings_deterministic: Permission::Allow,
            ..
        } => {
            let f = ribosome.zome_info(call_context.zome.clone());
            tokio_helper::block_on(f, std::time::Duration::from_secs(60))
                .map_err(|_| wasm_error!("60s timeout elapsed during zome_info()"))?
                .map_err(|e| match e {
                    RibosomeError::WasmRuntimeError(wasm_error) => wasm_error,
                    other_error => {
                        wasm_error!(WasmErrorInner::Host(other_error.to_string())).into()
                    }
                })
        }
        _ => Err(wasm_error!(WasmErrorInner::Host(
            RibosomeError::HostFnPermissions(
                call_context.zome.zome_name().clone(),
                call_context.function_name().clone(),
                "zome_info".into()
            )
            .to_string()
        ))
        .into()),
    }
}

#[cfg(test)]
#[cfg(feature = "slow_tests")]
pub mod test {
    use crate::core::ribosome::wasm_test::RibosomeTestFixture;
    use holochain_wasm_test_utils::TestWasm;
    use holochain_zome_types::prelude::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn zome_info_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::EntryDefs).await;

        let zome_info: ZomeInfo = conductor.call(&alice, "zome_info", ()).await;
        assert_eq!(zome_info.name, "entry_defs".into());
        assert_eq!(zome_info.id, ZomeIndex::new(1));
        assert_eq!(
            zome_info.entry_defs,
            vec![
                EntryDef {
                    id: "post".into(),
                    visibility: Default::default(),
                    required_validations: Default::default(),
                    ..Default::default()
                },
                EntryDef {
                    id: "comment".into(),
                    visibility: EntryVisibility::Private,
                    required_validations: Default::default(),
                    ..Default::default()
                }
            ]
            .into(),
        );

        let entries = vec![(ZomeIndex(0), vec![EntryDefIndex(0), EntryDefIndex(1)])];
        let links = vec![(ZomeIndex(0), vec![])];
        assert_eq!(
            zome_info.zome_types,
            ScopedZomeTypesSet {
                entries: ScopedZomeTypes(entries),
                links: ScopedZomeTypes(links),
            }
        );
    }

    #[cfg(feature = "wasmer_sys")]
    #[tokio::test(flavor = "multi_thread")]
    async fn zome_info_extern_fns_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::EntryDefs).await;

        let zome_info: ZomeInfo = conductor.call(&alice, "zome_info", ()).await;

        assert_eq!(
            zome_info.extern_fns,
            vec![
                FunctionName::new("__data_end"),
                FunctionName::new("__getrandom_custom"),
                FunctionName::new("__hc__allocate_1"),
                FunctionName::new("__hc__deallocate_1"),
                FunctionName::new("__heap_base"),
                FunctionName::new("assert_indexes"),
                FunctionName::new("entry_defs"),
                FunctionName::new("memory"),
                FunctionName::new("wasmer_metering_points_exhausted"),
                FunctionName::new("wasmer_metering_remaining_points"),
                FunctionName::new("zome_info"),
            ],
        );
    }


    // Same test, but excluding wasmer metering extern fns
    #[cfg(feature = "wasmer_wamr")]
    #[tokio::test(flavor = "multi_thread")]
    async fn zome_info_extern_fns_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor, alice, ..
        } = RibosomeTestFixture::new(TestWasm::EntryDefs).await;

        let zome_info: ZomeInfo = conductor.call(&alice, "zome_info", ()).await;

        assert_eq!(
            zome_info.extern_fns,
            vec![
                FunctionName::new("__data_end"),
                FunctionName::new("__getrandom_custom"),
                FunctionName::new("__hc__allocate_1"),
                FunctionName::new("__hc__deallocate_1"),
                FunctionName::new("__heap_base"),
                FunctionName::new("assert_indexes"),
                FunctionName::new("entry_defs"),
                FunctionName::new("memory"),
                FunctionName::new("zome_info"),
            ],
        );
    }
}



================================================
File: crates/holochain/src/core/ribosome/real_ribosome/wasmer_sys.rs
================================================
use crate::{
    core::ribosome::error::RibosomeResult, holochain_wasmer_host::module::WASM_METERING_LIMIT,
};
use holochain_wasmer_host::module::InstanceWithStore;
use holochain_zome_types::prelude::WasmZome;
use std::sync::Arc;
use wasmer::{AsStoreMut, Module};
use wasmer_middlewares::metering::{get_remaining_points, set_remaining_points, MeteringPoints};

pub fn reset_metering_points(instance_with_store: Arc<InstanceWithStore>) {
    let mut store_lock = instance_with_store.store.lock();
    let mut store_mut = store_lock.as_store_mut();
    set_remaining_points(
        &mut store_mut,
        instance_with_store.instance.as_ref(),
        WASM_METERING_LIMIT,
    );
}

pub fn get_used_metering_points(instance_with_store: Arc<InstanceWithStore>) -> u64 {
    let mut store_lock = instance_with_store.store.lock();
    let mut store_mut = store_lock.as_store_mut();

    match get_remaining_points(&mut store_mut, instance_with_store.instance.as_ref()) {
        MeteringPoints::Remaining(points) => WASM_METERING_LIMIT - points,
        MeteringPoints::Exhausted => WASM_METERING_LIMIT,
    }
}

/// DEPRECATED: Bundling precompiled and preserialized wasm for iOS is deprecated. Please use the wasm interpreter instead.
pub fn get_prebuilt_module(wasm_zome: &WasmZome) -> RibosomeResult<Option<Arc<Module>>> {
    match &wasm_zome.preserialized_path {
        Some(path) => {
            eprintln!("DEPRECATED: Bundling precompiled and preserialized wasm for iOS is deprecated. Please use the wasm interpreter instead.");
            let module = holochain_wasmer_host::module::get_ios_module_from_file(path.as_path())?;
            Ok(Some(Arc::new(module)))
        }
        None => Ok(None),
    }
}



================================================
File: crates/holochain/src/core/ribosome/real_ribosome/wasmer_wamr.rs
================================================
use crate::core::ribosome::error::RibosomeResult;
use holochain_wasmer_host::module::InstanceWithStore;
use holochain_zome_types::prelude::WasmZome;
use std::sync::Arc;
use tracing::warn;
use wasmer::Module;

// Metering is not supported in wasmer_wamr feature. This is a no-op.
pub fn reset_metering_points(_instance_with_store: Arc<InstanceWithStore>) {}

// Metering is not supported in wasmer_wamr feature. This is a no-op.
pub fn get_used_metering_points(_instance_with_store: Arc<InstanceWithStore>) -> u64 {
    0
}

// Use of precompiled and serialized modules is not supported in wasmer_wamr feature.
// If a preserialized_path is specified for the zome, it is ignored.
pub fn get_prebuilt_module(wasm_zome: &WasmZome) -> RibosomeResult<Option<Arc<Module>>> {
    if wasm_zome.preserialized_path.is_some() {
        warn!("A precompiled wasm path was specified, but the feature flag 'wasmer_sys' must be enabled to support use of precompiled wasm modules. Ignoring the precompiled path.");
    }

    Ok(None)
}



================================================
File: crates/holochain/src/core/sys_validate/error.rs
================================================
use holochain_conductor_services::DpkiServiceError;
use std::convert::TryFrom;

use super::SourceChainError;
use super::MAX_ENTRY_SIZE;
use crate::conductor::api::error::ConductorApiError;
use crate::conductor::entry_def_store::error::EntryDefStoreError;
use crate::core::validation::OutcomeOrError;
use crate::core::workflow::WorkflowError;
use crate::from_sub_error;
use holo_hash::ActionHash;
use holo_hash::AnyDhtHash;
use holochain_keystore::KeystoreError;
use holochain_sqlite::error::DatabaseError;
use holochain_state::workspace::WorkspaceError;
use holochain_types::prelude::*;
use holochain_zome_types::countersigning::CounterSigningError;
use holochain_zome_types::countersigning::CounterSigningSessionData;
use thiserror::Error;

/// Validation can result in either
/// - An Error
/// - Failed validation
/// - Successful validation
///
/// It is a lot cleaner to express this using
/// ? try's unfortunately try for custom types is
/// unstable but when it lands we should use:
/// <https://docs.rs/try-guard/0.2.0/try_guard/>
#[derive(Error, Debug)]
// TODO FIXME
#[allow(clippy::large_enum_variant)]
pub enum SysValidationError {
    #[error(transparent)]
    CascadeError(#[from] holochain_cascade::error::CascadeError),
    #[error(transparent)]
    DatabaseError(#[from] DatabaseError),
    #[error(transparent)]
    EntryDefStoreError(#[from] EntryDefStoreError),
    #[error(transparent)]
    KeystoreError(#[from] KeystoreError),
    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),
    #[error("Dna is missing for this hash {0:?}. Cannot validate without dna.")]
    DnaMissing(DnaHash),
    // NOTE: can remove this if SysValidationResult is replaced with SysValidationOutcome
    #[error(transparent)]
    ValidationOutcome(#[from] ValidationOutcome),
    #[error(transparent)]
    WorkflowError(#[from] Box<WorkflowError>),
    #[error(transparent)]
    WorkspaceError(#[from] WorkspaceError),
    #[error(transparent)]
    DpkiServiceError(#[from] DpkiServiceError),
    #[error(transparent)]
    ConductorApiError(#[from] Box<ConductorApiError>),
    #[error("Expected Entry-based Action, but got: {0:?}")]
    NonEntryAction(Action),
}

impl From<CounterSigningError> for SysValidationError {
    fn from(counter_signing_error: CounterSigningError) -> Self {
        SysValidationError::ValidationOutcome(ValidationOutcome::CounterSigningError(
            counter_signing_error,
        ))
    }
}

// #[deprecated = "This will be replaced with SysValidationOutcome as we shouldn't treat outcomes as errors"]
pub type SysValidationResult<T> = Result<T, SysValidationError>;

/// Return either:
/// - an Ok result
/// - ValidationOutcome
/// - SysValidationError
pub type SysValidationOutcome<T> = Result<T, OutcomeOrError<ValidationOutcome, SysValidationError>>;

from_sub_error!(SysValidationError, WorkspaceError);

impl<T> From<SysValidationError> for OutcomeOrError<T, SysValidationError> {
    fn from(e: SysValidationError) -> Self {
        OutcomeOrError::Err(e)
    }
}

/// Turn the OutcomeOrError into an Outcome or and Error
/// This is the best way to convert into an outcome or
/// exit early with a real error
impl<E> TryFrom<OutcomeOrError<ValidationOutcome, E>> for ValidationOutcome {
    type Error = E;

    fn try_from(value: OutcomeOrError<ValidationOutcome, E>) -> Result<Self, Self::Error> {
        match value {
            OutcomeOrError::Outcome(o) => Ok(o),
            OutcomeOrError::Err(e) => Err(e),
        }
    }
}

// TODO: use try guard crate to refactor this so it's not an "Error"
// https://docs.rs/try-guard/0.2.0/try_guard/
/// All the outcomes that can come from validation
/// This is not an error type it is the outcome of
/// failed validation.
#[derive(Error, Debug, PartialEq, Eq)]
pub enum ValidationOutcome {
    #[error("The record with signature {0:?} and action {1:?} was found to be counterfeit")]
    CounterfeitAction(Signature, Action),
    #[error("A warrant op was found to be counterfeit. Warrant: {0:?}")]
    CounterfeitWarrant(Warrant),
    #[error("A warrant op was found to be invalid. Reason: {1}, Warrant: {0:?}")]
    InvalidWarrant(Warrant, String),
    #[error("The action {1:?} is not found in the countersigning session data {0:?}")]
    ActionNotInCounterSigningSession(CounterSigningSessionData, NewEntryAction),
    #[error(transparent)]
    CounterSigningError(#[from] CounterSigningError),
    #[error("The dependency {0:?} was not found on the DHT")]
    DepMissingFromDht(AnyDhtHash),
    #[error("The agent {0:?} could not be found in DPKI")]
    DpkiAgentMissing(AgentPubKey),
    #[error("The agent {0:?} was found to be invalid at {1:?} according to the DPKI service")]
    DpkiAgentInvalid(AgentPubKey, Timestamp),
    #[error("Agent key {0} invalid")]
    InvalidAgentKey(AgentPubKey),
    #[error("The entry def index for {0:?} was out of range")]
    EntryDefId(AppEntryDef),
    #[error("The entry has a different hash to the action's entry hash")]
    EntryHash,
    #[error(
        "The entry size {0} was larger than the MAX_ENTRY_SIZE {}",
        MAX_ENTRY_SIZE
    )]
    EntryTooLarge(usize),
    #[error("The entry has a different type to the action's entry type")]
    EntryTypeMismatch,
    #[error("The visibility for {0:?} didn't match the zome")]
    EntryVisibility(AppEntryDef),
    #[error(
        "The link tag size {0} was larger than the MAX_TAG_SIZE {}",
        super::MAX_TAG_SIZE
    )]
    TagTooLarge(usize),
    #[error("An op with non-private entry type is missing its entry data. Action: {0:?}, Op type: {1:?} Reason: {2}")]
    MalformedDhtOp(Box<Action>, ChainOpType, String),
    #[error("The action with {0:?} was expected to be a link add action")]
    NotCreateLink(ActionHash),
    #[error("The action was expected to be a new entry action but was {0:?}")]
    NotNewEntry(Action),
    #[error("The PreflightResponse signature was not valid {0:?}")]
    PreflightResponseSignature(PreflightResponse),
    #[error(transparent)]
    PrevActionError(#[from] PrevActionError),
    #[error("Private entry data should never be included in any op other than StoreEntry.")]
    PrivateEntryLeaked,
    #[error("The DNA does not belong in this space! Action has {0:?}, expected {1:?}")]
    WrongDna(DnaHash, DnaHash),
    #[error("Update original: {0:?} doesn't match new: {1:?}")]
    UpdateTypeMismatch(EntryType, EntryType),
    #[error("Update original {0:?} doesn't match the {1:?} in the update")]
    UpdateHashMismatch(EntryHash, EntryHash),
    #[error("Signature {0:?} failed to verify for Action {1:?}")]
    VerifySignature(Signature, Action),
    #[error("The zome index for {0:?} was out of range")]
    ZomeIndex(AppEntryDef),
}

impl ValidationOutcome {
    pub fn not_found<I: Into<AnyDhtHash> + Clone>(h: &I) -> Self {
        Self::DepMissingFromDht(h.clone().into())
    }

    /// Convert into a OutcomeOrError<ValidationOutcome, SysValidationError>
    /// and exit early
    pub fn into_outcome<T>(self) -> SysValidationOutcome<T> {
        Err(OutcomeOrError::Outcome(self))
    }

    /// The outcome is pending further information, so no determination can be made at this time.
    /// If this is false, then the outcome is determinate, meaning we can reject validation now.
    pub fn is_indeterminate(&self) -> bool {
        if let ValidationOutcome::CounterfeitAction(_, _)
        | ValidationOutcome::CounterfeitWarrant(_) = self
        {
            // Just a helpful assertion for us
            unreachable!("Counterfeit ops are dropped before sys validation")
        }
        matches!(self, Self::DepMissingFromDht(_) | Self::DpkiAgentMissing(_))
    }
}



================================================
File: crates/holochain/src/core/sys_validate/tests.rs
================================================
//! Sys validation tests
//!
//! TESTED:
//! - Mismatched signatures are rejected
//! - Any action other than DNA cannot be at seq 0
//! - The DNA action can only be validated if the chain is empty,
//!     and its timestamp must not be less than the origin time
//! - Timestamps must increase monotonically
//! - Sequence numbers must increment by 1 for each new action
//! - Entry type in the action matches the entry variant
//! - Hash integrity check. The hash of an entry always matches what's in the action.
//! - The size of an entry does not exceed the max.
//! - Check that updates can't switch the entry type
//! - The link tag size is bounded
//! - Check the AppEntryDef is valid for the zome and the EntryDefId and ZomeIndex are in range.
//! - Check that StoreEntry never contains a private entry type
//! - Test that a given sequence of actions constitutes a valid chain w.r.t. its backlinks
//!
// TODO Add tests for:
// - Create and Update Agent can only be preceded by AgentValidationPkg
// - Author must match the entry hash of the most recent Create/Update Agent
// - Genesis must be correct:
//     - Explicitly check action seqs 0, 1, and 2.
// - There can only be one InitZomesCompleted
// - All backlinks are in-chain (prev action, etc.)
//
// TODO This "The DNA action can only be validated if the chain is empty" thing is a bit weird,
//  refactor to not look in the db

use super::*;
use crate::conductor::space::TestSpaces;
use crate::core::workflow::sys_validation_workflow::sys_validate_record;
use crate::sweettest::SweetAgents;
use crate::sweettest::SweetConductor;
use ::fixt::prelude::*;
use error::SysValidationError;
use holo_hash::fixt::ActionHashFixturator;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::DnaHashFixturator;
use holo_hash::fixt::EntryHashFixturator;
use holochain_cascade::MockCascade;
use holochain_keystore::test_keystore;
use holochain_keystore::AgentPubKeyExt;
use holochain_serialized_bytes::SerializedBytes;
use holochain_sqlite::prelude::DatabaseResult;
use holochain_types::test_utils::valid_arbitrary_chain;
use holochain_types::test_utils::ActionRefMut;
use holochain_zome_types::Action;
use matches::assert_matches;
use std::time::Duration;

/// Entry type in the action matches the entry variant
#[test]
fn check_entry_type_test() {
    let entry_fixt = EntryFixturator::new(Predictable);
    let et_fixt = EntryTypeFixturator::new(Predictable);

    for (e, et) in entry_fixt.zip(et_fixt).take(4) {
        assert_matches!(check_entry_type(&et, &e), Ok(()));
    }

    // Offset by 1
    let entry_fixt = EntryFixturator::new(Predictable);
    let mut et_fixt = EntryTypeFixturator::new(Predictable);
    et_fixt.next().unwrap();

    for (e, et) in entry_fixt.zip(et_fixt).take(4) {
        assert_matches!(
            check_entry_type(&et, &e),
            Err(SysValidationError::ValidationOutcome(
                ValidationOutcome::EntryTypeMismatch
            ))
        );
    }
}

/// Hash integrity check. The hash of an entry always matches what's in the action.
#[test]
fn check_entry_hash_test() {
    let mut ec = Create {
        author: fixt!(AgentPubKey),
        timestamp: Timestamp::now(),
        action_seq: 6,
        prev_action: fixt!(ActionHash),
        entry_type: EntryType::AgentPubKey,
        entry_hash: fixt!(EntryHash),
        weight: EntryRateWeight::default(),
    };
    let entry = Entry::App(AppEntryBytes(SerializedBytes::from(UnsafeBytes::from(
        vec![1, 3, 5],
    ))));
    let hash = EntryHash::with_data_sync(&entry);
    let action: Action = ec.clone().into();

    // First check it should have an entry
    assert_matches!(check_new_entry_action(&action), Ok(()));
    // Safe to unwrap if new entry
    let eh = action.entry_data().map(|(h, _)| h).unwrap();
    assert_matches!(
        check_entry_hash(eh, &entry),
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::EntryHash
        ))
    );

    ec.entry_hash = hash;
    let action: Action = ec.clone().into();

    let eh = action.entry_data().map(|(h, _)| h).unwrap();
    assert_matches!(check_entry_hash(eh, &entry), Ok(()));
    assert_matches!(
        check_new_entry_action(&Action::CreateLink(CreateLink {
            author: fixt!(AgentPubKey),
            timestamp: Timestamp::now(),
            action_seq: 8,
            prev_action: fixt!(ActionHash),
            base_address: fixt!(EntryHash).into(),
            target_address: fixt!(EntryHash).into(),
            zome_index: 0.into(),
            link_type: LinkType::new(3),
            tag: ().into(),
            weight: RateWeight::default(),
        })),
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::NotNewEntry(_)
        ))
    );
}

/// Check that StoreEntry does not have a private entry type
#[tokio::test(flavor = "multi_thread")]
async fn incoming_ops_filters_private_entry() {
    let dna = fixt!(DnaHash);
    let spaces = TestSpaces::new([dna.clone()]).await;
    let space = Arc::new(spaces.test_spaces[&dna].space.clone());
    let vault = space.dht_db.clone();
    let keystore = test_keystore();
    let (tx, _rx) = TriggerSender::new();

    let private_entry = Entry::App(AppEntryBytes(SerializedBytes::from(UnsafeBytes::from(
        vec![1, 3, 5],
    ))));
    let author = keystore.new_sign_keypair_random().await.unwrap();
    let app_entry_def = AppEntryDef::new(0.into(), 0.into(), EntryVisibility::Private);
    let create = Create {
        author: author.clone(),
        timestamp: Timestamp::now(),
        action_seq: 5,
        prev_action: fixt!(ActionHash),
        entry_type: EntryType::App(app_entry_def),
        entry_hash: EntryHash::with_data_sync(&private_entry),
        weight: EntryRateWeight::default(),
    };
    let action = Action::Create(create);
    let signature = author.sign(&keystore, &action).await.unwrap();

    let shh =
        SignedActionHashed::with_presigned(ActionHashed::from_content_sync(action), signature);
    let record = Record::new(shh, Some(private_entry));

    let ops_sender = IncomingDhtOpSender::new(space.clone(), tx.clone());
    ops_sender.send_store_entry(record.clone()).await.unwrap();
    let num_ops: usize = vault
        .read_async(move |txn| -> DatabaseResult<usize> {
            Ok(txn.query_row("SELECT COUNT(rowid) FROM DhtOp", [], |row| row.get(0))?)
        })
        .await
        .unwrap();
    assert_eq!(num_ops, 0);

    let ops_sender = IncomingDhtOpSender::new(space.clone(), tx.clone());
    ops_sender.send_store_record(record.clone()).await.unwrap();
    let num_ops: usize = vault
        .read_async(move |txn| -> DatabaseResult<usize> {
            Ok(txn.query_row("SELECT COUNT(rowid) FROM DhtOp", [], |row| row.get(0))?)
        })
        .await
        .unwrap();
    assert_eq!(num_ops, 1);
    let num_entries: usize = vault
        .read_async(move |txn| -> DatabaseResult<usize> {
            Ok(txn.query_row("SELECT COUNT(rowid) FROM Entry", [], |row| row.get(0))?)
        })
        .await
        .unwrap();
    assert_eq!(num_entries, 0);
}

/// Test that the valid_chain contrafact matches our chain validation function,
/// since many other tests will depend on this constraint
#[tokio::test(flavor = "multi_thread")]
// XXX: the valid_arbitrary_chain as used here can't handle actions with
// sys validation dependencies, so we filter out those action types.
// Also, there are several other problems here that need to be addressed
// to make this not flaky.
#[ignore = "flaky"]
async fn valid_chain_fact_test() {
    let n = 100;
    let keystore = SweetConductor::from_standard_config().await.keystore();
    let author = SweetAgents::one(keystore.clone()).await;

    let mut chain = valid_arbitrary_chain(&keystore, author, n).await;

    validate_chain(chain.iter().map(|r| r.signed_action()), &None).unwrap();

    let mut last = chain.pop().unwrap();
    let penult = chain.last().unwrap();

    // clean up this record so it's valid
    *last.as_action_mut().timestamp_mut() =
        (penult.action().timestamp() + Duration::from_secs(1)).unwrap();
    // re-sign it
    last.signed_action = SignedActionHashed::sign(
        &keystore,
        ActionHashed::from_content_sync(last.action().clone()),
    )
    .await
    .unwrap();

    let cascade = MockCascade::with_records(chain);

    sys_validate_record(&last, Arc::new(cascade)).await.unwrap();
}



================================================
File: crates/holochain/src/core/workflow/app_validation_workflow.rs
================================================
//! Holochain workflow to validate all incoming DHT operations with an
//! app-defined validation function.
//!
//! Triggered by system validation, this workflow iterates over a list of
//! [`DhtOp`]s that have passed system validation, validates each op, updates its validation status
//! in the database accordingly, and triggers op integration if necessary.
//!
//! ### Sequential validation
//!

// Even though ops are validated in sequence, they could all be validated in parallel too with the same result.
// All actions are written to the database straight away in the incoming dht ops workflow and do not require validation to be available for validating other ops. See https://github.com/holochain/holochain/issues/3724

//! Ops are validated in sequence based on their op type and the timestamp they
//! were authored (see [`OpOrder`] and [`OpNumericalOrder`]). Validating one op
//! after the other with this ordering was chosen so that ops that depend on earlier
//! ops will be validated after the earlier ops, and therefore have a higher chance
//! of being validated successfully. An example is an incoming delete
//! op that depends on a create op. Validated in order of their authoring, the
//! create op is validated first, followed at some stage by the delete op. If
//! the validation function references the original action when validating
//! delete ops, the create op will have been validated and is available in the
//! database. Otherwise the delete op could not be validated and its dependency,
//! the create op, would be awaited first.
//!
//! ### Op validation
//!
//! For each op the [corresponding app validation function](https://docs.rs/hdi/latest/hdi/#data-validation)
//! is executed. Entry and link CRUD actions, which the ops are derived from, have been
//! written with a particular integrity zome's entry and link types. Thus for
//! op validation, the validation function of the same integrity zome must be
//! used. Ops that do not relate to a specific entry or link like [`ChainOp::RegisterAgentActivity`]
//! or non-app entries like [`EntryType::CapGrant`] are validated with all
//! validation functions of the DNA's integrity zomes.
//!
//! Having established the relevant integrity zomes for validating an op, each
//! zome's validation callback is invoked.
//!
//! ### Outcome
//!
//! An op can be valid or invalid, which is considered "validated", or it could not be
//! validated because it is missing required dependencies such as actions,
//! entries, links or agent activity that the validation function is referencing. If
//! all ops were validated, the workflow completes with no further action. If
//! however some ops could not be validated, the workflow will trigger itself
//! again after a delay, while missing dependencies are being fetched in the
//! background.
//!
//! #### Errors
//!
//! If the validate invocation of an integrity zome returns an error while
//! validating an op, the op is considered not validated but also not missing
//! dependencies. In effect the workflow will not re-trigger itself.
//!
//! Such errors do not depend on op validity or presence of ops, but indicate
//! a more fundamental problem with either the conductor state, like a missing
//! zome or ribosome or DNA, or network access. For none of these errors is the
//! conductor able to recover itself.
//!
//! Ops that have not been validated due to validation errors will be retried
//! the next time app validation runs, when other ops from gossip or publish come in and
//! need to be validated.
//!
//! ### Missing dependencies
//!
//! Finding the zomes to invoke for validation oftentimes involves fetching a
//! referenced original action, like in the case of updates and deletes. Further
//! the validation function may require actions, entries or agent activity
//! (segments of an agent's source chain) that currently are not stored in the
//! local databases. These are dependencies of the op. If they are missing
//! locally, a network get request will be sent in the background. The op
//! validation outcome will be [`Outcome::AwaitingDeps`]. Validation of
//! remaining ops will carry on, as the network request's response is not
//! awaited within the op validation loop. Instead the whole workflow triggers
//! itself again after a delay.
//!
//! ### Workflow re-triggering
//!
//! Missing dependencies of ops re-trigger the validation workflow. After a delay of
//! a maximum of 3 seconds, which gives the background task that gets the missing
//! dependencies from the network some time to complete, the app validation workflow
//! runs again. All ops whose missing dependencies could be fetched during this interval
//! will successfully validate now.
//!
//! ### Integration workflow
//!

// This seems to mainly affect ops with system dependencies, as ops without such
// dependencies are set to integrated as part of this workflow.

//! If any ops have been validated (outcome valid or invalid), [`integrate_dht_ops_workflow`](crate::core::workflow::integrate_dht_ops_workflow)
//! is triggered, which completes integration of ops after successful validation.

use super::error::WorkflowResult;
use super::sys_validation_workflow::validation_query;

use crate::conductor::api::DpkiApi;
use crate::conductor::entry_def_store::get_entry_def;
use crate::conductor::Conductor;
use crate::conductor::ConductorHandle;
use crate::core::queue_consumer::TriggerSender;
use crate::core::queue_consumer::WorkComplete;
use crate::core::ribosome::guest_callback::validate::ValidateHostAccess;
use crate::core::ribosome::guest_callback::validate::ValidateInvocation;
use crate::core::ribosome::guest_callback::validate::ValidateResult;
use crate::core::ribosome::RibosomeT;
use crate::core::ribosome::ZomesToInvoke;
use crate::core::validation::OutcomeOrError;
use crate::core::SysValidationError;
use crate::core::SysValidationResult;
use crate::core::ValidationOutcome;

pub use error::*;
pub use types::Outcome;

use holo_hash::DhtOpHash;
use holochain_cascade::Cascade;
use holochain_cascade::CascadeImpl;
use holochain_keystore::MetaLairClient;
use holochain_p2p::actor::GetOptions as NetworkGetOptions;
use holochain_p2p::GenericNetwork;
use holochain_p2p::HolochainP2pDna;
use holochain_p2p::HolochainP2pDnaT;
use holochain_state::host_fn_workspace::HostFnWorkspace;
use holochain_state::host_fn_workspace::HostFnWorkspaceRead;
use holochain_state::prelude::*;

use parking_lot::Mutex;
use std::collections::HashSet;
use std::sync::atomic::AtomicUsize;
use std::sync::atomic::Ordering;
use std::sync::Arc;
use std::time::Duration;
use tracing::*;

#[cfg(test)]
mod tests;

#[cfg(test)]
mod validation_tests;

#[cfg(test)]
mod get_zomes_to_invoke_tests;

#[cfg(test)]
mod run_validation_callback_tests;

mod error;
mod types;

#[cfg_attr(
    feature = "instrument",
    instrument(skip(
        workspace,
        trigger_integration,
        trigger_publish,
        conductor_handle,
        network,
        dht_query_cache,
    ))
)]
#[allow(clippy::too_many_arguments)]
pub async fn app_validation_workflow(
    dna_hash: Arc<DnaHash>,
    workspace: Arc<AppValidationWorkspace>,
    trigger_integration: TriggerSender,
    trigger_publish: TriggerSender,
    conductor_handle: ConductorHandle,
    network: HolochainP2pDna,
    dht_query_cache: DhtDbQueryCache,
) -> WorkflowResult<WorkComplete> {
    let outcome_summary = app_validation_workflow_inner(
        dna_hash,
        workspace,
        conductor_handle,
        &network,
        dht_query_cache,
    )
    .await?;
    // --- END OF WORKFLOW, BEGIN FINISHER BOILERPLATE ---

    // If ops have been accepted or rejected, trigger integration.
    if outcome_summary.validated > 0 {
        trigger_integration.trigger(&"app_validation_workflow");
    }

    // If ops have been warranted, trigger publishing.
    if outcome_summary.warranted > 0 {
        trigger_publish.trigger(&"app_validation_workflow");
    }

    Ok(
        // If not all ops have been validated, trigger app validation workflow again.
        if outcome_summary.validated < outcome_summary.ops_to_validate {
            // Trigger app validation workflow again in 100-3000 milliseconds.
            let interval = 2900u64.saturating_sub(outcome_summary.missing as u64 * 100) + 100;
            WorkComplete::Incomplete(Some(Duration::from_millis(interval)))
        } else {
            WorkComplete::Complete
        },
    )
}

async fn app_validation_workflow_inner(
    dna_hash: Arc<DnaHash>,
    workspace: Arc<AppValidationWorkspace>,
    conductor: ConductorHandle,
    network: &HolochainP2pDna,
    dht_query_cache: DhtDbQueryCache,
) -> WorkflowResult<OutcomeSummary> {
    let db = workspace.dht_db.clone().into();
    let sorted_dht_ops = validation_query::get_ops_to_app_validate(&db).await?;
    let num_ops_to_validate = sorted_dht_ops.len();

    let cascade = Arc::new(workspace.full_cascade(network.clone()));
    let accepted_ops = Arc::new(AtomicUsize::new(0));
    let awaiting_ops = Arc::new(AtomicUsize::new(0));
    let rejected_ops = Arc::new(AtomicUsize::new(0));
    let warranted_ops = Arc::new(AtomicUsize::new(0));
    let failed_ops = Arc::new(Mutex::new(HashSet::new()));
    let mut agent_activity_ops = vec![];
    #[cfg(feature = "unstable-warrants")]
    let mut warrant_op_hashes = vec![];

    // Validate ops sequentially
    for sorted_dht_op in sorted_dht_ops.into_iter() {
        let (dht_op, dht_op_hash) = sorted_dht_op.into_inner();

        let chain_op = match dht_op {
            DhtOp::ChainOp(chain_op) => chain_op,
            _ => unreachable!("warrant ops are never sent to app validation"),
        };

        let op_type = chain_op.get_type();
        let action = chain_op.action();
        let dht_op_lite = chain_op.to_lite();

        // If this is agent activity, track it for the cache.
        let agent_activity_op = matches!(op_type, ChainOpType::RegisterAgentActivity)
            .then(|| (action.author().clone(), action.action_seq()));

        // Validate this op
        let validation_outcome = match chain_op_to_op(*chain_op.clone(), cascade.clone()).await {
            Ok(op) => {
                validate_op_outer(dna_hash.clone(), &op, &conductor, &workspace, network).await
            }
            Err(e) => Err(e),
        };
        // Flatten nested app validation outcome to either ok or error
        let validation_outcome = match validation_outcome {
            Ok(outcome) => AppValidationResult::Ok(outcome),
            Err(OutcomeOrError::Outcome(outcome)) => AppValidationResult::Ok(outcome),
            Err(OutcomeOrError::Err(err)) => AppValidationResult::Err(err),
        };

        match validation_outcome {
            Ok(outcome) => {
                // Collect all agent activity.
                if let Some(agent_activity_op) = agent_activity_op {
                    // If the activity is accepted or rejected then it's ready to integrate.
                    if matches!(&outcome, Outcome::Accepted | Outcome::Rejected(_)) {
                        agent_activity_ops.push(agent_activity_op);
                    }
                }
                if let Outcome::AwaitingDeps(_) | Outcome::Rejected(_) = &outcome {
                    warn!(?outcome, ?dht_op_lite, "DhtOp has failed app validation");
                }

                let accepted_ops = accepted_ops.clone();
                let awaiting_ops = awaiting_ops.clone();
                let rejected_ops = rejected_ops.clone();

                #[cfg(feature = "unstable-warrants")]
                if let Outcome::Rejected(_) = &outcome {
                    let warrant_op =
                        crate::core::workflow::sys_validation_workflow::make_warrant_op(
                            &conductor,
                            &dna_hash,
                            &chain_op,
                            ValidationType::App,
                        )
                        .await?;

                    warrant_op_hashes.push((warrant_op.to_hash(), warrant_op.dht_basis().clone()));

                    workspace
                        .authored_db
                        .write_async(move |txn| {
                            warn!("Inserting warrant op");
                            insert_op_authored(txn, &warrant_op)
                        })
                        .await?;

                    warranted_ops.fetch_add(1, Ordering::SeqCst);
                }

                let write_result = workspace
                    .dht_db
                    .write_async(move|txn| match outcome {
                        Outcome::Accepted => {
                            accepted_ops.fetch_add(1, Ordering::SeqCst);
                            put_integration_limbo(txn, &dht_op_hash, ValidationStatus::Valid)
                        }
                        Outcome::AwaitingDeps(_) => {
                            awaiting_ops.fetch_add(1, Ordering::SeqCst);
                            put_validation_limbo(
                                txn,
                                &dht_op_hash,
                                ValidationStage::AwaitingAppDeps,
                            )
                        }
                        Outcome::Rejected(_) => {
                            rejected_ops.fetch_add(1, Ordering::SeqCst);
                            tracing::info!("Received invalid op. The op author will be blocked. Op: {dht_op_lite:?}");
                            put_integration_limbo(txn, &dht_op_hash, ValidationStatus::Rejected)
                        }
                    })
                    .await;
                if let Err(err) = write_result {
                    tracing::error!(?chain_op, ?err, "Error updating dht op in database.");
                }
            }
            Err(err) => {
                tracing::error!(
                    ?chain_op,
                    ?err,
                    "App validation error when validating dht op."
                );
                failed_ops.lock().insert(dht_op_hash);
            }
        }
    }

    // "self-publish" warrants, i.e. insert them into the DHT db as if they were published to us by another node
    #[cfg(feature = "unstable-warrants")]
    holochain_state::integrate::authored_ops_to_dht_db(
        network.storage_arcs().await?,
        warrant_op_hashes,
        workspace.authored_db.clone().into(),
        workspace.dht_db.clone(),
        &workspace.dht_db_cache,
    )
    .await?;

    // Once the database transaction is committed, add agent activity to the cache
    // that is ready for integration.
    for (author, seq) in agent_activity_ops {
        dht_query_cache
            .set_activity_ready_to_integrate(&author, Some(seq))
            .await?;
    }

    let accepted_ops = accepted_ops.load(Ordering::SeqCst);
    let awaiting_ops = awaiting_ops.load(Ordering::SeqCst);
    let rejected_ops = rejected_ops.load(Ordering::SeqCst);
    let warranted_ops = warranted_ops.load(Ordering::SeqCst);
    let ops_validated = accepted_ops + rejected_ops;
    let failed_ops = Arc::try_unwrap(failed_ops)
        .expect("must be only reference")
        .into_inner();
    tracing::info!("{ops_validated} out of {num_ops_to_validate} validated: {accepted_ops} accepted, {awaiting_ops} awaiting deps, {rejected_ops} rejected, failed ops {failed_ops:?}.");

    let outcome_summary = OutcomeSummary {
        ops_to_validate: num_ops_to_validate,
        validated: ops_validated,
        accepted: accepted_ops,
        missing: awaiting_ops,
        rejected: rejected_ops,
        warranted: warranted_ops,
        failed: failed_ops,
    };
    Ok(outcome_summary)
}

// This fn is only used in the zome call workflow's inline validation.
pub async fn record_to_op(
    record: Record,
    op_type: ChainOpType,
    cascade: Arc<impl Cascade>,
) -> AppValidationOutcome<(Op, DhtOpHash, Option<Entry>)> {
    // Hide private data where appropriate
    let (record, mut hidden_entry) = if matches!(op_type, ChainOpType::StoreEntry) {
        // We don't want to hide private data for a StoreEntry, because when doing
        // inline validation as an author, we want to validate and integrate our own entry!
        // Publishing and gossip rules state that a private StoreEntry will never be transmitted
        // to another node.
        (record, None)
    } else {
        // All other records have private entry data hidden, including from ourselves if we are
        // authoring private data.
        record.privatized()
    };

    let (sah, entry) = record.into_inner();
    let mut entry = entry.into_option();
    let action = sah.into();
    // Register agent activity doesn't store the entry so we need to
    // save it so we can reconstruct the record later.
    if matches!(op_type, ChainOpType::RegisterAgentActivity) {
        hidden_entry = entry.take().or(hidden_entry);
    }
    let chain_op = ChainOp::from_type(op_type, action, entry)?;
    let chain_op_hash = chain_op.clone().to_hash();
    Ok((
        chain_op_to_op(chain_op, cascade).await?,
        chain_op_hash,
        hidden_entry,
    ))
}

async fn chain_op_to_op(chain_op: ChainOp, cascade: Arc<impl Cascade>) -> AppValidationOutcome<Op> {
    let op = match chain_op {
        ChainOp::StoreRecord(signature, action, entry) => Op::StoreRecord(StoreRecord {
            record: Record::new(
                SignedActionHashed::with_presigned(
                    ActionHashed::from_content_sync(action),
                    signature,
                ),
                entry.into_option(),
            ),
        }),
        ChainOp::StoreEntry(signature, action, entry) => Op::StoreEntry(StoreEntry {
            action: SignedHashed::new_unchecked(action.into(), signature),
            entry,
        }),
        ChainOp::RegisterAgentActivity(signature, action) => {
            Op::RegisterAgentActivity(RegisterAgentActivity {
                action: SignedActionHashed::with_presigned(
                    ActionHashed::from_content_sync(action),
                    signature,
                ),
                cached_entry: None,
            })
        }
        ChainOp::RegisterUpdatedContent(signature, update, entry)
        | ChainOp::RegisterUpdatedRecord(signature, update, entry) => {
            let new_entry = match update.entry_type.visibility() {
                EntryVisibility::Public => match entry.into_option() {
                    Some(entry) => Some(entry),
                    None => Some(
                        cascade
                            .retrieve_entry(update.entry_hash.clone(), Default::default())
                            .await?
                            .map(|(e, _)| e.into_content())
                            .ok_or_else(|| Outcome::awaiting(&update.entry_hash))?,
                    ),
                },
                _ => None,
            };
            Op::RegisterUpdate(RegisterUpdate {
                update: SignedHashed::new_unchecked(update, signature),
                new_entry,
            })
        }
        ChainOp::RegisterDeletedBy(signature, delete)
        | ChainOp::RegisterDeletedEntryAction(signature, delete) => {
            Op::RegisterDelete(RegisterDelete {
                delete: SignedHashed::new_unchecked(delete, signature),
            })
        }
        ChainOp::RegisterAddLink(signature, create_link) => {
            Op::RegisterCreateLink(RegisterCreateLink {
                create_link: SignedHashed::new_unchecked(create_link, signature),
            })
        }
        ChainOp::RegisterRemoveLink(signature, delete_link) => {
            let create_link = cascade
                .retrieve_action(delete_link.link_add_address.clone(), Default::default())
                .await?
                .and_then(|(sh, _)| CreateLink::try_from(sh.hashed.content).ok())
                .ok_or_else(|| Outcome::awaiting(&delete_link.link_add_address))?;
            Op::RegisterDeleteLink(RegisterDeleteLink {
                delete_link: SignedHashed::new_unchecked(delete_link, signature),
                create_link,
            })
        }
    };
    Ok(op)
}

async fn validate_op_outer(
    dna_hash: Arc<DnaHash>,
    op: &Op,
    conductor_handle: &ConductorHandle,
    workspace: &AppValidationWorkspace,
    network: &HolochainP2pDna,
) -> AppValidationOutcome<Outcome> {
    // Get the workspace for the validation calls
    let host_fn_workspace = workspace.validation_workspace().await?;

    // Get the ribosome
    let ribosome = conductor_handle
        .get_ribosome(dna_hash.as_ref())
        .map_err(|_| AppValidationError::DnaMissing((*dna_hash).clone()))?;

    let dpki = conductor_handle.running_services().dpki;

    validate_op(
        op,
        host_fn_workspace,
        network,
        &ribosome,
        conductor_handle,
        dpki,
        false, // is_inline
    )
    .await
}

#[allow(clippy::too_many_arguments)]
pub async fn validate_op(
    op: &Op,
    workspace: HostFnWorkspaceRead,
    network: &HolochainP2pDna,
    ribosome: &impl RibosomeT,
    conductor_handle: &ConductorHandle,
    dpki: DpkiApi,
    is_inline: bool,
) -> AppValidationOutcome<Outcome> {
    check_entry_def(op, &network.dna_hash(), conductor_handle)
        .await
        .map_err(AppValidationError::SysValidationError)?;

    let network = Arc::new(network.clone());

    let zomes_to_invoke = get_zomes_to_invoke(op, &workspace, network.clone(), ribosome).await;
    if let Err(OutcomeOrError::Err(err)) = &zomes_to_invoke {
        tracing::error!(?op, ?err, "Error getting zomes to invoke to validate op.");
    };
    let zomes_to_invoke = zomes_to_invoke?;
    let invocation = ValidateInvocation::new(zomes_to_invoke, op)
        .map_err(|e| AppValidationError::RibosomeError(e.into()))?;

    let outcome =
        run_validation_callback(invocation, ribosome, workspace, network, dpki, is_inline).await?;

    Ok(outcome)
}

/// Check the AppEntryDef is valid for the zome.
/// Check the EntryDefId and ZomeIndex are in range.
async fn check_entry_def(
    op: &Op,
    dna_hash: &DnaHash,
    conductor: &Conductor,
) -> SysValidationResult<()> {
    if let Some((_, EntryType::App(app_entry_def))) = op.entry_data() {
        check_app_entry_def(app_entry_def, dna_hash, conductor).await
    } else {
        Ok(())
    }
}

/// Check the AppEntryDef is valid for the zome.
/// Check the EntryDefId and ZomeIndex are in range.
async fn check_app_entry_def(
    app_entry_def: &AppEntryDef,
    dna_hash: &DnaHash,
    conductor: &Conductor,
) -> SysValidationResult<()> {
    // We want to be careful about holding locks open to the conductor api
    // so calls are made in blocks
    let ribosome = conductor
        .get_ribosome(dna_hash)
        .map_err(|_| SysValidationError::DnaMissing(dna_hash.clone()))?;

    // Check if the zome is found
    let zome = ribosome
        .get_integrity_zome(&app_entry_def.zome_index())
        .ok_or_else(|| ValidationOutcome::ZomeIndex(app_entry_def.clone()))?
        .into_inner()
        .1;

    let entry_def = get_entry_def(app_entry_def.entry_index(), zome, dna_hash, conductor).await?;

    // Check the visibility and return
    match entry_def {
        Some(entry_def) => {
            if entry_def.visibility == *app_entry_def.visibility() {
                Ok(())
            } else {
                Err(ValidationOutcome::EntryVisibility(app_entry_def.clone()).into())
            }
        }
        None => Err(ValidationOutcome::EntryDefId(app_entry_def.clone()).into()),
    }
}

// Zomes to invoke for app validation are determined based on app entries'
// zome index. Whenever an app entry is contained in the op, the zome index can
// directly be known. For other cases like deletes, the deleted action is
// retrieved with the expectation that it is the original create or an update,
// which again include the app entry type that specifies the zome index of the
// integrity zome.
//
// Special cases are non app entries like cap grants and claims and agent pub
// keys. None of them have an entry definition or a zome index of the integrity
// zome. Thus all integrity zomes are returned for validation invocation.
async fn get_zomes_to_invoke(
    op: &Op,
    workspace: &HostFnWorkspaceRead,
    network: GenericNetwork,
    ribosome: &impl RibosomeT,
) -> AppValidationOutcome<ZomesToInvoke> {
    match op {
        Op::RegisterAgentActivity(RegisterAgentActivity { .. }) => Ok(ZomesToInvoke::AllIntegrity),
        Op::StoreRecord(StoreRecord { record }) => {
            // For deletes there is no entry type to check, so we get the previous action.
            // In theory this can be yet another delete, in which case all
            // integrity zomes are returned for invocation.
            // Instead the delete could be followed up the chain to find the original
            // create, but since deleting a delete does not have much practical use,
            // it is neglected here.
            let action = match record.action() {
                Action::Delete(Delete {
                    deletes_address, ..
                })
                | Action::DeleteLink(DeleteLink {
                    link_add_address: deletes_address,
                    ..
                }) => {
                    let deleted_action =
                        retrieve_deleted_action(workspace, network, deletes_address).await?;
                    deleted_action.action().clone()
                }
                _ => record.action().clone(),
            };

            match action {
                Action::CreateLink(CreateLink { zome_index, .. })
                | Action::Create(Create {
                    entry_type: EntryType::App(AppEntryDef { zome_index, .. }),
                    ..
                })
                | Action::Update(Update {
                    entry_type: EntryType::App(AppEntryDef { zome_index, .. }),
                    ..
                }) => get_integrity_zome_from_ribosome(&zome_index, ribosome),
                _ => Ok(ZomesToInvoke::AllIntegrity),
            }
        }
        Op::StoreEntry(StoreEntry { action, .. }) => match &action.hashed.content {
            EntryCreationAction::Create(Create {
                entry_type: EntryType::App(app_entry_def),
                ..
            })
            | EntryCreationAction::Update(Update {
                entry_type: EntryType::App(app_entry_def),
                ..
            }) => get_integrity_zome_from_ribosome(&app_entry_def.zome_index, ribosome),
            _ => Ok(ZomesToInvoke::AllIntegrity),
        },
        Op::RegisterUpdate(RegisterUpdate { update, .. }) => match &update.hashed.entry_type {
            EntryType::App(app_entry_def) => {
                get_integrity_zome_from_ribosome(&app_entry_def.zome_index, ribosome)
            }
            _ => Ok(ZomesToInvoke::AllIntegrity),
        },
        Op::RegisterDelete(RegisterDelete { delete }) => {
            let deletes_address = &delete.hashed.deletes_address;
            let deleted_action =
                retrieve_deleted_action(workspace, network, deletes_address).await?;
            match deleted_action.hashed.content {
                Action::Create(Create {
                    entry_type: EntryType::App(app_entry_def),
                    ..
                })
                | Action::Update(Update {
                    entry_type: EntryType::App(app_entry_def),
                    ..
                }) => get_integrity_zome_from_ribosome(&app_entry_def.zome_index, ribosome),
                _ => Ok(ZomesToInvoke::AllIntegrity),
            }
        }
        Op::RegisterCreateLink(RegisterCreateLink {
            create_link:
                SignedHashed {
                    hashed:
                        HoloHashed {
                            content: action, ..
                        },
                    ..
                },
            ..
        })
        | Op::RegisterDeleteLink(RegisterDeleteLink {
            create_link: action,
            ..
        }) => get_integrity_zome_from_ribosome(&action.zome_index, ribosome),
    }
}

async fn retrieve_deleted_action(
    workspace: &HostFnWorkspaceRead,
    network: GenericNetwork,
    deletes_address: &ActionHash,
) -> AppValidationOutcome<SignedActionHashed> {
    let cascade = CascadeImpl::from_workspace_and_network(workspace, network.clone());
    let (deleted_action, _) = cascade
        .retrieve_action(deletes_address.clone(), NetworkGetOptions::default())
        .await?
        .ok_or_else(|| Outcome::awaiting(deletes_address))?;
    Ok(deleted_action)
}

fn get_integrity_zome_from_ribosome(
    zome_index: &ZomeIndex,
    ribosome: &impl RibosomeT,
) -> AppValidationOutcome<ZomesToInvoke> {
    let zome = ribosome.get_integrity_zome(zome_index).ok_or_else(|| {
        Outcome::rejected(format!("No integrity zome found with index {zome_index:?}"))
    })?;
    Ok(ZomesToInvoke::OneIntegrity(zome))
}

#[allow(clippy::too_many_arguments)]
async fn run_validation_callback(
    invocation: ValidateInvocation,
    ribosome: &impl RibosomeT,
    workspace: HostFnWorkspaceRead,
    network: GenericNetwork,
    dpki: DpkiApi,
    is_inline: bool,
) -> AppValidationResult<Outcome> {
    let validate_result = ribosome
        .run_validate(
            ValidateHostAccess::new(workspace.clone(), network.clone(), dpki, is_inline),
            invocation.clone(),
        )
        .await?;
    match validate_result {
        ValidateResult::Valid => Ok(Outcome::Accepted),
        ValidateResult::Invalid(reason) => Ok(Outcome::Rejected(reason)),
        ValidateResult::UnresolvedDependencies(UnresolvedDependencies::Hashes(hashes)) => {
            tracing::debug!(
                ?hashes,
                "Op validation returned unresolved dependencies -  Hashes"
            );
            // fetch all missing hashes in the background without awaiting them
            let cascade_workspace = workspace.clone();
            let cascade =
                CascadeImpl::from_workspace_and_network(&cascade_workspace, network.clone());

            // build a collection of futures to fetch the missing hashes
            let fetches = hashes.clone().into_iter().map(move |hash| {
                let cascade = cascade.clone();
                async move {
                    let result = cascade
                        .fetch_record(hash.clone(), NetworkGetOptions::must_get_options())
                        .await;
                    if let Err(err) = result {
                        tracing::warn!("error fetching dependent hash {hash:?}: {err}");
                    }
                }
            });
            // await all fetches in a separate task in the background
            tokio::spawn(async { futures::future::join_all(fetches).await });
            Ok(Outcome::AwaitingDeps(hashes))
        }
        ValidateResult::UnresolvedDependencies(UnresolvedDependencies::AgentActivity(
            author,
            filter,
        )) => {
            tracing::debug!(
                ?author,
                ?filter,
                "Op validation returned unresolved dependencies -  AgentActivity"
            );
            // fetch missing agent activities in the background without awaiting them
            let cascade_workspace = workspace.clone();
            let author = author.clone();
            let cascade =
                CascadeImpl::from_workspace_and_network(&cascade_workspace, network.clone());

            // fetch dependency
            tokio::spawn({
                let author = author.clone();
                async move {
                    let result = cascade
                        .must_get_agent_activity(author.clone(), filter)
                        .await;
                    if let Err(err) = result {
                        tracing::warn!("error fetching dependent chain of agent {author:?}: {err}");
                    }
                }
            });
            Ok(Outcome::AwaitingDeps(vec![author.into()]))
        }
    }
}

// accepted, missing and rejected are only used in tests
#[allow(dead_code)]
#[derive(Debug)]
struct OutcomeSummary {
    ops_to_validate: usize,
    validated: usize,
    accepted: usize,
    missing: usize,
    rejected: usize,
    warranted: usize,
    failed: HashSet<DhtOpHash>,
}

impl OutcomeSummary {
    fn new() -> Self {
        OutcomeSummary {
            ops_to_validate: 0,
            validated: 0,
            accepted: 0,
            missing: 0,
            rejected: 0,
            warranted: 0,
            failed: HashSet::new(),
        }
    }
}

impl Default for OutcomeSummary {
    fn default() -> Self {
        OutcomeSummary::new()
    }
}

pub struct AppValidationWorkspace {
    // Writeable because of warrants
    authored_db: DbWrite<DbKindAuthored>,
    dht_db: DbWrite<DbKindDht>,
    dht_db_cache: DhtDbQueryCache,
    cache: DbWrite<DbKindCache>,
    keystore: MetaLairClient,
    dna_def: Arc<DnaDef>,
}

impl AppValidationWorkspace {
    pub fn new(
        // Writeable because of warrants
        authored_db: DbWrite<DbKindAuthored>,
        dht_db: DbWrite<DbKindDht>,
        dht_db_cache: DhtDbQueryCache,
        cache: DbWrite<DbKindCache>,
        keystore: MetaLairClient,
        dna_def: Arc<DnaDef>,
    ) -> Self {
        Self {
            authored_db,
            dht_db,
            dht_db_cache,
            cache,
            keystore,
            dna_def,
        }
    }

    pub async fn validation_workspace(&self) -> AppValidationResult<HostFnWorkspaceRead> {
        Ok(HostFnWorkspace::new(
            self.authored_db.clone().into(),
            self.dht_db.clone().into(),
            self.dht_db_cache.clone(),
            self.cache.clone(),
            self.keystore.clone(),
            None,
            self.dna_def.clone(),
        )
        .await?)
    }

    pub fn full_cascade<Network: HolochainP2pDnaT>(&self, network: Network) -> CascadeImpl {
        CascadeImpl::empty()
            .with_authored(self.authored_db.clone().into())
            .with_dht(self.dht_db.clone().into())
            .with_network(Arc::new(network), self.cache.clone())
    }
}

pub fn put_validation_limbo(
    txn: &mut Txn<DbKindDht>,
    hash: &DhtOpHash,
    status: ValidationStage,
) -> WorkflowResult<()> {
    set_validation_stage(txn, hash, status)?;
    Ok(())
}

pub fn put_integration_limbo(
    txn: &mut Txn<DbKindDht>,
    hash: &DhtOpHash,
    status: ValidationStatus,
) -> WorkflowResult<()> {
    set_validation_status(txn, hash, status)?;
    set_validation_stage(txn, hash, ValidationStage::AwaitingIntegration)?;
    Ok(())
}



================================================
File: crates/holochain/src/core/workflow/call_zome_workflow.rs
================================================
use super::app_validation_workflow;
use super::app_validation_workflow::AppValidationError;
use super::app_validation_workflow::Outcome;
use super::error::WorkflowResult;
use super::sys_validation_workflow::sys_validate_record;
use crate::conductor::api::CellConductorApi;
use crate::conductor::api::CellConductorApiT;
use crate::conductor::api::DpkiApi;
use crate::conductor::ConductorHandle;
use crate::core::check_dpki_agent_validity_for_record;
use crate::core::queue_consumer::TriggerSender;
use crate::core::ribosome::error::RibosomeResult;
use crate::core::ribosome::guest_callback::post_commit::send_post_commit;
use crate::core::ribosome::RibosomeT;
use crate::core::ribosome::ZomeCallHostAccess;
use crate::core::ribosome::ZomeCallInvocation;
use crate::core::workflow::WorkflowError;
use holochain_keystore::MetaLairClient;
use holochain_p2p::{HolochainP2pDna, HolochainP2pDnaT};
use holochain_state::host_fn_workspace::SourceChainWorkspace;
use holochain_state::prelude::IncompleteCommitReason;
use holochain_state::source_chain::SourceChainError;
use holochain_types::prelude::*;
use holochain_zome_types::record::Record;
use std::sync::Arc;
use tokio::sync::broadcast;

#[cfg(test)]
mod validation_test;

/// Placeholder for the return value of a zome invocation
pub type ZomeCallResult = RibosomeResult<ZomeCallResponse>;

pub struct CallZomeWorkflowArgs<RibosomeT> {
    pub ribosome: RibosomeT,
    pub invocation: ZomeCallInvocation,
    pub signal_tx: broadcast::Sender<Signal>,
    pub conductor_handle: ConductorHandle,
    pub is_root_zome_call: bool,
    pub cell_id: CellId,
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn call_zome_workflow<Ribosome>(
    workspace: SourceChainWorkspace,
    network: HolochainP2pDna,
    keystore: MetaLairClient,
    args: CallZomeWorkflowArgs<Ribosome>,
    trigger_publish_dht_ops: TriggerSender,
    trigger_integrate_dht_ops: TriggerSender,
    trigger_countersigning: TriggerSender,
) -> WorkflowResult<ZomeCallResult>
where
    Ribosome: RibosomeT + 'static,
{
    let coordinator_zome = args
        .ribosome
        .dna_def()
        .get_coordinator_zome(args.invocation.zome.zome_name())
        .or_else(|_| {
            args.ribosome
                .dna_def()
                .get_integrity_zome(args.invocation.zome.zome_name())
                .map(CoordinatorZome::from)
        })
        .ok();
    let should_write = args.is_root_zome_call;
    let conductor_handle = args.conductor_handle.clone();
    let maybe_dpki = args.conductor_handle.running_services().dpki;
    let signal_tx = args.signal_tx.clone();
    let result = call_zome_workflow_inner(
        workspace.clone(),
        maybe_dpki,
        network.clone(),
        keystore.clone(),
        args,
        trigger_countersigning,
    )
    .await?;
    // --- END OF WORKFLOW, BEGIN FINISHER BOILERPLATE ---

    // commit the workspace
    if should_write {
        let countersigning_op = workspace.source_chain().countersigning_op()?;
        match workspace
            .source_chain()
            .flush(network.storage_arcs().await?, network.chc())
            .await
        {
            Ok(flushed_actions) => {
                // Skip if nothing was written
                if !flushed_actions.is_empty() {
                    match countersigning_op {
                        Some(op) => {
                            if let Err(error_response) =
                                super::countersigning_workflow::countersigning_publish(
                                    &network,
                                    op,
                                    (*workspace.author().ok_or_else(|| {
                                        WorkflowError::Other("author required".into())
                                    })?)
                                    .clone(),
                                )
                                .await
                            {
                                return Ok(Ok(error_response));
                            }
                        }
                        None => {
                            trigger_publish_dht_ops.trigger(&"call_zome_workflow");
                            trigger_integrate_dht_ops.trigger(&"call_zome_workflow");
                        }
                    }

                    // Only send post commit if this is a coordinator zome.
                    if let Some(coordinator_zome) = coordinator_zome {
                        send_post_commit(
                            conductor_handle,
                            workspace,
                            network,
                            keystore,
                            flushed_actions,
                            vec![coordinator_zome],
                            signal_tx,
                        )
                        .await?;
                    }
                }
            }
            err => {
                err?;
            }
        }
    };
    Ok(result)
}

async fn call_zome_workflow_inner<Ribosome>(
    workspace: SourceChainWorkspace,
    dpki: DpkiApi,
    network: HolochainP2pDna,
    keystore: MetaLairClient,
    args: CallZomeWorkflowArgs<Ribosome>,
    trigger_countersigning: TriggerSender,
) -> WorkflowResult<ZomeCallResult>
where
    Ribosome: RibosomeT + 'static,
{
    let CallZomeWorkflowArgs {
        ribosome,
        invocation,
        signal_tx,
        conductor_handle,
        cell_id,
        ..
    } = args;

    let call_zome_handle =
        CellConductorApi::new(conductor_handle.clone(), cell_id).into_call_zome_handle();

    tracing::trace!("Before zome call");
    let host_access = ZomeCallHostAccess::new(
        workspace.clone().into(),
        keystore,
        dpki,
        network.clone(),
        signal_tx,
        call_zome_handle,
    );
    let (ribosome, result) =
        call_zome_function_authorized(ribosome, host_access, invocation).await?;
    tracing::trace!("After zome call");

    let validation_result =
        inline_validation(workspace.clone(), network, conductor_handle, ribosome).await;

    // If the validation failed remove any active chain lock that matches the
    // entry that failed validation.
    // Note that missing dependencies will not produce an `InvalidCommit` but an `IncompleteCommit`
    // so that the commit can be retried later without terminating the countersigning session.
    if matches!(
        validation_result,
        Err(WorkflowError::SourceChainError(
            SourceChainError::InvalidCommit(_)
        ))
    ) {
        let scratch_records = workspace.source_chain().scratch_records()?;
        if scratch_records.len() == 1 {
            let lock_subject = holochain_state::source_chain::chain_lock_subject_for_entry(
                scratch_records[0].entry().as_option(),
            )?;

            // If this wasn't a countersigning commit then the lock will be empty.
            if !lock_subject.is_empty() {
                // Otherwise, we can check whether the chain was locked with a subject matching
                // the entry that failed validation.
                if let Some(chain_lock) = workspace.source_chain().get_chain_lock().await? {
                    // Here we know the chain is locked, and if the lock subject matches the entry
                    // that the app was trying to commit then we can unlock the chain.
                    if chain_lock.subject() == lock_subject {
                        if let Err(error) = workspace.source_chain().unlock_chain().await {
                            tracing::error!(?error);
                        }

                        // Immediately unlocking the chain is safe because we know that the
                        // countersigning commit hasn't been written to the source chain here.
                        // We still need to clean up the session state in the countersigning workspace
                        // though, so trigger the countersigning workflow and let it figure out
                        // what happened.
                        trigger_countersigning.trigger(&"invalid_countersigning_commit");
                    }
                }
            }
        }
    }

    validation_result?;
    Ok(result)
}

/// First check if we are authorized to call
/// the zome function.
/// Then send to a background thread and
/// call the zome function.
pub async fn call_zome_function_authorized<R>(
    ribosome: R,
    host_access: ZomeCallHostAccess,
    invocation: ZomeCallInvocation,
) -> WorkflowResult<(R, RibosomeResult<ZomeCallResponse>)>
where
    R: RibosomeT + 'static,
{
    match invocation.is_authorized(&host_access).await? {
        ZomeCallAuthorization::Authorized => {
            let r = ribosome.call_zome_function(host_access, invocation).await;
            Ok((ribosome, r))
        }
        not_authorized_reason => Ok((
            ribosome,
            Ok(ZomeCallResponse::Unauthorized(
                not_authorized_reason,
                invocation.cap_secret,
                invocation.zome.zome_name().clone(),
                invocation.fn_name.clone(),
            )),
        )),
    }
}

/// Run validation inline and wait for the result.
pub async fn inline_validation<Ribosome>(
    workspace: SourceChainWorkspace,
    network: HolochainP2pDna,
    conductor_handle: ConductorHandle,
    ribosome: Ribosome,
) -> WorkflowResult<()>
where
    Ribosome: RibosomeT + 'static,
{
    let cascade = Arc::new(holochain_cascade::CascadeImpl::from_workspace_and_network(
        &workspace,
        Arc::new(network.clone()),
    ));

    let scratch_records = workspace.source_chain().scratch_records()?;

    if let Some(dpki) = conductor_handle.running_services().dpki.clone() {
        // Don't check DPKI validity on DPKI itself!
        if !dpki.is_deepkey_dna(workspace.source_chain().cell_id().dna_hash()) {
            // Check the validity of the author as-at the first and the last record to be committed.
            // If these are valid, then the author is valid for the entire commit.
            let first = scratch_records.first();
            let last = scratch_records.last();
            if let Some(r) = first {
                check_dpki_agent_validity_for_record(&dpki, r).await?;
            }
            if let Some(r) = last {
                if first != last {
                    check_dpki_agent_validity_for_record(&dpki, r).await?;
                }
            }
        }
    }

    let records = {
        // collect all the records we need to validate in wasm
        let mut to_app_validate: Vec<Record> = Vec::with_capacity(scratch_records.len());
        // Loop forwards through all the new records
        for record in scratch_records {
            sys_validate_record(&record, cascade.clone())
                .await
                // If the was en error exit
                // If the validation failed, exit with an InvalidCommit
                // If the validation failed with a retryable error, exit with an IncompleteCommit
                // If it was ok continue
                .or_else(|outcome_or_err| outcome_or_err.into_workflow_error())?;
            to_app_validate.push(record);
        }

        to_app_validate
    };

    let dpki = conductor_handle.running_services().dpki;

    for mut chain_record in records {
        for op_type in action_to_op_types(chain_record.action()) {
            let outcome =
                app_validation_workflow::record_to_op(chain_record, op_type, cascade.clone()).await;

            let (op, _, omitted_entry) = match outcome {
                Ok(op) => op,
                Err(outcome_or_err) => return map_outcome(Outcome::try_from(outcome_or_err)),
            };

            let outcome = app_validation_workflow::validate_op(
                &op,
                workspace.clone().into(),
                &network,
                &ribosome,
                &conductor_handle,
                dpki.clone(),
                true, // is_inline
            )
            .await;
            let outcome = outcome.or_else(Outcome::try_from);
            map_outcome(outcome)?;
            chain_record = op_to_record(op, omitted_entry);
        }
    }

    Ok(())
}

fn op_to_record(op: Op, omitted_entry: Option<Entry>) -> Record {
    match op {
        Op::StoreRecord(StoreRecord { mut record }) => {
            if let Some(e) = omitted_entry {
                // NOTE: this is only possible in this situation because we already removed
                // this exact entry from this Record earlier. DON'T set entries on records
                // anywhere else without recomputing hashes and signatures!
                record.entry = RecordEntry::Present(e);
            }
            record
        }
        Op::StoreEntry(StoreEntry { action, entry }) => {
            Record::new(SignedActionHashed::raw_from_same_hash(action), Some(entry))
        }
        Op::RegisterUpdate(RegisterUpdate {
            update, new_entry, ..
        }) => Record::new(SignedActionHashed::raw_from_same_hash(update), new_entry),
        Op::RegisterDelete(RegisterDelete { delete, .. }) => Record::new(
            SignedActionHashed::raw_from_same_hash(delete),
            omitted_entry,
        ),
        Op::RegisterAgentActivity(RegisterAgentActivity { action, .. }) => Record::new(
            SignedActionHashed::raw_from_same_hash(action),
            omitted_entry,
        ),
        Op::RegisterCreateLink(RegisterCreateLink { create_link, .. }) => Record::new(
            SignedActionHashed::raw_from_same_hash(create_link),
            omitted_entry,
        ),
        Op::RegisterDeleteLink(RegisterDeleteLink { delete_link, .. }) => Record::new(
            SignedActionHashed::raw_from_same_hash(delete_link),
            omitted_entry,
        ),
    }
}

fn map_outcome(outcome: Result<Outcome, AppValidationError>) -> WorkflowResult<()> {
    match outcome.map_err(SourceChainError::other)? {
        app_validation_workflow::Outcome::Accepted => {}
        app_validation_workflow::Outcome::Rejected(reason) => {
            return Err(SourceChainError::InvalidCommit(format!(
                "Validation failed while committing: {reason}"
            ))
            .into());
        }
        // When the wasm is being called directly in a zome invocation, any state other than valid
        // is not allowed for new entries. E.g. we require that all dependencies are met when
        // committing an entry to a local source chain.
        // This is different to the case where we are validating data coming in from the network
        // where unmet dependencies would be rescheduled to attempt later due to partitions etc.
        // To allow the client to decide whether to retry later, we return a different error
        // variant here. This indicates that the validation did not fail because the data is
        // definitely invalid, but because validation could not make a decision yet.
        Outcome::AwaitingDeps(hashes) => {
            return Err(SourceChainError::IncompleteCommit(
                IncompleteCommitReason::DepMissingFromDht(hashes),
            )
            .into());
        }
    }
    Ok(())
}



================================================
File: crates/holochain/src/core/workflow/countersigning_workflow.rs
================================================
//! Countersigning workflow to maintain countersigning session state.

use crate::core::share::Share;
use holochain_p2p::{
    event::CountersigningSessionNegotiationMessage, HolochainP2pDna, HolochainP2pDnaT,
};
use holochain_state::prelude::*;
use std::time::Duration;

#[cfg(feature = "unstable-countersigning")]
use {
    super::error::WorkflowResult,
    crate::conductor::space::Space,
    crate::core::queue_consumer::{TriggerSender, WorkComplete},
    holo_hash::AgentPubKey,
    holochain_keystore::MetaLairClient,
    holochain_state::chain_lock::get_chain_lock,
    std::sync::Arc,
    tokio::sync::broadcast::Sender,
    tokio::task::AbortHandle,
};

#[cfg(feature = "unstable-countersigning")]
/// Accept handler for starting countersigning sessions.
mod accept;

#[cfg(feature = "unstable-countersigning")]
/// Inner workflow for resolving an incomplete countersigning session.
mod incomplete;

#[cfg(feature = "unstable-countersigning")]
/// Inner workflow for completing a countersigning session based on received signatures.
mod complete;

#[cfg(feature = "unstable-countersigning")]
/// State integrity function to ensure that the database and the workspace are in sync.
mod refresh;

#[cfg(feature = "unstable-countersigning")]
/// Success handler for receiving signature bundles from the network.
mod success;

#[cfg(feature = "unstable-countersigning")]
#[cfg(test)]
mod tests;

#[cfg(feature = "unstable-countersigning")]
pub(crate) use {accept::accept_countersigning_request, success::countersigning_success};

/// Countersigning workspace to hold session state.
#[derive(Clone)]
pub struct CountersigningWorkspace {
    inner: Share<CountersigningWorkspaceInner>,
    #[cfg(feature = "unstable-countersigning")]
    countersigning_resolution_retry_delay: Duration,
    #[cfg(feature = "unstable-countersigning")]
    countersigning_resolution_retry_limit: Option<usize>,
}

impl CountersigningWorkspace {
    /// Create a new countersigning workspace.
    #[allow(unused_variables)]
    pub fn new(
        countersigning_resolution_retry_delay: Duration,
        countersigning_resolution_retry_limit: Option<usize>,
    ) -> Self {
        Self {
            inner: Default::default(),
            #[cfg(feature = "unstable-countersigning")]
            countersigning_resolution_retry_delay,
            #[cfg(feature = "unstable-countersigning")]
            countersigning_resolution_retry_limit,
        }
    }

    pub fn get_countersigning_session_state(&self) -> Option<CountersigningSessionState> {
        self.inner
            .share_ref(|inner| Ok(inner.session.clone()))
            .unwrap()
    }

    pub fn mark_countersigning_session_for_force_abandon(
        &self,
        cell_id: &CellId,
    ) -> Result<(), CountersigningError> {
        self.inner
            .share_mut(|inner, _| {
                Ok(if let Some(session) = inner.session.as_mut() {
                    if let CountersigningSessionState::Unknown {
                        resolution,
                        force_abandon,
                        ..
                    } = session
                    {
                        if resolution.attempts >= 1 {
                            *force_abandon = true;
                            Ok(())
                        } else {
                            Err(CountersigningError::SessionNotUnresolved(cell_id.clone()))
                        }
                    } else {
                        Err(CountersigningError::SessionNotUnresolved(cell_id.clone()))
                    }
                } else {
                    Err(CountersigningError::SessionNotFound(cell_id.clone()))
                })
            })
            .unwrap()
    }

    pub fn mark_countersigning_session_for_force_publish(
        &self,
        cell_id: &CellId,
    ) -> Result<(), CountersigningError> {
        self.inner
            .share_mut(|inner, _| {
                Ok(if let Some(session) = inner.session.as_mut() {
                    if let CountersigningSessionState::Unknown {
                        resolution,
                        force_publish,
                        ..
                    } = session
                    {
                        if resolution.attempts >= 1 {
                            *force_publish = true;
                            Ok(())
                        } else {
                            Err(CountersigningError::SessionNotUnresolved(cell_id.clone()))
                        }
                    } else {
                        Err(CountersigningError::SessionNotUnresolved(cell_id.clone()))
                    }
                } else {
                    Err(CountersigningError::SessionNotFound(cell_id.clone()))
                })
            })
            .unwrap()
    }

    pub fn remove_countersigning_session(&self) -> Option<CountersigningSessionState> {
        self.inner
            .share_mut(|inner, _| Ok(inner.session.take()))
            .unwrap()
    }
}

/// The inner state of a countersigning workspace.
#[derive(Default)]
struct CountersigningWorkspaceInner {
    session: Option<CountersigningSessionState>,
    #[cfg(feature = "unstable-countersigning")]
    next_trigger: Option<NextTrigger>,
}

#[cfg(feature = "unstable-countersigning")]
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
#[allow(clippy::too_many_arguments)]
pub(crate) async fn countersigning_workflow(
    space: Space,
    workspace: Arc<CountersigningWorkspace>,
    network: Arc<impl HolochainP2pDnaT>,
    keystore: MetaLairClient,
    cell_id: CellId,
    signal_tx: Sender<Signal>,
    self_trigger: TriggerSender,
    integration_trigger: TriggerSender,
    publish_trigger: TriggerSender,
) -> WorkflowResult<WorkComplete> {
    tracing::debug!(
        "Starting countersigning workflow, with a session? {}",
        workspace
            .inner
            .share_ref(|inner| Ok(inner.session.is_some()))
            .unwrap()
    );

    // Clear trigger, if we need another one, it will be created later.
    workspace
        .inner
        .share_mut(|inner, _| {
            if let Some(next_trigger) = &mut inner.next_trigger {
                next_trigger.trigger_task.abort();
            }
            inner.next_trigger = None;
            Ok(())
        })
        .unwrap();

    // Ensure the workspace state knows about anything in the database on startup.
    refresh::refresh_workspace_state(
        &space,
        workspace.clone(),
        cell_id.clone(),
        signal_tx.clone(),
    )
    .await;

    // Abandon any sessions that have timed out.
    apply_timeout(&space, workspace.clone(), &cell_id, signal_tx.clone()).await?;

    // If the session is in an unknown state, try to recover it.
    try_recover_failed_session(
        &space,
        workspace.clone(),
        network.clone(),
        &cell_id,
        &signal_tx,
    )
    .await?;

    // If there are new signature bundles, verify them to complete the session.
    let maybe_signature_bundles = workspace
        .inner
        .share_mut(|inner, _| {
            Ok(match &mut inner.session {
                Some(CountersigningSessionState::SignaturesCollected {
                    signature_bundles, ..
                }) => Some(std::mem::take(signature_bundles)),
                _ => None,
            })
        })
        .unwrap();

    let mut completed = false;
    if let Some(signature_bundles) = maybe_signature_bundles {
        for signature_bundle in signature_bundles {
            // Try to complete the session using this signature bundle.

            match complete::inner_countersigning_session_complete(
                space.clone(),
                network.clone(),
                keystore.clone(),
                cell_id.agent_pubkey().clone(),
                signature_bundle.clone(),
                integration_trigger.clone(),
                publish_trigger.clone(),
            )
            .await
            {
                Ok(Some(_)) => {
                    completed = true;
                    break;
                }
                Ok(None) => {
                    tracing::warn!("Rejected signature bundle for countersigning session for agent: {:?}: {:?}", cell_id.agent_pubkey(), signature_bundle);
                }
                Err(e) => {
                    tracing::error!(
                        "Error completing countersigning session for agent: {:?}: {:?}",
                        cell_id.agent_pubkey(),
                        e
                    );
                }
            }
        }

        if !completed {
            // If we got these signatures from a resolution attempt, then we need to return to the
            // unknown state now that we've tried the signatures, and they can't be used to resolve
            // the session.
            workspace.inner.share_mut(|inner, _| {
                if let Some(session) = &mut inner.session {
                    match session {
                        CountersigningSessionState::SignaturesCollected {
                            preflight_request,
                            resolution,
                            ..
                        } => {
                            if resolution.is_some() {
                                *session = CountersigningSessionState::Unknown {
                                    preflight_request: preflight_request.clone(),
                                    resolution: resolution.clone().unwrap_or_default(),
                                    force_abandon: false,
                                    force_publish: false,
                                };
                            }
                        }
                        _ => {
                            tracing::error!("Countersigning session for agent {:?} was not in the expected state while trying to resolve it: {:?}", cell_id.agent_pubkey(), session);
                        }
                    }
                }
                Ok(())
            }).unwrap();
        }
    } else {
        // No signature bundles.
        // If the session is marked to be force-abandoned, execute that and send signal to the client.

        // If the session is marked to be force-published, execute that and set the completed status
        // for later removal of the session.

        // Get flags and preflight_request from session first if it is unresolved.
        if let Ok(Some((force_abandon, force_publish, preflight_request))) =
            workspace.inner.share_ref(|inner| {
                Ok(inner.session.as_ref().and_then(|session| {
                    // To set the force-abandon or force-publish flag, attempts to resolve must be > 0, so it does not have
                    // to be checked again now.
                    if let CountersigningSessionState::Unknown {
                        force_abandon,
                        force_publish,
                        preflight_request,
                        ..
                    } = session
                    {
                        Some((*force_abandon, *force_publish, preflight_request.clone()))
                    } else {
                        None
                    }
                }))
            })
        {
            if force_abandon {
                force_abandon_session(
                    space.clone(),
                    cell_id.agent_pubkey(),
                    &preflight_request,
                    workspace.clone(),
                    &signal_tx,
                )
                .await?;
            } else if force_publish {
                complete::force_publish_countersigning_session(
                    space.clone(),
                    network.clone(),
                    keystore.clone(),
                    integration_trigger.clone(),
                    publish_trigger.clone(),
                    cell_id.clone(),
                    preflight_request.clone(),
                )
                .await?;
                completed = true;
            }
        }
    }

    // Session is complete, either by incoming signature bundles or by force.
    if completed {
        // The session completed successfully, so we can remove it from the workspace.
        let countersigned_entry_hash = workspace
            .inner
            .share_mut(|inner, _| {
                let countersigned_entry_hash = inner.session.as_ref().map(|session| session.session_app_entry_hash().clone());
                tracing::trace!("Countersigning session completed successfully, removing from the workspace for agent: {:?}", cell_id.agent_pubkey());
                inner.session = None;
                Ok(countersigned_entry_hash)
            })
            .unwrap();

        // Signal to the UI.
        // If there are no active connections, this won't emit anything.
        if let Some(entry_hash) = countersigned_entry_hash {
            signal_tx
                .send(Signal::System(SystemSignal::SuccessfulCountersigning(
                    entry_hash,
                )))
                .ok();
        }
    }

    // At the end of the workflow, if we have a session still in progress, then schedule a
    // workflow run again at the end time.
    let maybe_end_time = workspace
        .inner
        .share_ref(|inner| {
            Ok(match &inner.session {
                Some(state) => match state {
                    CountersigningSessionState::Accepted(preflight_request)
                    | CountersigningSessionState::SignaturesCollected {
                        preflight_request, ..
                    } => Some(preflight_request.session_times.end),
                    CountersigningSessionState::Unknown { .. } => {
                        (Timestamp::now() + workspace.countersigning_resolution_retry_delay).ok()
                    }
                },
                None => None,
            })
        })
        .unwrap();

    tracing::info!("End time: {:?}", maybe_end_time);

    if let Some(end_time) = maybe_end_time {
        reschedule_self(workspace, self_trigger, end_time);
    }

    Ok(WorkComplete::Complete)
}

#[cfg(feature = "unstable-countersigning")]
async fn try_recover_failed_session(
    space: &Space,
    workspace: Arc<CountersigningWorkspace>,
    network: Arc<impl HolochainP2pDnaT + Sized>,
    cell_id: &CellId,
    signal_tx: &Sender<Signal>,
) -> WorkflowResult<()> {
    let maybe_session_in_unknown_state = workspace
        .inner
        .share_ref(|inner| {
            Ok(inner
                .session
                .as_ref()
                .and_then(|session_state| match session_state {
                    CountersigningSessionState::Unknown {
                        preflight_request, ..
                    } => Some(preflight_request.clone()),
                    _ => None,
                }))
        })
        .unwrap();

    if let Some(preflight_request) = maybe_session_in_unknown_state {
        tracing::info!(
            "Countersigning session for agent {:?} is in an unknown state, attempting to resolve",
            cell_id.agent_pubkey()
        );
        match incomplete::inner_countersigning_session_incomplete(
            space.clone(),
            network.clone(),
            cell_id.agent_pubkey().clone(),
            preflight_request.clone(),
        )
        .await
        {
            Ok((SessionCompletionDecision::Complete(_), outcomes)) => {
                // No need to do anything here. Signatures were found which may be able to complete
                // the session but the session isn't actually complete yet. We need to let the
                // workflow re-run and try those signatures.
                update_last_attempted(workspace.clone(), true, outcomes, cell_id);
            }
            Ok((SessionCompletionDecision::Abandoned, _)) => {
                // The session state has been resolved, so we can remove it from the workspace.
                workspace
                    .inner
                    .share_mut(|inner, _| {
                        tracing::trace!(
                            "Decision made for incomplete session, removing from workspace: {:?}",
                            cell_id.agent_pubkey()
                        );

                        inner.session = None;
                        Ok(())
                    })
                    .unwrap();

                signal_tx
                    .send(Signal::System(SystemSignal::AbandonedCountersigning(
                        preflight_request.app_entry_hash.clone(),
                    )))
                    .ok();
            }
            Ok((SessionCompletionDecision::Indeterminate, outcomes)) => {
                tracing::info!(
                    "No automated decision could be reached for the current countersigning session: {:?}",
                    cell_id.agent_pubkey()
                );

                // Record the attempt
                update_last_attempted(workspace.clone(), true, outcomes, cell_id);

                let resolution = get_resolution(workspace.clone());
                if let Some(SessionResolutionSummary { attempts, .. }) = resolution {
                    let limit = workspace.countersigning_resolution_retry_limit.unwrap_or(0);

                    // If we have reached the limit of attempts, then abandon the session.
                    if workspace.countersigning_resolution_retry_limit.is_none()
                        || (limit > 0 && attempts >= limit)
                    {
                        tracing::info!("Reached the limit ({}) of attempts ({}) to resolve countersigning session for agent: {:?}", limit, attempts, cell_id.agent_pubkey());

                        force_abandon_session(
                            space.clone(),
                            cell_id.agent_pubkey(),
                            &preflight_request,
                            workspace.clone(),
                            signal_tx,
                        )
                        .await?;
                    }
                }
            }
            Ok((SessionCompletionDecision::Failed, outcomes)) => {
                tracing::info!(
                    "Failed to resolve countersigning session for agent: {:?}",
                    cell_id.agent_pubkey()
                );

                // Record the attempt time, but not the attempt count.
                update_last_attempted(workspace.clone(), false, outcomes, cell_id);
            }
            Err(e) => {
                tracing::error!(
                    "Error resolving countersigning session for agent: {:?}: {:?}",
                    cell_id.agent_pubkey(),
                    e
                );
            }
        }
    }

    Ok(())
}

#[cfg(feature = "unstable-countersigning")]
fn reschedule_self(
    workspace: Arc<CountersigningWorkspace>,
    self_trigger: TriggerSender,
    at_timestamp: Timestamp,
) {
    workspace
        .inner
        .share_mut(|inner, _| {
            if let Some(next_trigger) = &mut inner.next_trigger {
                next_trigger.replace_if_sooner(at_timestamp, self_trigger.clone());
            } else {
                inner.next_trigger = Some(NextTrigger::new(at_timestamp, self_trigger.clone()));
            }

            Ok(())
        })
        .unwrap();
}

#[cfg(feature = "unstable-countersigning")]
fn update_last_attempted(
    workspace: Arc<CountersigningWorkspace>,
    add_to_attempts: bool,
    outcomes: Vec<SessionResolutionOutcome>,
    cell_id: &CellId,
) {
    workspace.inner.share_mut(|inner, _| {
        if let Some(session) = &mut inner.session {
            match session {
                CountersigningSessionState::SignaturesCollected { resolution, .. } => {
                    if let Some(resolution) = resolution {
                        if add_to_attempts {
                            resolution.attempts += 1;
                        }
                        resolution.last_attempt_at = Some(Timestamp::now());
                        resolution.outcomes = outcomes;
                    } else {
                        tracing::warn!("Countersigning session for agent {:?} is missing a resolution but we are trying to resolve it", cell_id.agent_pubkey());
                    }
                }
                CountersigningSessionState::Unknown { resolution, .. } => {
                    if add_to_attempts {
                        resolution.attempts += 1;
                    }
                    resolution.last_attempt_at = Some(Timestamp::now());
                    resolution.outcomes = outcomes;
                }
                state => {
                    tracing::error!("Countersigning session for agent {:?} was not in the expected state while trying to resolve it: {:?}", cell_id.agent_pubkey(), state);
                }
            }
        } else {
            tracing::error!("Countersigning session for agent {:?} was removed from the workspace while trying to resolve it", cell_id.agent_pubkey());
        }

        Ok(())
    }).unwrap()
}

#[cfg(feature = "unstable-countersigning")]
fn get_resolution(workspace: Arc<CountersigningWorkspace>) -> Option<SessionResolutionSummary> {
    workspace
        .inner
        .share_ref(|inner| {
            Ok(match &inner.session {
                Some(CountersigningSessionState::SignaturesCollected { resolution, .. }) => {
                    resolution.clone()
                }
                Some(CountersigningSessionState::Unknown { resolution, .. }) => {
                    Some(resolution.clone())
                }
                _ => None,
            })
        })
        .unwrap()
}

#[cfg(feature = "unstable-countersigning")]
async fn apply_timeout(
    space: &Space,
    workspace: Arc<CountersigningWorkspace>,
    cell_id: &CellId,
    signal_tx: Sender<Signal>,
) -> WorkflowResult<()> {
    let preflight_request = workspace
        .inner
        .share_ref(|inner| {
            Ok(inner
                .session
                .as_ref()
                .map(|session| session.preflight_request().clone()))
        })
        .unwrap();
    if preflight_request.is_none() {
        tracing::info!("Cannot check session timeout because there is no active session");
        return Ok(());
    }

    let authored = space.get_or_create_authored_db(cell_id.agent_pubkey().clone())?;

    let current_session = authored
        .read_async({
            let author = cell_id.agent_pubkey().clone();
            move |txn| current_countersigning_session(txn, Arc::new(author))
        })
        .await?;

    let mut has_committed_session = false;
    if let Some((_, _, session_data)) = current_session {
        if session_data.preflight_request.fingerprint() == preflight_request.unwrap().fingerprint()
        {
            has_committed_session = true;
        }
    }

    let timed_out = workspace
        .inner
        .share_mut(|inner, _| {
            Ok(inner.session.as_mut().and_then(|session| {
                let expired = match session {
                    CountersigningSessionState::Accepted(preflight_request) => {
                        if preflight_request.session_times.end < Timestamp::now() {
                            if has_committed_session {
                                *session = CountersigningSessionState::Unknown {
                                    preflight_request: preflight_request.clone(),
                                    resolution: SessionResolutionSummary {
                                        required_reason: ResolutionRequiredReason::Timeout,
                                        ..Default::default()
                                    },
                                    force_abandon: false,
                                    force_publish: false,
                                };
                                false
                            } else {
                                true
                            }
                        } else {
                            false
                        }
                    }
                    CountersigningSessionState::SignaturesCollected {
                        preflight_request,
                        signature_bundles,
                        resolution,
                    } => {
                        // Only change state if all signatures have been tried and this is not a recovery state
                        // because recovery should be dealt with separately.
                        if preflight_request.session_times.end < Timestamp::now()
                            && signature_bundles.is_empty()
                            && resolution.is_none()
                        {
                            *session = CountersigningSessionState::Unknown {
                                preflight_request: preflight_request.clone(),
                                resolution: SessionResolutionSummary {
                                    required_reason: ResolutionRequiredReason::Timeout,
                                    ..Default::default()
                                },
                                force_abandon: false,
                                force_publish: false,
                            };
                        }

                        false
                    }
                    _ => false,
                };

                if expired {
                    Some(session.preflight_request().clone())
                } else {
                    None
                }
            }))
        })
        .unwrap();

    if let Some(preflight_request) = timed_out {
        tracing::info!(
            "Countersigning session for agent {:?} has timed out, abandoning session",
            cell_id.agent_pubkey()
        );

        if let Err(e) = force_abandon_session(
            space.clone(),
            cell_id.agent_pubkey(),
            &preflight_request,
            workspace.clone(),
            &signal_tx,
        )
        .await
        {
            tracing::error!(
                "Error abandoning countersigning session for agent: {:?}: {:?}",
                cell_id.agent_pubkey(),
                e
            );
        }
    }

    Ok(())
}

#[cfg(feature = "unstable-countersigning")]
async fn force_abandon_session(
    space: Space,
    author: &AgentPubKey,
    preflight_request: &PreflightRequest,
    workspace: Arc<CountersigningWorkspace>,
    signal_tx: &Sender<Signal>,
) -> SourceChainResult<()> {
    let authored_db = space.get_or_create_authored_db(author.clone())?;

    let abandon_fingerprint = preflight_request.fingerprint()?;

    let maybe_session_data = authored_db
        .read_async({
            let author = author.clone();
            move |txn| current_countersigning_session(txn, Arc::new(author.clone()))
        })
        .await?;

    match maybe_session_data {
        Some((cs_action, cs_entry_hash, x))
            if x.preflight_request.fingerprint()? == abandon_fingerprint =>
        {
            tracing::info!("There is a committed session to remove for: {:?}", author);
            abandon_session(
                authored_db,
                author.clone(),
                cs_action.action().clone(),
                cs_entry_hash,
            )
            .await?;
        }
        _ => {
            // There is no matching, committed session but there may be a lock to remove
            authored_db
                .write_async({
                    let author = author.clone();
                    move |txn| {
                        let chain_lock = get_chain_lock(txn, &author)?;

                        match chain_lock {
                            Some(lock) if lock.subject() == abandon_fingerprint => {
                                unlock_chain(txn, &author)
                            }
                            _ => {
                                tracing::warn!(
                                    "No matching session or lock to remove for: {:?}",
                                    author
                                );
                                Ok(())
                            }
                        }
                    }
                })
                .await?;
        }
    }

    // Only once we've managed to remove the session do we remove the state for it.
    workspace
        .inner
        .share_mut(|inner, _| {
            tracing::trace!("Abandoning countersigning session for agent: {:?}", author);
            inner.session = None;
            Ok(())
        })
        .unwrap();

    // Then let the client know.
    signal_tx
        .send(Signal::System(SystemSignal::AbandonedCountersigning(
            preflight_request.app_entry_hash.clone(),
        )))
        .ok();

    Ok(())
}

/// Publish to entry authorities, so they can gather all the signed
/// actions for this session and respond with a session complete.
pub async fn countersigning_publish(
    network: &HolochainP2pDna,
    op: ChainOp,
    _author: AgentPubKey,
) -> Result<(), ZomeCallResponse> {
    if let Some(enzyme) = op.enzymatic_countersigning_enzyme() {
        if let Err(e) = network
            .countersigning_session_negotiation(
                vec![enzyme.clone()],
                CountersigningSessionNegotiationMessage::EnzymePush(Box::new(op)),
            )
            .await
        {
            tracing::error!(
                "Failed to push countersigning ops to enzyme because of: {:?}",
                e
            );
            return Err(ZomeCallResponse::CountersigningSession(e.to_string()));
        }
    } else {
        let basis = op.dht_basis();
        if let Err(e) = network.publish_countersign(true, basis, op.into()).await {
            tracing::error!(
                "Failed to publish to entry authorities for countersigning session because of: {:?}",
                e
            );
            return Err(ZomeCallResponse::CountersigningSession(e.to_string()));
        }
    }
    Ok(())
}

#[cfg(feature = "unstable-countersigning")]
/// Abandon a countersigning session.
async fn abandon_session(
    authored_db: DbWrite<DbKindAuthored>,
    author: AgentPubKey,
    cs_action: Action,
    cs_entry_hash: EntryHash,
) -> StateMutationResult<()> {
    authored_db
        .write_async(move |txn| -> StateMutationResult<()> {
            // Do the dangerous thing and remove the countersigning session.
            remove_countersigning_session(txn, cs_action, cs_entry_hash)?;

            // Once the session is removed we can unlock the chain.
            unlock_chain(txn, &author)?;

            Ok(())
        })
        .await?;

    Ok(())
}

#[cfg(feature = "unstable-countersigning")]
// TODO unify with the other mechanisms for re-triggering. This is currently working around
//      a performance issue with WorkComplete::Incomplete but is similar to the loop logic that
//      other workflows use - the difference being that this workflow varies the loop delay.
struct NextTrigger {
    trigger_at: Timestamp,
    trigger_task: AbortHandle,
}

#[cfg(feature = "unstable-countersigning")]
impl NextTrigger {
    fn new(trigger_at: Timestamp, trigger_sender: TriggerSender) -> Self {
        let delay = Self::calculate_delay(&trigger_at);

        let trigger_task = Self::start_trigger_task(delay, trigger_sender);

        Self {
            trigger_at,
            trigger_task,
        }
    }

    fn replace_if_sooner(&mut self, trigger_at: Timestamp, trigger_sender: TriggerSender) {
        // If the current trigger has expired, or the new one is sooner, then replace the
        // current trigger.
        if self.trigger_at < Timestamp::now() || trigger_at < self.trigger_at {
            let new_delay = Self::calculate_delay(&trigger_at);
            self.trigger_task.abort();
            self.trigger_at = trigger_at;
            self.trigger_task = Self::start_trigger_task(new_delay, trigger_sender);
        }
    }

    fn calculate_delay(trigger_at: &Timestamp) -> Duration {
        match trigger_at
            .checked_difference_signed(&Timestamp::now())
            .map(|d| d.to_std())
        {
            Some(Ok(d)) => d,
            _ => Duration::from_millis(100),
        }
    }

    fn start_trigger_task(delay: Duration, trigger_sender: TriggerSender) -> AbortHandle {
        tracing::trace!("Scheduling countersigning workflow in: {:?}", delay);
        tokio::task::spawn(async move {
            tokio::time::sleep(delay).await;
            trigger_sender.trigger(&"next trigger");
        })
        .abort_handle()
    }
}



================================================
File: crates/holochain/src/core/workflow/error.rs
================================================
// Error types are self-explanatory
#![allow(missing_docs)]

use super::app_validation_workflow::AppValidationError;
use crate::conductor::api::error::ConductorApiError;
use crate::conductor::conductor::DpkiServiceError;
use crate::conductor::CellError;
use crate::core::queue_consumer::QueueTriggerClosedError;
use crate::core::ribosome::error::RibosomeError;
use crate::core::SysValidationError;
use holochain_cascade::error::CascadeError;
use holochain_keystore::KeystoreError;
use holochain_p2p::HolochainP2pError;
use holochain_sqlite::error::DatabaseError;
use holochain_state::source_chain::SourceChainError;
use holochain_state::workspace::WorkspaceError;
use holochain_types::prelude::*;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum WorkflowError {
    #[error("The genesis self-check failed. App cannot be installed. Reason: {0}")]
    GenesisFailure(String),

    #[error(transparent)]
    AppValidationError(#[from] AppValidationError),

    #[error("Agent is invalid: {0:?}")]
    AgentInvalid(AgentPubKey),

    #[error("Conductor API error: {0}")]
    ConductorApi(#[from] Box<ConductorApiError>),

    #[error(transparent)]
    CascadeError(#[from] CascadeError),

    #[error(transparent)]
    CounterSigningError(#[from] CounterSigningError),

    #[error(transparent)]
    DpkiServiceError(#[from] DpkiServiceError),

    #[error("Workspace error: {0}")]
    WorkspaceError(#[from] WorkspaceError),

    #[error("Database error: {0}")]
    DatabaseError(#[from] DatabaseError),

    #[error(transparent)]
    RibosomeError(#[from] RibosomeError),

    #[error("Source chain error: {0}")]
    SourceChainError(#[from] SourceChainError),

    #[error("Capability token missing")]
    CapabilityMissing,

    #[error(transparent)]
    SerializedBytesError(#[from] SerializedBytesError),

    #[error(transparent)]
    CellError(#[from] CellError),

    #[error(transparent)]
    JoinError(#[from] tokio::task::JoinError),

    #[error(transparent)]
    QueueTriggerClosedError(#[from] QueueTriggerClosedError),

    #[error(transparent)]
    HolochainP2pError(#[from] HolochainP2pError),

    #[error(transparent)]
    HoloHashError(#[from] holo_hash::HoloHashError),

    #[error(transparent)]
    InterfaceError(#[from] crate::conductor::interface::error::InterfaceError),

    #[error(transparent)]
    DhtOpError(#[from] DhtOpError),

    #[error(transparent)]
    DbCacheError(#[from] holochain_types::db_cache::DbCacheError),

    #[error(transparent)]
    SysValidationError(#[from] SysValidationError),

    #[error(transparent)]
    KeystoreError(#[from] KeystoreError),

    #[error(transparent)]
    SqlError(#[from] holochain_sqlite::rusqlite::Error),

    #[error(transparent)]
    StateQueryError(#[from] holochain_state::query::StateQueryError),

    #[error(transparent)]
    StateMutationError(#[from] holochain_state::mutations::StateMutationError),

    #[error(transparent)]
    SystemTimeError(#[from] std::time::SystemTimeError),

    #[error(transparent)]
    TimestampError(#[from] holochain_zome_types::prelude::TimestampError),

    #[error("RecvError")]
    RecvError,

    #[error(transparent)]
    SendError(#[from] tokio::sync::mpsc::error::SendError<()>),

    /// Other
    #[error("Other: {0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

impl WorkflowError {
    /// promote a custom error type to a WorkflowError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self::Other(e.into())
    }

    /// True if a workflow encountering this error should bail, else it should
    /// continue executing/looping.
    pub fn workflow_should_bail(&self) -> bool {
        // Currently GenesisFailure is the only thing we abort the app for but
        // in the future this could be expanded to a more sophisticated match
        // statement covering more fatal issues.
        matches!(self, Self::GenesisFailure(_))
    }
}

impl From<one_err::OneErr> for WorkflowError {
    fn from(e: one_err::OneErr) -> Self {
        Self::other(e)
    }
}

/// Internal type to handle running workflows
pub type WorkflowResult<T> = Result<T, WorkflowError>;



================================================
File: crates/holochain/src/core/workflow/genesis_workflow.rs
================================================
//! Genesis Workflow: Initialize the source chain with the initial entries:
//! - Dna
//! - AgentValidationPkg
//! - AgentId

use std::sync::Arc;

use super::error::WorkflowError;
use super::error::WorkflowResult;
use crate::core::ribosome::guest_callback::genesis_self_check::v1::GenesisSelfCheckHostAccessV1;
use crate::core::ribosome::guest_callback::genesis_self_check::v1::GenesisSelfCheckInvocationV1;
use crate::core::ribosome::guest_callback::genesis_self_check::v2::GenesisSelfCheckHostAccessV2;
use crate::core::ribosome::guest_callback::genesis_self_check::v2::GenesisSelfCheckInvocationV2;
use crate::core::ribosome::guest_callback::genesis_self_check::{
    GenesisSelfCheckHostAccess, GenesisSelfCheckInvocation, GenesisSelfCheckResult,
};
use crate::{conductor::api::CellConductorApiT, core::ribosome::RibosomeT};
use derive_more::Constructor;
use holochain_chc::ChcImpl;
use holochain_sqlite::prelude::*;
use holochain_state::source_chain;
use holochain_state::workspace::WorkspaceResult;
use holochain_types::db_cache::DhtDbQueryCache;
use holochain_types::prelude::*;
use rusqlite::named_params;

/// The struct which implements the genesis Workflow
#[derive(Constructor)]
pub struct GenesisWorkflowArgs<Ribosome>
where
    Ribosome: RibosomeT + 'static,
{
    dna_file: DnaFile,
    agent_pubkey: AgentPubKey,
    membrane_proof: Option<MembraneProof>,
    ribosome: Ribosome,
    dht_db_cache: DhtDbQueryCache,
    chc: Option<ChcImpl>,
}

// #[cfg_attr(feature = "instrument", tracing::instrument(skip(workspace, api, args)))]
pub async fn genesis_workflow<'env, Api: CellConductorApiT, Ribosome>(
    mut workspace: GenesisWorkspace,
    api: Api,
    args: GenesisWorkflowArgs<Ribosome>,
) -> WorkflowResult<()>
where
    Ribosome: RibosomeT + 'static,
{
    genesis_workflow_inner(&mut workspace, args, api).await?;
    Ok(())
}

async fn genesis_workflow_inner<Api: CellConductorApiT, Ribosome>(
    workspace: &mut GenesisWorkspace,
    args: GenesisWorkflowArgs<Ribosome>,
    api: Api,
