) -> WorkflowResult<()>
where
    Ribosome: RibosomeT + 'static,
{
    let GenesisWorkflowArgs {
        dna_file,
        agent_pubkey,
        membrane_proof,
        ribosome,
        dht_db_cache,
        chc,
    } = args;

    if workspace.has_genesis(agent_pubkey.clone()).await? {
        return Ok(());
    }

    let dna_hash = ribosome.dna_def().to_hash();
    let DnaDef {
        name,
        modifiers: DnaModifiers { properties, .. },
        integrity_zomes,
        ..
    } = &ribosome.dna_def().content;
    let dna_info = DnaInfoV1 {
        zome_names: integrity_zomes.iter().map(|(n, _)| n.clone()).collect(),
        name: name.clone(),
        hash: dna_hash,
        properties: properties.clone(),
    };
    let result = ribosome
        .run_genesis_self_check(
            GenesisSelfCheckHostAccess {
                host_access_1: GenesisSelfCheckHostAccessV1,
                host_access_2: GenesisSelfCheckHostAccessV2,
            },
            GenesisSelfCheckInvocation {
                invocation_1: GenesisSelfCheckInvocationV1 {
                    payload: Arc::new(GenesisSelfCheckDataV1 {
                        dna_info,
                        membrane_proof: membrane_proof.clone(),
                        agent_key: agent_pubkey.clone(),
                    }),
                },
                invocation_2: GenesisSelfCheckInvocationV2 {
                    payload: Arc::new(GenesisSelfCheckDataV2 {
                        membrane_proof: membrane_proof.clone(),
                        agent_key: agent_pubkey.clone(),
                    }),
                },
            },
        )
        .await?;

    // If the self-check fails, fail genesis, and don't create the source chain.
    if let GenesisSelfCheckResult::Invalid(reason) = result {
        return Err(WorkflowError::GenesisFailure(reason));
    }

    // NOTE: we could check the key against DPKI state here, but the key hasn't even been
    //       registered at this point, so we can't.

    source_chain::genesis(
        workspace.vault.clone(),
        workspace.dht_db.clone(),
        &dht_db_cache,
        api.keystore().clone(),
        dna_file.dna_hash().clone(),
        agent_pubkey,
        membrane_proof,
        chc,
    )
    .await?;

    Ok(())
}

/// The workspace for Genesis
pub struct GenesisWorkspace {
    vault: DbWrite<DbKindAuthored>,
    dht_db: DbWrite<DbKindDht>,
}

impl GenesisWorkspace {
    /// Constructor
    pub fn new(env: DbWrite<DbKindAuthored>, dht_db: DbWrite<DbKindDht>) -> WorkspaceResult<Self> {
        Ok(Self { vault: env, dht_db })
    }

    pub async fn has_genesis(&self, author: AgentPubKey) -> DatabaseResult<bool> {
        let count = self
            .vault
            .read_async(move |txn| {
                let count: u32 = txn.query_row(
                    "
                SELECT
                COUNT(Action.hash)
                FROM Action
                JOIN DhtOp ON DhtOp.action_hash = Action.hash
                WHERE
                Action.author = :author
                LIMIT 3
                ",
                    named_params! {
                        ":author": author,
                    },
                    |row| row.get(0),
                )?;
                DatabaseResult::Ok(count)
            })
            .await?;
        Ok(count >= 3)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    use crate::conductor::api::MockCellConductorApiT;
    use crate::conductor::conductor::{mock_app_store, ConductorServices};
    use crate::core::ribosome::MockRibosomeT;
    use futures::FutureExt;
    use holochain_conductor_services::{DpkiService, KeyState, MockDpkiState};
    use holochain_keystore::test_keystore;
    use holochain_state::prelude::test_dht_db;
    use holochain_state::{prelude::test_authored_db, source_chain::SourceChain};
    use holochain_trace;
    use holochain_types::deepkey_roundtrip_backward;
    use holochain_types::test_utils::fake_agent_pubkey_1;
    use holochain_types::test_utils::fake_dna_file;
    use holochain_zome_types::Action;
    use matches::assert_matches;

    #[tokio::test(flavor = "multi_thread")]
    async fn genesis_initializes_source_chain() {
        holochain_trace::test_run();
        let test_db = test_authored_db();
        let dht_db = test_dht_db();
        let dht_db_cache = DhtDbQueryCache::new(dht_db.to_db().into());
        let keystore = test_keystore();
        let vault = test_db.to_db();
        let dna = fake_dna_file("a");
        let author = fake_agent_pubkey_1();

        let mut mock_dpki = MockDpkiState::new();
        let action =
            deepkey_roundtrip_backward!(SignedActionHashed, &::fixt::fixt!(SignedActionHashed));
        mock_dpki.expect_key_state().returning(move |_, _| {
            let action = action.clone();
            async move { Ok(KeyState::Valid(action)) }.boxed()
        });

        let dpki = DpkiService::new(::fixt::fixt!(CellId), mock_dpki);

        {
            let workspace = GenesisWorkspace::new(vault.clone(), dht_db.to_db()).unwrap();

            let mut api = MockCellConductorApiT::new();
            api.expect_conductor_services()
                .return_const(ConductorServices {
                    dpki: Some(Arc::new(dpki)),
                    app_store: Some(Arc::new(mock_app_store())),
                });
            api.expect_keystore().return_const(keystore.clone());
            let mut ribosome = MockRibosomeT::new();
            ribosome
                .expect_run_genesis_self_check()
                .returning(|_, _| Ok(GenesisSelfCheckResult::Valid));
            let dna_def = DnaDefHashed::from_content_sync(dna.dna_def().clone());
            ribosome.expect_dna_def().return_const(dna_def);
            let args = GenesisWorkflowArgs {
                dna_file: dna.clone(),
                agent_pubkey: author.clone(),
                membrane_proof: None,
                ribosome,
                dht_db_cache: dht_db_cache.clone(),
                chc: None,
            };
            let _: () = genesis_workflow(workspace, api, args).await.unwrap();
        }

        {
            let source_chain = SourceChain::new(
                vault.clone(),
                dht_db.to_db(),
                dht_db_cache,
                keystore,
                author.clone(),
            )
            .await
            .unwrap();
            let actions = source_chain
                .query(Default::default())
                .await
                .unwrap()
                .into_iter()
                .map(|e| e.action().clone())
                .collect::<Vec<_>>();

            assert_matches!(
                actions.as_slice(),
                [
                    Action::Dna(_),
                    Action::AgentValidationPkg(_),
                    Action::Create(_)
                ]
            );
        }
    }
}

/* TODO: update and rewrite as proper rust docs

Called from:

 - Conductor upon first ACTIVATION of an installed DNA (trace: follow)



Parameters (expected types/structures):

- DNA hash to pull from path to file (or HCHC [FUTURE] )

- AgentID [SEEDLING] (already registered in DeepKey [LEAPFROG])

- Membrane Access Payload (optional invitation code / to validate agent join) [possible for LEAPFROG]



Data X (data & structure) from Store Y:

- Get DNA from HCHC by DNA hash

- or Get DNA from filesystem by filename



----

Functions / Workflows:

- check that agent key is valid [MOCKED dpki] (via real dpki [LEAPFROG])

- retrieve DNA from file path [in the future from HCHC]

- initialize databases, save to conductor runtime config.

- commit DNA entry (w/ special enum action with NULL  prev_action)

- commit CapGrant for author (agent key) (w/ normal action)



    fn commit_DNA

    fn produce_action



Examples / Tests / Acceptance Criteria:

- check hash of DNA =



----



Persisted X Changes to Store Y (data & structure):

- source chain HEAD 2 new actions

- CAS commit actions and genesis entries: DNA & Author Capabilities Grant (Agent Key)



- bootstrapped peers from attempt to publish key and join network



Spawned Tasks (don't wait for result -signals/log/tracing=follow):

- ZomeCall:init (for processing app initialization with bridges & networking)

- DHT transforms of genesis entries in CAS



Returned Results (type & structure):

- None
*/



================================================
File: crates/holochain/src/core/workflow/incoming_dht_ops_workflow.rs
================================================
//! The workflow and queue consumer for DhtOp integration

use super::sys_validation_workflow::counterfeit_check_action;
use super::{error::WorkflowResult, sys_validation_workflow::counterfeit_check_warrant};
use crate::{conductor::space::Space, core::queue_consumer::TriggerSender};
use holo_hash::DhtOpHash;
use holochain_sqlite::error::DatabaseResult;
use holochain_sqlite::prelude::*;
use holochain_state::prelude::*;
use incoming_ops_batch::InOpBatchEntry;
use std::{collections::HashSet, sync::Arc};

mod incoming_ops_batch;

pub use incoming_ops_batch::IncomingOpsBatch;

#[cfg(test)]
mod tests;

struct OpsClaim {
    incoming_op_hashes: IncomingOpHashes,
    working_hashes: Vec<DhtOpHash>,
}

impl OpsClaim {
    fn acquire(
        incoming_op_hashes: IncomingOpHashes,
        ops: Vec<DhtOpHashed>,
    ) -> (Self, Vec<DhtOpHashed>) {
        let keep_incoming_op_hashes = incoming_op_hashes.clone();

        // Lock the shared state while we claim the ops we're going to work on
        let mut set = incoming_op_hashes.0.lock();

        // Track the hashes that we're going to work on, and should be removed from the shared state
        // when this claim is dropped.
        let mut working_hashes = Vec::with_capacity(ops.len());
        let mut working_ops = Vec::with_capacity(ops.len());

        for op in ops {
            if !set.contains(&op.hash) {
                set.insert(op.hash.clone());
                working_hashes.push(op.hash.clone());
                working_ops.push(op);
            }
        }

        (
            Self {
                incoming_op_hashes: keep_incoming_op_hashes,
                working_hashes,
            },
            working_ops,
        )
    }
}

impl Drop for OpsClaim {
    fn drop(&mut self) {
        // Lock the shared state while we remove the ops we're finished working with
        let incoming_op_hashes = self.incoming_op_hashes.clone();
        let mut set = incoming_op_hashes.0.lock();

        for hash in &self.working_hashes {
            set.remove(hash);
        }
    }
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(txn, ops)))]
fn batch_process_entry(
    txn: &mut Txn<DbKindDht>,
    request_validation_receipt: bool,
    ops: Vec<DhtOpHashed>,
) -> WorkflowResult<()> {
    // add incoming ops to the validation limbo
    let mut to_pending = Vec::with_capacity(ops.len());
    for op in ops {
        if !op_exists_inner(txn, &op.hash)? {
            to_pending.push(op);
        } else if request_validation_receipt {
            set_require_receipt(txn, &op.hash, true)?;
        }
    }

    tracing::debug!("Inserting {} ops", to_pending.len());
    add_to_pending(txn, &to_pending, request_validation_receipt)?;

    Ok(())
}

#[derive(Default, Clone)]
pub struct IncomingOpHashes(Arc<parking_lot::Mutex<HashSet<DhtOpHash>>>);

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(space, sys_validation_trigger, ops))
)]
pub async fn incoming_dht_ops_workflow(
    space: Space,
    sys_validation_trigger: TriggerSender,
    ops: Vec<DhtOp>,
    request_validation_receipt: bool,
) -> WorkflowResult<()> {
    let Space {
        incoming_op_hashes,
        incoming_ops_batch,
        dht_db,
        ..
    } = space;

    // Compute hashes for all the ops
    let ops = ops
        .into_iter()
        .map(DhtOpHashed::from_content_sync)
        .collect();

    // Filter out ops that are already being tracked, to avoid doing duplicate work
    let (_claim, ops) = OpsClaim::acquire(incoming_op_hashes, ops);

    // If everything we've been sent is already being worked on then this workflow run can be skipped
    if ops.is_empty() {
        return Ok(());
    }

    let num_ops = ops.len();
    let mut filter_ops = Vec::with_capacity(num_ops);
    for op in ops {
        // It's cheaper to check if the signature is valid before proceeding to open a write transaction.
        let keeper = should_keep(&op.content).await;
        match keeper {
            Ok(()) => filter_ops.push(op),
            Err(e) => {
                tracing::warn!(
                    ?op,
                    "Dropping batch of {} ops because the current op failed counterfeit checks",
                    num_ops,
                );
                // TODO we are returning here without blocking this author?
                return Err(e);
            }
        }
    }

    if !request_validation_receipt {
        // Filter the list of ops to only include those that are not already in the database.
        filter_ops = filter_existing_ops(&dht_db, filter_ops).await?;
    }

    // Check again whether everything has been filtered out and avoid launching a Tokio task if so
    if filter_ops.is_empty() {
        tracing::trace!(
            "Skipping the rest of the incoming_dht_ops_workflow because all ops were filtered out"
        );
        return Ok(());
    }

    let (mut maybe_batch, rcv) =
        incoming_ops_batch.check_insert(request_validation_receipt, filter_ops);

    let incoming_ops_batch = incoming_ops_batch.clone();
    if maybe_batch.is_some() {
        // there was no already running batch task, so spawn one:
        tokio::task::spawn({
            let dht_db = dht_db.clone();
            async move {
                while let Some(entries) = maybe_batch {
                    let senders = Arc::new(parking_lot::Mutex::new(Vec::new()));
                    let senders2 = senders.clone();
                    if let Err(err) = dht_db
                        .write_async(move |txn| {
                            for entry in entries {
                                let InOpBatchEntry {
                                    snd,
                                    request_validation_receipt,
                                    ops,
                                } = entry;
                                let res = batch_process_entry(txn, request_validation_receipt, ops);

                                // we can't send the results here...
                                // we haven't committed
                                senders2.lock().push((snd, res));
                            }

                            WorkflowResult::Ok(())
                        })
                        .await
                    {
                        tracing::error!(?err, "incoming_dht_ops_workflow error");
                    }

                    for (snd, res) in senders.lock().drain(..) {
                        let _ = snd.send(res);
                    }

                    // trigger validation of queued ops
                    tracing::debug!(
                        "Incoming dht ops workflow is now triggering the sys_validation_trigger"
                    );
                    sys_validation_trigger.trigger(&"incoming_dht_ops_workflow");

                    maybe_batch = incoming_ops_batch.check_end();
                }
            }
        });
    }

    rcv.await
        .map_err(|_| super::error::WorkflowError::RecvError)?
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(op)))]
/// If this op fails the counterfeit check it should be dropped
async fn should_keep(op: &DhtOp) -> WorkflowResult<()> {
    match op {
        DhtOp::ChainOp(op) => {
            let action = op.action();
            let signature = op.signature();
            counterfeit_check_action(signature, &action).await?;
        }
        DhtOp::WarrantOp(op) => counterfeit_check_warrant(op).await?,
    }
    Ok(())
}

fn add_to_pending(
    txn: &mut Txn<DbKindDht>,
    ops: &[DhtOpHashed],
    request_validation_receipt: bool,
) -> StateMutationResult<()> {
    for op in ops {
        insert_op_dht(txn, op, todo_no_cache_transfer_data())?;
        set_require_receipt(txn, op.as_hash(), request_validation_receipt)?;
    }

    Ok(())
}

fn op_exists_inner(txn: &rusqlite::Transaction<'_>, hash: &DhtOpHash) -> DatabaseResult<bool> {
    Ok(txn.query_row(
        "
        SELECT EXISTS(
            SELECT
            1
            FROM DhtOp
            WHERE
            DhtOp.hash = :hash
        )
        ",
        named_params! {
            ":hash": hash,
        },
        |row| row.get(0),
    )?)
}

pub async fn op_exists(vault: &DbWrite<DbKindDht>, hash: DhtOpHash) -> DatabaseResult<bool> {
    vault
        .read_async(move |txn| op_exists_inner(txn, &hash))
        .await
}

pub async fn filter_existing_ops(
    vault: &DbWrite<DbKindDht>,
    mut ops: Vec<DhtOpHashed>,
) -> DatabaseResult<Vec<DhtOpHashed>> {
    vault
        .read_async(move |txn| {
            ops.retain(|op| !op_exists_inner(txn, &op.hash).unwrap_or(true));
            Ok(ops)
        })
        .await
}



================================================
File: crates/holochain/src/core/workflow/initialize_zomes_workflow.rs
================================================
use super::error::WorkflowResult;
use crate::conductor::api::CellConductorApi;
use crate::conductor::api::CellConductorApiT;
use crate::conductor::ConductorHandle;
use crate::core::queue_consumer::TriggerSender;
use crate::core::ribosome::guest_callback::init::InitHostAccess;
use crate::core::ribosome::guest_callback::init::InitInvocation;
use crate::core::ribosome::guest_callback::init::InitResult;
use crate::core::ribosome::guest_callback::post_commit::send_post_commit;
use crate::core::ribosome::RibosomeT;
use derive_more::Constructor;
use holochain_keystore::MetaLairClient;
use holochain_p2p::{HolochainP2pDna, HolochainP2pDnaT};
use holochain_state::host_fn_workspace::SourceChainWorkspace;
use holochain_types::prelude::*;
use holochain_zome_types::action::builder;
use tokio::sync::broadcast;

#[derive(Constructor)]
pub struct InitializeZomesWorkflowArgs<Ribosome>
where
    Ribosome: RibosomeT + 'static,
{
    pub ribosome: Ribosome,
    pub conductor_handle: ConductorHandle,
    pub signal_tx: broadcast::Sender<Signal>,
    pub cell_id: CellId,
    pub integrate_dht_ops_trigger: TriggerSender,
}

impl<Ribosome> InitializeZomesWorkflowArgs<Ribosome>
where
    Ribosome: RibosomeT + 'static,
{
    pub fn dna_def(&self) -> &DnaDef {
        self.ribosome.dna_def().as_content()
    }
}

// #[cfg_attr(feature = "instrument", tracing::instrument(skip(network, keystore, workspace, args)))]
pub async fn initialize_zomes_workflow<Ribosome>(
    workspace: SourceChainWorkspace,
    network: HolochainP2pDna,
    keystore: MetaLairClient,
    args: InitializeZomesWorkflowArgs<Ribosome>,
) -> WorkflowResult<InitResult>
where
    Ribosome: RibosomeT + Clone + 'static,
{
    let conductor_handle = args.conductor_handle.clone();
    let coordinators = args.ribosome.dna_def().get_all_coordinators();
    let integrate_dht_ops_trigger = args.integrate_dht_ops_trigger.clone();
    let signal_tx = args.signal_tx.clone();
    let result =
        initialize_zomes_workflow_inner(workspace.clone(), network.clone(), keystore.clone(), args)
            .await?;

    // --- END OF WORKFLOW, BEGIN FINISHER BOILERPLATE ---

    // only commit if the result was successful
    if result == InitResult::Pass {
        let flushed_actions = workspace
            .source_chain()
            .flush(network.storage_arcs().await?, network.chc())
            .await?;

        send_post_commit(
            conductor_handle,
            workspace,
            network,
            keystore,
            flushed_actions,
            coordinators,
            signal_tx,
        )
        .await?;

        // Any ops that were moved to the dht_db as part of the flush but had dependencies will need to be integrated.
        integrate_dht_ops_trigger.trigger(&"initialize_zomes_workflow");
    }
    Ok(result)
}

async fn initialize_zomes_workflow_inner<Ribosome>(
    workspace: SourceChainWorkspace,
    network: HolochainP2pDna,
    keystore: MetaLairClient,
    args: InitializeZomesWorkflowArgs<Ribosome>,
) -> WorkflowResult<InitResult>
where
    Ribosome: RibosomeT + 'static,
{
    let dna_def = args.dna_def().clone();
    let InitializeZomesWorkflowArgs {
        ribosome,
        conductor_handle,
        signal_tx,
        cell_id,
        ..
    } = args;
    let call_zome_handle =
        CellConductorApi::new(conductor_handle.clone(), cell_id.clone()).into_call_zome_handle();
    let dpki = conductor_handle.running_services().dpki;

    // Call the init callback
    let result = {
        let host_access = InitHostAccess::new(
            workspace.clone().into(),
            keystore,
            dpki,
            network.clone(),
            signal_tx,
            call_zome_handle,
        );
        let invocation = InitInvocation { dna_def };
        ribosome.run_init(host_access, invocation).await?
    };

    // Insert the init marker
    // FIXME: For some reason if we don't spawn here
    // this future never gets polled again.
    let ws = workspace.clone();

    tokio::task::spawn(async move {
        ws.source_chain()
            .put(
                builder::InitZomesComplete {},
                None,
                ChainTopOrdering::Strict,
            )
            .await
    })
    .await??;

    // TODO: Validate scratch items
    super::inline_validation(workspace, network, conductor_handle, ribosome).await?;

    Ok(result)
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use super::*;
    use crate::conductor::Conductor;
    use crate::core::ribosome::guest_callback::validate::ValidateResult;
    use crate::core::ribosome::MockRibosomeT;
    use crate::fixt::DnaDefFixturator;
    use crate::fixt::MetaLairClientFixturator;
    use crate::sweettest::*;
    use crate::test_utils::fake_genesis;
    use ::fixt::prelude::*;
    use holochain_keystore::test_keystore;
    use holochain_p2p::HolochainP2pDnaFixturator;
    use holochain_state::prelude::*;
    use holochain_state::test_utils::test_db_dir;
    use holochain_types::db_cache::DhtDbQueryCache;
    use holochain_types::inline_zome::InlineZomeSet;
    use holochain_wasm_test_utils::TestWasm;
    use matches::assert_matches;

    async fn get_chain(cell: &SweetCell, keystore: MetaLairClient) -> SourceChain {
        SourceChain::new(
            cell.authored_db().clone(),
            cell.dht_db().clone(),
            DhtDbQueryCache::new(cell.dht_db().clone().into()),
            keystore,
            cell.agent_pubkey().clone(),
        )
        .await
        .unwrap()
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn adds_init_marker() {
        let test_db = test_authored_db();
        let test_cache = test_cache_db();
        let test_dht = test_dht_db();
        let keystore = test_keystore();
        let db = test_db.to_db();
        let author = fake_agent_pubkey_1();

        // Genesis
        fake_genesis(db.clone(), test_dht.to_db(), keystore.clone())
            .await
            .unwrap();

        let dna_def = DnaDefFixturator::new(Unpredictable).next().unwrap();
        let dna_def_hashed = DnaDefHashed::from_content_sync(dna_def.clone());

        let workspace = SourceChainWorkspace::new(
            db.clone(),
            test_dht.to_db(),
            DhtDbQueryCache::new(test_dht.to_db().into()),
            test_cache.to_db(),
            keystore,
            author.clone(),
            Arc::new(dna_def),
        )
        .await
        .unwrap();
        let mut ribosome = MockRibosomeT::new();

        // Setup the ribosome mock
        ribosome
            .expect_run_init()
            .returning(move |_workspace, _invocation| Ok(InitResult::Pass));
        ribosome
            .expect_run_validate()
            .returning(move |_, _| Ok(ValidateResult::Valid));
        ribosome
            .expect_dna_def()
            .return_const(dna_def_hashed.clone());

        let db_dir = test_db_dir();
        let conductor_handle = Conductor::builder()
            .config(SweetConductorConfig::standard().no_dpki().into())
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();
        let integrate_dht_ops_trigger = TriggerSender::new();

        let args = InitializeZomesWorkflowArgs {
            ribosome,
            conductor_handle,
            signal_tx: broadcast::channel(1).0,
            cell_id: CellId::new(dna_def_hashed.to_hash(), author.clone()),
            integrate_dht_ops_trigger: integrate_dht_ops_trigger.0.clone(),
        };
        let keystore = fixt!(MetaLairClient);
        let network = fixt!(HolochainP2pDna);

        initialize_zomes_workflow_inner(workspace.clone(), network, keystore, args)
            .await
            .unwrap();

        // Check init is added to the workspace
        let scratch = workspace.source_chain().snapshot().unwrap();
        assert_matches!(
            scratch.actions().next().unwrap().action(),
            Action::InitZomesComplete(_)
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn commit_during_init() {
        let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let mut conductor = SweetConductor::from_standard_config().await;
        let keystore = conductor.keystore();
        let app = conductor.setup_app("app", [&dna]).await.unwrap();
        let (cell,) = app.into_tuple();
        let zome = cell.zome("create_entry");

        assert_eq!(get_chain(&cell, keystore.clone()).await.len().unwrap(), 3);
        assert_eq!(
            get_chain(&cell, keystore.clone())
                .await
                .query(Default::default())
                .await
                .unwrap()
                .len(),
            3
        );

        let _: ActionHash = conductor.call(&zome, "create_entry", ()).await;

        let source_chain = get_chain(&cell, keystore.clone()).await;
        // - Ensure that the InitZomesComplete record got committed after the
        //   record committed during init()
        assert_matches!(
            source_chain.query(Default::default()).await.unwrap()[4].action(),
            Action::InitZomesComplete(_)
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn commit_during_init_one_zome_passes_one_fails() {
        let (dna, _, _) =
            SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create, TestWasm::InitFail]).await;
        let mut conductor = SweetConductor::from_standard_config().await;
        let keystore = conductor.keystore();
        let app = conductor.setup_app("app", [&dna]).await.unwrap();
        let (cell,) = app.into_tuple();
        let zome = cell.zome("create_entry");

        assert_eq!(get_chain(&cell, keystore.clone()).await.len().unwrap(), 3);

        // - Ensure that the chain does not advance due to init failing
        let r: Result<ActionHash, _> = conductor.call_fallible(&zome, "create_entry", ()).await;
        assert!(r.is_err());
        let source_chain = get_chain(&cell, keystore.clone());
        assert_eq!(source_chain.await.len().unwrap(), 3);

        // - Ensure idempotence of the above
        let r: Result<ActionHash, _> = conductor.call_fallible(&zome, "create_entry", ()).await;
        assert!(r.is_err());
        let source_chain = get_chain(&cell, keystore.clone());
        assert_eq!(source_chain.await.len().unwrap(), 3);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn commit_during_init_one_zome_unimplemented_one_fails() {
        let zome_fail = SweetInlineZomes::new(vec![], 0).function("init", |api, _: ()| {
            api.create(CreateInput::new(
                EntryDefLocation::CapGrant,
                EntryVisibility::Private,
                Entry::CapGrant(CapGrantEntry {
                    tag: "".into(),
                    access: ().into(),
                    functions: GrantedFunctions::Listed(
                        vec![("no-init".into(), "xxx".into())].into_iter().collect(),
                    ),
                }),
                ChainTopOrdering::default(),
            ))?;
            Ok(InitCallbackResult::Fail("reason".into()))
        });
        let zomes = InlineZomeSet::from((
            "create_entry",
            crate::conductor::conductor::tests::simple_create_entry_zome(),
        ))
        .merge(zome_fail.0);

        let (dna, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;

        let mut conductor = SweetConductor::from_standard_config().await;
        let keystore = conductor.keystore();
        let app = conductor.setup_app("app", [&dna]).await.unwrap();
        let (cell,) = app.into_tuple();
        let zome = cell.zome("create_entry");

        assert_eq!(get_chain(&cell, keystore.clone()).await.len().unwrap(), 3);

        // - Ensure that the chain does not advance due to init failing
        let r: Result<ActionHash, _> = conductor.call_fallible(&zome, "create_entry", ()).await;
        assert!(r.is_err());
        let source_chain = get_chain(&cell, keystore.clone());
        assert_eq!(source_chain.await.len().unwrap(), 3);

        // - Ensure idempotence of the above
        let r: Result<ActionHash, _> = conductor.call_fallible(&zome, "create_entry", ()).await;
        assert!(r.is_err());
        let source_chain = get_chain(&cell, keystore.clone());
        assert_eq!(source_chain.await.len().unwrap(), 3);
    }
}



================================================
File: crates/holochain/src/core/workflow/integrate_dht_ops_workflow.rs
================================================
//! The workflow and queue consumer for DhtOp integration

use super::*;
use crate::core::queue_consumer::TriggerSender;
use crate::core::queue_consumer::WorkComplete;
use holochain_p2p::HolochainP2pDna;
use holochain_p2p::HolochainP2pDnaT;
use holochain_state::prelude::*;

#[cfg(test)]
mod tests;

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(vault, trigger_receipt, network, dht_query_cache))
)]
pub async fn integrate_dht_ops_workflow(
    vault: DbWrite<DbKindDht>,
    dht_query_cache: DhtDbQueryCache,
    trigger_receipt: TriggerSender,
    network: HolochainP2pDna,
) -> WorkflowResult<WorkComplete> {
    let start = std::time::Instant::now();
    let time = holochain_zome_types::prelude::Timestamp::now();
    // Get any activity from the cache that is ready to be integrated.
    let activity_to_integrate = dht_query_cache.get_activity_to_integrate().await?;
    let (changed, activity_integrated) = vault
        .write_async(move |txn| {
            let mut total = 0;
            if !activity_to_integrate.is_empty() {
                let mut stmt = txn.prepare_cached(
                    holochain_sqlite::sql::sql_cell::UPDATE_INTEGRATE_DEP_ACTIVITY,
                )?;
                for (author, seq_range) in &activity_to_integrate {
                    let start = seq_range.start();
                    let end = seq_range.end();

                    total += stmt.execute(named_params! {
                        ":when_integrated": time,
                        ":register_activity": ChainOpType::RegisterAgentActivity,
                        ":seq_start": start,
                        ":seq_end": end,
                        ":author": author,
                    })?;
                }
            }
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::UPDATE_INTEGRATE_STORE_RECORD)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":store_record": ChainOpType::StoreRecord,
                })?;
            total += changed;
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::UPDATE_INTEGRATE_STORE_ENTRY)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":store_entry": ChainOpType::StoreEntry,
                })?;
            total += changed;
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::UPDATE_INTEGRATE_DEP_STORE_ENTRY)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":updated_content": ChainOpType::RegisterUpdatedContent,
                    ":deleted_entry_action": ChainOpType::RegisterDeletedEntryAction,
                    ":store_entry": ChainOpType::StoreEntry,
                })?;
            total += changed;
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::SET_ADD_LINK_OPS_TO_INTEGRATED)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":create_link": ChainOpType::RegisterAddLink,
                })?;
            total += changed;
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::UPDATE_INTEGRATE_DEP_STORE_RECORD)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":store_record": ChainOpType::StoreRecord,
                    ":updated_record": ChainOpType::RegisterUpdatedRecord,
                    ":deleted_by": ChainOpType::RegisterDeletedBy,
                })?;
            total += changed;
            let changed = txn
                .prepare_cached(holochain_sqlite::sql::sql_cell::SET_DELETE_LINK_OPS_TO_INTEGRATED)?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":create_link": ChainOpType::RegisterAddLink,
                    ":delete_link": ChainOpType::RegisterRemoveLink,

                })?;
            total += changed;
            let changed = txn
                .prepare_cached(
                    holochain_sqlite::sql::sql_cell::SET_CHAIN_INTEGRITY_WARRANT_OPS_TO_INTEGRATED,
                )?
                .execute(named_params! {
                    ":when_integrated": time,
                    ":chain_integrity_warrant": WarrantOpType::ChainIntegrityWarrant,
                })?;
            total += changed;
            WorkflowResult::Ok((total, activity_to_integrate))
        })
        .await?;
    // Once the database transaction is committed, update the cache with the
    // integrated activity.
    dht_query_cache
        .set_all_activity_to_integrated(activity_integrated)
        .await?;
    let ops_ps = changed as f64 / start.elapsed().as_micros() as f64 * 1_000_000.0;
    tracing::debug!(?changed, %ops_ps);
    if changed > 0 {
        trigger_receipt.trigger(&"integrate_dht_ops_workflow");
        network.new_integrated_data().await?;
        Ok(WorkComplete::Incomplete(None))
    } else {
        Ok(WorkComplete::Complete)
    }
}



================================================
File: crates/holochain/src/core/workflow/publish_dht_ops_workflow.rs
================================================
//! # Publish Dht Op Workflow
//!
//! ## Open questions
//! - [x] Publish add and remove links on private entries, what are the constraints on when to publish
//!
//! For now, Publish links on private entries
// TODO: B-01827 Make story about: later consider adding a flag to make a link private and not publish it.
//       Even for those private links, we may need to publish them to the author of the private entry
//       (and we'd have to reference its action  which actually exists on the DHT to make that work,
//       rather than the entry which does not exist on the DHT).
//!
//!

use super::error::WorkflowResult;
use crate::core::queue_consumer::TriggerSender;
use crate::core::queue_consumer::WorkComplete;
use holo_hash::*;
use holochain_p2p::HolochainP2pDnaT;
use holochain_state::prelude::*;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::OpHashSized;
use std::collections::HashMap;
use std::sync::Arc;
use std::time;
use std::time::Duration;
use tracing::*;

mod publish_query;
pub use publish_query::{get_ops_to_publish, num_still_needing_publish};

#[cfg(test)]
mod unit_tests;

/// Default redundancy factor for validation receipts
pub const DEFAULT_RECEIPT_BUNDLE_SIZE: u8 = 5;

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(db, network, trigger_self, min_publish_interval))
)]
pub async fn publish_dht_ops_workflow(
    db: DbWrite<DbKindAuthored>,
    network: Arc<impl HolochainP2pDnaT>,
    trigger_self: TriggerSender,
    agent: AgentPubKey,
    min_publish_interval: Duration,
) -> WorkflowResult<WorkComplete> {
    let mut complete = WorkComplete::Complete;
    let to_publish =
        publish_dht_ops_workflow_inner(db.clone().into(), agent.clone(), min_publish_interval)
            .await?;
    let to_publish_count: usize = to_publish.values().map(Vec::len).sum();

    if to_publish_count > 0 {
        info!("publishing {} ops", to_publish_count);
    }

    // Commit to the network
    let mut success = Vec::with_capacity(to_publish.len());
    for (basis, list) in to_publish {
        let (op_hash_list, op_data_list): (Vec<_>, Vec<_>) = list.into_iter().unzip();
        match network
            .publish(
                true,
                false,
                basis,
                agent.clone(),
                op_hash_list.clone(),
                None,
                Some(op_data_list),
            )
            .await
        {
            Err(e) => {
                // If we get a routing error it means the space hasn't started yet and we should try publishing again.
                if let holochain_p2p::HolochainP2pError::RoutingDnaError(_) = e {
                    // TODO if this doesn't change what is the loop terminate condition?
                    complete = WorkComplete::Incomplete(None);
                }
                warn!(failed_to_send_publish = ?e);
            }
            Ok(()) => {
                success.extend(op_hash_list);
            }
        }
    }

    if to_publish_count > 0 {
        info!("published {}/{} ops", success.len(), to_publish_count);
    }

    let now = time::SystemTime::now().duration_since(time::UNIX_EPOCH)?;
    let continue_publish = db
        .write_async(move |txn| {
            for hash in success {
                use holochain_p2p::DhtOpHashExt;
                let hash = DhtOpHash::from_kitsune(hash.data_ref());
                set_last_publish_time(txn, &hash, now)?;
            }
            WorkflowResult::Ok(publish_query::num_still_needing_publish(txn, agent)? > 0)
        })
        .await?;

    // If we have more ops that could be published then continue looping.
    if continue_publish {
        trigger_self.resume_loop();
    } else {
        trigger_self.pause_loop();
    }

    debug!("committed published ops");

    // --- END OF WORKFLOW, BEGIN FINISHER BOILERPLATE ---

    Ok(complete)
}

/// Read the authored for ops with receipt count < R
pub async fn publish_dht_ops_workflow_inner(
    db: DbRead<DbKindAuthored>,
    agent: AgentPubKey,
    min_publish_interval: Duration,
) -> WorkflowResult<HashMap<OpBasis, Vec<(OpHashSized, crate::prelude::DhtOp)>>> {
    // Ops to publish by basis
    let mut to_publish = HashMap::new();

    for (basis, op_hash, op) in get_ops_to_publish(agent, &db, min_publish_interval).await? {
        // For every op publish a request
        // Collect and sort ops by basis
        to_publish
            .entry(basis)
            .or_insert_with(Vec::new)
            .push((op_hash, op));
    }

    Ok(to_publish)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::fixt::CreateLinkFixturator;
    use crate::fixt::EntryFixturator;
    use crate::test_utils::fake_genesis;
    use crate::test_utils::test_network_with_events;
    use crate::test_utils::TestNetwork;
    use ::fixt::prelude::*;
    use futures::future::FutureExt;
    use holo_hash::fixt::AgentPubKeyFixturator;
    use holo_hash::fixt::DnaHashFixturator;
    use holochain_conductor_api::conductor::ConductorTuningParams;
    use holochain_p2p::actor::HolochainP2pSender;
    use holochain_p2p::HolochainP2pDna;
    use holochain_p2p::HolochainP2pRef;
    use holochain_state::mutations;
    use holochain_trace;
    use holochain_types::db_cache::DhtDbQueryCache;
    use std::collections::HashMap;
    use std::ops::Deref;
    use std::sync::atomic::AtomicU32;
    use std::sync::atomic::Ordering;
    use std::sync::Arc;
    use std::time::Duration;
    use test_case::test_case;
    use tokio::task::JoinHandle;
    use tracing_futures::Instrument;

    const RECV_TIMEOUT: Duration = Duration::from_millis(3000);

    /// publish ops setup
    async fn setup(
        db: DbWrite<DbKindAuthored>,
        num_agents: u32,
        num_hash: u32,
        panic_on_publish: bool,
    ) -> (
        TestNetwork,
        HolochainP2pDna,
        AgentPubKey,
        JoinHandle<()>,
        tokio::sync::oneshot::Receiver<()>,
    ) {
        // Create data fixts for op
        let mut sig_fixt = SignatureFixturator::new(Unpredictable);
        let mut link_add_fixt = CreateLinkFixturator::new(Unpredictable);
        let author = fake_agent_pubkey_1();

        db.write_async({
            let query_author = author.clone();

            move |txn| -> StateMutationResult<()> {
                for _ in 0..num_hash {
                    // Create data for op
                    let sig = sig_fixt.next().unwrap();
                    let mut link_add = link_add_fixt.next().unwrap();
                    link_add.author = query_author.clone();
                    // Create DhtOp
                    let op = ChainOp::RegisterAddLink(sig.clone(), link_add.clone());
                    // Get the hash from the op
                    let op_hashed = DhtOpHashed::from_content_sync(op.clone());
                    mutations::insert_op_authored(txn, &op_hashed)?;
                }
                Ok(())
            }
        })
        .await
        .unwrap();

        // Create cell data
        let dna = fixt!(DnaHash);
        let agents = AgentPubKeyFixturator::new(Unpredictable)
            .take(num_agents as usize)
            .collect::<Vec<_>>();

        // Create the network
        let filter_events =
            |evt: &_| matches!(evt, holochain_p2p::event::HolochainP2pEvent::Publish { .. });
        let (tx, mut recv) = tokio::sync::mpsc::channel(10);
        let test_network =
            test_network_with_events(Some(dna.clone()), Some(author.clone()), filter_events, tx)
                .await;
        let (tx_complete, rx_complete) = tokio::sync::oneshot::channel();
        let dna_network = test_network.dna_network();
        let network = test_network.network();
        let mut recv_count: u32 = 0;
        let total_expected = num_agents * num_hash;

        // Receive events and increment count
        let recv_task = tokio::task::spawn({
            async move {
                let mut tx_complete = Some(tx_complete);
                while let Some(evt) = recv.recv().await {
                    use holochain_p2p::event::HolochainP2pEvent::*;
                    if let Publish { respond, .. } = evt {
                        respond.respond(Ok(async move { Ok(()) }.boxed().into()));
                        if panic_on_publish {
                            panic!("Published, when expecting not to")
                        }
                        recv_count += 1;
                        if recv_count == total_expected {
                            // notify the test that all items have been received
                            tx_complete.take().unwrap().send(()).unwrap();
                            break;
                        }
                    }
                }
            }
        });

        // Join some agents onto the network
        // Skip the first agent as it has already joined
        for agent in agents.into_iter().skip(1) {
            HolochainP2pRef::join(&network, dna.clone(), agent, None, None)
                .await
                .unwrap();
        }

        (test_network, dna_network, author, recv_task, rx_complete)
    }

    /// Call the workflow
    async fn call_workflow(
        db: DbWrite<DbKindAuthored>,
        dna_network: HolochainP2pDna,
        author: AgentPubKey,
    ) {
        let (trigger_sender, _) = TriggerSender::new();
        publish_dht_ops_workflow(
            db.clone(),
            Arc::new(dna_network),
            trigger_sender,
            author,
            ConductorTuningParams::default().min_publish_interval(),
        )
        .await
        .unwrap();
    }

    /// There is a test that shows that network messages would be sent to all agents via broadcast.
    #[test_case(1, 1)]
    #[test_case(1, 10)]
    #[test_case(1, 100)]
    #[test_case(10, 1)]
    #[test_case(10, 10)]
    #[test_case(10, 100)]
    #[test_case(100, 1)]
    #[test_case(100, 10)]
    #[test_case(100, 100)]
    #[ignore = "(david.b) tests should be re-written using mock network"]
    fn test_sent_to_r_nodes(num_agents: u32, num_hash: u32) {
        tokio_helper::block_forever_on(async {
            holochain_trace::test_run();

            // Create test db
            let test_db = test_authored_db();
            let db = test_db.to_db();

            // Setup
            let (_network, dna_network, author, recv_task, rx_complete) =
                setup(db.clone(), num_agents, num_hash, false).await;

            call_workflow(db.clone(), dna_network, author).await;

            // Wait for expected # of responses, or timeout
            tokio::select! {
                _ = rx_complete => {}
                _ = tokio::time::sleep(RECV_TIMEOUT) => {
                    panic!("Timed out while waiting for expected responses.")
                }
            };

            let check = async move {
                recv_task.await.unwrap();
                db.read_async(move |txn| -> DatabaseResult<()> {
                    let unpublished_ops: bool = txn.query_row(
                        "SELECT EXISTS(SELECT 1 FROM DhtOp WHERE last_publish_time IS NULL)",
                        [],
                        |row| row.get(0),
                    )?;
                    assert!(!unpublished_ops);

                    Ok(())
                })
                .await
                .unwrap()
            };

            // Shutdown
            tokio::time::timeout(Duration::from_secs(10), check)
                .await
                .ok();
        });
    }

    /// There is a test to shows that DHTOps that were produced on private entries are not published.
    /// Some do get published
    /// Current private constraints:
    /// - No private Entry is ever published in any op
    /// - No StoreEntry
    /// - This workflow does not have access to private entries
    /// - Add / Remove links: Currently publish all.
    /// ## Explanation
    /// This test is a little big so a quick run down:
    /// 1. All ops that can contain entries are created with entries (StoreRecord, StoreEntry and RegisterUpdatedContent)
    /// 2. Then we create identical versions of these ops without the entries (set to None) (except StoreEntry)
    /// 3. The workflow is run and the ops are sent to the network receiver
    /// 4. We check that the correct number of ops are received (so we know there were no other ops sent)
    /// 5. StoreEntry is __not__ expected so would show up as an extra if it was produced
    /// 6. Every op that is received (StoreRecord and RegisterUpdatedContent) is checked to match the expected versions (entries removed)
    /// 7. Each op also has a count to check for duplicates
    #[test_case(1)]
    #[test_case(10)]
    #[test_case(100)]
    #[ignore = "(david.b) tests should be re-written using mock network"]
    fn test_private_entries(num_agents: u32) {
        tokio_helper::block_forever_on(
            async {
                holochain_trace::test_run();

                // Create test db
                let test_db = test_authored_db();
                let keystore = holochain_keystore::test_keystore();
                let dht_db = test_dht_db();
                let db = test_db.to_db();

                let dna = fixt!(DnaHash);
                let filter_events = |evt: &_| matches!(evt, holochain_p2p::event::HolochainP2pEvent::Publish { .. });
                let (tx, mut recv) = tokio::sync::mpsc::channel(10);
                let author = fake_agent_pubkey_1();
                let test_network = test_network_with_events(
                    Some(dna.clone()),
                    Some(author.clone()),
                    filter_events,
                    tx,
                )
                .await;
                let dna_network = test_network.dna_network();

                // Setup data
                let original_entry = fixt!(Entry);
                let new_entry = fixt!(Entry);
                let original_entry_hash = EntryHash::with_data_sync(&original_entry);
                let new_entry_hash = EntryHash::with_data_sync(&new_entry);

                // Make them private
                let visibility = EntryVisibility::Private;
                let mut entry_type_fixt =
                    AppEntryDefFixturator::new(visibility).map(EntryType::App);
                let ec_entry_type = entry_type_fixt.next().unwrap();
                let eu_entry_type = entry_type_fixt.next().unwrap();

                // Genesis and produce ops to clear these from the chains
                fake_genesis(db.clone(), dht_db.to_db(), keystore.clone())
                    .await
                    .unwrap();
                db.write_async(move |txn| -> DatabaseResult<usize> {
                    Ok(txn.execute("UPDATE DhtOp SET receipts_complete = 1", [])?)
                }).await.unwrap();
                let author = fake_agent_pubkey_1();

                // Put data in records
                let source_chain = SourceChain::new(
                    db.clone(),
                    dht_db.to_db(),
                    DhtDbQueryCache::new(dht_db.clone().into()),
                    keystore.clone(),
                    author.clone(),
                )
                .await
                .unwrap();
                // Produces 3 ops but minus 1 for store entry so 2 ops.
                let original_action_address = source_chain
                    .put_weightless(
                        builder::Create {
                            entry_type: ec_entry_type,
                            entry_hash: original_entry_hash.clone(),
                        },
                        Some(original_entry),
                        ChainTopOrdering::default(),
                    )
                    .await
                    .unwrap();

                // Produces 5 ops but minus 1 for store entry so 4 ops.
                let entry_update_hash = source_chain
                    .put_weightless(
                        builder::Update {
                            entry_type: eu_entry_type,
                            entry_hash: new_entry_hash,
                            original_action_address: original_action_address.clone(),
                            original_entry_address: original_entry_hash,
                        },
                        Some(new_entry),
                        ChainTopOrdering::default(),
                    )
                    .await
                    .unwrap();

                source_chain.flush(dna_network.storage_arcs().await.unwrap(), dna_network.chc()).await.unwrap();
                let (entry_create_action, entry_update_action) = db.write_async(move |writer| -> StateQueryResult<(SignedActionHashed, SignedActionHashed)> {
                        let store = CascadeTxnWrapper::from(writer.deref());
                        let ech = store.get_action(&original_action_address).unwrap().unwrap();
                        let euh = store.get_action(&entry_update_hash).unwrap().unwrap();
                        Ok((ech, euh))
                    })
                    .await
                    .unwrap();

                // Gather the expected op hashes, ops and basis
                // We are only expecting Store Record and Register Replaced By ops and nothing else
                let store_record_count = Arc::new(AtomicU32::new(0));
                let register_replaced_by_count = Arc::new(AtomicU32::new(0));
                let register_updated_record_count = Arc::new(AtomicU32::new(0));
                let register_agent_activity_count = Arc::new(AtomicU32::new(0));

                let expected = {
                    let mut map = HashMap::new();
                    // Op is expected to not contain the Entry even though the above contains the entry
                    let (entry_create_action, sig) = entry_create_action.into_inner();
                    let expected_op = ChainOp::RegisterAgentActivity(
                        sig.clone(),
                        entry_create_action.clone().into_content(),
                    );
                    let op_hash = expected_op.to_hash();
                    map.insert(
                        op_hash,
                        (expected_op, register_agent_activity_count.clone()),
                    );

                    let expected_op = ChainOp::StoreRecord(
                        sig,
                        entry_create_action.into_content(),
                        RecordEntry::NA,
                    );
                    let op_hash = expected_op.to_hash();

                    map.insert(op_hash, (expected_op, store_record_count.clone()));

                    // Create RegisterUpdatedContent
                    // Op is expected to not contain the Entry
                    let (entry_update_action, sig) = entry_update_action.into_inner();
                    let entry_update_action: Update =
                        entry_update_action.into_content().try_into().unwrap();
                    let expected_op = ChainOp::StoreRecord(
                        sig.clone(),
                        entry_update_action.clone().into(),
                        RecordEntry::NA,
                    );
                    let op_hash = expected_op.to_hash();

                    map.insert(op_hash, (expected_op, store_record_count.clone()));

                    let expected_op = ChainOp::RegisterUpdatedContent(
                        sig.clone(),
                        entry_update_action.clone(),
                        RecordEntry::NA,
                    );
                    let op_hash = expected_op.to_hash();

                    map.insert(op_hash, (expected_op, register_replaced_by_count.clone()));
                    let expected_op = ChainOp::RegisterUpdatedRecord(
                        sig.clone(),
                        entry_update_action.clone(),
                        RecordEntry::NA,
                    );
                    let op_hash = expected_op.to_hash();

                    map.insert(
                        op_hash,
                        (expected_op, register_updated_record_count.clone()),
                    );
                    let expected_op = ChainOp::RegisterAgentActivity(sig, entry_update_action.into());
                    let op_hash = expected_op.to_hash();
                    map.insert(
                        op_hash,
                        (expected_op, register_agent_activity_count.clone()),
                    );

                    map
                };

                // Create cell data
                let agents = AgentPubKeyFixturator::new(Unpredictable)
                    .take(num_agents as usize)
                    .collect::<Vec<_>>();

                // Create the network

                let (tx_complete, rx_complete) = tokio::sync::oneshot::channel();
                // We are expecting six ops per agent plus one for self.
                // The 7 genesis ops were already recently published, so
                // won't be published again this time.
                let total_expected = (num_agents + 1) * 6;
                let mut recv_count: u32 = 0;

                // Receive events and increment count
                tokio::task::spawn({
                    async move {
                        let mut tx_complete = Some(tx_complete);
                        while let Some(evt) = recv.recv().await {
                            use holochain_p2p::event::HolochainP2pEvent::*;
                            if let Publish { respond, ops, .. } = evt {
                                tracing::debug!(?ops);

                                // Check the ops are correct
                                for op in ops {
                                    let op_hash = DhtOpHash::with_data_sync(&op);
                                    match expected.get(&op_hash) {
                                        Some((expected_op, count)) => {
                                            assert_eq!(op, DhtOp::from(expected_op.clone()));
                                            count.fetch_add(1, Ordering::SeqCst);
                                        }
                                        None => {
                                            if let ChainOp::StoreEntry(_, h, _) = op.as_chain_op().expect("warrants not handled here") {
                                                if *h.visibility() == EntryVisibility::Private {
                                                    panic!(
                                                        "A private op has been published: {:?}",
                                                        h
                                                    )
                                                }
                                            }
                                        }
                                    }
                                    recv_count += 1;
                                }
                                respond.respond(Ok(async move { Ok(()) }.boxed().into()));
                                if recv_count == total_expected {
                                    tx_complete.take().unwrap().send(()).unwrap();
                                }
                            }
                        }
                    }
                    .instrument(debug_span!("private_entries_inner"))
                });

                // Join some agents onto the network
                {
                    let network = test_network.network();
                    for agent in agents {
                        HolochainP2pRef::join(&network, dna.clone(), agent, None, None)
                            .await
                            .unwrap()
                    }
                }

                call_workflow(db.clone(), dna_network, author).await;

                // Wait for expected # of responses, or timeout
                tokio::select! {
                    _ = rx_complete => {}
                    _ = tokio::time::sleep(RECV_TIMEOUT) => {
                        panic!("Timed out while waiting for expected responses.")
                    }
                };

                // We publish to ourself in a full sync network so we need
                // to expect one more op.
                let num_agents = num_agents + 1;
                // Check there is no ops left that didn't come through
                assert_eq!(
                    num_agents,
                    register_replaced_by_count.load(Ordering::SeqCst)
                );
                assert_eq!(
                    num_agents,
                    register_updated_record_count.load(Ordering::SeqCst)
                );
                assert_eq!(num_agents * 2, store_record_count.load(Ordering::SeqCst));
                assert_eq!(
                    num_agents * 2,
                    register_agent_activity_count.load(Ordering::SeqCst)
                );
            }
            .instrument(debug_span!("private_entries")),
        );
    }
}



================================================
File: crates/holochain/src/core/workflow/sys_validation_workflow.rs
================================================
//! ### The sys validation workflow
//!
//! This workflow runs against all [`DhtOp`]s that are in the DHT database, either coming from the authored database or from other nodes on the network via gossip and publishing.
//!
//! The purpose of the workflow is to make fundamental checks on the integrity of the data being put into the DHT. This ensures that invalid data is not served
//! to other nodes on the network. It also saves hApp developers from having to write these checks themselves since they set the minimum standards that all data
//! should meet regardless of the requirements of a given hApp.
//!
//! #### Validation checks
//!
//! The workflow operates on [`DhtOp`]s which are roughly equivalent to [`Record`]s but catered to the needs of a specific type of Authority.
//! Checks that you can rely on sys validation having performed are:
//! - For a [`ChainOp::StoreRecord`]
//!    - Check that the [`Action`] is either a [`Action::Dna`] at sequence number 0, or has a previous action with sequence number strictly greater than 0.
//!    - If the [`Entry`] is an [`Entry::CounterSign`], then the countersigning session data is mapped to a set of [`Action`]s and each of those actions must be found locally before this op can progress.
//!    - The [`Action`] must be either a [`Action::Create`] or an [`Action::Update`].
//!    - Run the [store entry checks](#store-entry-checks).
//! - For a [`ChainOp::StoreEntry`]
//!    - If the [`Entry`] is an [`Entry::CounterSign`], then the countersigning session data is mapped to a set of [`Action`]s and each of those actions must be found locally before this op is accepted.
//!    - Check that the [`Action`] is either a [`Action::Dna`] at sequence number 0, or has a previous action with sequence number strictly greater than 0.
//!    - Run the [store entry checks](#store-entry-checks).
//! - For a [`ChainOp::RegisterAgentActivity`]
//!    - Check that the [`Action`] is either a [`Action::Dna`] at sequence number 0, or has a previous action with sequence number strictly greater than 0.
//!    - If the [`Action`] is a [`Action::Dna`], then verify the contained DNA hash matches the DNA hash that sys validation is being run for.
//!    - Check that the previous action is never a [`Action::CloseChain`], since this is always required to be the last action in a chain.
//!    - Run the [store record checks](#store-record-checks).
//! - For a [`ChainOp::RegisterUpdatedContent`]
//!    - The [`Update::original_action_address`] reference to the [`Action`] being updated must point to an [`Action`] that can be found locally. Once the [`Action`] address has been resolved, the [`Update::original_entry_address`] is checked against the entry address that the referenced [`Action`] specified.
//!    - If there is an [`Entry`], then the [store entry checks](#store-entry-checks) are run.
//! - For a [`ChainOp::RegisterUpdatedRecord`]
//!    - The [`Update::original_action_address`] reference to the [`Action`] being updated must point to an [`Action`] that can be found locally. Once the [`Action`] address has been resolved, the [`Update::original_entry_address`] is checked against the entry address that the referenced [`Action`] specified.
//!    - If there is an [`Entry`], then the [store entry checks](#store-entry-checks) are run.
//! - For a [`ChainOp::RegisterDeletedBy`]
//!    - The [`Delete::deletes_address`] reference to the [`Action`] being deleted must point to an [`Action`] that can be found locally. The action being deleted must be a [`Action::Create`] or [`Action::Update`].
//! - For a [`ChainOp::RegisterDeletedEntryAction`]
//!    - The [`Delete::deletes_address`] reference to the [`Action`] being deleted must point to an [`Action`] that can be found locally. The action being deleted must be a [`Action::Create`] or [`Action::Update`].
//! - For a [`ChainOp::RegisterAddLink`]
//!   - The size of the [`CreateLink::tag`] must be less than or equal to the maximum size that is accepted for this link tag. This is specified in the constant [`MAX_TAG_SIZE`].
//! - For a [`ChainOp::RegisterRemoveLink`]
//!   - The [`DeleteLink::link_add_address`] reference to the [`Action`] of the link being deleted must point to an [`Action`] that can be found locally. That action being deleted must also
//!     be a [`Action::CreateLink`].
//!
//! ##### Store record checks
//!
//! These checks are run when storing a new action for a [`DhtOp`].
//!
//! - Check that the [`Action`] is either a [`Action::Dna`] at sequence number 0, or has a previous action with sequence number strictly greater than 0.
//! - Checks that the author of the current action is the same as the author of the previous action.
//! - Checks that the timestamp of the current action is greater than the timestamp of the previous action.
//! - Checks that the sequence number of the current action is exactly 1 more than the sequence number of the previous action.
//! - Checks that every [`Action::Create`] or [`Action::Update`] of an `AgentPubKey` is preceded by an [`Action::AgentValidationPkg`].
//!
//! ##### Store entry checks
//!
//! These checks are run when storing an entry that is included as part of a [`DhtOp`].
//!
//! - The entry type specified in the [`Action`] must match the entry type specified in the [`Entry`].
//! - The entry hash specified in the [`Action`] must match the entry hash specified in the [`Entry`], which will be hashed as part of the check to obtain a value that is deterministic.
//! - The size of the [`Entry`] must be less than or equal to the maximum size that is accepted for this entry type. This is specified in the constant [`MAX_ENTRY_SIZE`].
//! - If the [`Action`] is an [`Action::Update`], then the [`Update::original_action_address`] reference to the [`Action`] being updated must point to an [`Action`] that can be found locally. Once the [`Action`] address has been resolved, the [`Update::original_entry_address`] is checked against the entry address that the referenced [`Action`] specified.
//! - If the [`Entry`] is an [`Entry::CounterSign`], then the pre-flight response signatures are checked.
//!
//! #### Workflow description
//!
//! - The workflow starts by fetching all the ops that need to be validated from the database. The ops are processed as follows:
//!     - Ops are sorted by [`OpOrder`], to make it more likely that incoming ops will be processed in the order they were created.
//!     - The dependencies of these ops are then concurrently fetched from any of the local databases. Missing dependencies are handled later.
//!     - The [validation checks](#validation-checks) are run for each op.
//!     - For any ops that passed validation, they will be marked as ready for app validation in the database.
//!     - Any ops which were rejected will be marked rejected in the database.
//! - If any ops passed validation, then app validation will be triggered.
//! - For actions that were not found locally, the workflow will then attempt to fetch them from the network.
//! - If any actions that were missing are found on the network, then sys validation is re-triggered to see if the newly fetched actions allow any outstanding ops to pass validation.
//! - If fewer actions were fetched from the network than there were actions missing, then the workflow will sleep for a short time before re-triggering itself.
//! - Once all ops have an outcome, the workflow is complete and will wait to be triggered again by new incoming ops.
//!

use crate::conductor::Conductor;
use crate::core::queue_consumer::TriggerSender;
use crate::core::queue_consumer::WorkComplete;
use crate::core::sys_validate::*;
use crate::core::validation::*;
use crate::core::workflow::error::WorkflowResult;
use futures::FutureExt;
use futures::StreamExt;
use holo_hash::DhtOpHash;
use holochain_cascade::Cascade;
use holochain_cascade::CascadeImpl;
use holochain_conductor_services::DpkiImpl;
use holochain_keystore::MetaLairClient;
use holochain_p2p::GenericNetwork;
use holochain_p2p::HolochainP2pDnaT;
use holochain_sqlite::prelude::*;
use holochain_sqlite::sql::sql_cell::ACTION_HASH_BY_PREV;
use holochain_state::prelude::*;
use rusqlite::Transaction;
use std::convert::TryInto;
use std::sync::Arc;
use std::time::Duration;
use tracing::*;
use types::Outcome;

use self::validation_deps::SysValDeps;
use self::validation_deps::ValidationDependencies;
use self::validation_deps::ValidationDependencyState;

pub mod types;

pub mod validation_deps;
pub mod validation_query;

#[cfg(test)]
mod chain_test;
#[cfg(test)]
mod tests;
#[cfg(test)]
mod unit_tests;
#[cfg(test)]
mod validate_op_tests;

/// The sys validation workflow. It is described in the module level documentation.
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
#[allow(clippy::too_many_arguments)]
pub async fn sys_validation_workflow<Network: HolochainP2pDnaT + 'static>(
    workspace: Arc<SysValidationWorkspace>,
    current_validation_dependencies: SysValDeps,
    trigger_app_validation: TriggerSender,
    trigger_publish: TriggerSender,
    trigger_self: TriggerSender,
    network: Network,
    keystore: MetaLairClient,
    representative_agent: AgentPubKey,
) -> WorkflowResult<WorkComplete> {
    // Run the actual sys validation using data we have locally
    let outcome_summary = sys_validation_workflow_inner(
        workspace.clone(),
        current_validation_dependencies.clone(),
        &network,
        keystore,
        representative_agent,
    )
    .await?;

    // trigger app validation to process any ops that have been processed so far
    if outcome_summary.accepted > 0 {
        tracing::debug!("Sys validation accepted {} ops", outcome_summary.accepted);

        trigger_app_validation.trigger(&"sys_validation_workflow");
    }

    if outcome_summary.warranted > 0 {
        tracing::debug!(
            "Sys validation created {} warrants",
            outcome_summary.warranted
        );

        trigger_publish.trigger(&"sys_validation_workflow");
    }

    // Now go to the network to try to fetch missing dependencies
    let network_cascade = Arc::new(workspace.network_and_cache_cascade(Arc::new(network)));
    let missing_action_hashes = current_validation_dependencies
        .same_dht
        .lock()
        .get_missing_hashes();
    let num_fetched: usize = futures::stream::iter(missing_action_hashes.into_iter().map(|hash| {
        let network_cascade = network_cascade.clone();
        let current_validation_dependencies = current_validation_dependencies.clone();
        async move {
            match network_cascade
                .retrieve_action(hash, Default::default())
                .await
            {
                Ok(Some((action, source))) => {
                    let mut deps = current_validation_dependencies.same_dht.lock();

                    // If the source was local then that means some other fetch has put this action into the cache,
                    // that's fine we'll just grab it here.
                    if deps.insert(action, source) {
                        1
                    } else {
                        0
                    }
                }
                Ok(None) => {
                    // This is fine, we didn't find it on the network, so we'll have to try again.
                    // TODO This will hit the network again fairly quickly if sys validation is triggered again soon.
                    //      It would be more efficient to wait a bit before trying again.
                    0
                }
                Err(e) => {
                    tracing::error!(error = ?e, "Error fetching missing dependency");
                    0
                }
            }
        }
        .boxed()
    }))
    .buffer_unordered(10)
    .collect::<Vec<usize>>()
    .await
    .into_iter()
    .sum();

    if outcome_summary.missing > 0 {
        tracing::debug!(
            "Fetched {}/{} missing dependencies from the network",
            num_fetched,
            outcome_summary.missing
        );
    }

    if num_fetched > 0 {
        // If we fetched anything then we can re-run sys validation
        trigger_self.trigger(&"sys_validation_workflow");
    }

    if num_fetched < outcome_summary.missing {
        tracing::info!(
            "Sys validation sleeping for {:?}",
            workspace.sys_validation_retry_delay
        );
        Ok(WorkComplete::Incomplete(Some(
            workspace.sys_validation_retry_delay,
        )))
    } else {
        Ok(WorkComplete::Complete)
    }
}

type ForkedPair = ((ActionHash, Signature), (ActionHash, Signature));

async fn sys_validation_workflow_inner(
    workspace: Arc<SysValidationWorkspace>,
    current_validation_dependencies: SysValDeps,
    _network: &impl HolochainP2pDnaT,
    _keystore: MetaLairClient,
    _representative_agent: AgentPubKey,
) -> WorkflowResult<OutcomeSummary> {
    let db = workspace.dht_db.clone();
    let sorted_ops = validation_query::get_ops_to_sys_validate(&db).await?;

    // Forget what dependencies are currently in use
    current_validation_dependencies
        .same_dht
        .lock()
        .clear_retained_deps();

    if sorted_ops.is_empty() {
        tracing::trace!(
            "Skipping sys_validation_workflow because there are no ops to be validated"
        );

        // If there's nothing to validate then we can clear the dependencies and save some memory.
        current_validation_dependencies
            .same_dht
            .lock()
            .purge_held_deps();

        return Ok(OutcomeSummary::new());
    }

    let num_ops_to_validate = sorted_ops.len();
    tracing::debug!("Sys validating {} ops", num_ops_to_validate);

    let cascade = Arc::new(workspace.local_cascade());
    let dna_def = DnaDefHashed::from_content_sync((*workspace.dna_def()).clone());

    retrieve_previous_actions_for_ops(
        current_validation_dependencies.clone(),
        cascade.clone(),
        sorted_ops.clone().into_iter(),
    )
    .await;

    // Now drop all the dependencies that we didn't just try to access while searching the current set of ops to validate.
    current_validation_dependencies
        .same_dht
        .lock()
        .purge_held_deps();

    let mut validation_outcomes = Vec::with_capacity(sorted_ops.len());
    for hashed_op in sorted_ops {
        let dpki = workspace
            .dpki
            .clone()
            .filter(|dpki| !dpki.is_deepkey_dna(workspace.dna_def_hashed().as_hash()));

        // Note that this is async only because of the signature checks done during countersigning.
        // In most cases this will be a fast synchronous call.
        let r = validate_op(
            hashed_op.as_content(),
            &dna_def,
            current_validation_dependencies.clone(),
            dpki,
        )
        .await;

        match r {
            Ok(outcome) => validation_outcomes.push((hashed_op, outcome)),
            Err(e) => {
                tracing::error!(error = ?e, "Error validating op");
            }
        }
    }

    // Allow unused mutable, because it's mutated when feature unstable-warrants is enabled.
    #[allow(unused_mut)]
    let (mut summary, _invalid_ops, _forked_pairs) = workspace
        .dht_db
        .write_async(move |txn| {
            let mut summary = OutcomeSummary::default();
            let mut invalid_ops = vec![];
            let mut forked_pairs: Vec<(AgentPubKey, ForkedPair)> = vec![];

            for (hashed_op, outcome) in validation_outcomes {
                let (op, op_hash) = hashed_op.into_inner();
                let op_type = op.get_type();

                #[cfg(feature = "unstable-warrants")]
                if let DhtOp::ChainOp(chain_op) = &op {
                    // Author a ChainFork warrant if fork is detected
                    let action = chain_op.action();
                    if let Some(forked_action) = detect_fork(txn, &action)? {
                        let signature = chain_op.signature().clone();
                        let action_hash = action.to_hash();
                        forked_pairs.push((
                            action.author().clone(),
                            ((action_hash, signature), forked_action),
                        ));
                    }
                }

                // This is an optimization to skip app validation and integration for ops that are
                // rejected and don't have dependencies.
                let deps = op.sys_validation_dependencies();

                match outcome {
                    Outcome::Accepted => {
                        summary.accepted += 1;
                        match op_type {
                            DhtOpType::Chain(_) => {
                                put_validation_limbo(txn, &op_hash, ValidationStage::SysValidated)?
                            }
                            DhtOpType::Warrant(_) => {
                                // XXX: integrate accepted warrants immediately, because we don't
                                //      want them to go to app validation.
                                #[cfg(feature = "unstable-warrants")]
                                put_integrated(txn, &op_hash, ValidationStatus::Valid)?
                            }
                        };
                    }
                    Outcome::MissingDhtDep => {
                        summary.missing += 1;
                        let status = ValidationStage::AwaitingSysDeps;
                        put_validation_limbo(txn, &op_hash, status)?;
                    }
                    Outcome::Rejected(_) => {
                        invalid_ops.push((op_hash.clone(), op.clone()));

                        summary.rejected += 1;
                        if deps.is_empty() {
                            put_integrated(txn, &op_hash, ValidationStatus::Rejected)?;
                        } else {
                            put_integration_limbo(txn, &op_hash, ValidationStatus::Rejected)?;
                        }
                    }
                }
            }
            WorkflowResult::Ok((summary, invalid_ops, forked_pairs))
        })
        .await?;

    #[cfg(feature = "unstable-warrants")]
    {
        let mut warrants = vec![];
        for (_, op) in _invalid_ops {
            if let Some(chain_op) = op.as_chain_op() {
                let warrant_op = crate::core::workflow::sys_validation_workflow::make_invalid_chain_warrant_op_inner(
                &_keystore,
                _representative_agent.clone(),
                chain_op,
                ValidationType::Sys,
            )
            .await?;
                warrants.push(warrant_op);
            }
        }

        for (author, pair) in _forked_pairs {
            let warrant_op =
                make_fork_warrant_op_inner(&_keystore, _representative_agent.clone(), author, pair)
                    .await?;
            warrants.push(warrant_op);
        }

        let warrant_op_hashes = warrants
            .iter()
            .map(|w| (w.as_hash().clone(), w.dht_basis()))
            .collect::<Vec<_>>();

        workspace
            .authored_db
            .write_async(move |txn| {
                for warrant_op in warrants {
                    insert_op_authored(txn, &warrant_op)?;
                    summary.warranted += 1;
                }
                StateMutationResult::Ok(())
            })
            .await?;

        if let Some(cache) = workspace.dht_query_cache.as_ref() {
            // "self-publish" warrants, i.e. insert them into the DHT db as if they were published to us by another node
            holochain_state::integrate::authored_ops_to_dht_db(
                _network.storage_arcs().await?,
                warrant_op_hashes,
                workspace.authored_db.clone().into(),
                workspace.dht_db.clone(),
                cache,
            )
            .await?;
        }
    }

    tracing::debug!(
        ?summary,
        ?num_ops_to_validate,
        "Finished sys validation workflow"
    );

    Ok(summary)
}

async fn retrieve_actions(
    current_validation_dependencies: SysValDeps,
    cascade: Arc<impl Cascade + Send + Sync>,
    action_hashes: impl Iterator<Item = ActionHash>,
) {
    let action_fetches = action_hashes
        .filter(|hash| !current_validation_dependencies.same_dht.lock().has(hash))
        .map(|h| {
            // For each previous action that will be needed for validation, map the action to a fetch Action for its hash
            let cascade = cascade.clone();
            async move {
                let fetched = cascade.retrieve_action(h.clone(), Default::default()).await;
                tracing::trace!(hash = ?h, fetched = ?fetched, "Fetched action for validation");
                (h, fetched)
            }
            .boxed()
        });

    let new_deps: ValidationDependencies = ValidationDependencies::new_from_iter(futures::future::join_all(action_fetches)
        .await
        .into_iter()
        .filter_map(|r| {
            // Filter out errors, preparing the rest to be put into a HashMap for easy access.
            match r {
                (hash, Ok(Some((signed_action, source)))) => {
                    Some((hash, ValidationDependencyState::single(signed_action, source)))
                }
                (hash, Ok(None)) => {
                    Some((hash, ValidationDependencyState::new(None)))
                },
                (hash, Err(e)) => {
                    tracing::error!(error = ?e, action_hash = ?hash, "Error retrieving prev action");
                    None
                }
            }
        }));

    current_validation_dependencies
        .same_dht
        .lock()
        .merge(new_deps);
}

fn get_dependency_hashes_from_actions(actions: impl Iterator<Item = Action>) -> Vec<ActionHash> {
    actions
        .flat_map(|action| {
            vec![
                match action.prev_action().cloned() {
                    None => None,
                    hash => hash,
                },
                match action {
                    Action::Update(action) => Some(action.original_action_address),
                    Action::Delete(action) => Some(action.deletes_address),
                    Action::DeleteLink(action) => Some(action.link_add_address),
                    _ => None,
                },
            ]
            .into_iter()
            .flatten()
        })
        .collect()
}

/// Examine the list of provided actions and create a list of actions which are sys validation dependencies for those actions.
/// The actions are merged into `current_validation_dependencies`.
async fn fetch_previous_actions(
    current_validation_dependencies: SysValDeps,
    cascade: Arc<impl Cascade + Send + Sync>,
    actions: impl Iterator<Item = Action>,
) {
    retrieve_actions(
        current_validation_dependencies,
        cascade,
        get_dependency_hashes_from_actions(actions).into_iter(),
    )
    .await;
}

fn get_dependency_hashes_from_ops(ops: impl Iterator<Item = DhtOpHashed>) -> Vec<ActionHash> {
    ops.into_iter()
        .filter_map(|op| {
            // For each previous action that will be needed for validation, map the action to a fetch Record for its hash
            match &op.content {
                DhtOp::ChainOp(op) => match &**op {
                    ChainOp::StoreRecord(_, action, entry) => {
                        let mut actions = match entry {
                            // TODO we should be doing something similar to this in app validation!
                            RecordEntry::Present(entry @ Entry::CounterSign(session_data, _)) => {
                                // Discard errors here because we'll check later whether the input is valid. If it's not then it
                                // won't matter that we've skipped fetching deps for it
                                if let Ok(entry_rate_weight) = action_to_entry_rate_weight(action) {
                                    make_action_set_for_session_data(
                                        entry_rate_weight,
                                        entry,
                                        session_data,
                                    )
                                    .unwrap_or_else(|_| vec![])
                                    .into_iter()
                                    .map(|action| action.into_hash())
                                    .collect::<Vec<_>>()
                                } else {
                                    vec![]
                                }
                            }
                            _ => vec![],
                        };

                        if let Action::Update(update) = action {
                            actions.push(update.original_action_address.clone());
                        }
                        Some(actions)
                    }
                    ChainOp::StoreEntry(_, action, entry) => {
                        let mut actions = match entry {
                            Entry::CounterSign(session_data, _) => {
                                // Discard errors here because we'll check later whether the input is valid. If it's not then it
                                // won't matter that we've skipped fetching deps for it
                                make_action_set_for_session_data(
                                    new_entry_action_to_entry_rate_weight(action),
                                    entry,
                                    session_data,
                                )
                                .unwrap_or_else(|_| vec![])
                                .into_iter()
                                .map(|action| action.into_hash())
                                .collect::<Vec<_>>()
                            }
                            _ => vec![],
                        };

                        if let NewEntryAction::Update(update) = action {
                            actions.push(update.original_action_address.clone());
                        }
                        Some(actions)
                    }
                    ChainOp::RegisterAgentActivity(_, action) => action
                        .prev_action()
                        .map(|action| vec![action.as_hash().clone()]),
                    ChainOp::RegisterUpdatedContent(_, action, _) => {
                        Some(vec![action.original_action_address.clone()])
                    }
                    ChainOp::RegisterUpdatedRecord(_, action, _) => {
                        Some(vec![action.original_action_address.clone()])
                    }
                    ChainOp::RegisterDeletedBy(_, action) => {
                        Some(vec![action.deletes_address.clone()])
                    }
                    ChainOp::RegisterDeletedEntryAction(_, action) => {
                        Some(vec![action.deletes_address.clone()])
                    }
                    ChainOp::RegisterRemoveLink(_, action) => {
                        Some(vec![action.link_add_address.clone()])
                    }
                    _ => None,
                },
                DhtOp::WarrantOp(op) => match &op.proof {
                    WarrantProof::ChainIntegrity(warrant) => match warrant {
                        ChainIntegrityWarrant::InvalidChainOp {
                            action: (action_hash, _),
                            ..
                        } => Some(vec![action_hash.clone()]),
                        ChainIntegrityWarrant::ChainFork {
                            action_pair: ((a1, _), (a2, _)),
                            ..
                        } => Some(vec![a1.clone(), a2.clone()]),
                    },
                },
            }
        })
        .flatten()
        .collect()
}

/// Examine the list of provided ops and create a list of actions which are sys validation dependencies for those ops.
/// The actions are merged into `current_validation_dependencies`.
async fn retrieve_previous_actions_for_ops(
    current_validation_dependencies: SysValDeps,
    cascade: Arc<impl Cascade + Send + Sync>,
    ops: impl Iterator<Item = DhtOpHashed>,
) {
    retrieve_actions(
        current_validation_dependencies,
        cascade,
        get_dependency_hashes_from_ops(ops).into_iter(),
    )
    .await;
}

/// Validate a single DhtOp, using the supplied Cascade to draw dependencies from
pub(crate) async fn validate_op(
    op: &DhtOp,
    dna_def: &DnaDefHashed,
    validation_dependencies: SysValDeps,
    dpki: Option<DpkiImpl>,
) -> WorkflowResult<Outcome> {
    let result = match op {
        DhtOp::ChainOp(op) => validate_chain_op(op, dna_def, validation_dependencies, dpki).await,
        DhtOp::WarrantOp(op) => validate_warrant_op(op, dna_def, validation_dependencies).await,
    };
    match result {
        Ok(_) => Ok(Outcome::Accepted),
        // Handle the errors that result in pending or awaiting deps
        Err(SysValidationError::ValidationOutcome(e)) => {
            if e.is_indeterminate() {
                // This is expected if the dependency isn't held locally and needs to be fetched from the network
                // so downgrade the logging to trace.
                tracing::debug!(
                    msg = "DhtOp has a missing dependency",
                    ?op,
                    error = ?e,
                    error_msg = %e
                );
                Ok(Outcome::MissingDhtDep)
            } else {
                tracing::warn!(msg = "DhtOp was rejected during system validation.", ?op, error = ?e, error_msg = %e);
                Ok(Outcome::Rejected(e.to_string()))
            }
        }
        Err(e) => Err(e.into()),
    }
}

fn action_to_entry_rate_weight(action: &Action) -> SysValidationResult<EntryRateWeight> {
    action
        .entry_rate_data()
        .ok_or_else(|| SysValidationError::NonEntryAction(action.clone()))
}

fn new_entry_action_to_entry_rate_weight(action: &NewEntryAction) -> EntryRateWeight {
    match action {
        NewEntryAction::Create(h) => h.weight.clone(),
        NewEntryAction::Update(h) => h.weight.clone(),
    }
}

fn make_action_set_for_session_data(
    entry_rate_weight: EntryRateWeight,
    entry: &Entry,
    session_data: &CounterSigningSessionData,
) -> SysValidationResult<Vec<ActionHash>> {
    let entry_hash = EntryHash::with_data_sync(entry);
    Ok(session_data
        .build_action_set(entry_hash, entry_rate_weight)?
        .into_iter()
        .map(|action| ActionHash::with_data_sync(&action))
        .collect())
}

async fn validate_chain_op(
    op: &ChainOp,
    dna_def: &DnaDefHashed,
    validation_dependencies: SysValDeps,
    dpki: Option<DpkiImpl>,
) -> SysValidationResult<()> {
    check_entry_visibility(op)?;
    // Check agent validity in Deepkey first
    if let Some(dpki) = dpki {
        // Don't run DPKI agent validity checks on the DPKI service itself
        if !dpki.is_deepkey_dna(dna_def.as_hash()) {
            check_dpki_agent_validity_for_op(&dpki, op).await?;
        }
    }

    match op {
        ChainOp::StoreRecord(_, action, entry) => {
            check_prev_action(action)?;
            if let Some(entry) = entry.as_option() {
                // Retrieve for all other actions on countersigned entry.
                if let Entry::CounterSign(session_data, _) = entry {
                    for action_hash in make_action_set_for_session_data(
                        action_to_entry_rate_weight(action)?,
                        entry,
                        session_data,
                    )? {
                        // Just require that we are holding all the other actions
                        let validation_dependencies = validation_dependencies.same_dht.lock();
                        validation_dependencies
                            .get(&action_hash)
                            .and_then(|s| s.as_action())
                            .ok_or_else(|| {
                                ValidationOutcome::DepMissingFromDht(action_hash.clone().into())
                            })?;
                    }
                }
                // Has to be async because of signature checks being async
                store_entry(
                    (action)
                        .try_into()
                        .map_err(|_| ValidationOutcome::NotNewEntry(action.clone()))?,
                    entry,
                    validation_dependencies,
                )
                .await?;
            }
            Ok(())
        }
        ChainOp::StoreEntry(_, action, entry) => {
            // Check and hold for all other actions on countersigned entry.
            if let Entry::CounterSign(session_data, _) = entry {
                for action_hash in make_action_set_for_session_data(
                    new_entry_action_to_entry_rate_weight(action),
                    entry,
                    session_data,
                )? {
                    // Just require that we are holding all the other actions
                    let validation_dependencies = validation_dependencies.same_dht.lock();
                    validation_dependencies
                        .get(&action_hash)
                        .and_then(|s| s.as_action())
                        .ok_or_else(|| {
                            ValidationOutcome::DepMissingFromDht(action_hash.clone().into())
                        })?;
                }
            }

            check_prev_action(&action.clone().into())?;
            store_entry(action.into(), entry, validation_dependencies.clone()).await
        }
        ChainOp::RegisterAgentActivity(_, action) => {
            register_agent_activity(action, validation_dependencies.clone(), dna_def)?;
            store_record(action, validation_dependencies)
        }
        ChainOp::RegisterUpdatedContent(_, action, entry) => {
            register_updated_content(action, validation_dependencies.clone())?;
            if let Some(entry) = entry.as_option() {
                store_entry(
                    NewEntryActionRef::Update(action),
                    entry,
                    validation_dependencies,
                )
                .await?;
            }

            Ok(())
        }
        ChainOp::RegisterUpdatedRecord(_, action, entry) => {
            register_updated_record(action, validation_dependencies.clone())?;
            if let Some(entry) = entry.as_option() {
                store_entry(
                    NewEntryActionRef::Update(action),
                    entry,
                    validation_dependencies,
                )
                .await?;
            }

            Ok(())
        }
        ChainOp::RegisterDeletedBy(_, action) => {
            register_deleted_by(action, validation_dependencies)
        }
        ChainOp::RegisterDeletedEntryAction(_, action) => {
            register_deleted_entry_action(action, validation_dependencies)
        }
        ChainOp::RegisterAddLink(_, action) => register_add_link(action),
        ChainOp::RegisterRemoveLink(_, action) => {
            register_delete_link(action, validation_dependencies)
        }
    }
}

/// Verify agent key validity.
///
/// If the previous action is a `Delete` of the current agent pub key,
/// that agent key is invalid.
fn check_agent_validity(agent: &AgentPubKey, prev_action: &Action) -> SysValidationResult<()> {
    if let Action::Delete(delete) = prev_action {
        if delete.deletes_entry_address == agent.clone().into() {
            return Err(SysValidationError::ValidationOutcome(
                ValidationOutcome::InvalidAgentKey(agent.clone()),
            ));
        }
    }
    Ok(())
}

// TODO: should this check DPKI for agent validity?
async fn validate_warrant_op(
    op: &WarrantOp,
    _dna_def: &DnaDefHashed,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    match &op.proof {
        WarrantProof::ChainIntegrity(warrant) => match warrant {
            ChainIntegrityWarrant::InvalidChainOp {
                action: (action_hash, action_sig),
                action_author,
                ..
            } => {
                let action = {
                    let deps = validation_dependencies.same_dht.lock();
                    let action = deps
                        .get(action_hash)
                        .and_then(|s| s.as_action())
                        .ok_or_else(|| {
                            ValidationOutcome::DepMissingFromDht(action_hash.clone().into())
                        })?;

                    if action.author() != action_author {
                        return Err(ValidationOutcome::InvalidWarrant(
                            op.warrant().clone(),
                            "action author mismatch".into(),
                        )
                        .into());
                    }
                    action.clone()
                };
                verify_action_signature(action_sig, &action).await?;

                Ok(())
            }
            ChainIntegrityWarrant::ChainFork {
                action_pair: ((a1, a1_sig), (a2, a2_sig)),
                chain_author,
                ..
            } => {
                let (action1, action2) = {
                    let deps = validation_dependencies.same_dht.lock();
                    let action1 = deps
                        .get(a1)
                        .and_then(|s| s.as_action())
                        .ok_or_else(|| ValidationOutcome::DepMissingFromDht(a1.clone().into()))?;
                    let action2 = deps
                        .get(a2)
                        .and_then(|s| s.as_action())
                        .ok_or_else(|| ValidationOutcome::DepMissingFromDht(a2.clone().into()))?;

                    // chain_author basis must match the author of the action
                    if action1.author() != chain_author {
                        return Err(ValidationOutcome::InvalidWarrant(
                            op.warrant().clone(),
                            "chain author mismatch".into(),
                        )
                        .into());
                    }

                    // Both actions must be by same author
                    if action1.author() != action2.author() {
                        return Err(ValidationOutcome::InvalidWarrant(
                            op.warrant().clone(),
                            "action pair author mismatch".into(),
                        )
                        .into());
                    }

                    // A fork is evidenced by two actions with a common predecessor.
                    // NOTE: we could also check sequence numbers, but then we'd have to traverse
                    // both forks backwards until reaching a common ancestor to protect against an
                    // attack where someone authors a warrant using two actions from two different DNAs.
                    // Using seq numbers makes it easier to detect and prove a fork, but using prev_action
                    // prevents the attack.
                    if action1.prev_action() != action2.prev_action() {
                        return Err(ValidationOutcome::InvalidWarrant(
                            op.warrant().clone(),
                            "action pair seq mismatch".into(),
                        )
                        .into());
                    }

                    (action1.clone(), action2.clone())
                };

                verify_action_signature(a1_sig, &action1).await?;
                verify_action_signature(a2_sig, &action2).await?;

                Ok(())
            }
        },
    }
}

/// Run system validation for a single [`Record`] instead of the usual [`DhtOp`] input for the system validation workflow.
/// It is expected that the provided cascade will include a network so that dependencies which we either do not hold yet, or
/// should not hold, can be fetched and cached for use in validation.
///
/// Note that the conditions on the action being validated are slightly stronger than the usual system validation workflow. This is because
/// it is intended to be used for validation of records which have been authored locally so we should always be able to check the previous action.
pub async fn sys_validate_record(
    record: &Record,
    cascade: Arc<impl Cascade + Send + Sync>,
) -> SysValidationOutcome<()> {
    match sys_validate_record_inner(record, cascade).await {
        // Validation succeeded
        Ok(_) => Ok(()),
        // Validation failed so exit with that outcome
        Err(SysValidationError::ValidationOutcome(validation_outcome)) => {
            error!(
                msg = "Direct validation failed",
                ?validation_outcome,
                ?record,
            );
            validation_outcome.into_outcome()
        }
        // An error occurred so return it
        Err(e) => Err(OutcomeOrError::Err(e)),
    }
}

async fn sys_validate_record_inner(
    record: &Record,
    cascade: Arc<impl Cascade + Send + Sync>,
) -> SysValidationResult<()> {
    let signature = record.signature();
    let action = record.action();
    let maybe_entry = record.entry().as_option();
    counterfeit_check_action(signature, action).await?;

    async fn validate(
        action: &Action,
        maybe_entry: Option<&Entry>,
        cascade: Arc<impl Cascade + Send + Sync>,
    ) -> SysValidationResult<()> {
        let validation_dependencies = SysValDeps::default();
        fetch_previous_actions(
            validation_dependencies.clone(),
            cascade.clone(),
            vec![action.clone()].into_iter(),
        )
        .await;

        // Check agent validity
        if let Some(previous_action_hash) = action.prev_action() {
            let deps = validation_dependencies.same_dht.lock();
            // Previous action was fetched in the preceding call `fetch_previous_actions`.
            let previous_action = deps
                .get(previous_action_hash)
                .and_then(|s| s.as_action())
                .ok_or_else(|| {
                    ValidationOutcome::DepMissingFromDht(previous_action_hash.clone().into())
                })?;
            check_agent_validity(action.author(), previous_action)?;
        }

        store_record(action, validation_dependencies.clone())?;
        if let Some(maybe_entry) = maybe_entry {
            store_entry(
                action
                    .try_into()
                    .map_err(|_| ValidationOutcome::NotNewEntry(action.clone()))?,
                maybe_entry,
                validation_dependencies.clone(),
            )
            .await?;
        }
        match action {
            Action::Update(action) => {
                register_updated_content(action, validation_dependencies.clone())
            }
            Action::Delete(action) => {
                register_deleted_entry_action(action, validation_dependencies.clone())
            }
            Action::CreateLink(action) => register_add_link(action),
            Action::DeleteLink(action) => {
                register_delete_link(action, validation_dependencies.clone())
            }
            _ => Ok(()),
        }
    }

    match maybe_entry {
        Some(Entry::CounterSign(session, _)) => {
            if let Some(weight) = action.entry_rate_data() {
                let entry_hash = EntryHash::with_data_sync(maybe_entry.unwrap());
                for action in session.build_action_set(entry_hash, weight)? {
                    validate(&action, maybe_entry, cascade.clone()).await?;
                }
                Ok(())
            } else {
                tracing::error!("Got countersigning entry without rate assigned. This should be impossible. But, let's see what happens.");
                validate(action, maybe_entry, cascade.clone()).await
            }
        }
        _ => validate(action, maybe_entry, cascade).await,
    }
}

/// Check if the chain op has valid signature and author.
/// Ops that fail this check should be dropped.
pub async fn counterfeit_check_action(
    signature: &Signature,
    action: &Action,
) -> SysValidationResult<()> {
    verify_action_signature(signature, action).await?;
    Ok(())
}

/// Check if the warrant op has valid signature and author.
pub async fn counterfeit_check_warrant(warrant_op: &WarrantOp) -> SysValidationResult<()> {
    verify_warrant_signature(warrant_op).await?;
    author_key_is_valid(&warrant_op.author).await?;
    Ok(())
}

fn register_agent_activity(
    action: &Action,
    validation_dependencies: SysValDeps,
    dna_def: &DnaDefHashed,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let prev_action_hash = action.prev_action();

    // Checks
    check_prev_action(action)?;
    check_valid_if_dna(action, dna_def)?;
    if let Some(prev_action_hash) = prev_action_hash {
        // Just make sure we have the dependency and if not then don't mark this action as valid yet
        let validation_dependencies = validation_dependencies.same_dht.lock();
        let prev_action = validation_dependencies
            .get(prev_action_hash)
            .and_then(|s| s.as_action())
            .ok_or_else(|| ValidationOutcome::DepMissingFromDht(prev_action_hash.clone().into()))?;

        // Agent key updates are only validated by agent authorities.
        check_agent_validity(action.author(), prev_action)?;

        match prev_action {
            Action::CloseChain(_) => Err(ValidationOutcome::PrevActionError(
                (PrevActionErrorKind::ActionAfterChainClose, action.clone()).into(),
            )
            .into()),
            _ => Ok(()),
        }
    } else {
        Ok(())
    }
}

fn store_record(action: &Action, validation_dependencies: SysValDeps) -> SysValidationResult<()> {
    // Get data ready to validate
    let prev_action_hash = action.prev_action();

    // Checks
    check_prev_action(action)?;
    if let Some(prev_action_hash) = prev_action_hash {
        let validation_dependencies = validation_dependencies.same_dht.lock();
        let prev_action = validation_dependencies
            .get(prev_action_hash)
            .and_then(|s| s.as_action())
            .ok_or_else(|| ValidationOutcome::DepMissingFromDht(prev_action_hash.clone().into()))?;
        check_prev_author(action, prev_action)?;
        check_prev_timestamp(action, prev_action)?;
        check_prev_seq(action, prev_action)?;
        check_agent_validation_pkg_predecessor(action, prev_action)?;
    }

    Ok(())
}

async fn store_entry(
    action: NewEntryActionRef<'_>,
    entry: &Entry,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let entry_type = action.entry_type();
    let entry_hash = action.entry_hash();

    // Checks
    check_entry_type(entry_type, entry)?;
    check_entry_hash(entry_hash, entry)?;
    check_entry_size(entry)?;

    // Additional checks if this is an Update
    if let NewEntryActionRef::Update(entry_update) = action {
        let original_action_address = &entry_update.original_action_address;
        let validation_dependencies = validation_dependencies.same_dht.lock();
        let original_action = validation_dependencies
            .get(original_action_address)
            .and_then(|s| s.as_action())
            .ok_or_else(|| {
                ValidationOutcome::DepMissingFromDht(original_action_address.clone().into())
            })?;
        update_check(entry_update, original_action)?;
    }

    // Additional checks if this is a countersigned entry.
    if let Entry::CounterSign(session_data, _) = entry {
        check_countersigning_session_data(EntryHash::with_data_sync(entry), session_data, action)
            .await?;
    }
    Ok(())
}

fn register_updated_content(
    entry_update: &Update,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let original_action_address = &entry_update.original_action_address;

    let validation_dependencies = validation_dependencies.same_dht.lock();
    let original_action = validation_dependencies
        .get(original_action_address)
        .and_then(|s| s.as_action())
        .ok_or_else(|| {
            ValidationOutcome::DepMissingFromDht(original_action_address.clone().into())
        })?;

    update_check(entry_update, original_action)
}

fn register_updated_record(
    record_update: &Update,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let original_action_address = &record_update.original_action_address;

    let validation_dependencies = validation_dependencies.same_dht.lock();
    let original_action = validation_dependencies
        .get(original_action_address)
        .and_then(|s| s.as_action())
        .ok_or_else(|| {
            ValidationOutcome::DepMissingFromDht(original_action_address.clone().into())
        })?;

    update_check(record_update, original_action)
}

fn register_deleted_by(
    record_delete: &Delete,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let removed_action_address = &record_delete.deletes_address;

    let validation_dependencies = validation_dependencies.same_dht.lock();
    let action = validation_dependencies
        .get(removed_action_address)
        .and_then(|s| s.as_action())
        .ok_or_else(|| {
            ValidationOutcome::DepMissingFromDht(removed_action_address.clone().into())
        })?;

    check_new_entry_action(action)
}

fn register_deleted_entry_action(
    record_delete: &Delete,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let removed_action_address = &record_delete.deletes_address;

    let validation_dependencies = validation_dependencies.same_dht.lock();
    let action = validation_dependencies
        .get(removed_action_address)
        .and_then(|s| s.as_action())
        .ok_or_else(|| {
            ValidationOutcome::DepMissingFromDht(removed_action_address.clone().into())
        })?;

    check_new_entry_action(action)
}

fn register_add_link(link_add: &CreateLink) -> SysValidationResult<()> {
    check_tag_size(&link_add.tag)
}

fn register_delete_link(
    link_remove: &DeleteLink,
    validation_dependencies: SysValDeps,
) -> SysValidationResult<()> {
    // Get data ready to validate
    let link_add_address = &link_remove.link_add_address;

    // Just require that this link exists, don't need to check anything else about it here
    let validation_dependencies = validation_dependencies.same_dht.lock();
    let add_link_action = validation_dependencies
        .get(link_add_address)
        .and_then(|s| s.as_action())
        .ok_or_else(|| ValidationOutcome::DepMissingFromDht(link_add_address.clone().into()))?;

    match add_link_action {
        Action::CreateLink(_) => Ok(()),
        _ => Err(ValidationOutcome::NotCreateLink(add_link_action.to_hash()).into()),
    }
}

fn update_check(entry_update: &Update, original_action: &Action) -> SysValidationResult<()> {
    check_new_entry_action(original_action)?;
    // This shouldn't fail due to the above `check_new_entry_action` check
    let original_action: NewEntryActionRef = original_action
        .try_into()
        .map_err(|_| ValidationOutcome::NotNewEntry(original_action.clone()))?;
    check_update_reference(entry_update, &original_action)?;
    Ok(())
}

pub struct SysValidationWorkspace {
    scratch: Option<SyncScratch>,
    // Authored DB is writeable because warrants may be written.
    authored_db: DbWrite<DbKindAuthored>,
    dht_db: DbWrite<DbKindDht>,
    dht_query_cache: Option<DhtDbQueryCache>,
    cache: DbWrite<DbKindCache>,
    pub(crate) dna_def: Arc<DnaDef>,
    sys_validation_retry_delay: Duration,
    dpki: Option<DpkiImpl>,
}

impl SysValidationWorkspace {
    pub fn new(
        authored_db: DbWrite<DbKindAuthored>,
        dht_db: DbWrite<DbKindDht>,
        dht_query_cache: DhtDbQueryCache,
        cache: DbWrite<DbKindCache>,
        dna_def: Arc<DnaDef>,
        dpki: Option<DpkiImpl>,
        sys_validation_retry_delay: Duration,
    ) -> Self {
        Self {
            scratch: None,
            authored_db,
            dht_db,
            dht_query_cache: Some(dht_query_cache),
            cache,
            dna_def,
            dpki,
            sys_validation_retry_delay,
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn is_chain_empty(&self, author: &AgentPubKey) -> SourceChainResult<bool> {
        // If we have a query cache then this is an authority node and
        // we can quickly check if the chain is empty from the cache.
        if let Some(c) = &self.dht_query_cache {
            return Ok(c.is_chain_empty(author).await?);
        }

        // Otherwise we need to check this is an author node and
        // we need to check the author db.
        let author = author.clone();
        let chain_not_empty = self
            .authored_db
            .read_async(move |txn| {
                let mut stmt = txn.prepare(
                    "
                SELECT
                EXISTS (
                    SELECT
                    1
                    FROM Action
                    JOIN
                    DhtOp ON Action.hash = DhtOp.action_hash
                    WHERE
                    Action.author = :author
                    AND
                    DhtOp.when_integrated IS NOT NULL
                    AND
                    DhtOp.type = :activity
                    LIMIT 1
                )
                ",
                )?;
                DatabaseResult::Ok(stmt.query_row(
                    named_params! {
                        ":author": author,
                        ":activity": ChainOpType::RegisterAgentActivity,
                    },
                    |row| row.get(0),
                )?)
            })
            .await?;
        let chain_not_empty = match &self.scratch {
            Some(scratch) => scratch.apply(|scratch| !scratch.is_empty())? || chain_not_empty,
            None => chain_not_empty,
        };
        Ok(!chain_not_empty)
    }

    pub async fn action_seq_is_empty(&self, action: &Action) -> SourceChainResult<bool> {
        let author = action.author().clone();
        let seq = action.action_seq();
        let hash = ActionHash::with_data_sync(action);
        let action_seq_is_not_empty = self
            .dht_db
            .read_async({
                let hash = hash.clone();
                move |txn| {
                    DatabaseResult::Ok(txn.query_row(
                        "
                SELECT EXISTS(
                    SELECT
                    1
                    FROM Action
                    WHERE
                    Action.author = :author
                    AND
                    Action.seq = :seq
                    AND
                    Action.hash != :hash
                    LIMIT 1
                )
                ",
                        named_params! {
                            ":author": author,
                            ":seq": seq,
                            ":hash": hash,
                        },
                        |row| row.get(0),
                    )?)
                }
            })
            .await?;
        let action_seq_is_not_empty = match &self.scratch {
            Some(scratch) => {
                scratch.apply(|scratch| {
                    scratch.actions().any(|shh| {
                        shh.action().action_seq() == seq && *shh.action_address() != hash
                    })
                })? || action_seq_is_not_empty
            }
            None => action_seq_is_not_empty,
        };
        Ok(!action_seq_is_not_empty)
    }

    /// Create a cascade with local data only
    pub fn local_cascade(&self) -> CascadeImpl {
        let cascade = CascadeImpl::empty()
            .with_dht(self.dht_db.clone().into())
            .with_cache(self.cache.clone());
        match &self.scratch {
            Some(scratch) => cascade
                .with_authored(self.authored_db.clone().into())
                .with_scratch(scratch.clone()),
            None => cascade,
        }
    }

    pub fn network_and_cache_cascade(&self, network: GenericNetwork) -> CascadeImpl {
        CascadeImpl::empty().with_network(network, self.cache.clone())
    }

    /// Get a reference to the sys validation workspace's dna def.
    pub fn dna_def(&self) -> Arc<DnaDef> {
        self.dna_def.clone()
    }

    /// Get a reference to the sys validation workspace's dna def.
    pub fn dna_def_hashed(&self) -> DnaDefHashed {
        DnaDefHashed::from_content_sync((*self.dna_def).clone())
    }
}

fn put_validation_limbo(
    txn: &mut Transaction<'_>,
    hash: &DhtOpHash,
    status: ValidationStage,
) -> WorkflowResult<()> {
    set_validation_stage(txn, hash, status)?;
    Ok(())
}

fn put_integration_limbo(
    txn: &mut Transaction<'_>,
    hash: &DhtOpHash,
    status: ValidationStatus,
) -> WorkflowResult<()> {
    set_validation_status(txn, hash, status)?;
    set_validation_stage(txn, hash, ValidationStage::AwaitingIntegration)?;
    Ok(())
}

pub fn put_integrated(
    txn: &mut Transaction<'_>,
    hash: &DhtOpHash,
    status: ValidationStatus,
) -> WorkflowResult<()> {
    set_validation_status(txn, hash, status)?;
    // This set the validation stage to pending which is correct when
    // it's integrated.
    set_validation_stage(txn, hash, ValidationStage::Pending)?;
    set_when_integrated(txn, hash, Timestamp::now())?;
    Ok(())
}

pub async fn make_warrant_op(
    conductor: &Conductor,
    dna_hash: &DnaHash,
    op: &ChainOp,
    validation_type: ValidationType,
) -> WorkflowResult<DhtOpHashed> {
    let keystore = conductor.keystore();
    let warrant_author = get_representative_agent(conductor, dna_hash).expect("TODO: handle");
    make_invalid_chain_warrant_op_inner(keystore, warrant_author, op, validation_type).await
}

/// Gets an arbitrary agent with a cell running the given DNA, needed for processes
/// which require an agent signature but happen at the DNA level, i.e. not bound to any
/// particular cell.
pub fn get_representative_agent(conductor: &Conductor, dna_hash: &DnaHash) -> Option<AgentPubKey> {
    conductor
        .running_cell_ids()
        .into_iter()
        .find(|id| id.dna_hash() == dna_hash)
        .map(|id| id.agent_pubkey().clone())
}

pub async fn make_invalid_chain_warrant_op_inner(
    keystore: &MetaLairClient,
    warrant_author: AgentPubKey,
    op: &ChainOp,
    validation_type: ValidationType,
) -> WorkflowResult<DhtOpHashed> {
    let action = op.action();
    let action_author = action.author().clone();
    tracing::warn!("Authoring warrant for invalid op authored by {action_author}");

    let proof = WarrantProof::ChainIntegrity(ChainIntegrityWarrant::InvalidChainOp {
        action_author,
        action: (action.to_hash().clone(), op.signature().clone()),
        validation_type,
    });
    let warrant = Warrant::new(proof, warrant_author, Timestamp::now());
    let warrant_op = WarrantOp::sign(keystore, warrant)
        .await
        .map_err(|e| super::WorkflowError::Other(e.into()))?;
    let op: DhtOp = warrant_op.into();
    let op = op.into_hashed();
    Ok(op)
}

pub async fn make_fork_warrant_op_inner(
    keystore: &MetaLairClient,
    warrant_author: AgentPubKey,
    chain_author: AgentPubKey,
    action_pair: ((ActionHash, Signature), (ActionHash, Signature)),
) -> WorkflowResult<DhtOpHashed> {
    debug_assert_ne!(action_pair.0 .0, action_pair.1 .0);
    tracing::warn!(
        "Authoring warrant for chain fork by {chain_author}. Action hashes: ({}, {})",
        action_pair.0 .0,
        action_pair.1 .0
    );

    let warrant = Warrant::new(
        WarrantProof::ChainIntegrity(ChainIntegrityWarrant::ChainFork {
            chain_author,
            action_pair,
        }),
        warrant_author,
        Timestamp::now(),
    );
    let warrant_op = WarrantOp::sign(keystore, warrant)
        .await
        .map_err(|e| super::WorkflowError::Other(e.into()))?;
    let op: DhtOp = warrant_op.into();
    let op = op.into_hashed();
    Ok(op)
}

pub fn detect_fork(
    txn: &mut Transaction<'_>,
    action: &Action,
) -> StateQueryResult<Option<(ActionHash, Signature)>> {
    let mut statement = txn.prepare(ACTION_HASH_BY_PREV)?;
    let items = statement
        .query_map(
            named_params! {
                ":prev_hash": action.prev_action(),
                ":hash": action.to_hash(),
            },
            // First, try to deserialize the hash as an ActionHash...
            |row| match row.get("hash") {
                Ok(hash) => {
                    let action_blob: Vec<u8> = row.get("blob")?;
                    Ok(Some((hash, action_blob)))
                }
                // ...if that fails, we can skip it if it deserializes as a WarrantHash
                //    (checking the row type this way makes it so we don't have to join on the DhtOp table in the query)
                Err(err) => match row.get::<_, WarrantHash>("hash") {
                    Ok(_) => Ok(None),
                    Err(_) => Err(err),
                },
            },
        )?
        .collect::<Result<Vec<_>, _>>()?;
    items
        .into_iter()
        .filter_map(|maybe_tuple| {
            maybe_tuple.map(|(hash, action_blob)| {
                from_blob::<SignedAction>(action_blob)
                    .map(|action| (hash, action.signature().clone()))
            })
        })
        .next()
        .transpose()
}

#[derive(Debug, Clone)]
struct OutcomeSummary {
    accepted: usize,
    missing: usize,
    rejected: usize,
    warranted: usize,
}

impl OutcomeSummary {
    fn new() -> Self {
        OutcomeSummary {
            accepted: 0,
            missing: 0,
            rejected: 0,
            warranted: 0,
        }
    }
}

impl Default for OutcomeSummary {
    fn default() -> Self {
        OutcomeSummary::new()
    }
}



================================================
File: crates/holochain/src/core/workflow/validation_receipt_workflow.rs
================================================
use futures::future::BoxFuture;
use futures::{stream, StreamExt};
use itertools::Itertools;
use std::collections::HashSet;
use std::sync::Arc;

use holochain_keystore::MetaLairClient;
use holochain_p2p::HolochainP2pDnaT;
use holochain_state::prelude::*;
use tracing::*;

use super::error::WorkflowResult;
use crate::core::queue_consumer::WorkComplete;
use holochain_zome_types::block::Block;
use holochain_zome_types::block::BlockTarget;
use holochain_zome_types::block::CellBlockReason;

#[cfg(test)]
mod tests;

#[cfg(test)]
mod unit_tests;

#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(vault, network, keystore, apply_block))
)]
/// Send validation receipts to their authors in serial and without waiting for responses.
pub async fn validation_receipt_workflow<B>(
    dna_hash: Arc<DnaHash>,
    vault: DbWrite<DbKindDht>,
    network: impl HolochainP2pDnaT,
    keystore: MetaLairClient,
    running_cell_ids: HashSet<CellId>,
    apply_block: B,
) -> WorkflowResult<WorkComplete>
where
    B: Fn(Block) -> BoxFuture<'static, DatabaseResult<()>> + Clone,
{
    if running_cell_ids.is_empty() {
        return Ok(WorkComplete::Complete);
    }

    // This is making an assumption about the behaviour of validation: Once validation has run on this conductor
    // then all the cells running the same DNA agree on the result.
    let validators = running_cell_ids
        .into_iter()
        .filter_map(|id| {
            let (d, a) = id.into_dna_and_agent();
            if d == *dna_hash {
                Some(a)
            } else {
                None
            }
        })
        .collect::<Vec<_>>();

    // Get out all ops that are marked for sending receipt.
    let receipts = pending_receipts(&vault, validators.clone()).await?;

    let validators: HashSet<_> = validators.into_iter().collect();

    let grouped_by_author = receipts
        .into_iter()
        .group_by(|(_, author)| author.clone())
        .into_iter()
        .map(|(author, receipts)| {
            (
                author,
                receipts.into_iter().map(|(r, _)| r).collect::<Vec<_>>(),
            )
        })
        .collect::<Vec<(AgentPubKey, Vec<ValidationReceipt>)>>();

    for (author, receipts) in grouped_by_author {
        // Try to send the validation receipts
        match sign_and_send_receipts_to_author(
            &dna_hash,
            &network,
            &keystore,
            &validators,
            &author,
            receipts.clone(),
            apply_block.clone(),
        )
        .await
        {
            Ok(()) => {
                // Success, move on to mark them as sent
            }
            Err(e) => {
                info!(failed_to_sign_and_send_receipt = ?e);
            }
        }

        // Attempted to send the receipts so we now mark them to not send in the next workflow run.
        for receipt in receipts {
            vault
                .write_async(move |txn| set_require_receipt(txn, &receipt.dht_op_hash, false))
                .await?;
        }
    }

    Ok(WorkComplete::Complete)
}

/// Perform the signing and sending of
/// Requires that the receipts to send are all by the same author.
async fn sign_and_send_receipts_to_author<B>(
    dna_hash: &DnaHash,
    network: &impl HolochainP2pDnaT,
    keystore: &MetaLairClient,
    validators: &HashSet<AgentPubKey>,
    op_author: &AgentPubKey,
    receipts: Vec<ValidationReceipt>,
    apply_block: B,
) -> WorkflowResult<()>
where
    B: Fn(Block) -> BoxFuture<'static, DatabaseResult<()>>,
{
    // Don't send receipt to self. Don't block self.
    if validators.contains(op_author) {
        return Ok(());
    }

    let num_receipts = receipts.len();

    let receipts: Vec<SignedValidationReceipt> = stream::iter(receipts)
        .filter_map(|receipt| async {
            // Block authors of invalid ops.
            if matches!(receipt.validation_status, ValidationStatus::Rejected) {
                // Block BEFORE we integrate the outcome because this is not atomic
                // and if something goes wrong we know the integration will retry.
                if let Err(e) = apply_block(Block::new(
                    BlockTarget::Cell(
                        CellId::new((*dna_hash).clone(), op_author.clone()),
                        CellBlockReason::InvalidOp(receipt.dht_op_hash.clone()),
                    ),
                    match InclusiveTimestampInterval::try_new(Timestamp::MIN, Timestamp::MAX) {
                        Ok(interval) => interval,
                        Err(e) => {
                            error!("Failed to create timestamp interval: {:?}", e);
                            return None;
                        }
                    },
                ))
                .await
                {
                    error!("Failed to apply block to author {:?}: {:?}", op_author, e)
                }
            }

            // Sign on the dotted line.
            match ValidationReceipt::sign(receipt, keystore).await {
                Ok(r) => r,
                Err(e) => {
                    // TODO Which errors are retryable here? A fatal error would keep being retried and we don't want that;
                    //      aggressively give up for now.
                    info!(failed_to_sign_receipt = ?e);
                    None
                }
            }
        })
        .collect()
        .await;

    if receipts.is_empty() {
        info!("Dropped all validation receipts for author {:?}", op_author);
        return Ok(());
    } else if num_receipts < receipts.len() {
        info!(
            "Dropped {}/{} validation receipts for author {:?}, check previous errors to see why",
            num_receipts - receipts.len(),
            num_receipts,
            op_author,
        );
    }

    // Send it and don't wait for response.
    if let Err(e) = holochain_p2p::HolochainP2pDnaT::send_validation_receipts(
        network,
        op_author.clone(),
        receipts.into(),
    )
    .await
    {
        // No one home, they will need to publish again.
        info!(failed_send_receipt = ?e);
    }

    Ok(())
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
async fn pending_receipts(
    vault: &DbRead<DbKindDht>,
    validators: Vec<AgentPubKey>,
) -> StateQueryResult<Vec<(ValidationReceipt, AgentPubKey)>> {
    vault
        .read_async(move |txn| get_pending_validation_receipts(txn, validators))
        .await
}



================================================
File: crates/holochain/src/core/workflow/witnessing_workflow.rs
================================================
//! Witnessing workflow that is the counterpart to the countersigning workflow.

use super::{error::WorkflowResult, incoming_dht_ops_workflow::incoming_dht_ops_workflow};
use crate::conductor::space::Space;
use crate::core::queue_consumer::{TriggerSender, WorkComplete};
use crate::core::ribosome::weigh_placeholder;
use crate::core::share::Share;
use holo_hash::{ActionHash, AgentPubKey, DhtOpHash, EntryHash};
use holochain_p2p::event::CountersigningSessionNegotiationMessage;
use holochain_p2p::HolochainP2pDnaT;
use holochain_state::prelude::*;
use std::collections::HashMap;

/// A cheaply cloneable, thread-safe and in-memory store for
/// active countersigning sessions.
#[derive(Clone, Default)]
pub struct WitnessingWorkspace {
    inner: Share<WitnessingWorkspaceInner>,
}

/// Pending countersigning sessions.
#[derive(Default)]
pub struct WitnessingWorkspaceInner {
    pending: HashMap<EntryHash, Session>,
}

#[derive(Default)]
struct Session {
    /// Map of action hash for each signers action to the [`DhtOp`] and other required actions for
    /// this session to be considered complete.
    map: HashMap<ActionHash, (DhtOpHash, ChainOp, Vec<ActionHash>)>,
    /// When this session expires.
    ///
    /// If this is none the session is empty.
    expires: Option<Timestamp>,
}

/// Witnessing workflow that is the counterpart to the countersigning workflow.
///
/// This workflow is run by witnesses to countersigning sessions who are responsible for gathering
/// signatures during sessions. The workflow checks for complete sessions and pushes the complete
/// ops to validation then messages the session participants with the complete set of signatures
/// for the session.
pub(crate) async fn witnessing_workflow(
    space: Space,
    network: impl HolochainP2pDnaT,
    sys_validation_trigger: TriggerSender,
) -> WorkflowResult<WorkComplete> {
    // Get any complete sessions.
    let complete_sessions = space.witnessing_workspace.get_complete_sessions();
    let mut notify_agents = Vec::with_capacity(complete_sessions.len());

    // For each complete session send the ops to validation.
    for (agents, ops, actions) in complete_sessions {
        let non_enzymatic_ops: Vec<_> = ops
            .into_iter()
            .filter(|(_hash, dht_op)| dht_op.enzymatic_countersigning_enzyme().is_none())
            .collect();
        if !non_enzymatic_ops.is_empty() {
            incoming_dht_ops_workflow(
                space.clone(),
                sys_validation_trigger.clone(),
                non_enzymatic_ops
                    .into_iter()
                    .map(|(_h, o)| o.into())
                    .collect(),
                false,
            )
            .await?;
        }
        notify_agents.push((agents, actions));
    }

    // For each complete session notify the agents of success.
    for (agents, actions) in notify_agents {
        tracing::debug!("Witnessing ready, notifying agents {:?}", agents);
        if let Err(e) = network
            .countersigning_session_negotiation(
                agents,
                CountersigningSessionNegotiationMessage::AuthorityResponse(actions),
            )
            .await
        {
            // This could likely fail if a signer is offline, so it's not an error.
            tracing::warn!(
                "Failed to notify agents: counter signed actions because of {:?}",
                e
            );
        }
    }
    Ok(WorkComplete::Complete)
}

/// Receive incoming DhtOps for a countersigning session.
///
/// These ops are produced by participants in a countersigning session and sent to us to be checked.
/// This function will store the ops in the workspace and trigger the workflow.
pub(crate) fn receive_incoming_countersigning_ops(
    ops: Vec<(DhtOpHash, ChainOp)>,
    workspace: &WitnessingWorkspace,
    witnessing_workflow_trigger: TriggerSender,
) -> WorkflowResult<()> {
    let mut should_trigger = false;

    // For each op check it's the right type and extract the
    // entry hash, required actions and expires time.
    for (hash, op) in ops {
        // Must be a store entry op.
        if let ChainOp::StoreEntry(_, _, entry) = &op {
            // Must be a CounterSign entry type.
            if let Entry::CounterSign(session_data, _) = entry {
                let entry_hash = EntryHash::with_data_sync(entry);
                // Get the required actions for this session.
                let weight = weigh_placeholder();
                let action_set = session_data.build_action_set(entry_hash, weight)?;

                // Get the expires time for this session.
                let expires = *session_data.preflight_request().session_times.end();

                // Get the entry hash from an action.
                // If the actions have different entry hashes they will fail validation.
                if let Some(entry_hash) = action_set.first().and_then(|a| a.entry_hash().cloned()) {
                    // Hash the required actions.
                    let required_actions: Vec<_> = action_set
                        .into_iter()
                        .map(|a| ActionHash::with_data_sync(&a))
                        .collect();

                    // Only accept the op if the session is not expired.
                    if Timestamp::now() < expires {
                        // Put this op in the pending map.
                        workspace.put(entry_hash, hash, op, required_actions, expires);
                        // We have new ops, so we should trigger the workflow.
                        should_trigger = true;
                    }
                }
            } else {
                tracing::warn!(?op, "Incoming countersigning op is not a CounterSign entry");
            }
        } else {
            tracing::warn!(?op, "Incoming countersigning op is not a StoreEntry op");
        }
    }

    // Trigger the workflow if we have new ops.
    if should_trigger {
        witnessing_workflow_trigger.trigger(&"incoming_countersigning");
    }
    Ok(())
}

type AgentsToNotify = Vec<AgentPubKey>;
type Ops = Vec<(DhtOpHash, ChainOp)>;
type SignedActions = Vec<SignedAction>;

impl WitnessingWorkspace {
    /// Create a new empty countersigning workspace.
    pub fn new() -> WitnessingWorkspace {
        Self {
            inner: Share::new(Default::default()),
        }
    }

    /// Put a single signers store entry op in the workspace.
    fn put(
        &self,
        entry_hash: EntryHash,
        op_hash: DhtOpHash,
        op: ChainOp,
        required_actions: Vec<ActionHash>,
        expires: Timestamp,
    ) {
        // Hash the action of this op.
        let action_hash = ActionHash::with_data_sync(&op.action());
        self.inner
            .share_mut(|i, _| {
                // Get the session at this entry or create an empty one.
                let session = i.pending.entry(entry_hash).or_default();

                // Insert the op into the session.
                session
                    .map
                    .insert(action_hash, (op_hash, op, required_actions));

                // Set the expires time.
                session.expires = Some(expires);
                Ok(())
            })
            // We don't close this share, so we can ignore this error.
            .ok();
    }

    fn get_complete_sessions(&self) -> Vec<(AgentsToNotify, Ops, SignedActions)> {
        let now = Timestamp::now();
        self.inner
            .share_mut(|i, _| {
                // Remove any expired sessions.
                i.pending.retain(|_, session| {
                    session.expires.as_ref().map(|e| now < *e).unwrap_or(false)
                });

                // Get all complete session's entry hashes.
                let complete: Vec<_> = i
                    .pending
                    .iter()
                    .filter_map(|(entry_hash, session)| {
                        // If all session required actions are contained in the map
                        // then the session is complete.
                        if session.map.values().all(|(_, _, required_hashes)| {
                            required_hashes
                                .iter()
                                .all(|hash| session.map.contains_key(hash))
                        }) {
                            Some(entry_hash.clone())
                        } else {
                            None
                        }
                    })
                    .collect();

                let mut ret = Vec::with_capacity(complete.len());

                // For each complete session remove from the pending map
                // and fold into the signed actions to send to the agents
                // and the ops to validate.
                for hash in complete {
                    if let Some(session) = i.pending.remove(&hash) {
                        let map = session.map;
                        let r = map.into_iter().fold(
                            (Vec::new(), Vec::new(), Vec::new()),
                            |(mut agents, mut ops, mut actions), (_, (op_hash, op, _))| {
                                let action = op.action();
                                let signature = op.signature().clone();
                                // Agents to notify.
                                agents.push(action.author().clone());
                                // Signed actions to notify them with.
                                actions.push(SignedAction::new(action, signature));
                                // Ops to validate.
                                ops.push((op_hash, op));
                                (agents, ops, actions)
                            },
                        );
                        ret.push(r);
                    }
                }
                Ok(ret)
            })
            .unwrap_or_default()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ::fixt::*;
    use holo_hash::fixt::DhtOpHashFixturator;
    use holo_hash::fixt::EntryHashFixturator;

    /// Test that a session of 5 actions is complete when the expiry time is in the future and all
    /// required actions are present.
    #[test]
    fn gets_complete_sessions() {
        let workspace = WitnessingWorkspace::new();

        // - Create the ops.
        let data = || {
            let op_hash = fixt!(DhtOpHash);
            let op = ChainOp::RegisterAddLink(
                Signature(vec![1; 64].try_into().unwrap()),
                fixt!(CreateLink),
            );
            let action = op.action();
            (op_hash, op, action)
        };
        let entry_hash = fixt!(EntryHash);
        let mut op_hashes = Vec::new();
        let mut ops = Vec::new();
        let mut required_actions = Vec::new();
        for _ in 0..5 {
            let (op_hash, op, action) = data();
            let action_hash = ActionHash::with_data_sync(&action);
            op_hashes.push(op_hash);
            ops.push(op);
            required_actions.push(action_hash);
        }

        // - Put the ops in the workspace with expiry set to one hour from now.
        for (op_h, op) in op_hashes.into_iter().zip(ops.into_iter()) {
            let expires = (Timestamp::now() + std::time::Duration::from_secs(60 * 60)).unwrap();
            workspace.put(
                entry_hash.clone(),
                op_h,
                op,
                required_actions.clone(),
                expires,
            );
        }

        // - Get all complete sessions.
        let r = workspace.get_complete_sessions();
        // - Expect we have one.
        assert_eq!(r.len(), 1);

        workspace
            .inner
            .share_mut(|i, _| {
                // - Check we have none pending.
                assert_eq!(i.pending.len(), 0);
                Ok(())
            })
            .unwrap();
    }

    /// Test that expired sessions are removed.
    #[test]
    fn expired_sessions_removed() {
        let workspace = WitnessingWorkspace::new();

        // - Create an op for a session that has expired in the past.
        let op_hash = fixt!(DhtOpHash);
        let op = ChainOp::RegisterAddLink(
            Signature(vec![1; 64].try_into().unwrap()),
            fixt!(CreateLink),
        );
        let action = op.action();
        let entry_hash = fixt!(EntryHash);
        let action_hash = ActionHash::with_data_sync(&action);
        let expires = (Timestamp::now() - std::time::Duration::from_secs(60 * 60)).unwrap();

        // - Add it to the workspace.
        workspace.put(entry_hash, op_hash, op, vec![action_hash], expires);
        let r = workspace.get_complete_sessions();

        // - Expect we have no complete sessions.
        assert_eq!(r.len(), 0);
        workspace
            .inner
            .share_mut(|i, _| {
                // - Check we have none pending.
                assert_eq!(i.pending.len(), 0);
                Ok(())
            })
            .unwrap();
    }
}



================================================
File: crates/holochain/src/core/workflow/app_validation_workflow/error.rs
================================================
use holochain_p2p::HolochainP2pError;
use holochain_types::prelude::*;
use thiserror::Error;

use crate::conductor::entry_def_store::error::EntryDefStoreError;
use crate::core::ribosome::error::RibosomeError;
use crate::core::validation::OutcomeOrError;
use crate::core::{SourceChainError, SysValidationError};
use crate::from_sub_error;

use super::types::Outcome;

#[derive(Error, Debug)]
pub enum AppValidationError {
    #[error(transparent)]
    CascadeError(#[from] holochain_cascade::error::CascadeError),
    #[error("Dna is missing {0:?}. Cannot validate without dna.")]
    DnaMissing(DnaHash),
    #[error(transparent)]
    DhtOpError(#[from] DhtOpError),
    #[error(transparent)]
    EntryDefStoreError(#[from] EntryDefStoreError),
    #[error(transparent)]
    HolochainP2pError(#[from] HolochainP2pError),
    #[error("Links cannot be called on multiple zomes for validation")]
    LinkMultipleZomes,
    #[error(transparent)]
    RibosomeError(#[from] RibosomeError),
    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),
    // Sys validation that requires calls to zomes happen during app validation
    #[error(transparent)]
    SysValidationError(#[from] SysValidationError),
    #[error("The app entry type {0:?} zome index was out of range")]
    ZomeIndex(ZomeIndex),
}

pub type AppValidationResult<T> = Result<T, AppValidationError>;
/// This is a way to return a success or immediately exit with an outcome
/// or immediately exit with an error
pub(super) type AppValidationOutcome<T> = Result<T, OutcomeOrError<Outcome, AppValidationError>>;

impl<T> From<AppValidationError> for OutcomeOrError<T, AppValidationError> {
    fn from(e: AppValidationError) -> Self {
        OutcomeOrError::Err(e)
    }
}
use holochain_cascade::error::CascadeError;
// These need to match the #[from] in AppValidationError
from_sub_error!(AppValidationError, RibosomeError);
from_sub_error!(AppValidationError, CascadeError);
from_sub_error!(AppValidationError, EntryDefStoreError);
from_sub_error!(AppValidationError, SourceChainError);
from_sub_error!(AppValidationError, DhtOpError);



================================================
File: crates/holochain/src/core/workflow/app_validation_workflow/get_zomes_to_invoke_tests.rs
================================================
use crate::conductor::space::TestSpace;
use crate::core::ribosome::{MockRibosomeT, ZomesToInvoke};
use crate::core::validation::OutcomeOrError;
use crate::core::workflow::app_validation_workflow::{
    get_zomes_to_invoke, put_validation_limbo, Outcome,
};
use crate::fixt::MetaLairClientFixturator;
use crate::sweettest::{SweetDnaFile, SweetInlineZomes};
use fixt::fixt;
use holo_hash::fixt::{ActionHashFixturator, AgentPubKeyFixturator, EntryHashFixturator};
use holo_hash::{HasHash, HashableContentExtSync};
use holochain_p2p::MockHolochainP2pDnaT;
use holochain_state::host_fn_workspace::HostFnWorkspaceRead;
use holochain_state::mutations::insert_op_dht;
use holochain_state::validation_db::ValidationStage;
use holochain_types::dht_op::{ChainOp, DhtOpHashed};
use holochain_types::rate_limit::{EntryRateWeight, RateWeight};
use holochain_zome_types::action::{AppEntryDef, Create, Delete, EntryType, Update, ZomeIndex};
use holochain_zome_types::fixt::{
    ActionFixturator, CreateFixturator, CreateLinkFixturator, DeleteLinkFixturator,
    EntryFixturator, SignatureFixturator, UpdateFixturator,
};
use holochain_zome_types::op::{
    EntryCreationAction, Op, RegisterAgentActivity, RegisterCreateLink, RegisterDelete,
    RegisterDeleteLink, RegisterUpdate, StoreEntry, StoreRecord,
};
use holochain_zome_types::record::{Record, RecordEntry, SignedActionHashed, SignedHashed};
use holochain_zome_types::timestamp::Timestamp;
use holochain_zome_types::Action;
use matches::assert_matches;
use std::sync::Arc;

#[tokio::test(flavor = "multi_thread")]
async fn register_agent_activity() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let ribosome = MockRibosomeT::new();

    let action = fixt!(Action);
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let op = Op::RegisterAgentActivity(RegisterAgentActivity {
        action: action.clone(),
        cached_entry: None,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_entry_create_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let create = Create {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: entry.clone().to_hash(),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let action = EntryCreationAction::Create(create);
    let action = SignedHashed::new_unchecked(action, fixt!(Signature));
    let op = Op::StoreEntry(StoreEntry {
        action: action.clone(),
        entry,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_entry_create_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let create = Create {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: entry.clone().to_hash(),
        entry_type: EntryType::CapClaim,
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let action = EntryCreationAction::Create(create);
    let action = SignedHashed::new_unchecked(action, fixt!(Signature));
    let op = Op::StoreEntry(StoreEntry {
        action: action.clone(),
        entry,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_entry_update_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let update = Update {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        entry_hash: entry.to_hash(),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        original_action_address: fixt!(ActionHash),
        original_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let action = EntryCreationAction::Update(update);
    let action = SignedHashed::new_unchecked(action, fixt!(Signature));
    let op = Op::StoreEntry(StoreEntry {
        action: action.clone(),
        entry,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_entry_update_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let update = Update {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        entry_hash: entry.to_hash(),
        entry_type: EntryType::AgentPubKey,
        original_action_address: fixt!(ActionHash),
        original_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let action = EntryCreationAction::Update(update);
    let action = SignedHashed::new_unchecked(action, fixt!(Signature));
    let op = Op::StoreEntry(StoreEntry {
        action: action.clone(),
        entry,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_create_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let action = Action::Create(Create {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: entry.clone().to_hash(),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_create_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let ribosome = MockRibosomeT::new();

    let action = Action::Create(Create {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: fixt!(AgentPubKey).into(),
        entry_type: EntryType::AgentPubKey,
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_create_wrong_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    // zome with index 1 does not exist
    let zome_index = ZomeIndex(1);
    let mut ribosome = MockRibosomeT::new();
    ribosome
        .expect_get_integrity_zome()
        .return_once(move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            None
        });

    let entry = fixt!(Entry);
    let action = Action::Create(Create {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: entry.clone().to_hash(),
        entry_type: EntryType::App(AppEntryDef {
            zome_index: 1.into(),
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome).await;
    assert_matches!(
        zomes_to_invoke,
        Err(OutcomeOrError::Outcome(Outcome::Rejected(_)))
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_create_link() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create_link = fixt!(CreateLink);
    create_link.zome_index = zome_index;
    let action = Action::CreateLink(create_link);
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_update_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create = fixt!(Create);
    create.entry_type = EntryType::App(AppEntryDef {
        zome_index,
        entry_index: 0.into(),
        visibility: Default::default(),
    });
    let action = Action::Update(Update {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: fixt!(EntryHash),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        original_action_address: create.to_hash(),
        original_entry_address: create.entry_hash.clone(),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_update_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let ribosome = MockRibosomeT::new();

    let mut create = fixt!(Create);
    create.entry_type = EntryType::CapGrant;
    let action = Action::Update(Update {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: fixt!(EntryHash),
        entry_type: EntryType::AgentPubKey,
        original_action_address: create.to_hash(),
        original_entry_address: create.entry_hash.clone(),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_update_of_update_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create = fixt!(Create);
    create.entry_type = EntryType::App(AppEntryDef {
        zome_index,
        entry_index: 0.into(),
        visibility: Default::default(),
    });
    let update = Action::Update(Update {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: fixt!(EntryHash),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        original_action_address: create.to_hash(),
        original_entry_address: create.entry_hash.clone(),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = Action::Update(Update {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        entry_hash: fixt!(EntryHash),
        entry_type: EntryType::App(AppEntryDef {
            zome_index: 0.into(),
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        original_action_address: update.to_hash(),
        original_entry_address: update.entry_hash().unwrap().clone(),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_delete_without_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create = fixt!(Create);
    create.entry_type = EntryType::App(AppEntryDef {
        zome_index,
        entry_index: 0.into(),
        visibility: Default::default(),
    });
    let original_action = Action::Create(create);
    let action = Action::Delete(Delete {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_delete_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let ribosome = MockRibosomeT::new();

    let mut create = fixt!(Create);
    create.entry_type = EntryType::CapGrant;
    let original_action = Action::Create(create);
    let action = Action::Delete(Delete {
        action_seq: 0,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    });
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn store_record_delete_link() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create_link = fixt!(CreateLink);
    create_link.zome_index = zome_index;
    let original_action = Action::CreateLink(create_link.clone());
    let mut delete_link = fixt!(DeleteLink);
    delete_link.link_add_address = original_action.to_hash();
    let action = Action::DeleteLink(delete_link);
    let action = SignedActionHashed::new_unchecked(action, fixt!(Signature));
    let record = Record::new(action.clone(), None);
    let op = Op::StoreRecord(StoreRecord { record });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_update_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let entry = fixt!(Entry);
    let update = Update {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        entry_hash: entry.to_hash(),
        entry_type: EntryType::App(AppEntryDef {
            zome_index,
            entry_index: 0.into(),
            visibility: Default::default(),
        }),
        original_action_address: fixt!(ActionHash),
        original_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let update = SignedHashed::new_unchecked(update, fixt!(Signature));
    let op = Op::RegisterUpdate(RegisterUpdate {
        update: update.clone(),
        new_entry: Some(entry),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_update_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, _, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let ribosome = MockRibosomeT::new();

    let entry = fixt!(Entry);
    let update = Update {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        entry_hash: entry.to_hash(),
        entry_type: EntryType::CapClaim,
        original_action_address: fixt!(ActionHash),
        original_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: EntryRateWeight::default(),
    };
    let update = SignedHashed::new_unchecked(update, fixt!(Signature));
    let op = Op::RegisterUpdate(RegisterUpdate {
        update: update.clone(),
        new_entry: Some(entry),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_delete_create_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create = fixt!(Create);
    create.entry_type = EntryType::App(AppEntryDef {
        zome_index,
        entry_index: 0.into(),
        visibility: Default::default(),
    });
    let original_action = Action::Create(create);
    let delete = Delete {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    };
    let delete = SignedHashed::new_unchecked(delete, fixt!(Signature));
    let op = Op::RegisterDelete(RegisterDelete {
        delete: delete.clone(),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_delete_create_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create = fixt!(Create);
    create.entry_type = EntryType::CapGrant;
    let original_action = Action::Create(create);
    let delete = Delete {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    };
    let delete = SignedHashed::new_unchecked(delete, fixt!(Signature));
    let op = Op::RegisterDelete(RegisterDelete {
        delete: delete.clone(),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_delete_update_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut update = fixt!(Update);
    update.entry_type = EntryType::App(AppEntryDef {
        zome_index,
        entry_index: 0.into(),
        visibility: Default::default(),
    });
    let original_action = Action::Update(update);
    let delete = Delete {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    };
    let delete = SignedHashed::new_unchecked(delete, fixt!(Signature));
    let op = Op::RegisterDelete(RegisterDelete {
        delete: delete.clone(),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_delete_update_non_app_entry() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut update = fixt!(Update);
    update.entry_type = EntryType::CapClaim;
    let original_action = Action::Update(update);
    let delete = Delete {
        action_seq: 1,
        author: fixt!(AgentPubKey),
        deletes_address: original_action.to_hash(),
        deletes_entry_address: fixt!(EntryHash),
        prev_action: fixt!(ActionHash),
        timestamp: Timestamp::now(),
        weight: RateWeight::default(),
    };
    let delete = SignedHashed::new_unchecked(delete, fixt!(Signature));
    let op = Op::RegisterDelete(RegisterDelete {
        delete: delete.clone(),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    // write original action to dht db
    let dht_op = DhtOpHashed::from_content_sync(ChainOp::StoreRecord(
        fixt!(Signature),
        original_action,
        RecordEntry::NA,
    ));
    test_space.space.dht_db.test_write(move |txn| {
        insert_op_dht(txn, &dht_op, None).unwrap();
        put_validation_limbo(txn, dht_op.as_hash(), ValidationStage::SysValidated).unwrap();
    });

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::AllIntegrity);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_create_link() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create_link = fixt!(CreateLink);
    create_link.zome_index = zome_index;
    let create_link = SignedHashed::new_unchecked(create_link, fixt!(Signature));
    let op = Op::RegisterCreateLink(RegisterCreateLink {
        create_link: create_link.clone(),
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}

#[tokio::test(flavor = "multi_thread")]
async fn register_delete_link() {
    let zomes = SweetInlineZomes::new(vec![], 0);
    let (dna_file, integrity_zomes, _) = SweetDnaFile::unique_from_inline_zomes(zomes).await;
    let zome = &integrity_zomes[0];
    let zome_index = ZomeIndex(0);
    let mut ribosome = MockRibosomeT::new();
    ribosome.expect_get_integrity_zome().return_once({
        let zome = zome.clone();
        move |index| {
            assert_eq!(index, &zome_index, "expected zome index {zome_index:?}");
            Some(zome)
        }
    });

    let mut create_link = fixt!(CreateLink);
    create_link.zome_index = zome_index;
    let delete_link = SignedHashed::new_unchecked(fixt!(DeleteLink), fixt!(Signature));
    let op = Op::RegisterDeleteLink(RegisterDeleteLink {
        create_link: create_link.clone(),
        delete_link,
    });

    let test_space = TestSpace::new(dna_file.dna_hash().clone());
    let workspace = HostFnWorkspaceRead::new(
        test_space
            .space
            .get_or_create_authored_db(fixt!(AgentPubKey))
            .unwrap()
            .into(),
        test_space.space.dht_db.clone().into(),
        test_space.space.dht_query_cache.clone(),
        test_space.space.cache_db.clone(),
        fixt!(MetaLairClient),
        None,
        Arc::new(dna_file.dna_def().clone()),
    )
    .await
    .unwrap();
    let network = Arc::new(MockHolochainP2pDnaT::new());

    let zomes_to_invoke = get_zomes_to_invoke(&op, &workspace, network, &ribosome)
        .await
        .unwrap();
    assert_matches!(zomes_to_invoke, ZomesToInvoke::OneIntegrity(z) if z.name == zome.name);
}



================================================
File: crates/holochain/src/core/workflow/app_validation_workflow/run_validation_callback_tests.rs
================================================
use crate::{
    conductor::space::TestSpace,
    core::{
        ribosome::{
            guest_callback::validate::ValidateInvocation, real_ribosome::RealRibosome,
            ZomesToInvoke,
        },
        workflow::app_validation_workflow::{run_validation_callback, Outcome},
    },
    fixt::MetaLairClientFixturator,
    sweettest::{SweetDnaFile, SweetInlineZomes},
};
use fixt::fixt;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::{ActionHash, AgentPubKey, HashableContentExtSync};
use holochain_p2p::{HolochainP2pDnaFixturator, MockHolochainP2pDnaT};
use holochain_sqlite::exports::FallibleIterator;
use holochain_state::{host_fn_workspace::HostFnWorkspaceRead, prelude::insert_op_cache};
use holochain_types::{
    chain::MustGetAgentActivityResponse,
    db::{DbKindCache, DbWrite},
    dht_op::{ChainOp, DhtOpHashed, WireOps},
    record::WireRecordOps,
};
use holochain_wasmer_host::module::ModuleCache;
use holochain_zome_types::{
    chain::{ChainFilter, ChainFilters, MustGetAgentActivityInput},
    dependencies::holochain_integrity_types::{UnresolvedDependencies, ValidateCallbackResult},
    entry::MustGetActionInput,
    fixt::{CreateFixturator, DeleteFixturator, SignatureFixturator},
    judged::Judged,
    op::{Op, RegisterAgentActivity, RegisterDelete},
    record::{SignedActionHashed, SignedHashed},
    validate::ValidationStatus,
    Action,
};
use matches::assert_matches;
use parking_lot::RwLock;
use std::{collections::HashSet, sync::Arc, time::Duration};

// test app validation with a must get action where the original action of
// a delete is not in the cache db and then added to it
#[tokio::test(flavor = "multi_thread")]
async fn validation_callback_must_get_action() {
    let zomes = SweetInlineZomes::new(vec![], 0).integrity_function("validate", {
        move |api, op: Op| {
            if let Op::RegisterDelete(RegisterDelete { delete }) = op {
                let result =
                    api.must_get_action(MustGetActionInput(delete.hashed.deletes_address.clone()));
                if result.is_ok() {
                    Ok(ValidateCallbackResult::Valid)
                } else {
                    Ok(ValidateCallbackResult::UnresolvedDependencies(
                        UnresolvedDependencies::Hashes(vec![delete
                            .hashed
                            .deletes_address
                            .clone()
                            .into()]),
                    ))
                }
            } else {
                unreachable!()
            }
        }
    });

    let TestCase {
        ribosome,
        test_space,
        workspace,
        zomes_to_invoke,
        alice,
        bob,
        ..
    } = TestCase::new(zomes).await;

    let network = Arc::new(fixt!(HolochainP2pDna));
    let dpki = None;

    // a create by alice
    let mut create = fixt!(Create);
    create.author = alice.clone();
    let create_action = Action::Create(create.clone());
    // a delete by bob that references alice's create
    let mut delete = fixt!(Delete);
    delete.author = bob.clone();
    delete.deletes_address = create_action.clone().to_hash();
    let delete_action_signed_hashed = SignedHashed::new_unchecked(delete.clone(), fixt!(Signature));
    let delete_action_op = Op::RegisterDelete(RegisterDelete {
        delete: delete_action_signed_hashed.clone(),
    });
    let invocation = ValidateInvocation::new(zomes_to_invoke, &delete_action_op).unwrap();

    // action has not been written to a database yet
    // validation should indicate it is awaiting create action hash
    let outcome = run_validation_callback(
        invocation.clone(),
        &ribosome,
        workspace.clone(),
        network.clone(),
        dpki.clone(),
        false,
    )
    .await
    .unwrap();
    assert_matches!(outcome, Outcome::AwaitingDeps(hashes) if hashes == vec![create_action.to_hash().into()]);

    // write action to be must got during validation to dht cache db
    let dht_op = ChainOp::RegisterAgentActivity(fixt!(Signature), create_action.clone());
    let dht_op_hashed = DhtOpHashed::from_content_sync(dht_op);
    test_space.space.cache_db.test_write(move |txn| {
        insert_op_cache(txn, &dht_op_hashed).unwrap();
    });

    // the same validation should now successfully validate the op
    let outcome = run_validation_callback(invocation, &ribosome, workspace, network, dpki, false)
        .await
        .unwrap();
    assert_matches!(outcome, Outcome::Accepted);
}

// same as previous test but this time awaiting the background task that
// fetches the missing original create of a delete
// instead of explicitly writing the missing op to the cache
#[tokio::test(flavor = "multi_thread")]
async fn validation_callback_awaiting_deps_hashes() {
    holochain_trace::test_run();

    let zomes = SweetInlineZomes::new(vec![], 0).integrity_function("validate", {
        move |api, op: Op| {
            if let Op::RegisterDelete(RegisterDelete { delete }) = op {
                let result =
                    api.must_get_action(MustGetActionInput(delete.hashed.deletes_address.clone()));
                if result.is_ok() {
                    Ok(ValidateCallbackResult::Valid)
                } else {
                    Ok(ValidateCallbackResult::UnresolvedDependencies(
                        UnresolvedDependencies::Hashes(vec![delete
                            .hashed
                            .deletes_address
                            .clone()
                            .into()]),
                    ))
                }
            } else {
                unreachable!()
            }
        }
    });

    let TestCase {
        zomes_to_invoke,
        ribosome,
        alice,
        bob,
        workspace,
        test_space,
    } = TestCase::new(zomes).await;

    // a create by alice
    let mut create = fixt!(Create);
    create.author = alice.clone();
    let create_action = Action::Create(create.clone());
    let create_action_signed_hashed =
