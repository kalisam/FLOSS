        }
        .boxed()
        .into()
    }

    fn peer_extrapolated_coverage(
        &self,
        space: std::sync::Arc<kitsune_p2p::KitsuneSpace>,
        dht_arc_set: holochain_p2p::dht_arc::DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>> {
        async move {
            let db = self.spaces.p2p_agents_db(&DnaHash::from_kitsune(&space))?;
            let coverage = db.p2p_extrapolated_coverage(dht_arc_set).await?;
            Ok(coverage)
        }
        .boxed()
        .into()
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn record_metrics(
        &self,
        space: std::sync::Arc<kitsune_p2p::KitsuneSpace>,
        records: Vec<MetricRecord>,
    ) -> KitsuneHostResult<()> {
        async move {
            let db = self.spaces.p2p_metrics_db(&DnaHash::from_kitsune(&space))?;
            Ok(db
                .write_async(move |txn| txn.p2p_log_metrics(records))
                .await?)
        }
        .boxed()
        .into()
    }

    fn get_agent_info_signed(
        &self,
        GetAgentInfoSignedEvt { space, agent }: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<Option<AgentInfoSigned>> {
        let dna_hash = DnaHash::from_kitsune(&space);
        let db = self.spaces.p2p_agents_db(&dna_hash);
        async move {
            Ok(super::p2p_agent_store::get_agent_info_signed(db?.into(), space, agent).await?)
        }
        .boxed()
        .into()
    }

    fn remove_agent_info_signed(
        &self,
        GetAgentInfoSignedEvt { space, agent }: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<bool> {
        let dna_hash = DnaHash::from_kitsune(&space);
        let db = self.spaces.p2p_agents_db(&dna_hash);
        async move { Ok(db?.p2p_remove_agent(&agent).await?) }
            .boxed()
            .into()
    }

    fn query_region_set(
        &self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        arq_set: ArqSet,
    ) -> KitsuneHostResult<holochain_p2p::dht::region_set::RegionSetLtcs> {
        let dna_hash = DnaHash::from_kitsune(&space);
        async move {
            let topology = self.get_topology(space.clone()).await?;
            let db = self.spaces.dht_db(&dna_hash)?;
            let region_set =
                query_region_set(db, topology.clone(), &self.strat, arq_set.into()).await?;
            Ok(region_set)
        }
        .boxed()
        .into()
    }

    fn query_size_limited_regions(
        &self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        size_limit: u32,
        regions: Vec<holochain_p2p::dht::region::Region>,
    ) -> KitsuneHostResult<Vec<holochain_p2p::dht::region::Region>> {
        let dna_hash = DnaHash::from_kitsune(&space);
        async move {
            let topology = self.get_topology(space).await?;
            let db = self.spaces.dht_db(&dna_hash)?;
            Ok(query_size_limited_regions::query_size_limited_regions(
                db, topology, regions, size_limit,
            )
            .await?)
        }
        .boxed()
        .into()
    }

    fn query_op_hashes_by_region(
        &self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        region: holochain_p2p::dht::region::RegionCoords,
    ) -> KitsuneHostResult<Vec<OpHashSized>> {
        let dna_hash = DnaHash::from_kitsune(&space);
        async move {
            let db = self.spaces.dht_db(&dna_hash)?;
            let topology = self.get_topology(space).await?;
            let bounds = region.to_bounds(&topology);
            Ok(query_region_op_hashes::query_region_op_hashes(db.clone(), bounds).await?)
        }
        .boxed()
        .into()
    }

    fn get_topology(&self, space: Arc<kitsune_p2p::KitsuneSpace>) -> KitsuneHostResult<Topology> {
        let dna_hash = DnaHash::from_kitsune(&space);
        let dna_def = self
            .ribosome_store
            .share_mut(|ds| ds.get_dna_def(&dna_hash))
            .ok_or(DnaError::DnaMissing(dna_hash));
        let cutoff = self
            .config
            .network
            .tuning_params
            .danger_gossip_recent_threshold();
        async move { Ok(dna_def?.topology(cutoff)) }.boxed().into()
    }

    fn op_hash(&self, op_data: KOpData) -> KitsuneHostResult<KOpHash> {
        async move {
            let op = holochain_p2p::WireDhtOpData::decode(op_data.0.clone())?;

            let op_hash = DhtOpHash::with_data_sync(&op.op_data).into_kitsune();

            Ok(op_hash)
        }
        .boxed()
        .into()
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    fn check_op_data(
        &self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        op_hash_list: Vec<KOpHash>,
        context: Option<kitsune_p2p::dependencies::kitsune_p2p_fetch::FetchContext>,
    ) -> KitsuneHostResult<Vec<bool>> {
        async move {
            let db = self.spaces.dht_db(&DnaHash::from_kitsune(&space))?;
            let results = db
                .write_async(move |txn| {
                    let mut out = Vec::new();
                    for op_hash in op_hash_list {
                        let op_hash = DhtOpHash::from_kitsune(&op_hash);
                        match txn.query_row(
                            "SELECT 1 FROM DhtOp WHERE hash = ?",
                            [&op_hash],
                            |_row| Ok(()),
                        ) {
                            Ok(_) => {
                                // might be tempted to remove this given we
                                // are currently reflecting publishes,
                                // but we still need this for the delegate
                                // broadcast case.
                                if let Some(context) = context {
                                    if context.has_request_validation_receipt() {
                                        txn.execute(
                                            "UPDATE DhtOp SET require_receipt = ? WHERE DhtOp.hash = ?",
                                            [&true as &dyn ToSql, &op_hash as &dyn ToSql],
                                        )?;
                                    }
                                }
                                out.push(true)
                            }
                            Err(_) => out.push(false),
                        }
                    }
                    holochain_sqlite::prelude::DatabaseResult::Ok(out)
                })
                .await?;

            Ok(results)
        }
        .boxed()
        .into()
    }

    fn handle_op_hash_received(
        &self,
        _space: &KitsuneSpace,
        _op_hash: &RoughSized<KOpHash>,
        _transfer_method: TransferMethod,
    ) {
    }

    fn handle_op_hash_transmitted(
        &self,
        _space: &KitsuneSpace,
        _op_hash: &RoughSized<KOpHash>,
        _transfer_method: TransferMethod,
    ) {
    }

    fn lair_tag(&self) -> Option<Arc<str>> {
        self.lair_tag.clone()
    }

    fn lair_client(&self) -> Option<lair_keystore_api::LairClient> {
        self.lair_client.clone()
    }
}



================================================
File: crates/holochain/src/conductor/logger.rs
================================================



================================================
File: crates/holochain/src/conductor/metrics.rs
================================================
use opentelemetry_api::{global::meter_with_version, metrics::*};

pub type P2pEventDurationMetric = Histogram<f64>;
pub type PostCommitDurationMetric = Histogram<f64>;

pub fn create_p2p_event_duration_metric() -> P2pEventDurationMetric {
    meter_with_version(
        "hc.conductor",
        None::<&'static str>,
        None::<&'static str>,
        Some(vec![]),
    )
    .f64_histogram("hc.conductor.p2p_event.duration")
    .with_unit(Unit::new("s"))
    .with_description("The time spent processing a p2p event")
    .init()
}

pub fn create_post_commit_duration_metric() -> P2pEventDurationMetric {
    meter_with_version(
        "hc.conductor",
        None::<&'static str>,
        None::<&'static str>,
        Some(vec![]),
    )
    .f64_histogram("hc.conductor.post_commit.duration")
    .with_unit(Unit::new("s"))
    .with_description("The time spent executing a post commit")
    .init()
}



================================================
File: crates/holochain/src/conductor/p2p_agent_store.rs
================================================
//! Queries for the P2pAgentStore db
use futures::StreamExt;
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_conductor_api::AgentInfoDump;
use holochain_conductor_api::P2pAgentsDump;
use holochain_p2p::dht::spacetime::Topology;
use holochain_p2p::dht::PeerStrat;
use holochain_p2p::dht::PeerView;
use holochain_p2p::dht_arc::DhtArc;
use holochain_p2p::kitsune_p2p::agent_store::AgentInfoSigned;
use holochain_p2p::AgentPubKeyExt;
use holochain_sqlite::prelude::*;
use holochain_state::prelude::*;
use holochain_state::query::StateQueryError;
use kitsune_p2p_types::bootstrap::AgentInfoPut;
use std::sync::Arc;
use thiserror::Error;

use super::error::ConductorResult;

/// A set of agent information that are to be committed
/// with any other active batches.
pub struct P2pBatch {
    /// Agent information to be committed.
    pub peer_data: Vec<AgentInfoSigned>,
    /// The result of this commit.
    pub result_sender: tokio::sync::oneshot::Sender<Result<Vec<AgentInfoPut>, P2pBatchError>>,
}

#[derive(Debug, Error)]
#[allow(missing_docs)]
pub enum P2pBatchError {
    #[error(transparent)]
    DatabaseError(#[from] DatabaseError),
    #[error("Batch transaction failed {0}")]
    BatchFailed(String),
}

/// Inject multiple agent info entries into the peer store
pub async fn inject_agent_infos<'iter, I: IntoIterator<Item = &'iter AgentInfoSigned> + Send>(
    env: DbWrite<DbKindP2pAgents>,
    iter: I,
) -> StateMutationResult<()> {
    p2p_put_all(&env, iter.into_iter()).await?;
    Ok(())
}

/// Inject multiple agent info entries into the peer store in batches.
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn p2p_put_all_batch(
    env: DbWrite<DbKindP2pAgents>,
    rx: tokio::sync::mpsc::Receiver<P2pBatch>,
) {
    let space = env.kind().0.clone();
    let stream = tokio_stream::wrappers::ReceiverStream::new(rx);
    let mut stream = stream.ready_chunks(100);
    while let Some(batch) = stream.next().await {
        let mut responses = Vec::with_capacity(batch.len());
        let (tx, rx) = tokio::sync::oneshot::channel();
        let result = env
            .write_async({
                let space = space.clone();
                move |txn| {
                    'batch: for P2pBatch {
                        peer_data: batch,
                        result_sender: response,
                    } in batch
                    {
                        let mut put_infos = Vec::with_capacity(batch.len());
                        for info in batch {
                            match p2p_put_single(space.clone(), txn, &info) {
                                Ok(put_info) => put_infos.push(put_info),
                                Err(e) => {
                                    responses.push((Err(e), response));
                                    continue 'batch;
                                }
                            }
                        }
                        responses.push((Ok(put_infos), response));
                    }
                    tx.send(responses).map_err(|_| {
                        DatabaseError::Other(anyhow::anyhow!(
                            "Failed to send response from background thread"
                        ))
                    })?;
                    DatabaseResult::Ok(())
                }
            })
            .await;
        let responses = rx.await;
        match result {
            Ok(_) => {
                if let Ok(responses) = responses {
                    for (result, response) in responses {
                        let _ = response.send(result.map_err(P2pBatchError::from));
                    }
                }
            }
            Err(e) => {
                if let Ok(responses) = responses {
                    for (_, response) in responses {
                        let _ = response.send(Err(P2pBatchError::BatchFailed(format!("{:?}", e))));
                    }
                }
            }
        }
    }
}

/// Helper function to get all the peer data from this conductor
pub async fn all_agent_infos(
    env: DbRead<DbKindP2pAgents>,
) -> StateQueryResult<Vec<AgentInfoSigned>> {
    Ok(env.p2p_list_agents().await?)
}

/// Helper function to get a single agent info
pub async fn get_single_agent_info(
    env: DbRead<DbKindP2pAgents>,
    _space: DnaHash,
    agent: AgentPubKey,
) -> StateQueryResult<Option<AgentInfoSigned>> {
    let agent = agent.to_kitsune();
    Ok(env.p2p_get_agent(&agent).await?)
}

/// Share all current agent infos known to all provided peer dbs with each other.
#[cfg(any(test, feature = "test_utils"))]
pub async fn exchange_peer_info(envs: Vec<DbWrite<DbKindP2pAgents>>) {
    use std::collections::HashSet;
    let mut all_infos: HashSet<AgentInfoSigned> = HashSet::new();

    for env in envs.iter() {
        let infos: HashSet<AgentInfoSigned> = all_agent_infos(env.clone().into())
            .await
            .unwrap()
            .into_iter()
            .collect();
        all_infos.extend(infos);
    }

    for env in envs.iter() {
        inject_agent_infos(env.clone(), all_infos.iter())
            .await
            .unwrap();
    }
}

/// Drop the specified agent keys from each conductor's peer db.
#[cfg(any(test, feature = "test_utils"))]
pub async fn forget_peer_info(
    all_envs: Vec<DbWrite<DbKindP2pAgents>>,
    agents_to_forget: impl IntoIterator<Item = &AgentPubKey>,
) {
    use kitsune_p2p_types::KAgent;

    let agents_to_forget: Vec<KAgent> = agents_to_forget
        .into_iter()
        .map(|a| a.to_kitsune())
        .collect();

    futures::future::join_all(all_envs.clone().into_iter().map(move |env| {
        let agents = agents_to_forget.clone();

        async move {
            for agent in agents.iter() {
                env.p2p_remove_agent(agent).await.unwrap();
            }
        }
    }))
    .await;
}

/// Interconnect provided pair of conductors via their peer store databases,
/// according to the connectivity matrix
#[cfg(any(test, feature = "test_utils"))]
pub async fn exchange_peer_info_sparse(
    envs: Vec<DbWrite<DbKindP2pAgents>>,
    connectivity: Vec<std::collections::HashSet<usize>>,
) {
    assert_eq!(envs.len(), connectivity.len());
    for (i, a) in envs.iter().enumerate() {
        let infos_a = all_agent_infos(a.clone().into()).await.unwrap();
        for (j, b) in envs.iter().enumerate() {
            if i == j {
                continue;
            }
            if !connectivity[j].contains(&i) {
                continue;
            }
            // let infos_b = all_agent_infos(b.clone().into()).await.unwrap();
            // inject_agent_infos(a.clone(), infos_b.iter()).await.unwrap();
            inject_agent_infos(b.clone(), infos_a.iter()).await.unwrap();
        }
    }
}

/// Reveal every agent in a single conductor to every agent in another.
#[cfg(any(test, feature = "test_utils"))]
pub async fn reveal_peer_info(
    observer_envs: Vec<DbWrite<DbKindP2pAgents>>,
    seen_envs: Vec<DbWrite<DbKindP2pAgents>>,
) {
    for observer in observer_envs.iter() {
        for seen in seen_envs.iter() {
            inject_agent_infos(
                observer.clone(),
                all_agent_infos(seen.clone().into()).await.unwrap().iter(),
            )
            .await
            .unwrap();
        }
    }
}

/// Get agent info for a single agent
pub async fn get_agent_info_signed(
    environ: DbRead<DbKindP2pAgents>,
    _kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
    kitsune_agent: Arc<kitsune_p2p::KitsuneAgent>,
) -> ConductorResult<Option<AgentInfoSigned>> {
    Ok(environ.p2p_get_agent(&kitsune_agent).await?)
}

/// Get all agent info for a single space
pub async fn list_all_agent_info(
    environ: DbRead<DbKindP2pAgents>,
    _kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
) -> ConductorResult<Vec<AgentInfoSigned>> {
    Ok(environ.p2p_list_agents().await?)
}

/// Get all agent info for a single space near a basis loc
pub async fn list_all_agent_info_signed_near_basis(
    environ: DbRead<DbKindP2pAgents>,
    _kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
    basis_loc: u32,
    limit: u32,
) -> ConductorResult<Vec<AgentInfoSigned>> {
    Ok(environ.p2p_query_near_basis(basis_loc, limit).await?)
}

/// Get the peer density an agent is currently seeing within
/// a given [`DhtArc`]
pub async fn query_peer_density(
    env: DbRead<DbKindP2pAgents>,
    topology: Topology,
    strat: PeerStrat,
    kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
    _dht_arc: DhtArc,
) -> ConductorResult<PeerView> {
    let now = now();
    let infos = env.p2p_list_agents().await?;
    let arqs: Vec<_> = infos
        .into_iter()
        .filter_map(|v| {
            if v.space == kitsune_space && !is_expired(now, &v) {
                Some(v.storage_arq)
            } else {
                None
            }
        })
        .collect();

    // contains is already checked in the iterator
    Ok(strat.view(topology, arqs.as_slice()))
}

fn now() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis() as u64
}

fn is_expired(now: u64, info: &AgentInfoSigned) -> bool {
    now >= info.expires_at_ms
}

/// Dump the agents currently in the peer store
pub async fn dump_state(
    env: DbRead<DbKindP2pAgents>,
    cell_id: Option<CellId>,
) -> StateQueryResult<P2pAgentsDump> {
    use std::fmt::Write;
    let cell_id = cell_id.map(|c| c.into_dna_and_agent()).map(|c| {
        (
            (c.0.clone(), holochain_p2p::space_holo_to_kit(c.0)),
            (c.1.clone(), holochain_p2p::agent_holo_to_kit(c.1)),
        )
    });
    let agent_infos = all_agent_infos(env).await?;
    let agent_infos = agent_infos.into_iter().filter(|a| match &cell_id {
        Some((s, _)) => s.1 == *a.space,
        None => true,
    });
    let mut this_agent_info = None;
    let mut peers = Vec::new();
    for info in agent_infos {
        let mut dump = String::new();

        use chrono::{DateTime, Duration, NaiveDateTime, Utc};
        let duration = Duration::try_milliseconds(info.signed_at_ms as i64).ok_or_else(|| {
            StateQueryError::Other("Agent info timestamp out of range".to_string())
        })?;
        let s = duration.num_seconds();
        let n = duration.clone().to_std().unwrap().subsec_nanos();
        // TODO FIXME
        #[allow(deprecated)]
        let dt = DateTime::<Utc>::from_utc(NaiveDateTime::from_timestamp(s, n), Utc);
        let duration = Duration::try_milliseconds(info.expires_at_ms as i64).ok_or_else(|| {
            StateQueryError::Other("Agent info timestamp out of range".to_string())
        })?;
        let s = duration.num_seconds();
        let n = duration.clone().to_std().unwrap().subsec_nanos();
        // TODO FIXME
        #[allow(deprecated)]
        let exp = DateTime::<Utc>::from_utc(NaiveDateTime::from_timestamp(s, n), Utc);
        let now = Utc::now();

        writeln!(dump, "signed at {}", dt).ok();
        writeln!(
            dump,
            "expires at {} in {}mins",
            exp,
            (exp - now).num_minutes()
        )
        .ok();
        writeln!(dump, "urls: {:?}", info.url_list).ok();
        let info = AgentInfoDump {
            kitsune_agent: info.agent.clone(),
            kitsune_space: info.space.clone(),
            dump,
        };
        match &cell_id {
            Some((s, a)) if *info.kitsune_agent == a.1 && *info.kitsune_space == s.1 => {
                this_agent_info = Some(info);
            }
            None | Some(_) => peers.push(info),
        }
    }

    Ok(P2pAgentsDump {
        this_agent_info,
        this_dna: cell_id.clone().map(|(s, _)| s),
        this_agent: cell_id.clone().map(|(_, a)| a),
        peers,
    })
}

#[cfg(test)]
mod tests {
    use super::*;
    use ::fixt::prelude::*;
    use holochain_state::test_utils::test_p2p_agents_db;
    use kitsune_p2p_types::fixt::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_store_agent_info_signed() {
        holochain_trace::test_run();

        let test_db = test_p2p_agents_db();
        let db = test_db.to_db();

        let agent_info_signed = fixt!(AgentInfoSigned, Predictable);

        p2p_put(&db, &agent_info_signed).await.unwrap();

        let ret = db.p2p_get_agent(&agent_info_signed.agent).await.unwrap();

        assert_eq!(ret, Some(agent_info_signed));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn add_agent_info_to_db() {
        holochain_trace::test_run();
        let t_db = test_p2p_agents_db();
        let db = t_db.to_db();

        // - Check no data in the store to start
        let count = db.p2p_count_agents().await.unwrap();

        assert_eq!(count, 0);

        // - Get agents and space
        let agent_infos = AgentInfoSignedFixturator::new(Unpredictable)
            .take(5)
            .collect::<Vec<_>>();

        let mut expect = agent_infos.clone();
        expect.sort_by(|a, b| a.agent.partial_cmp(&b.agent).unwrap());

        // - Inject some data
        inject_agent_infos(db.clone(), agent_infos.iter())
            .await
            .unwrap();

        // - Check the same data is now in the store
        let mut agents = all_agent_infos(db.clone().into()).await.unwrap();

        agents.sort_by(|a, b| a.agent.partial_cmp(&b.agent).unwrap());

        assert_eq!(expect, agents);
    }
}



================================================
File: crates/holochain/src/conductor/paths.rs
================================================
//! Defines default paths for various resources

pub use holochain_conductor_api::config::conductor::paths::*;



================================================
File: crates/holochain/src/conductor/ribosome_store.rs
================================================
use holochain_types::{prelude::*, share::RwShare};
use holochain_zome_types::entry_def::EntryDef;
use std::collections::HashMap;

use crate::core::ribosome::{real_ribosome::RealRibosome, RibosomeT};

#[derive(Default)]
pub struct RibosomeStore {
    ribosomes: HashMap<DnaHash, RealRibosome>,
    entry_defs: HashMap<EntryDefBufferKey, EntryDef>,
}

impl RibosomeStore {
    pub fn new() -> RwShare<Self> {
        RwShare::new(RibosomeStore {
            ribosomes: HashMap::new(),
            entry_defs: HashMap::new(),
        })
    }

    pub fn add_ribosome(&mut self, ribosome: RealRibosome) {
        self.ribosomes.insert(ribosome.dna_hash().clone(), ribosome);
    }

    pub fn add_ribosomes<T: IntoIterator<Item = (DnaHash, RealRibosome)> + 'static>(
        &mut self,
        ribosomes: T,
    ) {
        self.ribosomes.extend(ribosomes);
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub fn list(&self) -> Vec<DnaHash> {
        self.ribosomes.keys().cloned().collect()
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub fn get_dna_def(&self, hash: &DnaHash) -> Option<DnaDef> {
        self.ribosomes
            .get(hash)
            .map(|d| d.dna_def().clone().into_content())
    }

    // TODO: use Arc, eliminate cloning
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    pub fn get_dna_file(&self, hash: &DnaHash) -> Option<DnaFile> {
        self.ribosomes.get(hash).map(|r| r.dna_file().clone())
    }

    pub fn get_ribosome(&self, hash: &DnaHash) -> Option<RealRibosome> {
        self.ribosomes.get(hash).cloned()
    }

    pub fn add_entry_def(&mut self, k: EntryDefBufferKey, entry_def: EntryDef) {
        self.entry_defs.insert(k, entry_def);
    }

    pub fn add_entry_defs<T: IntoIterator<Item = (EntryDefBufferKey, EntryDef)> + 'static>(
        &mut self,
        entry_defs: T,
    ) {
        self.entry_defs.extend(entry_defs);
    }

    pub fn get_entry_def(&self, k: &EntryDefBufferKey) -> Option<EntryDef> {
        self.entry_defs.get(k).cloned()
    }
}

impl DnaStore for RibosomeStore {
    fn get_dna(&self, dna_hash: &DnaHash) -> Option<DnaFile> {
        self.get_dna_file(dna_hash)
    }
}



================================================
File: crates/holochain/src/conductor/space.rs
================================================
//! This module contains data and functions for running operations
//! at the level of a [`DnaHash`] space.
//! Multiple [`Cell`](crate::conductor::Cell)'s could share the same space.
use std::{
    cell::Cell,
    collections::{hash_map, HashMap},
    sync::Arc,
    time::Duration,
};

use super::{
    conductor::RwShare,
    error::ConductorResult,
    p2p_agent_store::{self, P2pBatch},
};
use crate::conductor::{error::ConductorError, state::ConductorState};
use crate::core::workflow::countersigning_workflow::CountersigningWorkspace;
use crate::core::{
    queue_consumer::QueueConsumerMap,
    workflow::{
        incoming_dht_ops_workflow::{
            incoming_dht_ops_workflow, IncomingOpHashes, IncomingOpsBatch,
        },
        witnessing_workflow::{receive_incoming_countersigning_ops, WitnessingWorkspace},
    },
};
use holo_hash::{AgentPubKey, DhtOpHash, DnaHash};
use holochain_conductor_api::conductor::paths::DatabasesRootPath;
use holochain_conductor_api::conductor::ConductorConfig;
use holochain_keystore::MetaLairClient;
use holochain_p2p::AgentPubKeyExt;
use holochain_p2p::DnaHashExt;
use holochain_p2p::{
    dht::region::RegionBounds,
    dht_arc::{DhtArcRange, DhtArcSet},
    event::FetchOpDataQuery,
};
use holochain_sqlite::prelude::{
    DatabaseResult, DbKey, DbKindAuthored, DbKindCache, DbKindConductor, DbKindDht,
    DbKindP2pAgents, DbKindP2pMetrics, DbKindWasm, DbSyncLevel, DbSyncStrategy, DbWrite,
    PoolConfig, ReadAccess,
};
use holochain_state::{
    host_fn_workspace::SourceChainWorkspace,
    mutations,
    prelude::*,
    query::{map_sql_dht_op_common, StateQueryError},
};
use holochain_util::timed;
use kitsune_p2p::event::{TimeWindow, TimeWindowInclusive};
use kitsune_p2p_block::NodeId;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use rusqlite::{named_params, OptionalExtension};
use std::convert::TryInto;
use std::path::PathBuf;

#[derive(Clone)]
/// This is the set of all current
/// [`DnaHash`] spaces for all cells
/// installed on this conductor.
pub struct Spaces {
    map: RwShare<HashMap<DnaHash, Space>>,
    pub(crate) db_dir: Arc<DatabasesRootPath>,
    pub(crate) config: Arc<ConductorConfig>,
    /// The map of running queue consumer workflows.
    pub(crate) queue_consumer_map: QueueConsumerMap,
    pub(crate) conductor_db: DbWrite<DbKindConductor>,
    pub(crate) wasm_db: DbWrite<DbKindWasm>,
    db_key: DbKey,
}

#[derive(Clone)]
/// This is the set of data required at the
/// [`DnaHash`] space level.
/// All cells in the same [`DnaHash`] space
/// will share these.
pub struct Space {
    /// The dna hash for this space.
    pub dna_hash: Arc<DnaHash>,

    /// The caches databases. These are shared across cells.
    /// There is one per unique Dna.
    pub cache_db: DbWrite<DbKindCache>,

    /// The conductor database. There is only one of these.
    pub conductor_db: DbWrite<DbKindConductor>,

    /// The authored databases. These are per-agent.
    /// There is one per unique combination of Dna and AgentPubKey.
    pub authored_dbs: Arc<parking_lot::Mutex<HashMap<AgentPubKey, DbWrite<DbKindAuthored>>>>,

    /// The dht databases. These are shared across cells.
    /// There is one per unique Dna.
    pub dht_db: DbWrite<DbKindDht>,

    /// The database for storing AgentInfoSigned
    pub p2p_agents_db: DbWrite<DbKindP2pAgents>,

    /// The database for storing p2p MetricDatum(s)
    pub p2p_metrics_db: DbWrite<DbKindP2pMetrics>,

    /// The batch sender for writes to the p2p database.
    pub p2p_batch_sender: tokio::sync::mpsc::Sender<P2pBatch>,

    /// A cache for slow database queries.
    pub dht_query_cache: DhtDbQueryCache,

    /// Countersigning workspace for session state.
    pub countersigning_workspaces:
        Arc<parking_lot::Mutex<HashMap<CellId, Arc<CountersigningWorkspace>>>>,

    /// Witnessing workspace that is shared across this cell.
    pub witnessing_workspace: WitnessingWorkspace,

    /// Incoming op hashes that are queued for processing.
    pub incoming_op_hashes: IncomingOpHashes,

    /// Incoming ops batch for this space.
    pub incoming_ops_batch: IncomingOpsBatch,

    root_db_dir: Arc<PathBuf>,
    db_key: DbKey,
}

/// Test spaces
#[cfg(test)]
pub struct TestSpaces {
    /// The spaces
    pub spaces: Spaces,
    /// The test spaces
    pub test_spaces: HashMap<DnaHash, TestSpace>,
    /// The queue consumer map
    pub queue_consumer_map: QueueConsumerMap,
}

/// A test space
#[cfg(test)]
pub struct TestSpace {
    /// The space
    pub space: Space,
    _temp_dir: tempfile::TempDir,
}

thread_local!(static DANGER_PRINT_DB_SECRETS: Cell<bool> = const { Cell::new(false) });

/// WARNING!! DANGER!! This exposes your database decryption secrets!
/// Print the database decryption secrets to stderr.
/// With these PRAGMA commands, you'll be able to run sqlcipher
/// directly to manipulate holochain databases.
pub fn set_danger_print_db_secrets(v: bool) {
    DANGER_PRINT_DB_SECRETS.set(v);
}

impl Spaces {
    /// Create a new empty set of [`DnaHash`] spaces.
    pub async fn new(
        config: Arc<ConductorConfig>,
        passphrase: sodoken::BufRead,
    ) -> ConductorResult<Self> {
        // do this before any awaits
        let danger_print_db_secrets = DANGER_PRINT_DB_SECRETS.get();
        // clear the value
        DANGER_PRINT_DB_SECRETS.set(false);

        let root_db_dir: DatabasesRootPath = config
            .data_root_path
            .clone()
            .ok_or(ConductorError::NoDataRootPath)?
            .try_into()?;

        let db_key_path = root_db_dir.join("db.key");
        let db_key = match tokio::fs::read_to_string(db_key_path.clone()).await {
            Ok(locked) => DbKey::load(locked, passphrase).await?,
            Err(_) => {
                let db_key = DbKey::generate(passphrase).await?;
                tokio::fs::write(db_key_path, db_key.locked.clone()).await?;
                db_key
            }
        };

        if danger_print_db_secrets {
            eprintln!(
                "--beg-db-secrets--{}--end-db-secrets--",
                &String::from_utf8_lossy(&db_key.unlocked.read_lock())
            );
        }

        let db_sync_strategy = config.db_sync_strategy;
        let db_sync_level = match db_sync_strategy {
            DbSyncStrategy::Fast => DbSyncLevel::Off,
            DbSyncStrategy::Resilient => DbSyncLevel::Normal,
        };

        let (conductor_db, wasm_db) = tokio::task::block_in_place(|| {
            let conductor_db = DbWrite::open_with_pool_config(
                root_db_dir.as_ref(),
                DbKindConductor,
                PoolConfig {
                    synchronous_level: db_sync_level,
                    key: db_key.clone(),
                },
            )?;
            let wasm_db = DbWrite::open_with_pool_config(
                root_db_dir.as_ref(),
                DbKindWasm,
                PoolConfig {
                    synchronous_level: db_sync_level,
                    key: db_key.clone(),
                },
            )?;
            ConductorResult::Ok((conductor_db, wasm_db))
        })?;

        Ok(Spaces {
            map: RwShare::new(HashMap::new()),
            db_dir: Arc::new(root_db_dir),
            config,
            queue_consumer_map: QueueConsumerMap::new(),
            conductor_db,
            wasm_db,
            db_key,
        })
    }

    /// Block some target.
    pub async fn block(&self, input: Block) -> DatabaseResult<()> {
        holochain_state::block::block(&self.conductor_db, input).await
    }

    /// Unblock some target.
    pub async fn unblock(&self, input: Block) -> DatabaseResult<()> {
        holochain_state::block::unblock(&self.conductor_db, input).await
    }

    async fn node_agents_in_spaces(
        &self,
        node_id: NodeId,
        dnas: Vec<DnaHash>,
    ) -> DatabaseResult<Vec<CellId>> {
        let mut agent_lists: Vec<Vec<AgentInfoSigned>> = vec![];
        for dna in dnas {
            // @todo join_all for these awaits
            agent_lists.push(self.p2p_agents_db(&dna)?.p2p_list_agents().await?);
        }

        Ok(agent_lists
            .into_iter()
            .flatten()
            .filter(|agent| {
                agent.url_list.iter().any(|url| {
                    kitsune_p2p_types::tx_utils::ProxyUrl::from(url.as_str())
                        .digest()
                        .map(|u| u.0 == *node_id)
                        .unwrap_or(false)
                })
            })
            .map(|agent_info| {
                CellId::new(
                    DnaHash::from_kitsune(&agent_info.space),
                    AgentPubKey::from_kitsune(&agent_info.agent),
                )
            })
            .collect())
    }

    /// Check if some target is blocked.
    pub async fn is_blocked(
        &self,
        target_id: BlockTargetId,
        timestamp: Timestamp,
    ) -> DatabaseResult<bool> {
        let cell_ids = match &target_id {
            BlockTargetId::Cell(cell_id) => vec![cell_id.to_owned()],
            BlockTargetId::NodeDna(node_id, dna_hash) => {
                self.node_agents_in_spaces((*node_id).clone(), vec![dna_hash.clone()])
                    .await?
            }
            BlockTargetId::Node(node_id) => {
                self.node_agents_in_spaces(
                    (*node_id).clone(),
                    self.map
                        .share_ref(|m| m.keys().cloned().collect::<Vec<DnaHash>>()),
                )
                .await?
            }
            // @todo
            BlockTargetId::Ip(_) => {
                vec![]
            }
        };

        // If node_agents_in_spaces is not yet initialized, we can't know anything about
        // which cells are blocked, so avoid the race condition by returning false
        // TODO: actually fix the preflight, because this could be a loophole for someone
        //       to evade a block in some circumstances
        if cell_ids.is_empty() {
            return Ok(false);
        }

        self.conductor_db
            .read_async(move |txn| {
                Ok(
                    // If the target_id is directly blocked then we always return true.
                    holochain_state::block::query_is_blocked(txn, target_id, timestamp)?
            // If there are zero unblocked cells then return true.
            || {
                let mut all_blocked_cell_ids = true;
                for cell_id in cell_ids {
                    if !holochain_state::block::query_is_blocked(
                        txn,
                        BlockTargetId::Cell(cell_id), timestamp)? {
                            all_blocked_cell_ids = false;
                            break;
                        }
                }
                all_blocked_cell_ids
            },
                )
            })
            .await
    }

    /// Get the holochain conductor state
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn get_state(&self) -> ConductorResult<ConductorState> {
        timed!([1, 10, 1000], "get_state", {
            match query_conductor_state(&self.conductor_db).await? {
                Some(state) => Ok(state),
                // update_state will again try to read the state. It's a little
                // inefficient in the infrequent case where we haven't saved the
                // state yet, but more atomic, so worth it.
                None => self.update_state(Ok).await,
            }
        })
    }

    /// Update the internal state with a pure function mapping old state to new
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn update_state<F>(&self, f: F) -> ConductorResult<ConductorState>
    where
        F: Send + FnOnce(ConductorState) -> ConductorResult<ConductorState> + 'static,
    {
        let (state, _) = self.update_state_prime(|s| Ok((f(s)?, ()))).await?;
        Ok(state)
    }

    /// Update the internal state with a pure function mapping old state to new,
    /// which may also produce an output value which will be the output of
    /// this function
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn update_state_prime<F, O>(&self, f: F) -> ConductorResult<(ConductorState, O)>
    where
        F: FnOnce(ConductorState) -> ConductorResult<(ConductorState, O)> + Send + 'static,
        O: Send + 'static,
    {
        timed!([1, 10, 1000], "update_state_prime", {
            self.conductor_db
                .write_async(move |txn| {
                    let state = txn
                        .query_row("SELECT blob FROM ConductorState WHERE id = 1", [], |row| {
                            row.get("blob")
                        })
                        .optional()?;
                    let state = match state {
                        Some(state) => from_blob(state)?,
                        None => ConductorState::default(),
                    };
                    let (new_state, output) = f(state)?;
                    mutations::insert_conductor_state(txn, (&new_state).try_into()?)?;
                    Result::<_, ConductorError>::Ok((new_state, output))
                })
                .await
        })
    }

    /// Get something from every space
    pub fn get_from_spaces<R, F: FnMut(&Space) -> R>(&self, f: F) -> Vec<R> {
        self.map
            .share_ref(|spaces| spaces.values().map(f).collect())
    }

    /// Get the space if it exists or create it if it doesn't.
    pub fn get_or_create_space(&self, dna_hash: &DnaHash) -> DatabaseResult<Space> {
        self.get_or_create_space_ref(dna_hash, |s| s.clone())
    }

    fn get_or_create_space_ref<F, R>(&self, dna_hash: &DnaHash, f: F) -> DatabaseResult<R>
    where
        F: Fn(&Space) -> R,
    {
        match self.map.share_ref(|spaces| spaces.get(dna_hash).map(&f)) {
            Some(r) => Ok(r),
            None => self
                .map
                .share_mut(|spaces| match spaces.entry(dna_hash.clone()) {
                    hash_map::Entry::Occupied(entry) => Ok(f(entry.get())),
                    hash_map::Entry::Vacant(entry) => {
                        let space = Space::new(
                            Arc::new(dna_hash.clone()),
                            self.db_dir.to_path_buf(),
                            self.config.db_sync_strategy,
                            self.db_key.clone(),
                        )?;

                        let r = f(&space);
                        entry.insert(space);
                        Ok(r)
                    }
                }),
        }
    }

    /// Get the cache database (this will create the space if it doesn't already exist).
    pub fn cache(&self, dna_hash: &DnaHash) -> DatabaseResult<DbWrite<DbKindCache>> {
        self.get_or_create_space_ref(dna_hash, |space| space.cache_db.clone())
    }

    /// Get or create the authored database for this author (this will create the space if it doesn't already exist).
    pub fn get_or_create_authored_db(
        &self,
        dna_hash: &DnaHash,
        author: AgentPubKey,
    ) -> DatabaseResult<DbWrite<DbKindAuthored>> {
        self.get_or_create_space_ref(dna_hash, |space| {
            space.get_or_create_authored_db(author.clone())
        })?
    }

    /// Get all the authored databases for this space (this will create the space if it doesn't already exist).
    pub fn get_all_authored_dbs(
        &self,
        dna_hash: &DnaHash,
    ) -> DatabaseResult<Vec<DbWrite<DbKindAuthored>>> {
        self.get_or_create_space_ref(dna_hash, |space| space.get_all_authored_dbs())
    }

    /// Get the dht database (this will create the space if it doesn't already exist).
    pub fn dht_db(&self, dna_hash: &DnaHash) -> DatabaseResult<DbWrite<DbKindDht>> {
        self.get_or_create_space_ref(dna_hash, |space| space.dht_db.clone())
    }

    /// Get the peer database (this will create the space if it doesn't already exist).
    pub fn p2p_agents_db(&self, dna_hash: &DnaHash) -> DatabaseResult<DbWrite<DbKindP2pAgents>> {
        self.get_or_create_space_ref(dna_hash, |space| space.p2p_agents_db.clone())
    }

    /// Get the peer database (this will create the space if it doesn't already exist).
    pub fn p2p_metrics_db(&self, dna_hash: &DnaHash) -> DatabaseResult<DbWrite<DbKindP2pMetrics>> {
        self.get_or_create_space_ref(dna_hash, |space| space.p2p_metrics_db.clone())
    }

    /// Get the batch sender (this will create the space if it doesn't already exist).
    pub fn p2p_batch_sender(
        &self,
        dna_hash: &DnaHash,
    ) -> DatabaseResult<tokio::sync::mpsc::Sender<P2pBatch>> {
        self.get_or_create_space_ref(dna_hash, |space| space.p2p_batch_sender.clone())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    /// the network module is requesting a list of dht op hashes
    /// Get the [`DhtOpHash`]es and authored timestamps for a given time window.
    pub async fn handle_query_op_hashes(
        &self,
        dna_hash: &DnaHash,
        dht_arc_set: DhtArcSet,
        window: TimeWindow,
        max_ops: usize,
        include_limbo: bool,
    ) -> ConductorResult<Option<(Vec<DhtOpHash>, TimeWindowInclusive)>> {
        // The exclusive window bounds.
        let start = window.start;
        let end = window.end;
        let max_ops: u32 = max_ops.try_into().unwrap_or(u32::MAX);

        let db = self.dht_db(dna_hash)?;

        let include_limbo = if include_limbo {
            "\n"
        } else {
            "AND DhtOp.when_integrated IS NOT NULL\n"
        };

        let intervals = dht_arc_set.intervals();
        let sql = if let Some(DhtArcRange::Full) = intervals.first() {
            format!(
                "{} {} {}",
                holochain_sqlite::sql::sql_cell::FETCH_OP_HASHES_P1,
                include_limbo,
                holochain_sqlite::sql::sql_cell::FETCH_OP_HASHES_P2,
            )
        } else {
            let sql_ranges = intervals
                .into_iter()
                .filter(|i| matches!(i, &DhtArcRange::Bounded(_, _)))
                .map(|interval| match interval {
                    DhtArcRange::Bounded(start_loc, end_loc) => {
                        if start_loc <= end_loc {
                            format!(
                                "AND storage_center_loc >= {} AND storage_center_loc <= {} \n ",
                                start_loc, end_loc
                            )
                        } else {
                            format!(
                                "AND (storage_center_loc < {} OR storage_center_loc > {}) \n ",
                                end_loc, start_loc
                            )
                        }
                    }
                    _ => unreachable!(),
                })
                .collect::<String>();
            format!(
                "{} {} {} {}",
                holochain_sqlite::sql::sql_cell::FETCH_OP_HASHES_P1,
                include_limbo,
                sql_ranges,
                holochain_sqlite::sql::sql_cell::FETCH_OP_HASHES_P2,
            )
        };
        let results = db
            .read_async(move |txn| {
                let mut stmt = txn.prepare_cached(&sql)?;
                let hashes = stmt
                    .query_map(
                        named_params! {
                            ":from": start,
                            ":to": end,
                            ":limit": max_ops,
                        },
                        |row| row.get("hash"),
                    )?
                    .collect::<rusqlite::Result<Vec<DhtOpHash>>>()?;
                let range = hashes.first().and_then(|s| hashes.last().map(|e| (s, e)));
                match range {
                    Some((start, end)) => {
                        let start: Timestamp = txn.query_row(
                            "SELECT authored_timestamp FROM DhtOp WHERE hash = ?",
                            [start],
                            |row| row.get(0),
                        )?;
                        let end: Timestamp = txn.query_row(
                            "SELECT authored_timestamp FROM DhtOp WHERE hash = ?",
                            [end],
                            |row| row.get(0),
                        )?;
                        DatabaseResult::Ok(Some((
                            hashes,
                            kitsune_p2p_types::Timestamp::from_micros(start.0)
                                ..=kitsune_p2p_types::Timestamp::from_micros(end.0),
                        )))
                    }
                    None => Ok(None),
                }
            })
            .await?;

        Ok(results)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, query)))]
    /// The network module is requesting the content for dht ops
    pub async fn handle_fetch_op_data(
        &self,
        dna_hash: &DnaHash,
        query: FetchOpDataQuery,
    ) -> ConductorResult<Vec<(holo_hash::DhtOpHash, holochain_types::dht_op::DhtOp)>> {
        match query {
            FetchOpDataQuery::Hashes {
                op_hash_list,
                include_limbo,
            } => {
                self.handle_fetch_op_data_by_hashes(dna_hash, op_hash_list, include_limbo)
                    .await
            }
            FetchOpDataQuery::Regions(regions) => {
                self.handle_fetch_op_data_by_regions(dna_hash, regions)
                    .await
            }
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, regions)))]
    /// The network module is requesting the content for dht ops
    pub async fn handle_fetch_op_data_by_regions(
        &self,
        dna_hash: &DnaHash,
        regions: Vec<RegionBounds>,
    ) -> ConductorResult<Vec<(holo_hash::DhtOpHash, holochain_types::dht_op::DhtOp)>> {
        let sql = holochain_sqlite::sql::sql_cell::FETCH_OPS_BY_REGION;
        Ok(self
            .dht_db(dna_hash)?
            .read_async(move |txn| {
                let mut stmt = txn.prepare_cached(sql).map_err(StateQueryError::from)?;
                let results = regions
                    .into_iter()
                    .map(|bounds| {
                        let (x0, x1) = bounds.x;
                        let (t0, t1) = bounds.t;
                        stmt.query_and_then(
                            named_params! {
                                ":storage_start_loc": x0,
                                ":storage_end_loc": x1,
                                ":timestamp_min": t0,
                                ":timestamp_max": t1,
                            },
                            |row| {
                                let hash: DhtOpHash =
                                    row.get("hash").map_err(StateQueryError::from)?;
                                Ok(map_sql_dht_op_common(false, false, "type", row)?
                                    .map(|op| (hash, op)))
                            },
                        )
                        .map_err(StateQueryError::from)?
                        .collect::<Result<Vec<Option<_>>, StateQueryError>>()
                    })
                    .collect::<Result<Vec<Vec<Option<_>>>, _>>()?
                    .into_iter()
                    .flatten()
                    .flatten()
                    .collect();
                StateQueryResult::Ok(results)
            })
            .await?)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, op_hashes)))]
    /// The network module is requesting the content for dht ops
    pub async fn handle_fetch_op_data_by_hashes(
        &self,
        dna_hash: &DnaHash,
        op_hashes: Vec<holo_hash::DhtOpHash>,
        include_limbo: bool,
    ) -> ConductorResult<Vec<(holo_hash::DhtOpHash, holochain_types::dht_op::DhtOp)>> {
        let mut sql = "
            SELECT DhtOp.hash, DhtOp.type AS dht_type,
            Action.blob AS action_blob, 
            Action.author as author,
            Entry.blob AS entry_blob
            FROM DHtOp
            JOIN Action ON DhtOp.action_hash = Action.hash
            LEFT JOIN Entry ON Action.entry_hash = Entry.hash
            WHERE
            DhtOp.hash = ?
        "
        .to_string();

        if !include_limbo {
            sql.push_str(
                "
                AND
                DhtOp.when_integrated IS NOT NULL
            ",
            );
        }

        let db = self.dht_db(dna_hash)?;
        let results = db
            .read_async(move |txn| {
                let mut out = Vec::with_capacity(op_hashes.len());
                for hash in op_hashes {
                    let mut stmt = txn.prepare_cached(&sql)?;
                    let mut rows = stmt.query([hash])?;
                    if let Some(row) = rows.next()? {
                        let op = holochain_state::query::map_sql_dht_op(false, "dht_type", row)?;
                        let hash: DhtOpHash = row.get("hash")?;
                        out.push((hash, op));
                    } else {
                        return Err(holochain_state::query::StateQueryError::Sql(
                            rusqlite::Error::QueryReturnedNoRows,
                        ));
                    }
                }
                StateQueryResult::Ok(out)
            })
            .await?;
        Ok(results)
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, request_validation_receipt, ops))
    )]
    /// we are receiving a "publish" event from the network
    pub async fn handle_publish(
        &self,
        dna_hash: &DnaHash,
        request_validation_receipt: bool,
        countersigning_session: bool,
        ops: Vec<DhtOp>,
    ) -> ConductorResult<()> {
        // If this is a countersigning session then
        // send it to the countersigning workflow otherwise
        // send it to the incoming ops workflow.
        if countersigning_session {
            use futures::StreamExt;
            let ops = futures::stream::iter(ops.into_iter().filter_map(|op| match op {
                DhtOp::ChainOp(op) => {
                    let hash = DhtOpHash::with_data_sync(&*op);
                    Some((hash, *op))
                }
                _ => {
                    tracing::warn!(
                        ?op,
                        "Invalid DhtOp in countersigning session, only ChainOps will be handled"
                    );
                    None
                }
            }))
            .collect()
            .await;

            let (workspace, trigger) = self.get_or_create_space_ref(dna_hash, |space| {
                (
                    space.witnessing_workspace.clone(),
                    self.queue_consumer_map
                        .witnessing_trigger(space.dna_hash.clone()),
                )
            })?;
            let trigger = match trigger {
                Some(t) => t,
                // If the workflow has not been spawned yet, we can't handle incoming messages.
                None => return Ok(()),
            };
            receive_incoming_countersigning_ops(ops, &workspace, trigger)?;
        } else {
            let space = self.get_or_create_space(dna_hash)?;
            let trigger = match self
                .queue_consumer_map
                .sys_validation_trigger(space.dna_hash.clone())
            {
                Some(t) => t,
                // If the workflow has not been spawned yet we can't handle incoming messages.
                // Note this is not an error because only a validation receipt is proof of a publish.
                None => {
                    tracing::warn!("No sys validation trigger yet for space: {}", dna_hash);
                    return Ok(());
                }
            };
            incoming_dht_ops_workflow(space, trigger, ops, request_validation_receipt).await?;
        }
        Ok(())
    }

    /// Get the recent_threshold based on the kitsune network config
    pub fn recent_threshold(&self) -> Duration {
        self.config
            .network
            .tuning_params
            .danger_gossip_recent_threshold()
    }
}

impl Space {
    fn new(
        dna_hash: Arc<DnaHash>,
        root_db_dir: PathBuf,
        db_sync_strategy: DbSyncStrategy,
        db_key: DbKey,
    ) -> DatabaseResult<Self> {
        let space = dna_hash.to_kitsune();
        let db_sync_level = match db_sync_strategy {
            DbSyncStrategy::Fast => DbSyncLevel::Off,
            DbSyncStrategy::Resilient => DbSyncLevel::Normal,
        };

        let (cache, dht_db, p2p_agents_db, p2p_metrics_db, conductor_db) =
            tokio::task::block_in_place(|| {
                let cache = DbWrite::open_with_pool_config(
                    root_db_dir.as_ref(),
                    DbKindCache(dna_hash.clone()),
                    PoolConfig {
                        synchronous_level: db_sync_level,
                        key: db_key.clone(),
                    },
                )?;
                let dht_db = DbWrite::open_with_pool_config(
                    root_db_dir.as_ref(),
                    DbKindDht(dna_hash.clone()),
                    PoolConfig {
                        synchronous_level: db_sync_level,
                        key: db_key.clone(),
                    },
                )?;
                let p2p_agents_db = DbWrite::open_with_pool_config(
                    root_db_dir.as_ref(),
                    DbKindP2pAgents(space.clone()),
                    PoolConfig {
                        synchronous_level: db_sync_level,
                        key: db_key.clone(),
                    },
                )?;
                let p2p_metrics_db = DbWrite::open_with_pool_config(
                    root_db_dir.as_ref(),
                    DbKindP2pMetrics(space),
                    PoolConfig {
                        synchronous_level: db_sync_level,
                        key: db_key.clone(),
                    },
                )?;
                let conductor_db: DbWrite<DbKindConductor> = DbWrite::open_with_pool_config(
                    root_db_dir.as_ref(),
                    DbKindConductor,
                    PoolConfig {
                        synchronous_level: db_sync_level,
                        key: db_key.clone(),
                    },
                )?;
                DatabaseResult::Ok((cache, dht_db, p2p_agents_db, p2p_metrics_db, conductor_db))
            })?;

        let (tx, rx) = tokio::sync::mpsc::channel(100);
        tokio::spawn(p2p_agent_store::p2p_put_all_batch(
            p2p_agents_db.clone(),
            rx,
        ));
        let p2p_batch_sender = tx;

        let witnessing_workspace = WitnessingWorkspace::default();
        let incoming_op_hashes = IncomingOpHashes::default();
        let incoming_ops_batch = IncomingOpsBatch::default();
        let dht_query_cache = DhtDbQueryCache::new(dht_db.clone().into());
        let r = Self {
            dna_hash,
            cache_db: cache,
            authored_dbs: Arc::new(parking_lot::Mutex::new(HashMap::new())),
            dht_db,
            p2p_agents_db,
            p2p_metrics_db,
            p2p_batch_sender,
            countersigning_workspaces: Default::default(),
            witnessing_workspace,
            incoming_op_hashes,
            incoming_ops_batch,
            dht_query_cache,
            conductor_db,
            root_db_dir: Arc::new(root_db_dir),
            db_key,
        };
        Ok(r)
    }

    /// Construct a SourceChain for an author in this Space
    pub async fn source_chain(
        &self,
        keystore: MetaLairClient,
        author: AgentPubKey,
    ) -> SourceChainResult<SourceChain> {
        SourceChain::raw_empty(
            self.get_or_create_authored_db(author.clone())?,
            self.dht_db.clone(),
            self.dht_query_cache.clone(),
            keystore,
            author,
        )
        .await
    }

    /// Create a SourceChainWorkspace from this Space
    pub async fn source_chain_workspace(
        &self,
        keystore: MetaLairClient,
        author: AgentPubKey,
        dna_def: Arc<DnaDef>,
    ) -> ConductorResult<SourceChainWorkspace> {
        Ok(SourceChainWorkspace::new(
            self.get_or_create_authored_db(author.clone())?.clone(),
            self.dht_db.clone(),
            self.dht_query_cache.clone(),
            self.cache_db.clone(),
            keystore,
            author,
            dna_def,
        )
        .await?)
    }

    /// Get or create the authored database for an agent in this space
    pub fn get_or_create_authored_db(
        &self,
        author: AgentPubKey,
    ) -> DatabaseResult<DbWrite<DbKindAuthored>> {
        match self.authored_dbs.lock().entry(author.clone()) {
            hash_map::Entry::Occupied(entry) => Ok(entry.get().clone()),
            hash_map::Entry::Vacant(entry) => {
                let db = tokio::task::block_in_place(|| {
                    DbWrite::open_with_pool_config(
                        self.root_db_dir.as_ref(),
                        DbKindAuthored(Arc::new(CellId::new((*self.dna_hash).clone(), author))),
                        PoolConfig {
                            synchronous_level: DbSyncLevel::Normal,
                            key: self.db_key.clone(),
                        },
                    )
                })?;

                entry.insert(db.clone());
                Ok(db)
            }
        }
    }

    /// Gets authored databases for this space, for every author.
    pub fn get_all_authored_dbs(&self) -> Vec<DbWrite<DbKindAuthored>> {
        self.authored_dbs.lock().values().cloned().collect()
    }
}

/// Get the holochain conductor state
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn query_conductor_state(
    db: &DbRead<DbKindConductor>,
) -> ConductorResult<Option<ConductorState>> {
    db.read_async(|txn| {
        let state = txn
            .query_row("SELECT blob FROM ConductorState WHERE id = 1", [], |row| {
                row.get("blob")
            })
            .optional()?;
        match state {
            Some(state) => ConductorResult::Ok(Some(from_blob(state)?)),
            None => ConductorResult::Ok(None),
        }
    })
    .await
}

#[cfg(test)]
impl TestSpaces {
    /// Create a new test space
    pub async fn new(dna_hashes: impl IntoIterator<Item = DnaHash>) -> Self {
        let queue_consumer_map = QueueConsumerMap::new();
        Self::with_queue_consumer(dna_hashes, queue_consumer_map).await
    }

    /// Create a new test space with a queue consumer
    pub async fn with_queue_consumer(
        dna_hashes: impl IntoIterator<Item = DnaHash>,
        queue_consumer_map: QueueConsumerMap,
    ) -> Self {
        let mut test_spaces: HashMap<DnaHash, _> = HashMap::new();
        for hash in dna_hashes.into_iter() {
            test_spaces.insert(hash.clone(), TestSpace::new(hash));
        }
        let temp_dir = tempfile::Builder::new()
            .prefix("holochain-test-environments")
            .tempdir()
            .unwrap();
        let spaces = Spaces::new(
            ConductorConfig {
                data_root_path: Some(temp_dir.path().to_path_buf().into()),
                ..Default::default()
            }
            .into(),
            sodoken::BufRead::new_no_lock(b"passphrase"),
        )
        .await
        .unwrap();
        spaces.map.share_mut(|map| {
            map.extend(
                test_spaces
                    .iter()
                    .map(|(k, v)| (k.clone(), v.space.clone())),
            );
        });
        Self {
            queue_consumer_map,
            spaces,
            test_spaces,
        }
    }
}

#[cfg(test)]
impl TestSpace {
    /// Create a new test space
    pub fn new(dna_hash: DnaHash) -> Self {
        let temp_dir = tempfile::Builder::new()
            .prefix("holochain-test-environments")
            .tempdir()
            .unwrap();

        Self {
            space: Space::new(
                Arc::new(dna_hash),
                temp_dir.path().to_path_buf(),
                Default::default(),
                Default::default(),
            )
            .unwrap(),
            _temp_dir: temp_dir,
        }
    }
}



================================================
File: crates/holochain/src/conductor/state.rs
================================================
//! Structs which allow the Conductor's state to be persisted across
//! startups and shutdowns

use holochain_conductor_api::config::InterfaceDriver;
use holochain_conductor_api::signal_subscription::SignalSubscription;
use holochain_conductor_services::DeepkeyInstallation;
use holochain_conductor_services::DPKI_APP_ID;
use holochain_p2p::NetworkCompatParams;
use holochain_types::prelude::*;
use holochain_types::websocket::AllowedOrigins;
use serde::Deserialize;
use serde::Serialize;
use std::collections::HashMap;
use std::collections::HashSet;
use std::sync::Arc;

use super::error::{ConductorError, ConductorResult};

/// Unique conductor tag / identifier.
#[derive(Clone, Deserialize, Serialize, Debug, SerializedBytes)]
#[cfg_attr(test, derive(PartialEq))]
#[serde(transparent)]
pub struct ConductorStateTag(pub Arc<str>);

impl Default for ConductorStateTag {
    fn default() -> Self {
        Self(nanoid::nanoid!().into())
    }
}

/// Info required to re-initialize conductor services upon restart
#[derive(Clone, PartialEq, Eq, Deserialize, Serialize, Default, Debug, SerializedBytes)]
pub struct ConductorServicesState {
    /// Data needed to initialize the DPKI service, if installed
    pub dpki: Option<DeepkeyInstallation>,
}

/// Mutable conductor state, stored in a DB and writable only via Admin interface.
///
/// References between structs (cell configs pointing to
/// the agent and DNA to be instantiated) are implemented
/// via string IDs.
#[serde_with::serde_as]
#[derive(Clone, Deserialize, Serialize, Default, Debug, SerializedBytes)]
#[cfg_attr(test, derive(PartialEq))]
pub struct ConductorState {
    /// Unique conductor tag / identifier.
    #[serde(default)]
    tag: ConductorStateTag,
    /// Apps (and services) that have been installed, regardless of status.
    #[serde(default)]
    installed_apps_and_services: InstalledAppMap,

    /// Number of agent keys that have ever been derived from the device seed.
    /// Only increases, never decreases. Used for deriving reconstructible
    /// agent keys from the lair "device seed".
    #[serde(default)]
    pub derived_agent_key_count: u32,

    /// Conductor services that have been installed, to enable initialization
    /// upon restart
    #[serde(default)]
    pub(crate) conductor_services: ConductorServicesState,

    /// List of interfaces any UI can use to access zome functions.
    #[serde_as(as = "Vec<(_, _)>")]
    #[serde(default)]
    pub(crate) app_interfaces: HashMap<AppInterfaceId, AppInterfaceConfig>,
}

/// A unique identifier used to refer to an App Interface internally.
#[derive(Clone, Deserialize, Serialize, Debug, Hash, PartialEq, Eq)]
pub struct AppInterfaceId {
    /// The port used to create this interface
    port: u16,
    /// If the port is 0 then it will be assigned by the OS
    /// so we need a unique identifier for that case.
    id: Option<String>,
}

impl Default for AppInterfaceId {
    fn default() -> Self {
        Self::new(0)
    }
}

impl AppInterfaceId {
    /// Create an id from the port
    pub fn new(port: u16) -> Self {
        let id = if port == 0 {
            Some(nanoid::nanoid!())
        } else {
            None
        };
        Self { port, id }
    }
    /// Get the port intended for this interface
    pub fn port(&self) -> u16 {
        self.port
    }
}

/// Does the given InstalledAppId refer to an app, or a service?
pub fn is_app(id: &InstalledAppId) -> bool {
    !is_service(id)
}

/// Does the given InstalledAppId refer to a service, or an app?
pub fn is_service(id: &InstalledAppId) -> bool {
    id.as_str() == DPKI_APP_ID
}

impl ConductorState {
    /// A unique identifier for this conductor
    pub fn tag(&self) -> &ConductorStateTag {
        &self.tag
    }

    /// Set the tag for this conductor
    #[cfg(test)]
    pub fn set_tag(&mut self, tag: ConductorStateTag) {
        self.tag = tag;
    }

    /// Immutable access to the inner collection of all apps and services
    pub fn installed_apps_and_services(&self) -> &InstalledAppMap {
        &self.installed_apps_and_services
    }

    /// Mutable access to the inner collection of all apps
    // #[cfg(test)]
    #[deprecated = "Bare mutable access isn't the best idea"]
    pub fn installed_apps_and_services_mut(&mut self) -> &mut InstalledAppMap {
        &mut self.installed_apps_and_services
    }

    /// Iterate over only the "enabled" apps and services
    pub fn enabled_apps_and_services(
        &self,
    ) -> impl Iterator<Item = (&InstalledAppId, &InstalledApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(_, app)| app.status().is_enabled())
    }

    /// Iterate over only the "enabled" apps
    pub fn enabled_apps(&self) -> impl Iterator<Item = (&InstalledAppId, &InstalledApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(_, app)| app.status().is_enabled())
            .filter(|(id, _)| is_app(id))
    }

    /// Iterate over only the "disabled" apps
    pub fn disabled_apps(&self) -> impl Iterator<Item = (&InstalledAppId, &InstalledApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(id, _)| is_app(id))
            .filter(|(_, app)| !app.status().is_enabled())
    }

    /// Iterate over only the "running" apps
    pub fn running_apps(&self) -> impl Iterator<Item = (&InstalledAppId, RunningApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(id, _)| is_app(id))
            .filter_map(|(id, app)| {
                if *app.status() == AppStatus::Running {
                    let running = RunningApp::from(app.as_ref().clone());
                    Some((id, running))
                } else {
                    None
                }
            })
    }

    /// Iterate over only the paused apps
    pub fn paused_apps(&self) -> impl Iterator<Item = (&InstalledAppId, StoppedApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(id, _)| is_app(id))
            .filter_map(|(id, app)| {
                if app.status.is_paused() {
                    StoppedApp::from_app(app).map(|stopped| (id, stopped))
                } else {
                    None
                }
            })
    }

    /// Iterate over only the "stopped" apps (paused OR disabled)
    pub fn stopped_apps(&self) -> impl Iterator<Item = (&InstalledAppId, StoppedApp)> + '_ {
        self.installed_apps_and_services
            .iter()
            .filter(|(id, _)| is_app(id))
            .filter_map(|(id, app)| StoppedApp::from_app(app).map(|stopped| (id, stopped)))
    }

    /// Getter for a single app. Returns error if app missing.
    pub fn get_app(&self, id: &InstalledAppId) -> ConductorResult<&InstalledApp> {
        self.installed_apps_and_services
            .get(id)
            .ok_or_else(|| ConductorError::AppNotInstalled(id.clone()))
    }

    /// Getter for a mutable reference to a single app. Returns error if app missing.
    pub fn get_app_mut(&mut self, id: &InstalledAppId) -> ConductorResult<&mut InstalledApp> {
        self.installed_apps_and_services
            .get_mut(id)
            .ok_or_else(|| ConductorError::AppNotInstalled(id.clone()))
    }

    /// Getter for a single app. Returns error if app missing.
    pub fn remove_app(&mut self, id: &InstalledAppId) -> ConductorResult<InstalledApp> {
        self.installed_apps_and_services
            .swap_remove(id)
            .ok_or_else(|| ConductorError::AppNotInstalled(id.clone()))
    }

    /// Add an app in the Disabled state. Returns an error if an app is already
    /// present at the given ID.
    pub fn add_app(&mut self, app: InstalledAppCommon) -> ConductorResult<StoppedApp> {
        if self.installed_apps_and_services.contains_key(app.id()) {
            return Err(ConductorError::AppAlreadyInstalled(app.id().clone()));
        }
        let stopped_app = StoppedApp::new_fresh(app);
        self.installed_apps_and_services
            .insert(stopped_app.id().clone(), stopped_app.clone().into());
        Ok(stopped_app)
    }

    /// Add an app in the AwaitingMemproofs state. Returns an error if an app is already
    /// present at the given ID.
    pub fn add_app_awaiting_memproofs(
        &mut self,
        app: InstalledAppCommon,
    ) -> ConductorResult<InstalledApp> {
        if self.installed_apps_and_services.contains_key(app.id()) {
            return Err(ConductorError::AppAlreadyInstalled(app.id().clone()));
        }
        let app = InstalledApp::new(app, AppStatus::AwaitingMemproofs);
        self.installed_apps_and_services
            .insert(app.id().clone(), app.clone());
        Ok(app)
    }

    /// Update the status of an installed app in-place.
    /// Return a reference to the (possibly updated) app.
    /// Additionally, if an update occurred, return the previous state. If no update occurred, return None.
    pub fn transition_app_status(
        &mut self,
        id: &InstalledAppId,
        transition: AppStatusTransition,
    ) -> ConductorResult<(&InstalledApp, AppStatusFx)> {
        match transition {
            AppStatusTransition::Disable(_) | AppStatusTransition::Pause(_) => {
                let dependents: Vec<_> = self
                    .get_dependent_apps(id, true)?
                    .into_iter()
                    .filter(|id| {
                        self.installed_apps_and_services
                            .get(id)
                            .map(|app| app.status().is_running())
                            .unwrap_or(false)
                    })
                    .collect();
                if !dependents.is_empty() {
                    tracing::warn!(
                        "Disabling/pausing app '{}' which has running protected dependent apps: {:?}",
                        id,
                        dependents
                    );
                }
            }
            AppStatusTransition::Enable | AppStatusTransition::Start => {
                let dependencies: Vec<_> = self
                    .get_app(id)?
                    .roles()
                    .values()
                    .flat_map(|r| match r {
                        AppRoleAssignment::Primary(_) => vec![],
                        AppRoleAssignment::Dependency(AppRoleDependency { cell_id, protected }) => {
                            if *protected {
                                self.installed_apps_and_services
                                    .iter()
                                    .filter_map(|(id, app)| {
                                        (!app.status().is_running()
                                            && app.all_cells().any(|id| id == *cell_id))
                                        .then_some(id)
                                    })
                                    .collect()
                            } else {
                                vec![]
                            }
                        }
                    })
                    .collect();
                if !dependencies.is_empty() {
                    return Err(ConductorError::AppStatusError(format!(
                        "Enabling/starting App '{}' which has protected dependencies that are not running: {:?}",
                        id, dependencies
                    )));
                }
            }
        }
        let app = self
            .installed_apps_and_services
            .get_mut(id)
            .ok_or_else(|| ConductorError::AppNotInstalled(id.clone()))?;
        let delta = app.status.transition(transition);
        Ok((app, delta))
    }

    /// Returns the interface configuration with the given ID if present
    pub fn interface_by_id(&self, id: &AppInterfaceId) -> Option<AppInterfaceConfig> {
        self.app_interfaces.get(id).cloned()
    }

    /// Find the app which contains the given cell by its [CellId].
    pub fn find_app_containing_cell(&self, cell_id: &CellId) -> Option<&InstalledApp> {
        self.installed_apps_and_services
            .values()
            .find(|app| app.all_cells().any(|id| id == *cell_id))
    }

    /// Get network compability params
    /// (but this can't actually be on the Conductor since it must be retrieved before
    /// conductor initialization)
    pub fn get_network_compat(&self) -> NetworkCompatParams {
        NetworkCompatParams {
            dpki_uuid: {
                tracing::warn!("Using default NetworkCompatParams");
                None
            },
        }
    }

    /// Find all installed apps that have a role which depends on a cell in this app
    /// via `AppRoleAssignment::Dependency`.
    ///
    /// The `protected_only` field is a filter. If false, all dependent apps are returned.
    /// If true, only dependent apps with at least one protected dependency are returned.
    pub fn get_dependent_apps(
        &self,
        id: &InstalledAppId,
        protected_only: bool,
    ) -> ConductorResult<Vec<InstalledAppId>> {
        let app = self.get_app(id)?;
        let cell_ids: HashSet<_> = app.all_cells().collect();
        Ok(self
            .installed_apps_and_services
            .iter()
            .filter(|(_, app)| {
                app.role_assignments.values().any(|r| match r {
                    AppRoleAssignment::Primary(_) => false,
                    AppRoleAssignment::Dependency(d) => {
                        cell_ids.contains(&d.cell_id) && (!protected_only || d.protected)
                    }
                })
            })
            .map(|(id, _)| id.clone())
            .collect())
    }

    /// Find all installed apps which have a cell that this app depends on
    /// via `AppRoleAssignment::Dependency`.
    ///
    /// The `protected_only` field is a filter. If false, all dependency apps are returned.
    /// If true, only protected dependencies are returned.
    pub fn get_depdency_apps(
        &self,
        id: &InstalledAppId,
        protected_only: bool,
    ) -> ConductorResult<Vec<InstalledAppId>> {
        let dependencies: Vec<_> = self
            .get_app(id)?
            .roles()
            .values()
            .flat_map(|r| match r {
                AppRoleAssignment::Primary(_) => vec![],
                AppRoleAssignment::Dependency(AppRoleDependency { cell_id, protected }) => {
                    if !protected_only || *protected {
                        self.installed_apps_and_services
                            .iter()
                            .filter_map(|(id, app)| {
                                (app.all_cells().any(|id| id == *cell_id)
                                    && !app.status().is_running())
                                .then_some(id)
                            })
                            .collect()
                    } else {
                        vec![]
                    }
                }
            })
            .cloned()
            .collect();
        Ok(dependencies)
    }
}

/// Here, interfaces are user facing and make available zome functions to
/// GUIs, browser based web UIs, local native UIs, other local applications and scripts.
/// We currently have:
/// * websockets
///
/// We will also soon develop
/// * Unix domain sockets
///
/// The cells (referenced by ID) that are to be made available via that interface should be listed.
#[derive(Clone, Deserialize, Serialize, Debug)]
#[cfg_attr(test, derive(PartialEq))]
pub struct AppInterfaceConfig {
    /// The signal subscription settings for each App
    pub signal_subscriptions: HashMap<InstalledAppId, SignalSubscription>,

    /// The application that this interface is for. If `Some`, then this interface will only allow
    /// connections which use a token that has been issued for the same app id. Otherwise, any app
    /// is allowed to connect.
    pub installed_app_id: Option<InstalledAppId>,

    /// The driver for the interface, e.g. Websocket
    pub driver: InterfaceDriver,
}

impl AppInterfaceConfig {
    /// Create config for a websocket interface
    pub fn websocket(
        port: u16,
        allowed_origins: AllowedOrigins,
        installed_app_id: Option<InstalledAppId>,
    ) -> Self {
        Self {
            signal_subscriptions: HashMap::new(),
            installed_app_id,
            driver: InterfaceDriver::Websocket {
                port,
                allowed_origins,
            },
        }
    }
}

// TODO: Tons of consistency check tests were ripped out in the great legacy code cleanup
// We should add these back in when we've landed the new Dna format
// See https://github.com/holochain/holochain/blob/7750a0291e549be006529e4153b3b6cf0d686462/crates/holochain/src/conductor/state/tests.rs#L1
// for all old tests



================================================
File: crates/holochain/src/conductor/api/api_cell.rs
================================================
//! The CellConductorApi allows Cells to talk to their Conductor

use std::sync::Arc;

use super::error::ConductorApiError;
use super::error::ConductorApiResult;
use super::DpkiApi;
use crate::conductor::conductor::ConductorServices;
use crate::conductor::error::ConductorResult;
use crate::conductor::ConductorHandle;
use crate::core::ribosome::guest_callback::post_commit::PostCommitArgs;
use crate::core::ribosome::real_ribosome::RealRibosome;
use crate::core::workflow::ZomeCallResult;
use async_trait::async_trait;
use holo_hash::DnaHash;
use holochain_keystore::MetaLairClient;
use holochain_state::host_fn_workspace::SourceChainWorkspace;
use holochain_state::nonce::WitnessNonceResult;
use holochain_state::prelude::DatabaseResult;
use holochain_types::prelude::*;
use holochain_zome_types::block::Block;
use holochain_zome_types::block::BlockTargetId;
use tokio::sync::mpsc::error::SendError;
use tokio::sync::mpsc::OwnedPermit;

/// The concrete implementation of [`CellConductorApiT`], which is used to give
/// Cells an API for calling back to their [`Conductor`](crate::conductor::Conductor).
#[derive(Clone)]
pub struct CellConductorApi {
    conductor_handle: ConductorHandle,
    cell_id: CellId,
}

/// Alias
pub type CellConductorHandle = Arc<dyn CellConductorApiT + Send + 'static>;

/// A minimal set of functionality needed from the conductor by
/// host functions.
pub type CellConductorReadHandle = Arc<dyn CellConductorReadHandleT + Send + 'static>;

impl CellConductorApi {
    /// Instantiate from a Conductor reference and a CellId to identify which Cell
    /// this API instance is associated with
    pub fn new(conductor_handle: ConductorHandle, cell_id: CellId) -> Self {
        Self {
            conductor_handle,
            cell_id,
        }
    }
}

#[async_trait]
impl CellConductorApiT for CellConductorApi {
    fn cell_id(&self) -> &CellId {
        &self.cell_id
    }

    fn conductor_services(&self) -> ConductorServices {
        self.conductor_handle
            .running_services
            .share_ref(|s| s.clone())
    }

    fn keystore(&self) -> &MetaLairClient {
        self.conductor_handle.keystore()
    }

    fn get_dna(&self, dna_hash: &DnaHash) -> Option<DnaFile> {
        self.conductor_handle.get_dna_file(dna_hash)
    }

    fn get_this_dna(&self) -> ConductorApiResult<DnaFile> {
        self.conductor_handle
            .get_dna_file(self.cell_id.dna_hash())
            .ok_or_else(|| ConductorApiError::DnaMissing(self.cell_id.dna_hash().clone()))
    }

    fn get_this_ribosome(&self) -> ConductorApiResult<RealRibosome> {
        Ok(self
            .conductor_handle
            .get_ribosome(self.cell_id.dna_hash())?)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self)))]
    fn get_zome(&self, dna_hash: &DnaHash, zome_name: &ZomeName) -> ConductorApiResult<Zome> {
        let dna = self
            .get_dna(dna_hash)
            .ok_or_else(|| ConductorApiError::DnaMissing(dna_hash.clone()))?;
        Ok(dna.dna_def().get_zome(zome_name)?)
    }

    fn get_entry_def(&self, key: &EntryDefBufferKey) -> Option<EntryDef> {
        self.conductor_handle.get_entry_def(key)
    }

    fn into_call_zome_handle(self) -> CellConductorReadHandle {
        Arc::new(self)
    }

    async fn post_commit_permit(&self) -> Result<OwnedPermit<PostCommitArgs>, SendError<()>> {
        self.conductor_handle.post_commit_permit().await
    }
}

/// The "internal" Conductor API interface, for a Cell to talk to its calling Conductor.
#[async_trait]
#[cfg_attr(feature = "test_utils", mockall::automock)]
pub trait CellConductorApiT: Send + Sync {
    /// Get this cell id
    fn cell_id(&self) -> &CellId;

    /// Access to the conductor services
    fn conductor_services(&self) -> ConductorServices;

    /// Request access to this conductor's keystore
    fn keystore(&self) -> &MetaLairClient;

    /// Get a [`Dna`](holochain_types::prelude::Dna) from the [`RibosomeStore`](crate::conductor::ribosome_store::RibosomeStore)
    fn get_dna(&self, dna_hash: &DnaHash) -> Option<DnaFile>;

    /// Get the [`Dna`](holochain_types::prelude::Dna) of this cell from the [`RibosomeStore`](crate::conductor::ribosome_store::RibosomeStore)
    fn get_this_dna(&self) -> ConductorApiResult<DnaFile>;

    /// Get the [`RealRibosome`] of this cell from the [`RibosomeStore`](crate::conductor::ribosome_store::RibosomeStore)
    fn get_this_ribosome(&self) -> ConductorApiResult<RealRibosome>;

    /// Get a [`Zome`](holochain_types::prelude::Zome) from this cell's Dna
    fn get_zome(&self, dna_hash: &DnaHash, zome_name: &ZomeName) -> ConductorApiResult<Zome>;

    /// Get a [`EntryDef`] from the [`EntryDefBufferKey`]
    fn get_entry_def(&self, key: &EntryDefBufferKey) -> Option<EntryDef>;

    /// Turn this into a call zome handle
    fn into_call_zome_handle(self) -> CellConductorReadHandle;

    /// Get an OwnedPermit to the post commit task.
    async fn post_commit_permit(&self) -> Result<OwnedPermit<PostCommitArgs>, SendError<()>>;
}

#[async_trait]
#[cfg_attr(feature = "test_utils", mockall::automock)]
/// A minimal set of functionality needed from the conductor by
/// host functions.
pub trait CellConductorReadHandleT: Send + Sync {
    /// Get this cell id
    fn cell_id(&self) -> &CellId;

    /// Invoke a zome function on a Cell
    async fn call_zome(
        &self,
        params: ZomeCallParams,
        workspace_lock: SourceChainWorkspace,
    ) -> ConductorApiResult<ZomeCallResult>;

    /// Get a zome from this cell's Dna
    fn get_zome(&self, dna_hash: &DnaHash, zome_name: &ZomeName) -> ConductorApiResult<Zome>;

    /// Get a [`EntryDef`] from the [`EntryDefBufferKey`]
    fn get_entry_def(&self, key: &EntryDefBufferKey) -> Option<EntryDef>;

    /// Call into DPKI
    fn get_dpki(&self) -> DpkiApi;

    /// Try to put the nonce from a calling agent in the db. Fails with a stale result if a newer nonce exists.
    async fn witness_nonce_from_calling_agent(
        &self,
        agent: AgentPubKey,
        nonce: Nonce256Bits,
        expires: Timestamp,
    ) -> ConductorApiResult<WitnessNonceResult>;

    /// Find the first cell ID across all apps the given cell id is in that
    /// is assigned to the given role.
    async fn find_cell_with_role_alongside_cell(
        &self,
        cell_id: &CellId,
        role_name: &RoleName,
    ) -> ConductorResult<Option<CellId>>;

    /// Expose block functionality to zomes.
    async fn block(&self, input: Block) -> DatabaseResult<()>;

    /// Expose unblock functionality to zomes.
    async fn unblock(&self, input: Block) -> DatabaseResult<()>;

    /// Expose is_blocked functionality to zomes.
    async fn is_blocked(&self, input: BlockTargetId, timestamp: Timestamp) -> DatabaseResult<bool>;

    /// Find an installed app by one of its [CellId]s.
    async fn find_app_containing_cell(
        &self,
        cell_id: &CellId,
    ) -> ConductorResult<Option<InstalledApp>>;

    /// Expose create_clone_cell functionality to zomes.
    async fn create_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: CreateCloneCellPayload,
    ) -> ConductorResult<ClonedCell>;

    /// Expose disable_clone_cell functionality to zomes.
    async fn disable_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: DisableCloneCellPayload,
    ) -> ConductorResult<()>;

    /// Expose enable_clone_cell functionality to zomes.
    async fn enable_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: EnableCloneCellPayload,
    ) -> ConductorResult<ClonedCell>;

    /// Expose delete_clone_cell functionality to zomes.
    async fn delete_clone_cell(&self, payload: DeleteCloneCellPayload) -> ConductorResult<()>;

    #[cfg(feature = "unstable-countersigning")]
    /// Accept a countersigning session.
    async fn accept_countersigning_session(
        &self,
        cell_id: CellId,
        request: PreflightRequest,
    ) -> ConductorResult<PreflightRequestAcceptance>;
}

#[async_trait]
impl CellConductorReadHandleT for CellConductorApi {
    fn cell_id(&self) -> &CellId {
        &self.cell_id
    }

    async fn call_zome(
        &self,
        params: ZomeCallParams,
        workspace_lock: SourceChainWorkspace,
    ) -> ConductorApiResult<ZomeCallResult> {
        if self.cell_id == params.cell_id {
            self.conductor_handle
                .call_zome_with_workspace(params, workspace_lock)
                .await
        } else {
            self.conductor_handle.call_zome(params).await
        }
    }

    fn get_zome(&self, dna_hash: &DnaHash, zome_name: &ZomeName) -> ConductorApiResult<Zome> {
        CellConductorApiT::get_zome(self, dna_hash, zome_name)
    }

    fn get_entry_def(&self, key: &EntryDefBufferKey) -> Option<EntryDef> {
        CellConductorApiT::get_entry_def(self, key)
    }

    fn get_dpki(&self) -> DpkiApi {
        CellConductorApiT::conductor_services(self).dpki
    }

    async fn witness_nonce_from_calling_agent(
        &self,
        agent: AgentPubKey,
        nonce: Nonce256Bits,
        expires: Timestamp,
    ) -> ConductorApiResult<WitnessNonceResult> {
        Ok(self
            .conductor_handle
            .witness_nonce_from_calling_agent(agent, nonce, expires)
            .await?)
    }

    async fn find_cell_with_role_alongside_cell(
        &self,
        cell_id: &CellId,
        role_name: &RoleName,
    ) -> ConductorResult<Option<CellId>> {
        self.conductor_handle
            .find_cell_with_role_alongside_cell(cell_id, role_name)
            .await
    }

    async fn block(&self, input: Block) -> DatabaseResult<()> {
        self.conductor_handle.block(input).await
    }

    async fn unblock(&self, input: Block) -> DatabaseResult<()> {
        self.conductor_handle.unblock(input).await
    }

    async fn is_blocked(&self, input: BlockTargetId, timestamp: Timestamp) -> DatabaseResult<bool> {
        self.conductor_handle.is_blocked(input, timestamp).await
    }

    async fn find_app_containing_cell(
        &self,
        cell_id: &CellId,
    ) -> ConductorResult<Option<InstalledApp>> {
        self.conductor_handle
            .find_app_containing_cell(cell_id)
            .await
    }

    async fn create_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: CreateCloneCellPayload,
    ) -> ConductorResult<ClonedCell> {
        self.conductor_handle
            .clone()
            .create_clone_cell(installed_app_id, payload)
            .await
    }

    async fn disable_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: DisableCloneCellPayload,
    ) -> ConductorResult<()> {
        self.conductor_handle
            .clone()
            .disable_clone_cell(installed_app_id, &payload)
            .await
    }

    async fn enable_clone_cell(
        &self,
        installed_app_id: &InstalledAppId,
        payload: EnableCloneCellPayload,
    ) -> ConductorResult<ClonedCell> {
        self.conductor_handle
            .clone()
            .enable_clone_cell(installed_app_id, &payload)
            .await
    }

    async fn delete_clone_cell(&self, payload: DeleteCloneCellPayload) -> ConductorResult<()> {
        self.conductor_handle
            .clone()
            .delete_clone_cell(&payload)
            .await
    }

    #[cfg(feature = "unstable-countersigning")]
    async fn accept_countersigning_session(
        &self,
        cell_id: CellId,
        request: PreflightRequest,
    ) -> ConductorResult<PreflightRequestAcceptance> {
        self.conductor_handle
            .accept_countersigning_session(cell_id, request)
            .await
    }
}



================================================
File: crates/holochain/src/conductor/api/api_dpki.rs
================================================
//! The DpkiApi allows access to the DPKI service of a conductor.

use std::sync::Arc;

use holochain_conductor_services::DpkiService;

/// Alias for an optional DPKI conductor service.
pub type DpkiApi = Option<Arc<DpkiService>>;



================================================
File: crates/holochain/src/conductor/api/api_external.rs
================================================
use crate::conductor::interface::error::InterfaceResult;
use holochain_serialized_bytes::prelude::*;

mod admin_interface;
mod app_interface;
pub use admin_interface::*;
pub use app_interface::*;
use holochain_types::prelude::InstalledAppId;

/// A trait that unifies both the admin and app interfaces
#[async_trait::async_trait]
pub trait InterfaceApi: 'static + Send + Sync + Clone {
    /// An authentication payload to establish a connection.
    /// This is the first message sent on a connection.
    type Auth: TryFrom<SerializedBytes, Error = SerializedBytesError>
        + Send
        + Sync
        + std::fmt::Debug;

    /// Which request is being made
    type ApiRequest: TryFrom<SerializedBytes, Error = SerializedBytesError>
        + Send
        + Sync
        + std::fmt::Debug;

    /// Which response is sent to the above request
    type ApiResponse: TryInto<SerializedBytes, Error = SerializedBytesError>
        + Send
        + Sync
        + std::fmt::Debug;

    /// Authentication a connection request.
    async fn auth(&self, auth: Self::Auth) -> InterfaceResult<InstalledAppId>;

    /// Handle a request on this API
    async fn handle_request(
        &self,
        request: Result<Self::ApiRequest, SerializedBytesError>,
    ) -> InterfaceResult<Self::ApiResponse>;
}



================================================
File: crates/holochain/src/conductor/api/error.rs
================================================
//! Errors occurring during a [`CellConductorApi`](super::CellConductorApi) or [`InterfaceApi`](super::InterfaceApi) call
use crate::conductor::error::ConductorError;
use crate::conductor::interface::error::InterfaceError;
use crate::conductor::CellError;
use crate::core::ribosome::error::RibosomeError;
use crate::core::workflow::WorkflowError;
use holo_hash::DnaHash;
use holochain_chc::ChcError;
use holochain_sqlite::error::DatabaseError;
use holochain_state::source_chain::SourceChainError;
use holochain_state::workspace::WorkspaceError;
use holochain_types::prelude::*;
use holochain_zome_types::cell::CellId;
use mr_bundle::error::MrBundleError;
use serde::de::DeserializeOwned;
use thiserror::Error;

/// Errors occurring during a [`CellConductorApi`](super::CellConductorApi) or [`InterfaceApi`](super::InterfaceApi) call
#[derive(Error, Debug)]
pub enum ConductorApiError {
    /// The Dna for this Cell is not installed in the conductor.
    #[error("The Dna for this Cell is not installed in the conductor! DnaHash: {0}")]
    DnaMissing(DnaHash),

    /// Cell was referenced, but is missing from the conductor.
    #[error(
        "A Cell attempted to use an CellConductorApi it was not given.\nAPI CellId: {api_cell_id:?}\nInvocation CellId: {call_cell_id:?}"
    )]
    ZomeCallCellMismatch {
        /// The CellId which is referenced by the CellConductorApi
        api_cell_id: CellId,
        /// The CellId which is referenced by the ZomeCallInvocation
        call_cell_id: CellId,
    },

    /// Conductor threw an error during API call.
    #[error("Conductor returned an error while using a ConductorApi: {0:?}")]
    ConductorError(#[from] ConductorError),

    /// Io error.
    #[error("Io error while using a Interface Api: {0:?}")]
    Io(#[from] std::io::Error),

    /// Serialization error
    #[error("Serialization error while using a InterfaceApi: {0:?}")]
    SerializationError(#[from] SerializationError),

    /// Database error
    #[error(transparent)]
    DatabaseError(#[from] DatabaseError),

    /// Workspace error.
    #[error(transparent)]
    WorkspaceError(#[from] WorkspaceError),

    /// Workflow error.
    #[error(transparent)]
    WorkflowError(#[from] WorkflowError),

    /// ZomeError
    #[error("ZomeError: {0}")]
    ZomeError(#[from] holochain_zome_types::zome::ZomeError),

    /// DnaError
    #[error("DnaError: {0}")]
    DnaError(#[from] holochain_types::dna::DnaError),

    /// The Dna file path provided was invalid
    #[error("The Dna file path provided was invalid")]
    DnaReadError(String),

    /// KeystoreError
    #[error("KeystoreError: {0}")]
    KeystoreError(#[from] holochain_keystore::KeystoreError),

    /// Cell error
    #[error(transparent)]
    CellError(#[from] CellError),

    /// App error
    #[error(transparent)]
    AppError(#[from] AppError),

    /// Error in the Interface
    #[error("An error occurred in the interface: {0:?}")]
    InterfaceError(#[from] InterfaceError),

    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),

    #[error(transparent)]
    AppBundleError(#[from] AppBundleError),

    #[error(transparent)]
    MrBundleError(#[from] MrBundleError),

    #[error(transparent)]
    JsonDumpError(#[from] serde_json::Error),

    #[error(transparent)]
    StateQueryError(#[from] holochain_state::query::StateQueryError),

    #[error(transparent)]
    StateMutationError(#[from] holochain_state::mutations::StateMutationError),

    #[error(transparent)]
    RusqliteError(#[from] rusqlite::Error),

    #[error(transparent)]
    ChcError(#[from] ChcError),

    #[error(transparent)]
    RibosomeError(#[from] crate::core::ribosome::error::RibosomeError),

    /// Other
    #[error("Other: {0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

impl ConductorApiError {
    /// promote a custom error type to a KitsuneP2pError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self::Other(e.into())
    }
}

impl From<one_err::OneErr> for ConductorApiError {
    fn from(e: one_err::OneErr) -> Self {
        Self::other(e)
    }
}

/// All the serialization errors that can occur
#[derive(Error, Debug)]
pub enum SerializationError {
    /// Denotes inability to move into or out of SerializedBytes
    #[error(transparent)]
    Bytes(#[from] holochain_serialized_bytes::SerializedBytesError),

    /// Denotes inability to parse a UUID
    #[error(transparent)]
    Uuid(#[from] uuid::Error),
}

/// Type alias
pub type ConductorApiResult<T> = Result<T, ConductorApiError>;

pub use holochain_conductor_api::ExternalApiWireError;

impl From<ConductorApiError> for ExternalApiWireError {
    fn from(err: ConductorApiError) -> Self {
        match err {
            ConductorApiError::DnaReadError(e) => ExternalApiWireError::DnaReadError(e),
            e => ExternalApiWireError::internal(e),
        }
    }
}

impl From<SerializationError> for ExternalApiWireError {
    fn from(e: SerializationError) -> Self {
        ExternalApiWireError::Deserialization(format!("{:?}", e))
    }
}

impl From<RibosomeError> for ExternalApiWireError {
    fn from(e: RibosomeError) -> Self {
        ExternalApiWireError::RibosomeError(e.to_string())
    }
}

pub fn zome_call_response_to_conductor_api_result<T: DeserializeOwned + std::fmt::Debug>(
    zcr: ZomeCallResponse,
) -> ConductorApiResult<T> {
    match zcr {
        ZomeCallResponse::Ok(bytes) => Ok(bytes.decode().map_err(SerializationError::from)?),
        other => Err(ConductorApiError::other(format!("{:?}", other))),
    }
}



================================================
File: crates/holochain/src/conductor/api/api_external/admin_interface.rs
================================================
use std::collections::HashSet;

use crate::conductor::api::error::ConductorApiError;
use crate::conductor::api::error::ConductorApiResult;
use crate::conductor::api::error::SerializationError;
use crate::conductor::error::ConductorError;
use crate::conductor::interface::error::InterfaceError;
use crate::conductor::interface::error::InterfaceResult;
use crate::conductor::ConductorHandle;
use holochain_serialized_bytes::prelude::*;
use holochain_types::dna::DnaBundle;
use holochain_types::prelude::*;
use mr_bundle::Bundle;

use tracing::*;

pub use holochain_conductor_api::*;

/// The admin interface that external connections
/// can use to make requests to the conductor
/// The concrete (non-mock) implementation of the AdminInterfaceApi
#[derive(Clone)]
pub struct AdminInterfaceApi {
    /// Mutable access to the Conductor
    conductor_handle: ConductorHandle,
}

impl AdminInterfaceApi {
    /// Create an admin interface api.
    pub fn new(conductor_handle: ConductorHandle) -> Self {
        AdminInterfaceApi { conductor_handle }
    }

    /// Handle an [AdminRequest] and return an [AdminResponse].
    pub async fn handle_request(
        &self,
        request: Result<AdminRequest, SerializedBytesError>,
    ) -> InterfaceResult<AdminResponse> {
        // Don't hold the read across both awaits
        {
            self.conductor_handle
                .check_running()
                .map_err(Box::new)
                .map_err(InterfaceError::RequestHandler)?;
        }
        match request {
            Ok(request) => Ok(self.handle_admin_request(request).await),
            Err(e) => Ok(AdminResponse::Error(SerializationError::from(e).into())),
        }
    }

    /// Deal with error cases produced by `handle_admin_request_inner`
    pub(crate) async fn handle_admin_request(&self, request: AdminRequest) -> AdminResponse {
        debug!("admin request: {:?}", request);

        let res = self
            .handle_admin_request_inner(request)
            .await
            .unwrap_or_else(|e| AdminResponse::Error(e.into()));
        debug!("admin response: {:?}", res);
        res
    }

    async fn handle_admin_request_inner(
        &self,
        request: AdminRequest,
    ) -> ConductorApiResult<AdminResponse> {
        use AdminRequest::*;
        match request {
            AddAdminInterfaces(configs) => {
                self.conductor_handle
                    .clone()
                    .add_admin_interfaces(configs)
                    .await?;
                Ok(AdminResponse::AdminInterfacesAdded)
            }
            RegisterDna(payload) => {
                trace!(register_dna_payload = ?payload);
                let RegisterDnaPayload { modifiers, source } = *payload;
                let modifiers = modifiers.serialized().map_err(SerializationError::Bytes)?;
                // network seed and properties from the register call will override any in the bundle
                let dna = match source {
                    DnaSource::Hash(ref hash) => {
                        if !modifiers.has_some_option_set() {
                            return Err(ConductorApiError::DnaReadError(
                                "DnaSource::Hash requires `properties` or `network_seed` or `origin_time` to create a derived Dna"
                                    .to_string(),
                            ));
                        }
                        self.conductor_handle
                            .get_dna_file(hash)
                            .ok_or_else(|| {
                                ConductorApiError::DnaReadError(format!(
                                    "Unable to create derived Dna: {} not registered",
                                    hash
                                ))
                            })?
                            .update_modifiers(modifiers)
                    }
                    DnaSource::Path(ref path) => {
                        let bundle = Bundle::read_from_file(path).await?;
                        let bundle: DnaBundle = bundle.into();
                        let (dna_file, _original_hash) = bundle.into_dna_file(modifiers).await?;
                        dna_file
                    }
                    DnaSource::Bundle(bundle) => {
                        let (dna_file, _original_hash) = bundle.into_dna_file(modifiers).await?;
                        dna_file
                    }
                };

                let hash = dna.dna_hash().clone();
                let dna_list = self.conductor_handle.list_dnas();
                if !dna_list.contains(&hash) {
                    self.conductor_handle.register_dna(dna).await?;
                }
                Ok(AdminResponse::DnaRegistered(hash))
            }
            GetDnaDefinition(dna_hash) => {
                let dna_def = self
                    .conductor_handle
                    .get_dna_def(&dna_hash)
                    .ok_or(ConductorApiError::DnaMissing(*dna_hash))?;
                Ok(AdminResponse::DnaDefinitionReturned(dna_def))
            }
            UpdateCoordinators(payload) => {
                let UpdateCoordinatorsPayload { dna_hash, source } = *payload;
                let (coordinator_zomes, wasms) = match source {
                    CoordinatorSource::Path(ref path) => {
                        let bundle = Bundle::read_from_file(path).await?;
                        let bundle: CoordinatorBundle = bundle.into();
                        bundle.into_zomes().await?
                    }
                    CoordinatorSource::Bundle(bundle) => bundle.into_zomes().await?,
                };

                self.conductor_handle
                    .update_coordinators(&dna_hash, coordinator_zomes, wasms)
                    .await?;

                Ok(AdminResponse::CoordinatorsUpdated)
            }
            InstallApp(payload) => {
                let app: InstalledApp = self
                    .conductor_handle
                    .clone()
                    .install_app_bundle(*payload)
                    .await?;
                let dna_definitions = self.conductor_handle.get_dna_definitions(&app)?;
                Ok(AdminResponse::AppInstalled(AppInfo::from_installed_app(
                    &app,
                    &dna_definitions,
                )))
            }
            UninstallApp {
                installed_app_id,
                force,
            } => {
                self.conductor_handle
                    .clone()
                    .uninstall_app(&installed_app_id, force)
                    .await?;
                Ok(AdminResponse::AppUninstalled)
            }
            ListDnas => {
                let dna_list = self.conductor_handle.list_dnas();
                Ok(AdminResponse::DnasListed(dna_list))
            }
            GenerateAgentPubKey => {
                let agent_pub_key = self
                    .conductor_handle
                    .keystore()
                    .clone()
                    .new_sign_keypair_random()
                    .await?;
                Ok(AdminResponse::AgentPubKeyGenerated(agent_pub_key))
            }
            RevokeAgentKey(payload) => {
                let RevokeAgentKeyPayload { agent_key, app_id } = *payload;
                let results = self
                    .conductor_handle
                    .clone()
                    .revoke_agent_key_for_app(agent_key, app_id)
                    .await?;
                // Convert errors to strings
                let results: Vec<(CellId, String)> = results
                    .into_iter()
                    .filter_map(|(cell_id, result)| {
                        if let Err(err) = result {
                            Some((cell_id, err.to_string()))
                        } else {
                            None
                        }
                    })
                    .collect();
                Ok(AdminResponse::AgentKeyRevoked(results))
            }
            ListCellIds => {
                let cell_ids = self
                    .conductor_handle
                    .running_cell_ids()
                    .into_iter()
                    .collect();
                Ok(AdminResponse::CellIdsListed(cell_ids))
            }
            ListApps { status_filter } => {
                let apps = self.conductor_handle.list_apps(status_filter).await?;
                Ok(AdminResponse::AppsListed(apps))
            }
            EnableApp { installed_app_id } => {
                // Enable app
                let (app, errors) = self
                    .conductor_handle
                    .clone()
                    .enable_app(installed_app_id.clone())
                    .await?;

                let app_cells: HashSet<_> = app.required_cells().collect();

                let app_info = self
                    .conductor_handle
                    .get_app_info(&installed_app_id)
                    .await?
                    .ok_or(ConductorError::AppNotInstalled(installed_app_id))?;

                let errors: Vec<_> = errors
                    .into_iter()
                    .filter(|(cell_id, _)| app_cells.contains(cell_id))
                    .map(|(cell_id, error)| (cell_id, error.to_string()))
                    .collect();

                Ok(AdminResponse::AppEnabled {
                    app: app_info,
                    errors,
                })
            }
            DisableApp { installed_app_id } => {
                // Disable app
                self.conductor_handle
                    .clone()
                    .disable_app(installed_app_id, DisabledAppReason::User)
                    .await?;
                Ok(AdminResponse::AppDisabled)
            }
            AttachAppInterface {
                port,
                allowed_origins,
                installed_app_id,
            } => {
                let port = port.unwrap_or(0);
                let port = self
                    .conductor_handle
                    .clone()
                    .add_app_interface(
                        either::Either::Left(port),
                        allowed_origins,
                        installed_app_id,
                    )
                    .await?;
                Ok(AdminResponse::AppInterfaceAttached { port })
            }
            ListAppInterfaces => {
                let interfaces = self.conductor_handle.list_app_interfaces().await?;
                Ok(AdminResponse::AppInterfacesListed(interfaces))
            }
            DumpState { cell_id } => {
                let state = self.conductor_handle.dump_cell_state(&cell_id).await?;
                Ok(AdminResponse::StateDumped(state))
            }
            DumpConductorState => {
                let state = self.conductor_handle.dump_conductor_state().await?;
                Ok(AdminResponse::ConductorStateDumped(state))
            }
            DumpFullState {
                cell_id,
                dht_ops_cursor,
            } => {
                let state = self
                    .conductor_handle
                    .dump_full_cell_state(&cell_id, dht_ops_cursor)
                    .await?;
                Ok(AdminResponse::FullStateDumped(state))
            }
            DumpNetworkMetrics { dna_hash } => {
                let dump = self.conductor_handle.dump_network_metrics(dna_hash).await?;
                Ok(AdminResponse::NetworkMetricsDumped(dump))
            }
            DumpNetworkStats => {
                let stats = self.conductor_handle.dump_network_stats().await?;
                Ok(AdminResponse::NetworkStatsDumped(stats))
            }
            AddAgentInfo { agent_infos } => {
                self.conductor_handle.add_agent_infos(agent_infos).await?;
                Ok(AdminResponse::AgentInfoAdded)
            }
            AgentInfo { cell_id } => {
                let r = self.conductor_handle.get_agent_infos(cell_id).await?;
                Ok(AdminResponse::AgentInfo(r))
            }
            GraftRecords {
                cell_id,
                validate,
                records,
            } => {
                self.conductor_handle
                    .clone()
                    .graft_records_onto_source_chain(cell_id, validate, records)
                    .await?;
                Ok(AdminResponse::RecordsGrafted)
            }
            GrantZomeCallCapability(payload) => {
                self.conductor_handle
                    .clone()
                    .grant_zome_call_capability(*payload)
                    .await?;
                Ok(AdminResponse::ZomeCallCapabilityGranted)
            }

            ListCapabilityGrants {
                installed_app_id,
                include_revoked,
            } => {
                let state = self.conductor_handle.clone().get_state().await?;
                let app = state.get_app(&installed_app_id)?;
                let app_cells: HashSet<CellId> = app.required_cells().collect();
                let cap_info = self
                    .conductor_handle
                    .clone()
                    .capability_grant_info(&app_cells, include_revoked)
                    .await?;
                Ok(AdminResponse::CapabilityGrantsInfo(cap_info))
            }

            DeleteCloneCell(payload) => {
                self.conductor_handle
                    .clone()
                    .delete_clone_cell(&payload)
                    .await?;
                Ok(AdminResponse::CloneCellDeleted)
            }
            StorageInfo => Ok(AdminResponse::StorageInfo(
                self.conductor_handle.storage_info().await?,
            )),
            IssueAppAuthenticationToken(payload) => {
                Ok(AdminResponse::AppAuthenticationTokenIssued(
                    self.conductor_handle
                        .issue_app_authentication_token(payload)?,
                ))
            }
            RevokeAppAuthenticationToken(token) => {
                self.conductor_handle
                    .revoke_app_authentication_token(token)?;
                Ok(AdminResponse::AppAuthenticationTokenRevoked)
            }
            GetCompatibleCells(dna_hash) => Ok(AdminResponse::CompatibleCells(
                self.conductor_handle
                    .cells_by_dna_lineage(&dna_hash)
                    .await?,
            )),
        }
    }
}

#[cfg(test)]
mod test {
    use super::*;
    use crate::conductor::Conductor;
    use anyhow::Result;
    use holochain_state::prelude::*;
    use holochain_trace;
    use holochain_types::test_utils::fake_dna_zomes;
    use holochain_types::test_utils::write_fake_dna_file;
    use holochain_wasm_test_utils::TestWasm;
    use matches::assert_matches;
    use uuid::Uuid;

    #[tokio::test(flavor = "multi_thread")]
    async fn register_list_dna_app() -> Result<()> {
        holochain_trace::test_run();
        let env_dir = test_db_dir();
        let handle = Conductor::builder()
            .with_data_root_path(env_dir.path().to_path_buf().into())
            .test(&[])
            .await?;

        let admin_api = AdminInterfaceApi::new(handle.clone());
        let network_seed = Uuid::new_v4();
        let dna = fake_dna_zomes(
            &network_seed.to_string(),
            vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
        );
        let dna_hash = dna.dna_hash().clone();
        let (dna_path, _tempdir) = write_fake_dna_file(dna.clone()).await.unwrap();
        let path_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none(),
            source: DnaSource::Path(dna_path.clone()),
        };
        let path_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(path_payload)))
            .await;
        assert_matches!(
            path_install_response,
            AdminResponse::DnaRegistered(h) if h == dna_hash
        );

        // re-register idempotent
        let path_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none(),
            source: DnaSource::Path(dna_path.clone()),
        };
        let path1_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(path_payload)))
            .await;
        assert_matches!(
            path1_install_response,
            AdminResponse::DnaRegistered(h) if h == dna_hash
        );

        let dna_list = admin_api.handle_admin_request(AdminRequest::ListDnas).await;
        let expects = vec![dna_hash.clone()];
        assert_matches!(dna_list, AdminResponse::DnasListed(a) if a == expects);

        // register by hash
        let hash_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none(),
            source: DnaSource::Hash(dna_hash.clone()),
        };

        // without modifiers seed should throw error
        let hash_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(hash_payload)))
            .await;
        assert_matches!(
            hash_install_response,
            AdminResponse::Error(ExternalApiWireError::DnaReadError(e)) if e == *"DnaSource::Hash requires `properties` or `network_seed` or `origin_time` to create a derived Dna"
        );

        // with a property should install and produce a different hash
        let json: serde_yaml::Value = serde_yaml::from_str("some prop: \"foo\"").unwrap();
        let hash_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none().with_properties(YamlProperties::new(json.clone())),
            source: DnaSource::Hash(dna_hash.clone()),
        };
        let install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(hash_payload)))
            .await;
        assert_matches!(
            install_response,
            AdminResponse::DnaRegistered(hash) if hash != dna_hash
        );

        // with a network seed should install and produce a different hash
        let hash_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none()
                .with_network_seed(String::from("12345678900000000000000")),
            source: DnaSource::Hash(dna_hash.clone()),
        };
        let hash2_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(hash_payload)))
            .await;

        let new_hash = if let AdminResponse::DnaRegistered(ref h) = hash2_install_response {
            h.clone()
        } else {
            unreachable!()
        };

        assert_matches!(
            hash2_install_response,
            AdminResponse::DnaRegistered(hash) if hash != dna_hash
        );

        // from a path with a same network seed should return the already registered hash so it's idempotent
        let path_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none()
                .with_network_seed(String::from("12345678900000000000000")),
            source: DnaSource::Path(dna_path.clone()),
        };
        let path2_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(path_payload)))
            .await;
        assert_matches!(
            path2_install_response,
            AdminResponse::DnaRegistered(hash) if hash == new_hash
        );

        // from a path with different network seed should produce different hash
        let path_payload = RegisterDnaPayload {
            modifiers: DnaModifiersOpt::none().with_network_seed(String::from("foo")),
            source: DnaSource::Path(dna_path),
        };
        let path3_install_response = admin_api
            .handle_admin_request(AdminRequest::RegisterDna(Box::new(path_payload)))
            .await;
        assert_matches!(
            path3_install_response,
            AdminResponse::DnaRegistered(hash) if hash != dna_hash
        );

        tokio::time::timeout(std::time::Duration::from_secs(1), handle.shutdown())
            .await
            .ok();
        Ok(())
    }

    // @todo fix test by using new InstallApp call
    // #[tokio::test(flavor = "multi_thread")]
    // async fn install_list_dna_app() {
    // holochain_trace::test_run();
    // let db_dir = test_db_dir();
    // let handle = Conductor::builder().test(db_dir.path(), &[]).await.unwrap();
    // let shutdown = handle.take_shutdown_handle().unwrap();
    // let admin_api = RealAdminInterfaceApi::new(handle.clone());
    // let network_seed = Uuid::new_v4();
    // let dna = fake_dna_zomes(
    //     &network_seed.to_string(),
    //     vec![(TestWasm::Foo.into(), TestWasm::Foo.into())],
    // );
    // let (dna_path, _tempdir) = write_fake_dna_file(dna.clone()).await.unwrap();
    // let agent_key1 = fake_agent_pubkey_1();

    // attempt install with a hash before the DNA has been registered
    // let dna_hash = dna.dna_hash().clone();
    // let hash_payload = InstallAppDnaPayload::hash_only(dna_hash.clone(), "".to_string());
    // let hash_install_payload = InstallAppPayload {
    //     dnas: vec![hash_payload],
    //     installed_app_id: "test-by-hash".to_string(),
    //     agent_key: agent_key1,
    // };
    // let install_response = admin_api
    //     .handle_admin_request(AdminRequest::InstallApp(Box::new(
    //         hash_install_payload.clone(),
    //     )))
    //     .await;
    // assert_matches!(
    //     install_response,
    //     AdminResponse::Error(ExternalApiWireError::DnaReadError(e)) if e == format!("Given dna has not been registered: {}", dna_hash)
    // );

    // now register a DNA
    // let path_payload = RegisterDnaPayload {
    //     modifiers: DnaModifiersOpt::none(),
    //     source: DnaSource::Path(dna_path),
    // };
    // let path_install_response = admin_api
    //     .handle_admin_request(AdminRequest::RegisterDna(Box::new(path_payload)))
    //     .await;
    // assert_matches!(
    //     path_install_response,
    //     AdminResponse::DnaRegistered(h) if h == dna_hash
    // );

    // let agent_key2 = fake_agent_pubkey_2();
    // let path_payload = InstallAppDnaPayload::hash_only(dna_hash.clone(), "".to_string());
    // let cell_id2 = CellId::new(dna_hash.clone(), agent_key2.clone());
    // let expected_installed_app = InstalledApp::new_fresh(
    //     InstalledAppCommon::new_legacy(
    //         "test-by-path".to_string(),
    //         vec![InstalledCell::new(cell_id2.clone(), "".to_string())],
    //     )
    //     .unwrap(),
    // );
    // let expected_installed_app_info: InstalledAppInfo = (&expected_installed_app).into();
    // let path_install_payload = InstallAppPayload {
    //     dnas: vec![path_payload],
    //     installed_app_id: "test-by-path".to_string(),
    //     agent_key: agent_key2,
    // };

    // let install_response = admin_api
    //     .handle_admin_request(AdminRequest::InstallApp(Box::new(path_install_payload)))
    //     .await;
    // assert_matches!(
    //     install_response,
    //     AdminResponse::AppInstalled(info) if info == expected_installed_app_info
    // );
    // let dna_list = admin_api.handle_admin_request(AdminRequest::ListDnas).await;
    // let expects = vec![dna_hash.clone()];
    // assert_matches!(dna_list, AdminResponse::DnasListed(a) if a == expects);

    // let expected_enabled_app = InstalledApp::new_running(
    //     InstalledAppCommon::new_legacy(
    //         "test-by-path".to_string(),
    //         vec![InstalledCell::new(cell_id2.clone(), "".to_string())],
    //     )
    //     .unwrap(),
    // );
    // let expected_enabled_app_info: InstalledAppInfo = (&expected_enabled_app).into();
    // let res = admin_api
    //     .handle_admin_request(AdminRequest::EnableApp {
    //         installed_app_id: "test-by-path".to_string(),
    //     })
    //     .await;
    // assert_matches!(res,
    //     AdminResponse::AppEnabled {app, ..} if app == expected_enabled_app_info
    // );

    // let res = admin_api
    //     .handle_admin_request(AdminRequest::ListCellIds)
    //     .await;

    // assert_matches!(res, AdminResponse::CellIdsListed(v) if v == vec![cell_id2]);

    // now try to install the happ using the hash
    // let _install_response = admin_api
    //     .handle_admin_request(AdminRequest::InstallApp(Box::new(hash_install_payload)))
    //     .await;
    // let _res = admin_api
    //     .handle_admin_request(AdminRequest::EnableApp {
    //         installed_app_id: "test-by-hash".to_string(),
    //     })
    //     .await;

    // let res = admin_api
    //     .handle_admin_request(AdminRequest::ListApps {
    //         status_filter: Some(AppStatusFilter::Enabled),
    //     })
    //     .await;

    // assert_matches!(res, AdminResponse::AppsListed(v) if v.iter().find(|app_info| app_info.installed_app_id.as_str() == "test-by-path").is_some() && v.iter().find(|app_info| app_info.installed_app_id.as_str() == "test-by-hash").is_some());

    // handle.shutdown();
    // tokio::time::timeout(std::time::Duration::from_secs(1), shutdown)
    //     .await
    //     .ok();
    // }
}



================================================
File: crates/holochain/src/conductor/api/api_external/app_interface.rs
================================================
use crate::conductor::api::error::ConductorApiError;
use crate::conductor::api::error::ConductorApiResult;
use crate::conductor::api::error::SerializationError;
use crate::conductor::interface::error::InterfaceError;
use crate::conductor::interface::error::InterfaceResult;
use crate::conductor::ConductorHandle;

use holochain_serialized_bytes::prelude::*;

use holochain_types::prelude::*;

pub use holochain_conductor_api::*;

/// The Conductor lives inside an Arc<RwLock<_>> which is shared with all
/// other Api references
#[derive(Clone)]
pub struct AppInterfaceApi {
    conductor_handle: ConductorHandle,
}

impl AppInterfaceApi {
    /// Create a new instance from a shared Conductor reference
    pub fn new(conductor_handle: ConductorHandle) -> Self {
        Self { conductor_handle }
    }

    /// Check an authentication request and return the app that access has been granted
    /// for on success.
    pub async fn auth(&self, auth: AppAuthentication) -> InterfaceResult<InstalledAppId> {
        self.conductor_handle
            .authenticate_app_token(auth.token, auth.installed_app_id)
            .map_err(Box::new)
            .map_err(InterfaceError::RequestHandler)
    }

    /// Handle an [AppRequest] in the context of an [InstalledAppId], and return an [AppResponse].
    pub async fn handle_request(
        &self,
        installed_app_id: InstalledAppId,
        request: Result<AppRequest, SerializedBytesError>,
    ) -> InterfaceResult<AppResponse> {
        {
            self.conductor_handle
                .check_running()
                .map_err(Box::new)
                .map_err(InterfaceError::RequestHandler)?;
        }
        match request {
            Ok(request) => Ok(self.handle_app_request(installed_app_id, request).await),
            Err(e) => Ok(AppResponse::Error(SerializationError::from(e).into())),
        }
    }

    /// Deal with error cases produced by `handle_app_request_inner`
    async fn handle_app_request(
        &self,
        installed_app_id: InstalledAppId,
        request: AppRequest,
    ) -> AppResponse {
        tracing::debug!("app request: {:?}", request);

        let res = self
            .handle_app_request_inner(installed_app_id, request)
            .await
            .unwrap_or_else(|e| AppResponse::Error(e.into()));
        tracing::debug!("app response: {:?}", res);
        res
    }

    /// Routes the [AppRequest] to the [AppResponse]
    async fn handle_app_request_inner(
        &self,
        installed_app_id: InstalledAppId,
        request: AppRequest,
    ) -> ConductorApiResult<AppResponse> {
        match request {
            AppRequest::AppInfo => Ok(AppResponse::AppInfo(
                self.conductor_handle
                    .get_app_info(&installed_app_id)
                    .await?,
            )),
            AppRequest::CallZome(zome_call_params_signed) => {
                match self.conductor_handle.handle_external_zome_call(*zome_call_params_signed).await? {
                    Ok(ZomeCallResponse::Ok(output)) => Ok(AppResponse::ZomeCalled(Box::new(output))),
                    Ok(ZomeCallResponse::AuthenticationFailed(signature, provenance)) => Ok(AppResponse::Error(
                        ExternalApiWireError::ZomeCallAuthenticationFailed(format!(
                            "Authentication failure. Bad signature {:?} by provenance {:?}.",
                            signature, provenance,
                        )),
                    )),
                    Ok(ZomeCallResponse::Unauthorized(zome_call_authorization, cap_secret, zome_name, fn_name)) => Ok(AppResponse::Error(
                        ExternalApiWireError::ZomeCallUnauthorized(format!(
                            "Call was not authorized with reason {:?}, cap secret {:?} to call the function {} in zome {}",
                            zome_call_authorization, cap_secret, fn_name, zome_name
                        )),
                    )),
                    Ok(ZomeCallResponse::NetworkError(e)) => unreachable!(
                        "Interface zome calls should never be routed to the network. This is a bug. Got {}",
                        e
                    ),
                    Ok(ZomeCallResponse::CountersigningSession(e)) => Ok(AppResponse::Error(
                        ExternalApiWireError::CountersigningSessionError(format!(
                            "A countersigning session has failed to start on this zome call because: {}",
                            e
                        )),
                    )),
                    Err(e) => Ok(AppResponse::Error(e.into())),
                }
            }
            #[cfg(feature = "unstable-countersigning")]
            AppRequest::GetCountersigningSessionState(payload) => {
                let countersigning_session_state = self
                    .conductor_handle
                    .clone()
                    .get_countersigning_session_state(&payload)
                    .await?;
                Ok(AppResponse::CountersigningSessionState(Box::new(
                    countersigning_session_state,
                )))
            }
            #[cfg(feature = "unstable-countersigning")]
            AppRequest::AbandonCountersigningSession(payload) => {
                self.conductor_handle
                    .clone()
                    .abandon_countersigning_session(&payload)
                    .await?;
                Ok(AppResponse::CountersigningSessionAbandoned)
            }
            #[cfg(feature = "unstable-countersigning")]
            AppRequest::PublishCountersigningSession(payload) => {
                self.conductor_handle
                    .clone()
                    .publish_countersigning_session(&payload)
                    .await?;
                Ok(AppResponse::PublishCountersigningSessionTriggered)
            }
            AppRequest::CreateCloneCell(payload) => {
                let clone_cell = self
                    .conductor_handle
                    .clone()
                    .create_clone_cell(&installed_app_id, *payload)
                    .await?;
                Ok(AppResponse::CloneCellCreated(clone_cell))
            }
            AppRequest::DisableCloneCell(payload) => {
                self.conductor_handle
                    .clone()
                    .disable_clone_cell(&installed_app_id, &payload)
                    .await?;
                Ok(AppResponse::CloneCellDisabled)
            }
            AppRequest::EnableCloneCell(payload) => {
                let enabled_cell = self
                    .conductor_handle
                    .clone()
                    .enable_clone_cell(&installed_app_id, &payload)
                    .await?;
                Ok(AppResponse::CloneCellEnabled(enabled_cell))
            }
            AppRequest::NetworkInfo(payload) => {
                let info = self
                    .conductor_handle
                    .network_info(&installed_app_id, &payload)
                    .await?;
                Ok(AppResponse::NetworkInfo(info))
            }
            AppRequest::ListWasmHostFunctions => Ok(AppResponse::ListWasmHostFunctions(
                self.conductor_handle.list_wasm_host_functions().await?,
            )),
            AppRequest::ProvideMemproofs(memproofs) => {
                self.conductor_handle
                    .clone()
                    .provide_memproofs(&installed_app_id, memproofs)
                    .await?;
                Ok(AppResponse::Ok)
            }
            AppRequest::EnableApp => {
                let status = self
                    .conductor_handle
                    .get_app_info(&installed_app_id)
                    .await?
                    .ok_or(ConductorApiError::other("app not found".to_string()))?
                    .status;
                match status {
                    AppInfoStatus::Running
                    | AppInfoStatus::Disabled {
                        reason: DisabledAppReason::NotStartedAfterProvidingMemproofs,
                    } => {
                        self.conductor_handle
                            .clone()
                            .enable_app(installed_app_id.clone())
                            .await?;
                        Ok(AppResponse::Ok)
                    }
                    _ => Err(ConductorApiError::other(
                        "app not in correct state to enable".to_string(),
                    )),
                }
            } //
              // TODO: implement after DPKI lands
              // AppRequest::RotateAppAgentKey => {
              //     let new_key = self
              //         .conductor_handle
              //         .rotate_app_agent_key(&installed_app_id)
              //         .await?;
              //     Ok(AppResponse::AppAgentKeyRotated(new_key))
              // }
        }
    }
}

/// The payload for authenticating an app interface connection
#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct AppAuthentication {
    /// The token received from the admin interface, demonstrating that the app is allowed
    /// to connect.
    pub token: Vec<u8>,

    /// If the app interface is bound to an installed app, this is the ID of that app. This field
    /// must be provided by Holochain and not the client.
    pub installed_app_id: Option<InstalledAppId>,
}



================================================
File: crates/holochain/src/conductor/cell/error.rs
================================================
use super::INIT_MUTEX_TIMEOUT_SECS;
use crate::conductor::entry_def_store::error::EntryDefStoreError;
use crate::conductor::{api::error::ConductorApiError, error::ConductorError};
use crate::core::ribosome::error::RibosomeError;
use crate::core::ribosome::guest_callback::init::InitResult;
use crate::core::workflow::WorkflowError;
use crate::core::SourceChainError;
use holochain_cascade::error::CascadeError;
use holochain_p2p::HolochainP2pError;
use holochain_sqlite::error::DatabaseError;
use holochain_types::prelude::*;
use holochain_zome_types::cell::CellId;

use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum CellError {
    #[error("error dealing with workspace state: {0}")]
    DatabaseError(#[from] DatabaseError),
    #[error(transparent)]
    CascadeError(#[from] CascadeError),
    #[error("Failed to join the create cell task: {0}")]
    JoinError(#[from] tokio::task::JoinError),
    #[error("Genesis failed: {0}")]
    Genesis(Box<ConductorApiError>),
    #[error(transparent)]
    ActionError(#[from] ActionError),
    #[error("This cell has not had a successful genesis and cannot be created")]
    CellWithoutGenesis(CellId),
    #[error("The cell with id {0} is disabled.")]
    CellDisabled(CellId),
    #[error("Authentication failure. Bad signature {0:?} by provenance {1}.")]
    ZomeCallAuthenticationFailed(Signature, AgentPubKey),
    #[error(
        "The cell failed to cleanup its environment because: {0}. Recommend manually deleting the database at: {1}"
    )]
    Cleanup(String, PathBuf),
    #[error(transparent)]
    DnaError(#[from] DnaError),
    #[error(transparent)]
    EntryDefStoreError(#[from] EntryDefStoreError),
    #[error(transparent)]
    WorkflowError(#[from] Box<WorkflowError>),
    #[error(transparent)]
    WorkspaceError(#[from] holochain_state::workspace::WorkspaceError),
    #[error(transparent)]
    RibosomeError(#[from] RibosomeError),
    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),
    #[error("The cell tried to run the initialize zomes callback but failed because {0:?}")]
    InitFailed(InitResult),
    #[error(
        "Another zome function has triggered the `init()` callback, which has been blocking this zome call for longer than {} seconds. Giving up.",
        INIT_MUTEX_TIMEOUT_SECS
    )]
    InitTimeout,
    #[error("Failed to get or create the cache for this dna {0:?}")]
    FailedToCreateCache(Box<ConductorError>),
    #[error("Failed to get or create the authored db for this dna {0:?}")]
    FailedToCreateAuthoredDb(Box<ConductorError>),
    #[error("Failed to get or create the DHT db for this dna {0:?}")]
    FailedToCreateDhtDb(Box<ConductorError>),
    #[error("Failed to get or create the dna space {0:?}")]
    FailedToCreateDnaSpace(Box<ConductorError>),
    #[error(transparent)]
    HolochainP2pError(#[from] HolochainP2pError),
    #[error(transparent)]
    ConductorError(#[from] Box<ConductorError>),
    #[error(transparent)]
    ConductorApiError(#[from] Box<ConductorApiError>),
    #[error(transparent)]
    SerializedBytesError(#[from] holochain_serialized_bytes::SerializedBytesError),
    #[error("Todo")]
    Todo,
    #[error("The op: {0:?} is missing for this receipt")]
    OpMissingForReceipt(DhtOpHash),
    #[error(transparent)]
    StateQueryError(#[from] holochain_state::query::StateQueryError),
    #[error(transparent)]
    StateMutationError(#[from] holochain_state::mutations::StateMutationError),
    #[error(transparent)]
    OneErr(#[from] one_err::OneErr),
}

pub type CellResult<T> = Result<T, CellError>;



================================================
File: crates/holochain/src/conductor/cell/gossip_test.rs
================================================
use crate::sweettest::*;
use crate::test_utils::inline_zomes::simple_create_read_zome;
use hdk::prelude::*;
use holochain_conductor_api::conductor::ConductorConfig;
use holochain_sqlite::store::AsP2pStateReadExt;
use holochain_test_wasm_common::AnchorInput;
use holochain_wasm_test_utils::TestWasm;
use kitsune_p2p_types::config::TransportConfig;

#[tokio::test(flavor = "multi_thread")]
#[ignore = "This test is flaky"]
async fn gossip_test() {
    holochain_trace::test_run();
    let config = SweetConductorConfig::standard().no_publish();
    let mut conductors = SweetConductorBatch::from_config(2, config).await;

    let (dna_file, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Anchor]).await;

    let apps = conductors.setup_app("app", &[dna_file]).await.unwrap();
    let ((cell_1,), (cell_2,)) = apps.into_tuples();
    conductors.exchange_peer_info().await;

    let anchor = AnchorInput("alice".to_string(), "0".to_string());
    let _: EntryHash = conductors[0]
        .call(&cell_1.zome(TestWasm::Anchor), "anchor", anchor)
        .await;

    await_consistency(60, [&cell_1, &cell_2]).await.unwrap();

    let hashes: EntryHashes = conductors[1]
        .call(
            &cell_2.zome(TestWasm::Anchor),
            "list_anchor_addresses",
            "alice",
        )
        .await;
    assert_eq!(hashes.0.len(), 1);
}

#[tokio::test(flavor = "multi_thread")]
async fn signature_smoke_test() {
    holochain_trace::test_run();

    let rendezvous = SweetLocalRendezvous::new().await;

    let mut config = ConductorConfig::default();
    config.network.transport_pool = vec![TransportConfig::Mem {}];
    // Hit a bootstrap service so it can blow up and return an error if we get our end of
    // things totally wrong.
    config.network.bootstrap_service = Some(url2::url2!("{}", rendezvous.bootstrap_addr()));
    let zomes = vec![TestWasm::Anchor];
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(zomes).await;
    let mut conductor = SweetConductor::from_config_rendezvous(config, rendezvous).await;

    conductor.setup_app("app", [&dna]).await.unwrap();

    // TODO should check that the app is running otherwise we don't know if bootstrap was called
    conductor.shutdown().await;
}

#[tokio::test(flavor = "multi_thread")]
async fn agent_info_test() {
    holochain_trace::test_run();
    let config = SweetConductorConfig::standard().no_publish().no_dpki();
    let mut conductors = SweetConductorBatch::from_config(2, config).await;

    let (dna_file, _, _) =
        SweetDnaFile::unique_from_inline_zomes(("zome", simple_create_read_zome())).await;

    let apps = conductors.setup_app("app", &[dna_file]).await.unwrap();
    let ((cell_1,), (cell_2,)) = apps.into_tuples();
    conductors.exchange_peer_info().await;

    let p2p_agents_dbs: Vec<_> = conductors
        .iter()
        .filter_map(|c| {
            c.spaces
                .get_from_spaces(|s| s.p2p_agents_db.clone())
                .first()
                .cloned()
        })
        .collect();

    await_consistency(10, [&cell_1, &cell_2]).await.unwrap();
    assert_eq!(p2p_agents_dbs.len(), 2);
    for p2p_agents_db in p2p_agents_dbs {
        let len = p2p_agents_db.p2p_count_agents().await.unwrap();
        assert_eq!(len, 2);
    }
}



================================================
File: crates/holochain/src/conductor/cell/test.rs
================================================
use crate::conductor::space::TestSpaces;
use crate::conductor::Conductor;
use crate::core::ribosome::real_ribosome::{ModuleCacheLock, RealRibosome};
use crate::core::workflow::incoming_dht_ops_workflow::op_exists;
use crate::test_utils::{fake_valid_dna_file, test_network};
use holo_hash::HasHash;
use holochain_conductor_api::conductor::paths::DataRootPath;
use holochain_state::prelude::*;
use holochain_wasmer_host::module::ModuleCache;
use holochain_zome_types::action;
use std::sync::Arc;
use tokio::sync::broadcast;

#[tokio::test(flavor = "multi_thread")]
async fn test_cell_handle_publish() {
    let keystore = holochain_keystore::test_keystore();

    let agent_key = keystore.new_sign_keypair_random().await.unwrap();
    let dna_file = fake_valid_dna_file("test_cell_handle_publish");
    let cell_id = CellId::new(dna_file.dna_hash().clone(), agent_key);
    let dna = cell_id.dna_hash().clone();
    let agent = cell_id.agent_pubkey().clone();

    let spaces = TestSpaces::new([dna.clone()]).await;
    let db = spaces.test_spaces[&dna]
        .space
        .get_or_create_authored_db(cell_id.agent_pubkey().clone())
        .unwrap();
    let dht_db = spaces.test_spaces[&dna].space.dht_db.clone();
    let dht_db_cache = spaces.test_spaces[&dna].space.dht_query_cache.clone();

    let test_network = test_network(Some(dna.clone()), Some(agent.clone())).await;
    let holochain_p2p_cell = test_network.dna_network();

    let db_dir = test_db_dir().path().to_path_buf();
    let data_root_path: DataRootPath = db_dir.clone().into();
    let handle = Conductor::builder()
        .config(
            crate::sweettest::SweetConductorConfig::standard()
                .no_dpki()
                .into(),
        )
        .with_keystore(keystore.clone())
        .with_data_root_path(data_root_path.clone())
        .test(&[])
        .await
        .unwrap();
    handle.register_dna(dna_file.clone()).await.unwrap();
    let wasmer_module_cache = Some(Arc::new(ModuleCacheLock::new(ModuleCache::new(Some(
        db_dir.join("wasm-cache"),
    )))));

    let ribosome = RealRibosome::new(dna_file, wasmer_module_cache)
        .await
        .unwrap();

    super::Cell::genesis(
        cell_id.clone(),
        handle.clone(),
        db.clone(),
        dht_db.clone(),
        dht_db_cache.clone(),
        ribosome,
        None,
        None,
    )
    .await
    .unwrap();

    let (_cell, _) = super::Cell::create(
        cell_id,
        handle.clone(),
        spaces.test_spaces[&dna].space.clone(),
        holochain_p2p_cell,
        broadcast::channel(10).0,
    )
    .await
    .unwrap();

    let action = action::Action::Dna(action::Dna {
        author: agent.clone(),
        timestamp: Timestamp::now(),
        hash: dna.clone(),
    });
    let hh = ActionHashed::from_content_sync(action.clone());
    let shh = SignedActionHashed::sign(&keystore, hh).await.unwrap();
    let op = ChainOp::StoreRecord(shh.signature().clone(), action.clone(), RecordEntry::NA);
    let op_hash = DhtOpHashed::from_content_sync(op.clone()).into_hash();

    spaces
        .spaces
        .handle_publish(&dna, true, false, vec![op.clone().into()])
        .await
        .unwrap();

    op_exists(&dht_db, op_hash).await.unwrap();

    handle.shutdown().await.unwrap().unwrap();
}



================================================
File: crates/holochain/src/conductor/conductor/agent_key_operations.rs
================================================
//! Tests related to key revocation are located under [tests/agent_key_revocation](tests).

use holochain_types::deepkey_roundtrip_backward;

use super::*;

/// The result type of an agent key revocation for an app.
pub type RevokeAgentKeyForAppResult = HashMap<CellId, ConductorApiResult<()>>;

impl Conductor {
    /// Revoke an agent's key pair for all cells of an app.
    ///
    /// Writes a `Delete` action to the source chain of all cells of the app, which renders them read-only.
    /// If DPKI is installed as conductor service, the agent key will be revoked there too and becomes
    /// invalid.
    pub async fn revoke_agent_key_for_app(
        self: Arc<Self>,
        agent_key: AgentPubKey,
        app_id: InstalledAppId,
    ) -> ConductorResult<RevokeAgentKeyForAppResult> {
        // Disable app while revoking key
        self.clone()
            .disable_app(app_id.clone(), DisabledAppReason::DeletingAgentKey)
            .await?;

        // Revoke key in DPKI first, if installed, and then in cells' source chains.
        // Call separate function so that in case a part of key revocation fails, the app is still enabled again.
        let revocation_per_cell_results =
            Conductor::revoke_agent_key_for_app_inner(self.clone(), agent_key, app_id.clone())
                .await;

        // Enable app again.
        self.clone().enable_app(app_id.clone()).await?;

        let revocation_per_cell_results = revocation_per_cell_results?;

        // Publish 'Delete' actions of cells where successful.
        // Triggering workflow is only possible when cells are enabled.
        let publish_workflow_triggers = revocation_per_cell_results
            .iter()
            .filter(|(_, result)| result.is_ok())
            .map({
                |(cell_id, _)| {
                    let conductor = self.clone();
                    async move {
                        match conductor.cell_by_id(cell_id).await {
                            Ok(cell) => {
                                cell.publish_authored_ops();
                                // Even though integration somehow happens in multi-conductor tests,
                                // it's not clear why it does, so it's safer to trigger it explicitly.
                                cell.notify_authored_ops_moved_to_limbo();
                            }
                            Err(err) => tracing::warn!(
                                ?err,
                                ?cell_id,
                                "Could not find cell to publish agent key deletion"
                            ),
                        }
                    }
                }
            });
        futures::future::join_all(publish_workflow_triggers).await;

        // Return cell ids with their agent key deletion result
        Ok(revocation_per_cell_results)
    }

    /// Revoke agent key in Deepkey first, if installed, and then write a [`Delete`] of the key to the source chain.
    async fn revoke_agent_key_for_app_inner(
        conductor: Arc<Conductor>,
        agent_key: AgentPubKey,
        app_id: InstalledAppId,
    ) -> ConductorResult<RevokeAgentKeyForAppResult> {
        // If DPKI service is installed, revoke agent key there first
        if let Some(dpki_service) = conductor.running_services().dpki {
            let dpki_state = dpki_service.state().await;
            let timestamp = Timestamp::now();
            let key_state = dpki_state.key_state(agent_key.clone(), timestamp).await?;
            match key_state {
                KeyState::NotFound => {
                    return Err(ConductorError::DpkiError(
                        DpkiServiceError::DpkiAgentMissing(agent_key.clone()),
                    ))
                }
                // If the key already is invalid, do nothing. Operation should be idempotent to allow for
                // retries if agent key of some source chain could not be deleted successfully.
                KeyState::Invalid(_) => (),
                KeyState::Valid(_) => {
                    // Get action hash of key registration
                    let key_meta = dpki_state.query_key_meta(agent_key.clone()).await?;
                    // Sign revocation request
                    let signature = dpki_service
                        .cell_id
                        .agent_pubkey()
                        .sign_raw(
                            &conductor.keystore,
                            key_meta.key_registration_addr.get_raw_39().into(),
                        )
                        .await
                        .map_err(|e| DpkiServiceError::Lair(e.into()))?;
                    let signature = deepkey_roundtrip_backward!(Signature, &signature);
                    // Revoke key in DPKI
                    let _revocation = dpki_state
                        .revoke_key(RevokeKeyInput {
                            key_revocation: KeyRevocation {
                                prior_key_registration: key_meta.key_registration_addr,
                                revocation_authorization: vec![(0, signature)],
                            },
                        })
                        .await?;
                }
            };
        }

        // Write 'Delete' action to source chains of all cells of the app
        let state = conductor.get_state().await?;
        let app = state.get_app(&app_id)?;
        if *app.agent_key() != agent_key {
            return Err(ConductorError::AppError(AppError::AgentKeyMissing(
                agent_key, app_id,
            )));
        }
        let all_cells: Vec<CellId> = app.all_cells().collect();
        let delete_agent_key_of_all_cells = all_cells.clone().into_iter().map(|cell_id| {
            let conductor = conductor.clone();
            let agent_key = agent_key.clone();
            async move {
                // Instantiate source chain
                let source_chain = SourceChain::new(
                    conductor.get_or_create_authored_db(cell_id.dna_hash(), agent_key.clone())?,
                    conductor.get_or_create_dht_db(cell_id.dna_hash())?,
                    conductor
                        .get_or_create_space(cell_id.dna_hash())?
                        .dht_query_cache,
                    conductor.keystore().clone(),
                    agent_key.clone(),
                )
                .await?;

                // Insert `Delete` action of agent pub key into source chain
                source_chain.delete_valid_agent_pub_key().await?;
                let network = conductor
                    .holochain_p2p
                    .to_dna(cell_id.dna_hash().clone(), conductor.get_chc(&cell_id));
                source_chain
                    .flush(
                        network
                            .storage_arcs()
                            .await
                            .map_err(ConductorApiError::other)?,
                        network.chc(),
                    )
                    .await?;

                Ok::<_, ConductorApiError>(())
            }
        });
        let delete_agent_key_results =
            futures::future::join_all(delete_agent_key_of_all_cells).await;
        // Build result map with cell id as key and deletion result as value
        let cell_results: HashMap<_, _> = delete_agent_key_results
            .into_iter()
            .enumerate()
            .map(|(index, result)| (all_cells[index].clone(), result))
            .collect();

        Ok(cell_results)
    }
}



================================================
File: crates/holochain/src/conductor/conductor/app_auth_token_store.rs
================================================
use holochain_conductor_api::AppAuthenticationToken;
use rand::RngCore;
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::time::SystemTime;

use crate::conductor::error::{ConductorError, ConductorResult};
use holochain_types::prelude::InstalledAppId;

pub struct AppAuthTokenStore {
    issued_tokens: HashMap<AppAuthenticationToken, TokenMeta>,
}

impl AppAuthTokenStore {
    pub fn new() -> Self {
        Self {
            issued_tokens: HashMap::new(),
        }
    }

    /// Issue a token that can be used to authenticate a connection. The token will only be valid
    /// for use with the specified `installed_app_id` and will expire after `expiry_seconds`.
    ///
    /// If `single_use` is true, the token will be invalidated after the first use, successful or not.
    pub fn issue_token(
        &mut self,
        installed_app_id: InstalledAppId,
        expiry_seconds: u64,
        single_use: bool,
    ) -> (AppAuthenticationToken, Option<SystemTime>) {
        let mut token = [0u8; 64];
        rand::thread_rng().fill_bytes(&mut token);
        let token = token.to_vec();

        let expires_at = if expiry_seconds > 0 {
            SystemTime::now().checked_add(std::time::Duration::from_secs(expiry_seconds))
        } else {
            None
        };

        self.issued_tokens.insert(
            token.clone(),
            TokenMeta {
                installed_app_id,
                expires_at,
                single_use,
            },
        );
        self.remove_expired_tokens();

        (token, expires_at)
    }

    /// Revoke a token, making it invalid for future authentication. This should be used when a
    /// token is no longer needed.
    ///
    /// It will not error when the token does
    /// not exist, so that it is safe to revoke single-use tokens without needing a way to check if
    /// they have been used.
    pub fn revoke_token(&mut self, token: AppAuthenticationToken) {
        self.issued_tokens.remove(&token);
    }

    /// Authenticate a token and return the `InstalledAppId` that the token was issued for.
    ///
    /// If `app_id_restriction` is provided, the token will only be valid for the specified `InstalledAppId`.
    /// This is useful when an app interface is restricted to a single app and tokens that would
    /// otherwise be valid, are not valid for connecting to this app interface.
    pub fn authenticate_token(
        &mut self,
        token: AppAuthenticationToken,
        app_id_restriction: Option<InstalledAppId>,
    ) -> ConductorResult<InstalledAppId> {
        self.remove_expired_tokens();

        match self.issued_tokens.entry(token) {
            Entry::Occupied(entry) => {
                let meta = { entry.get().clone() };

                if meta.single_use {
                    entry.remove();
                }

                if let Some(app_id_restriction) = app_id_restriction {
                    if app_id_restriction != meta.installed_app_id {
                        return Err(ConductorError::FailedAuthenticationError(
                            "Attempt to use token in the context of another application"
                                .to_string(),
                        ));
                    }
                }

                let app_id = meta.installed_app_id.clone();

                Ok(app_id)
            }
            Entry::Vacant(_) => Err(ConductorError::FailedAuthenticationError(
                "Invalid token".to_string(),
            )),
        }
    }

    fn remove_expired_tokens(&mut self) {
        let current_time = SystemTime::now();

        self.issued_tokens.retain(|_, meta| {
            if let Some(expires_at) = meta.expires_at {
                expires_at > current_time
            } else {
                // Always keep tokens that are set to not expire
                true
            }
        });
    }

    #[cfg(test)]
    fn age_tokens(&mut self) {
        self.issued_tokens.iter_mut().for_each(|(_, meta)| {
            if meta.expires_at.is_none() {
                return;
            }

            meta.expires_at = Some(
                SystemTime::now()
                    .checked_sub(std::time::Duration::from_secs(10))
                    .unwrap(),
            );
        });
    }

    #[cfg(test)]
    fn get_tokens(&self) -> &HashMap<AppAuthenticationToken, TokenMeta> {
        &self.issued_tokens
    }
}

impl Default for AppAuthTokenStore {
    fn default() -> Self {
        Self::new()
    }
}

#[derive(Debug, Clone)]
struct TokenMeta {
    installed_app_id: InstalledAppId,
    expires_at: Option<SystemTime>,
    single_use: bool,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn issue_and_use_single_use_token() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 10, true);

        let authenticated_for_app = auth.authenticate_token(token.clone(), None).unwrap();
        assert_eq!(authenticated_for_app, installed_app_id);

        let result = auth.authenticate_token(token.clone(), None);
        assert!(result.is_err());
    }

    #[test]
    fn reuse_token() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 10, false);

        let authenticated_for_app = auth.authenticate_token(token.clone(), None).unwrap();
        assert_eq!(authenticated_for_app, installed_app_id);

        let authenticated_for_app = auth.authenticate_token(token.clone(), None).unwrap();
        assert_eq!(authenticated_for_app, installed_app_id);
    }

    #[test]
    fn attempt_with_token_that_does_not_exist() {
        let mut auth = AppAuthTokenStore::new();
        let result = auth.authenticate_token(vec![0; 16], None);
        assert!(result.is_err());
    }

    #[test]
    fn use_token_with_app_restriction() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 1, true);

        let result = auth.authenticate_token(token.clone(), Some(installed_app_id.clone()));
        assert_eq!(result.unwrap(), installed_app_id);
    }

    #[test]
    fn use_token_with_app_restriction_mismatch() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 1, true);

        let other_app_id = "other_app".to_string();
        let result = auth.authenticate_token(token.clone(), Some(other_app_id));
        assert!(result.is_err());

        // Token was invalidated by the use in a failed attempt
        let result = auth.authenticate_token(token.clone(), Some(installed_app_id.clone()));
        assert!(result.is_err());
    }

    #[test]
    fn use_token_with_app_restriction_mismatch_multi_use() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 1, false);

        let other_app_id = "other_app".to_string();
        let result = auth.authenticate_token(token.clone(), Some(other_app_id));
        assert!(result.is_err());

        // Token was retained through the failed attempt because the caller has used it with a
        // websocket connection that is restricted to another app.
        let result = auth.authenticate_token(token.clone(), Some(installed_app_id.clone()));
        assert_eq!(result.unwrap(), installed_app_id);
    }

    #[test]
    fn use_expired_token() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        let (token, _) = auth.issue_token(installed_app_id.clone(), 1, true);

        auth.age_tokens();

        let result = auth.authenticate_token(token.clone(), None);
        assert!(result.is_err());

        assert!(auth.get_tokens().is_empty());
    }

    #[test]
    fn issuing_new_tokens_removes_expired_tokens() {
        let mut auth = AppAuthTokenStore::new();
        let installed_app_id = "test_app".to_string();
        for _ in 0..3 {
            auth.issue_token(installed_app_id.clone(), 1, true);
        }

        assert_eq!(3, auth.get_tokens().len());
        auth.age_tokens();
        // Just to show that the test code didn't mess with the state
        assert_eq!(3, auth.get_tokens().len());

        // Having this clear out the expired tokens means that even if a client is issuing tokens that
        // don't get used, older tokens will still be dropped.
        let (token, _) = auth.issue_token(installed_app_id.clone(), 1, true);

        assert_eq!(1, auth.get_tokens().len());
        assert_eq!(token, *auth.get_tokens().iter().next().unwrap().0);
    }

    #[test]
    fn create_token_which_does_not_expire() {
        let mut auth = AppAuthTokenStore::new();
        let (token, expiry) = auth.issue_token("test_app".to_string(), 0, false);
        assert!(expiry.is_none());

        for _ in 0..3 {
            let r = auth.authenticate_token(token.clone(), None);
            assert!(r.is_ok());
        }
    }
}



================================================
File: crates/holochain/src/conductor/conductor/app_broadcast.rs
================================================
use holochain_types::app::InstalledAppId;
use holochain_types::prelude::*;
use std::collections::hash_map::Entry;
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::broadcast;

// Number of signals in buffer before we start dropping them.
// 64 gives us a good burst buffer incase multiple threads are
// sending signals at the same time and we need to catch up,
// but not so many that we have to be overly concerned about
// the memory usage implications.
const SIGNAL_BUFFER_SIZE: usize = 64;

#[derive(Debug, Clone)]
pub struct AppBroadcast {
    channels: Arc<parking_lot::Mutex<HashMap<InstalledAppId, broadcast::Sender<Signal>>>>,
}

impl AppBroadcast {
    pub(crate) fn new() -> Self {
        Self {
            channels: Arc::new(parking_lot::Mutex::new(HashMap::new())),
        }
    }

    /// Create a signal sender for a specific installed app.
    ///
    /// The app does not actually need to be installed to call this and it does not need to be
    /// called before subscribing to signals.
    pub(crate) fn create_send_handle(
        &self,
        installed_app_id: InstalledAppId,
    ) -> broadcast::Sender<Signal> {
        match self.channels.lock().entry(installed_app_id) {
            Entry::Occupied(e) => e.get().clone(),
            Entry::Vacant(e) => e.insert(broadcast::channel(SIGNAL_BUFFER_SIZE).0).clone(),
        }
    }

    /// Subscribe to signals for a specific installed app.
    ///
    /// The app does not actually need to be installed to call this and the sdner does not need to
    /// be created before subscribing.
    pub(crate) fn subscribe(
        &self,
        installed_app_id: InstalledAppId,
    ) -> broadcast::Receiver<Signal> {
        match self.channels.lock().entry(installed_app_id) {
            Entry::Occupied(e) => e.get().subscribe(),
            Entry::Vacant(e) => {
                let (tx, rx) = broadcast::channel(SIGNAL_BUFFER_SIZE);
                e.insert(tx);

                rx
            }
        }
    }

    /// Given a list of currently installed apps, retain only the channels for those apps.
    /// This is useful for cleaning up channels for apps that have been uninstalled.
    pub(crate) fn retain(&self, installed_apps: HashSet<InstalledAppId>) {
        self.channels
            .lock()
            .retain(|k, _| installed_apps.contains(k));
    }

    #[cfg(test)]
    fn keys(&self) -> Vec<InstalledAppId> {
        self.channels.lock().keys().cloned().collect()
    }
}

impl Default for AppBroadcast {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ::fixt::prelude::*;
    use hdk::prelude::CellIdFixturator;
    use hdk::prelude::ZomeNameFixturator;
    use holochain_zome_types::signal::AppSignal;

    #[tokio::test]
    async fn create_send_handle_and_broadcast() {
        let app_broadcast = AppBroadcast::new();
        let installed_app_id: InstalledAppId = "test".into();

        // Create sender first
        let tx = app_broadcast.create_send_handle(installed_app_id.clone());

        // Now subscribe
        let mut rx = app_broadcast.subscribe(installed_app_id.clone());

        // Send and receive the signal
        let signal = Signal::App {
            cell_id: fixt!(CellId),
            zome_name: fixt!(ZomeName),
            signal: AppSignal::new(ExternIO::from(vec![])),
        };
        tx.send(signal.clone()).unwrap();
        let received_signal = rx.recv().await.unwrap();

        assert_eq!(signal, received_signal);
    }

    #[tokio::test]
    async fn subscribe_and_broadcast() {
        let app_broadcast = AppBroadcast::new();
        let installed_app_id: InstalledAppId = "test".into();

        // Subscribe first
        let mut rx = app_broadcast.subscribe(installed_app_id.clone());

        // Now create sender
        let tx = app_broadcast.create_send_handle(installed_app_id.clone());

        // Send and receive the signal
        let signal = Signal::App {
            cell_id: fixt!(CellId),
            zome_name: fixt!(ZomeName),
            signal: AppSignal::new(ExternIO::from(vec![])),
        };
        tx.send(signal.clone()).unwrap();
        let received_signal = rx.recv().await.unwrap();

        assert_eq!(signal, received_signal);
    }

    #[tokio::test]
    async fn multiple_senders_and_subscribers() {
        let app_broadcast = AppBroadcast::new();
        let installed_app_id: InstalledAppId = "test".into();

        // Create sender 1 and subscriber 1
        let tx_1 = app_broadcast.create_send_handle(installed_app_id.clone());
        let mut rx_1 = app_broadcast.subscribe(installed_app_id.clone());

        // Create sender 2 and subscriber 2
        let tx_2 = app_broadcast.create_send_handle(installed_app_id.clone());
        let mut rx_2 = app_broadcast.subscribe(installed_app_id.clone());

        let signal_1 = Signal::App {
            cell_id: fixt!(CellId),
            zome_name: fixt!(ZomeName),
            signal: AppSignal::new(ExternIO::from(vec![])),
        };
        tx_1.send(signal_1.clone()).unwrap();

        let signal_1_rcv_1 = rx_1.recv().await.unwrap();
        let signal_1_rcv_2 = rx_2.recv().await.unwrap();
        assert_eq!(signal_1, signal_1_rcv_1);
        assert_eq!(signal_1, signal_1_rcv_2);

        let signal_2 = Signal::App {
            cell_id: fixt!(CellId),
            zome_name: fixt!(ZomeName),
            signal: AppSignal::new(ExternIO::from(vec![])),
        };
        tx_2.send(signal_2.clone()).unwrap();

        let signal_2_rcv_1 = rx_1.recv().await.unwrap();
        let signal_2_rcv_2 = rx_2.recv().await.unwrap();
        assert_eq!(signal_2, signal_2_rcv_1);
        assert_eq!(signal_2, signal_2_rcv_2);
    }

    #[tokio::test]
    async fn clean_up_unused_senders() {
        let app_broadcast = AppBroadcast::new();

        let installed_app_id_1: InstalledAppId = "test 1".into();
        let _tx_1 = app_broadcast.create_send_handle(installed_app_id_1.clone());

        let installed_app_id_2: InstalledAppId = "test 2".into();
        let _tx_2 = app_broadcast.create_send_handle(installed_app_id_2.clone());

        assert_eq!(2, app_broadcast.keys().len());

        let mut still_installed = HashSet::new();
        still_installed.insert(installed_app_id_1.clone());
        app_broadcast.retain(still_installed);

        assert_eq!(1, app_broadcast.keys().len());
        assert_eq!(vec![installed_app_id_1], app_broadcast.keys());
    }
}



================================================
File: crates/holochain/src/conductor/conductor/builder.rs
================================================
use super::*;
use crate::conductor::kitsune_host_impl::KitsuneHostImpl;
use crate::conductor::manager::OutcomeReceiver;
use crate::conductor::metrics::{create_post_commit_duration_metric, PostCommitDurationMetric};
use crate::conductor::paths::DataRootPath;
use crate::conductor::ribosome_store::RibosomeStore;
use crate::conductor::ConductorHandle;
use holochain_conductor_api::conductor::paths::KeystorePath;
use holochain_p2p::NetworkCompatParams;

/// A configurable Builder for Conductor and sometimes ConductorHandle
#[derive(Default)]
pub struct ConductorBuilder {
    /// The configuration
    pub config: ConductorConfig,

    /// The RibosomeStore (mockable)
    pub ribosome_store: RibosomeStore,

    /// For new lair, passphrase is required
    pub passphrase: Option<sodoken::BufRead>,

    /// Optional keystore override
    pub keystore: Option<MetaLairClient>,

    /// Optional state override (for testing)
    #[cfg(any(test, feature = "test_utils"))]
    pub state: Option<ConductorState>,

    /// Optional DPKI service implementation, used to override the service specified in the config,
    /// especially for testing with a mock
    #[cfg(any(test, feature = "test_utils"))]
    pub dpki: Option<DpkiImpl>,

    /// If specified here and a device seed is not already specified in the config,
    /// a new seed will be generated in lair with a random unique tag, and the conductor config
    /// will be updated to use this seed.
    #[cfg(any(test, feature = "test_utils"))]
    pub generate_test_device_seed: bool,

    /// Skip printing setup info to stdout
    pub no_print_setup: bool,

    /// WARNING!! DANGER!! This exposes your database decryption secrets!
    /// Print the database decryption secrets to stderr.
    /// With these PRAGMA commands, you'll be able to run sqlcipher
    /// directly to manipulate holochain databases.
    pub danger_print_db_secrets: bool,
}

impl ConductorBuilder {
    /// Default ConductorBuilder.
    pub fn new() -> Self {
        Self::default()
    }
}

impl ConductorBuilder {
    /// Set the ConductorConfig used to build this Conductor
    pub fn config(mut self, config: ConductorConfig) -> Self {
        self.config = config;
        self
    }

    /// Set the passphrase for use in keystore initialization
    pub fn passphrase(mut self, passphrase: Option<sodoken::BufRead>) -> Self {
        self.passphrase = passphrase;
        self
    }

    /// Set up the builder to skip printing setup
    pub fn no_print_setup(mut self) -> Self {
        self.no_print_setup = true;
        self
    }

    /// WARNING!! DANGER!! This exposes your database decryption secrets!
    /// Print the database decryption secrets to stderr.
    /// With these PRAGMA commands, you'll be able to run sqlcipher
    /// directly to manipulate holochain databases.
    pub fn danger_print_db_secrets(mut self, v: bool) -> Self {
        self.danger_print_db_secrets = v;
        self
    }

    /// Set the data root path for the conductor that will be built.
    pub fn with_data_root_path(mut self, data_root_path: DataRootPath) -> Self {
        self.config.data_root_path = Some(data_root_path);
        self
    }

    /// If a device seed is not already specified in the config, one will
    /// be generated and used for the test keystore.
    #[cfg(any(test, feature = "test_utils"))]
    pub fn with_test_device_seed(mut self) -> Self {
        self.generate_test_device_seed = true;
        self
    }

    #[cfg(any(test, feature = "test_utils"))]
    async fn setup_test_device_seed(mut self, keystore: MetaLairClient) -> ConductorResult<Self> {
        // Set up device seed if specified
        if self.generate_test_device_seed && self.config.device_seed_lair_tag.is_none() {
            let tag = format!("_hc_test_device_seed_{}", nanoid::nanoid!());
            keystore
                .lair_client()
                .new_seed(tag.clone().into(), None, false)
                .await?;
            self.config.device_seed_lair_tag = Some(tag);
        }
        Ok(self)
    }

    /// Initialize a "production" Conductor
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(scope = self.config.network.tracing_scope)))]
    pub async fn build(self) -> ConductorResult<ConductorHandle> {
        let builder = self;
        tracing::debug!(?builder.config);

        let passphrase = match &builder.passphrase {
            Some(p) => p.clone(),
            None => sodoken::BufRead::new_no_lock(&[]),
        };

        let keystore = if let Some(keystore) = builder.keystore.clone() {
            keystore.clone()
        } else {
            pub(crate) fn warn_no_encryption() {
                #[cfg(not(feature = "sqlite-encrypted"))]
                {
                    const MSG: &str = "WARNING: running without local db encryption";
                    eprintln!("{}", MSG);
                    println!("{}", MSG);
                    tracing::warn!("{}", MSG);
                }
            }
            let get_passphrase = || -> ConductorResult<sodoken::BufRead> {
                match builder.passphrase.as_ref() {
                    None => Err(
                        one_err::OneErr::new("passphrase required for lair keystore api").into(),
                    ),
                    Some(p) => Ok(p.to_owned()),
                }
            };
            match &builder.config.keystore {
                KeystoreConfig::DangerTestKeystore => {
                    holochain_keystore::spawn_test_keystore().await?
                }
                KeystoreConfig::LairServer { connection_url } => {
                    warn_no_encryption();
                    let passphrase = get_passphrase()?;
                    match spawn_lair_keystore(connection_url.clone(), passphrase).await {
                        Ok(keystore) => keystore,
                        Err(err) => {
                            tracing::error!(?err, "Failed to spawn Lair keystore");
                            return Err(err.into());
                        }
                    }
                }
                KeystoreConfig::LairServerInProc { lair_root } => {
                    warn_no_encryption();

                    let keystore_root_path: KeystorePath = match lair_root {
                        Some(lair_root) => lair_root.clone(),
                        None => builder
                            .config
                            .data_root_path
                            .as_ref()
                            .ok_or(ConductorError::NoDataRootPath)?
                            .clone()
                            .try_into()?,
                    };
                    let keystore_config_path = keystore_root_path
                        .as_ref()
                        .join("lair-keystore-config.yaml");
                    let passphrase = get_passphrase()?;

                    match spawn_lair_keystore_in_proc(&keystore_config_path, passphrase).await {
                        Ok(keystore) => keystore,
                        Err(err) => {
                            tracing::error!(?err, "Failed to spawn Lair keystore in process");
                            return Err(err.into());
                        }
                    }
                }
            }
        };

        info!("Conductor startup: passphrase obtained.");

        #[cfg(any(test, feature = "test_utils"))]
        let builder = builder.setup_test_device_seed(keystore.clone()).await?;

        let Self {
            ribosome_store,
            config,
            ..
        } = builder;

        let config = Arc::new(config);

        let ribosome_store = RwShare::new(ribosome_store);

        crate::conductor::space::set_danger_print_db_secrets(builder.danger_print_db_secrets);
        let spaces = Spaces::new(config.clone(), passphrase).await?;
        let tag = spaces.get_state().await?.tag().clone();

        let tag_ed: Arc<str> = format!("{}_ed", tag.0).into_boxed_str().into();
        let _ = keystore
            .lair_client()
            .new_seed(tag_ed.clone(), None, false)
            .await;

        let network_config = config.network.clone();
        let (cert_digest, cert, cert_priv_key) = keystore
            .get_or_create_tls_cert_by_tag(tag.0.clone())
            .await?;
        let tls_config =
            holochain_p2p::kitsune_p2p::dependencies::kitsune_p2p_types::tls::TlsConfig {
                cert,
                cert_priv_key,
                cert_digest,
            };

        info!("Conductor startup: TLS cert created.");

        let strat = network_config.tuning_params.to_arq_strat();

        let host = KitsuneHostImpl::new(
            spaces.clone(),
            config.clone(),
            ribosome_store.clone(),
            strat,
            Some(tag_ed),
            Some(keystore.lair_client()),
        );

        // TODO: when we make DPKI optional, we can remove the unwrap_or and just let it be None,
        let dpki_config = Some(config.dpki.clone());

        let dpki_dna_to_install = match &dpki_config {
            Some(config) => {
                if config.no_dpki {
                    None
                } else {
                    let dna = get_dpki_dna(config)
                        .await?
                        .into_dna_file(Default::default())
                        .await?
                        .0;

                    Some(dna)
                }
            }
            _ => unreachable!(
                "We currently require DPKI to be used, but this may change in the future"
            ),
        };

        let dpki_uuid = dpki_dna_to_install
            .as_ref()
            .map(|dna| dna.dna_hash().get_raw_32().try_into().expect("32 bytes"));
        let network_compat = NetworkCompatParams { dpki_uuid };

        let (holochain_p2p, p2p_evt) = match holochain_p2p::spawn_holochain_p2p(
            network_config,
            tls_config,
            host,
            network_compat,
        )
        .await
        {
            Ok(r) => r,
            Err(err) => {
                tracing::error!(?err, "Error spawning networking");
                return Err(err.into());
            }
        };

        info!("Conductor startup: networking started.");

        let (post_commit_sender, post_commit_receiver) =
            tokio::sync::mpsc::channel(POST_COMMIT_CHANNEL_BOUND);

        let (outcome_tx, outcome_rx) = futures::channel::mpsc::channel(8);

        let conductor = Conductor::new(
            config.clone(),
            ribosome_store,
            keystore,
            holochain_p2p,
            spaces,
            post_commit_sender,
            outcome_tx,
        );

        let shutting_down = conductor.shutting_down.clone();

        #[cfg(any(test, feature = "test_utils"))]
        let conductor = Self::update_fake_state(builder.state, conductor).await?;

        // Create handle
        let handle: ConductorHandle = Arc::new(conductor);

        {
            let handle = handle.clone();
            tokio::task::spawn(async move {
                while !shutting_down.load(std::sync::atomic::Ordering::Relaxed) {
                    tokio::time::sleep(std::time::Duration::from_secs(60)).await;
                    if let Err(e) = handle.prune_p2p_agents_db().await {
                        tracing::error!("failed to prune p2p_agents_db: {:?}", e);
                    }
                }
            });
        }

        Self::finish(
            handle,
            config,
            dpki_dna_to_install,
            p2p_evt,
            post_commit_receiver,
            outcome_rx,
            builder.no_print_setup,
        )
        .await
    }

    pub(crate) async fn spawn_post_commit(
        conductor_handle: ConductorHandle,
        receiver: tokio::sync::mpsc::Receiver<PostCommitArgs>,
        stop: StopReceiver,
        duration_metric: PostCommitDurationMetric,
    ) {
        let receiver_stream = tokio_stream::wrappers::ReceiverStream::new(receiver);
        stop.fuse_with(receiver_stream)
            .for_each_concurrent(POST_COMMIT_CONCURRENT_LIMIT, move |post_commit_args| {
                let start = Instant::now();
                let conductor_handle = conductor_handle.clone();
                let duration_metric = duration_metric.clone();
                async move {
                    let PostCommitArgs {
                        host_access,
                        invocation,
                        cell_id,
                    } = post_commit_args;
                    match conductor_handle.clone().get_ribosome(cell_id.dna_hash()) {
                        Ok(ribosome) => {
                            if let Err(e) = ribosome.run_post_commit(host_access, invocation).await
                            {
                                tracing::error!(?e);
                            }
                        }
                        Err(e) => {
                            tracing::error!(?e);
                        }
                    }

                    duration_metric.record(
                        start.elapsed().as_secs_f64(),
                        &[
                            opentelemetry_api::KeyValue::new(
                                "dna_hash",
                                format!("{:?}", cell_id.dna_hash()),
                            ),
                            opentelemetry_api::KeyValue::new(
                                "agent",
                                format!("{:?}", cell_id.agent_pubkey()),
                            ),
                        ],
                    );
                }
            })
            .await;
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub(crate) async fn finish(
        conductor: ConductorHandle,
        config: Arc<ConductorConfig>,
        dpki_dna_to_install: Option<DnaFile>,
        p2p_evt: holochain_p2p::event::HolochainP2pEventReceiver,
        post_commit_receiver: tokio::sync::mpsc::Receiver<PostCommitArgs>,
        outcome_receiver: OutcomeReceiver,
        no_print_setup: bool,
    ) -> ConductorResult<ConductorHandle> {
        conductor
            .clone()
            .start_scheduler(SCHEDULER_INTERVAL)
            .await?;

        info!("Conductor startup: scheduler task started.");

        tokio::task::spawn(p2p_event_task(p2p_evt, conductor.clone()).in_current_span());

        info!("Conductor startup: p2p event task started.");

        let tm = conductor.task_manager();
        let conductor2 = conductor.clone();
        let post_commit_duration_metric = create_post_commit_duration_metric();
        tm.add_conductor_task_unrecoverable("post_commit_receiver", move |stop| {
            Self::spawn_post_commit(
                conductor2,
                post_commit_receiver,
                stop,
                post_commit_duration_metric,
            )
            .map(Ok)
        });

        let configs = config.admin_interfaces.clone().unwrap_or_default();
        let cell_startup_errors = conductor
            .clone()
            .initialize_conductor(outcome_receiver, configs)
            .await?;

        // TODO: This should probably be emitted over the admin interface
        if !cell_startup_errors.is_empty() {
            error!(
                msg = "Failed to create the following active apps",
                ?cell_startup_errors
            );
        }

        // Install DPKI from DNA
        if let Some(dna) = dpki_dna_to_install {
            let dna_hash = dna.dna_hash().clone();
            match conductor.clone().install_dpki(dna, true).await {
                Ok(_) => tracing::info!("Installed DPKI from DNA {}", dna_hash),
                Err(ConductorError::AppAlreadyInstalled(_)) => {
                    tracing::debug!("DPKI already installed, skipping installation")
                }
                Err(e) => return Err(e),
            }
        }

        if !no_print_setup {
            conductor.print_setup();
        }

        Ok(conductor)
    }

    /// Pass a test keystore in, to ensure that generated test agents
    /// are actually available for signing (especially for tryorama compat)
    pub fn with_keystore(mut self, keystore: MetaLairClient) -> Self {
        self.keystore = Some(keystore);
        self
    }

    #[cfg(any(test, feature = "test_utils"))]
    /// Sets some fake conductor state for tests
    pub fn fake_state(mut self, state: ConductorState) -> Self {
        self.state = Some(state);
        self
    }

    #[cfg(any(test, feature = "test_utils"))]
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub(crate) async fn update_fake_state(
        state: Option<ConductorState>,
        conductor: Conductor,
    ) -> ConductorResult<Conductor> {
        if let Some(state) = state {
            conductor.update_state(move |_| Ok(state)).await?;
        }
        Ok(conductor)
    }

    /// Build a Conductor with a test environment
    #[cfg(any(test, feature = "test_utils"))]
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(scope = self.config.network.tracing_scope)))]
    pub async fn test(self, extra_dnas: &[DnaFile]) -> ConductorResult<ConductorHandle> {
        let builder = self;

        let keystore = builder
            .keystore
            .clone()
            .unwrap_or_else(holochain_keystore::test_keystore);

        let builder = builder
            .with_test_device_seed()
            .setup_test_device_seed(keystore.clone())
            .await?;

        let config = Arc::new(builder.config);
        let spaces =
            Spaces::new(config.clone(), sodoken::BufRead::new_no_lock(b"passphrase")).await?;
        let tag = spaces.get_state().await?.tag().clone();

        let tag_ed: Arc<str> = format!("{}_ed", tag.0).into_boxed_str().into();
        let _ = keystore
            .lair_client()
            .new_seed(tag_ed.clone(), None, false)
            .await;

        let network_config = config.network.clone();
        let strat = network_config.tuning_params.to_arq_strat();

        let ribosome_store = RwShare::new(builder.ribosome_store);
        let host = KitsuneHostImpl::new(
            spaces.clone(),
            config.clone(),
            ribosome_store.clone(),
            strat,
            Some(tag_ed),
            Some(keystore.lair_client()),
        );

        // TODO: when we make DPKI optional, we can remove the unwrap_or and just let it be None,
        let dpki_config = Some(config.dpki.clone());

        let (dpki_uuid, dpki_dna_to_install) = match (&builder.dpki, &dpki_config) {
            // If a DPKI impl was provided to the builder, use that
            (Some(dpki_impl), _) => (Some(dpki_impl.uuid()), None),

            // Otherwise load the DNA from config if specified
            (None, Some(dpki_config)) => {
                if dpki_config.no_dpki {
                    (None, None)
                } else {
                    let dna = get_dpki_dna(dpki_config)
                        .await?
                        .into_dna_file(Default::default())
                        .await?
                        .0;
                    (
                        Some(dna.dna_hash().get_raw_32().try_into().expect("32 bytes")),
                        Some(dna),
                    )
                }
            }

            (None, None) => unreachable!(
                "We currently require DPKI to be used, but this may change in the future"
            ),
        };

        let network_compat = NetworkCompatParams { dpki_uuid };

        let (holochain_p2p, p2p_evt) =
                holochain_p2p::spawn_holochain_p2p(network_config, holochain_p2p::kitsune_p2p::dependencies::kitsune_p2p_types::tls::TlsConfig::new_ephemeral().await.unwrap(), host, network_compat)
                    .await?;

        let (post_commit_sender, post_commit_receiver) =
            tokio::sync::mpsc::channel(POST_COMMIT_CHANNEL_BOUND);

        let (outcome_tx, outcome_rx) = futures::channel::mpsc::channel(8);

        let conductor = Conductor::new(
            config.clone(),
            ribosome_store,
            keystore,
            holochain_p2p,
            spaces,
            post_commit_sender,
            outcome_tx,
        );

        let conductor = Self::update_fake_state(builder.state, conductor).await?;

        // Create handle
        let handle: ConductorHandle = Arc::new(conductor);

        // Install DPKI from DNA or mock
        if let Some(dpki_impl) = builder.dpki {
            // This is a mock DPKI impl, so inject it into the conductor directly
            handle.running_services_mutex().share_mut(|s| {
                s.dpki = Some(dpki_impl);
            });
        }

        // Install extra DNAs, in particular:
        // the ones with InlineZomes will not be registered in the Wasm DB
        // and cannot be automatically loaded on conductor restart.

        for dna_file in extra_dnas {
            handle
                .register_dna(dna_file.clone())
                .await
                .expect("Could not install DNA");
        }

        Self::finish(
            handle,
            config,
            dpki_dna_to_install,
            p2p_evt,
            post_commit_receiver,
            outcome_rx,
            builder.no_print_setup,
        )
        .await
    }
}



================================================
File: crates/holochain/src/conductor/conductor/chc.rs
================================================
use holochain_chc::ChcImpl;

use super::*;

impl Conductor {
    /// Get access to the CHC used by the conductor
    #[allow(unused_variables)]
    pub fn get_chc(&self, cell_id: &CellId) -> Option<ChcImpl> {
        cfg_if::cfg_if! {
            if #[cfg(feature = "chc")] {
                crate::conductor::chc::build_chc(self.config.chc_url.as_ref().map(|u| u.as_ref()), self.keystore().clone(), cell_id)
            } else {
                None
            }
        }
    }

    #[cfg(test)]
    #[allow(dead_code)]
    pub(crate) async fn chc_sync(
        self: Arc<Self>,
        cell_id: CellId,
        enable_app: Option<InstalledAppId>,
    ) -> ConductorApiResult<()> {
        if let Some(chc) = self.get_chc(&cell_id) {
            let db =
                self.get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())?;
            let author = cell_id.agent_pubkey().clone();
            let top_hash = db
                .read_async(move |txn| {
                    SourceChainResult::Ok(chain_head_db(txn, Arc::new(author))?.map(|h| h.action))
                })
                .await?;
            let records = chc.get_record_data(top_hash).await?;

            self.clone()
                .graft_records_onto_source_chain(cell_id, true, records)
                .await?;
            if let Some(app_id) = enable_app {
                self.enable_app(app_id).await?;
            }
        }
        Ok(())
    }
}



================================================
File: crates/holochain/src/conductor/conductor/graft_records_onto_source_chain.rs
================================================
use holochain_state::source_chain::SourceChain;
use holochain_types::prelude::ChainItem;

use super::*;

impl Conductor {
    /// Inject records into a source chain for a cell.
    /// If the records form a chain segment that can be "grafted" onto the existing chain, it will be.
    /// Otherwise, a new chain will be formed using the specified records.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn graft_records_onto_source_chain(
        self: Arc<Self>,
        cell_id: CellId,
        validate: bool,
        records: Vec<Record>,
    ) -> ConductorApiResult<()> {
        // Require that the cell is installed.
        if let err @ Err(ConductorError::CellMissing(_)) = self.cell_by_id(&cell_id).await {
            let _ = err?;
        }

        // Get or create the space for this cell.
        let space = self.get_or_create_space(cell_id.dna_hash())?;

        let chc = None;
        let network = self.holochain_p2p().to_dna(cell_id.dna_hash().clone(), chc);

        let source_chain: SourceChain = space
            .source_chain(self.keystore().clone(), cell_id.agent_pubkey().clone())
            .await?;

        let existing = source_chain
            .query(ChainQueryFilter::new().descending())
            .await?
            .into_iter()
            .map(|r| r.signed_action)
            .collect::<Vec<SignedActionHashed>>();

        let graft = ChainGraft::new(existing, records).rebalance();
        let chain_top = graft.existing_chain_top();

        if validate {
            self.clone()
                .validate_records(&cell_id, &chain_top, graft.incoming())
                .await?;
        }

        // Produce the op lites for each record.
        let data = graft
            .incoming
            .into_iter()
            .map(|el| {
                let ops = produce_op_lites_from_records(vec![&el])?;
                // Check have the same author as cell.
                let (sah, entry) = el.into_inner();
                if sah.action().author() != cell_id.agent_pubkey() {
                    return Err(StateMutationError::AuthorsMustMatch);
                }
                Ok((sah, ops, entry.into_option()))
            })
            .collect::<StateMutationResult<Vec<_>>>()?;

        // Commit the records to the source chain.
        let ops_to_integrate = space
            .get_or_create_authored_db(cell_id.agent_pubkey().clone())?
            .write_async({
                let cell_id = cell_id.clone();
                move |txn| {
                    if let Some((_, seq)) = chain_top {
                        // Remove records above the grafting position.
                        //
                        // NOTES:
                        // - the chain top may have moved since the grafting call began,
                        //   but it doesn't really matter, since we explicitly want to
                        //   clobber anything beyond the grafting point anyway.
                        // - if there is an existing fork, there may still be a fork after the
                        //   grafting. A more rigorous approach would thin out the existing
                        //   actions until a single fork is obtained.
                        txn.execute(
                            holochain_sqlite::sql::sql_cell::DELETE_ACTIONS_AFTER_SEQ,
                            rusqlite::named_params! {
                                ":author": cell_id.agent_pubkey(),
                                ":seq": seq
                            },
                        )
                        .map_err(StateMutationError::from)?;
                    }

                    let mut ops_to_integrate = Vec::new();

                    // Commit the records and ops to the authored db.
                    for (sah, ops, entry) in data {
                        // Clippy is wrong :(
                        #[allow(clippy::needless_collect)]
                        let basis = ops
                            .iter()
                            .map(|op| op.dht_basis().clone())
                            .collect::<Vec<_>>();
                        ops_to_integrate.extend(
                            source_chain::put_raw(txn, sah, ops, entry)?
                                .into_iter()
                                .zip(basis.into_iter()),
                        );
                    }
                    SourceChainResult::Ok(ops_to_integrate)
                }
            })
            .await?;

        // Check which ops need to be integrated.
        // Only integrated if a cell is installed.
        if self.running_cell_ids().contains(&cell_id) {
            holochain_state::integrate::authored_ops_to_dht_db(
                network
                    .storage_arcs()
                    .await
                    .map_err(ConductorApiError::other)?,
                ops_to_integrate,
                space
                    .get_or_create_authored_db(cell_id.agent_pubkey().clone())?
                    .into(),
                space.dht_db.clone(),
                &space.dht_query_cache,
            )
            .await?;

            // Any ops that were moved to the dht_db but had dependencies will need to be integrated.
            self.cell_by_id(&cell_id)
                .await?
                .notify_authored_ops_moved_to_limbo();
        }
        Ok(())
    }

    async fn validate_records(
        self: Arc<Self>,
        cell_id: &CellId,
        chain_top: &Option<(ActionHash, u32)>,
        records: &[Record],
    ) -> ConductorApiResult<()> {
        let space = self.get_or_create_space(cell_id.dna_hash())?;
        let ribosome = self.get_ribosome(cell_id.dna_hash())?;
        let chc = None;
        let network = self.holochain_p2p().to_dna(cell_id.dna_hash().clone(), chc);

        // Create a raw source chain to validate against because
        // genesis may not have been run yet.
        let workspace = SourceChainWorkspace::raw_empty(
            space.get_or_create_authored_db(cell_id.agent_pubkey().clone())?,
            space.dht_db.clone(),
            space.dht_query_cache.clone(),
            space.cache_db.clone(),
            self.keystore().clone(),
            cell_id.agent_pubkey().clone(),
            Arc::new(ribosome.dna_def().as_content().clone()),
        )
        .await?;

        let sc = workspace.source_chain();

        // Validate the chain.
        validate_chain(records.iter().map(|e| e.signed_action()), chain_top)
            .map_err(|e| SourceChainError::InvalidCommit(e.to_string()))?;

        // Add the records to the source chain so we can validate them.
        sc.scratch()
            .apply(|scratch| {
                for r in records {
                    holochain_state::prelude::insert_record_scratch(
                        scratch,
                        r.clone(),
                        Default::default(),
                    );
                }
            })
            .map_err(SourceChainError::from)?;

        // Run the individual record validations.
        crate::core::workflow::inline_validation(
            workspace.clone(),
            network.clone(),
            self.clone(),
            ribosome,
        )
        .await?;

        Ok(())
    }
}

/// Specifies a set of existing actions forming a chain, and a set of incoming actions
/// to attempt to "graft" onto the existing chain.
///
/// The existing actions are guaranteed to be ordered in descending sequence order,
/// and the incoming actions are guaranteed to be ordered in increasing sequence order.
/// This is just easier for implementation purposes.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct ChainGraft<A, B> {
    existing: Vec<A>,
    incoming: Vec<B>,
}

#[derive(Clone, Debug, PartialEq, Eq)]
enum Pivot {
    None,
    NewRoot,
    Index(usize),
}

impl<A: ChainItem, B: Clone + AsRef<A>> ChainGraft<A, B> {
    /// Constructor, ensuring that existing items are sorted descending,
    /// and incoming items are sorted ascending.
    pub fn new(mut existing: Vec<A>, mut incoming: Vec<B>) -> Self {
        existing.sort_unstable_by_key(|r| u32::MAX - r.seq());
        incoming.sort_unstable_by_key(|r| r.as_ref().seq());
        Self { existing, incoming }
    }

    pub fn existing_chain_top(&self) -> Option<(A::Hash, u32)> {
        self.existing
            .first()
            .map(|a| (a.get_hash().clone(), a.seq()))
    }

    /// Given a set of incoming actions, find the maximal set of existing hashes
    /// which can be preserved, and the minimal set of incoming actions to be committed,
    /// such that the new source chain will include all of the incoming actions, all of
    /// the existing hashes returned, and none of the actions which fall outside of
    /// either group.
    ///
    /// Assumptions:
    /// - The existing actions form a chain, with no forks.
    /// - The incoming actions form a chain, with no forks.
    ///
    /// This has the effect of attempting to "graft" the incoming actions onto the existing
    /// source chain. If the grafting causes a fork, then the existing items after the fork
    /// point get deleted, so that there remains a single unforked chain containing the incoming items.
    ///
