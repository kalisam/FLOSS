        let entry_hash = EntryHash::with_data_sync(&entry);

        let to_agent_activity_op =
            |h, sig| ChainOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(sig, h));

        let to_record_and_op = |a: Action, sig: Signature| {
            let op = ChainOpHashed::from_content_sync(ChainOp::StoreEntry(
                sig.clone(),
                match a {
                    Action::Create(ref c) => NewEntryAction::Create(c.clone()),
                    _ => unreachable!(),
                },
                entry.clone(),
            ));
            let shh = SignedActionHashed::with_presigned(ActionHashed::from_content_sync(a), sig);
            (Record::new(shh, Some(entry.clone())), op)
        };

        let to_record_dna_op = |a: Action, sig: Signature| {
            let op = ChainOpHashed::from_content_sync(ChainOp::StoreRecord(
                sig.clone(),
                a.clone(),
                RecordEntry::NA,
            ));
            let shh = SignedActionHashed::with_presigned(ActionHashed::from_content_sync(a), sig);
            (Record::new(shh, None), op)
        };

        // The hashes we are expecting to get returned by the below activity set.
        let mut valid_hashes = Vec::new();

        // The records on the chain. Needs to match the activity set.
        let mut valid_records = Vec::new();

        // The store record ops for the actual data on the chain which should
        // match the set of activity ops.
        let mut store_entry_ops = Vec::new();

        // A set of activity ops:
        // - Must be on the above agents chain.
        // - Create a valid, unbroken chain.
        // - All actions are valid:
        //    - Prev hash actually match prev action's hash
        //    - Seq numbers are in order.
        //    - First action must be a Dna.
        let mut agent_activity_ops = Vec::new();
        let mut dna = fixt!(Dna);
        dna.author = agent.clone();
        let dna = Action::Dna(dna);

        // Insert the dna
        let dna_sig = fixt!(Signature);
        let (el, op) = to_record_dna_op(dna.clone(), dna_sig.clone());
        valid_records.push(el);
        store_entry_ops.push(op);
        agent_activity_ops.push(to_agent_activity_op(dna.clone(), dna_sig));

        let creates: Vec<_> = CreateFixturator::new(Unpredictable)
            .enumerate()
            .take(50)
            .collect();
        let mut prev_hash = ActionHash::with_data_sync(&dna);
        valid_hashes.push((0, prev_hash.clone()));
        for (seq, mut create) in creates {
            let action_seq = (seq + 1) as u32;
            create.author = agent.clone();
            create.action_seq = action_seq;
            create.prev_action = prev_hash.clone();
            create.entry_hash = entry_hash.clone();
            create.entry_type = EntryType::App(AppEntryDef::new(
                1.into(),
                1.into(),
                if private_entries {
                    EntryVisibility::Private
                } else {
                    EntryVisibility::Public
                },
            ));
            let action = Action::Create(create);
            let sig = fixt!(Signature);
            prev_hash = ActionHash::with_data_sync(&action);
            agent_activity_ops.push(to_agent_activity_op(action.clone(), sig.clone()));

            valid_hashes.push((action_seq, prev_hash.clone()));

            let (el, op) = to_record_and_op(action, sig);
            valid_records.push(el);
            store_entry_ops.push(op);
        }

        // The head of the chain is the last valid hash
        // because we are going to insert all ops as valid and integrated.
        let last = valid_hashes.last().unwrap();
        let chain_head = ChainHead {
            action_seq: last.0,
            hash: last.1.clone(),
        };

        // Highest Observed is the same as the chain head.
        let highest_observed = HighestObserved {
            action_seq: last.0,
            hash: vec![last.1.clone()],
        };

        // Finally add some random noise, so we know we are getting the correct items.
        let noise_agent_activity_ops = ActionFixturator::new(Unpredictable)
            .take(50)
            .map(|a| to_agent_activity_op(a, fixt!(Signature)))
            .collect();

        Self {
            agent_activity_ops,
            noise_agent_activity_ops,
            store_entry_ops,
            agent,
            valid_hashes: ChainItems::Hashes(valid_hashes),
            valid_records: ChainItems::Full(valid_records),
            highest_observed,
            chain_head,
        }
    }
}



================================================
File: crates/holochain_cascade/src/test_utils/entry_test_data.rs
================================================
use holo_hash::ActionHash;
use holo_hash::EntryHash;
use holochain_serialized_bytes::UnsafeBytes;
use holochain_state::prelude::*;
use std::convert::TryInto;

use ::fixt::prelude::*;

/// A collection of test fixtures used to test entry-related cascade functionality
#[derive(Debug)]
#[allow(missing_docs)]
pub struct EntryTestData {
    pub store_entry_op: ChainOpHashed,
    pub wire_create: Judged<WireNewEntryAction>,
    pub create_hash: ActionHash,
    pub delete_entry_action_op: ChainOpHashed,
    pub wire_delete: Judged<WireDelete>,
    pub delete_hash: ActionHash,
    pub update_content_op: ChainOpHashed,
    pub wire_update: Judged<WireUpdateRelationship>,
    pub update_hash: ActionHash,
    pub hash: EntryHash,
    pub entry: EntryData,
    // Links
    pub create_link_op: ChainOpHashed,
    pub delete_link_op: ChainOpHashed,
    pub wire_create_link: WireCreateLink,
    pub wire_create_link_base: WireCreateLink,
    pub wire_delete_link: WireDeleteLink,
    pub create_link_action: SignedActionHashed,
    pub delete_link_action: SignedActionHashed,
    pub link_key: WireLinkKey,
    pub link_key_tag: WireLinkKey,
    pub links: Vec<Link>,
    pub link_query: WireLinkQuery,
}

impl EntryTestData {
    /// Create the test fixtures
    pub fn create() -> Self {
        let mut create = fixt!(Create);
        let mut update = fixt!(Update);
        let mut delete = fixt!(Delete);

        let mut create_link = fixt!(CreateLink);
        create_link.zome_index = 0.into();
        let mut delete_link = fixt!(DeleteLink);

        let entry: AppEntryBytes = SerializedBytes::from(UnsafeBytes::from(vec![3u8]))
            .try_into()
            .unwrap();
        let entry = Entry::App(entry);
        let entry_hash = EntryHash::with_data_sync(&entry);
        let update_entry: AppEntryBytes = SerializedBytes::from(UnsafeBytes::from(vec![4u8]))
            .try_into()
            .unwrap();
        let update_entry = Entry::App(update_entry);
        let update_entry_hash = EntryHash::with_data_sync(&update_entry);

        let mut entry_type_fixt =
            AppEntryDefFixturator::new(EntryVisibility::Public).map(EntryType::App);

        create.entry_hash = entry_hash.clone();
        create.entry_type = entry_type_fixt.next().unwrap();
        update.entry_hash = update_entry_hash;
        update.entry_type = entry_type_fixt.next().unwrap();

        let create_action = Action::Create(create.clone());
        let create_hash = ActionHash::with_data_sync(&create_action);

        delete.deletes_entry_address = entry_hash.clone();
        delete.deletes_address = create_hash.clone();

        update.original_entry_address = entry_hash.clone();
        update.original_action_address = create_hash.clone();

        create_link.base_address = entry_hash.clone().into();
        delete_link.base_address = entry_hash.clone().into();
        let create_link_action = Action::CreateLink(create_link.clone());
        let delete_action = Action::Delete(delete.clone());
        let update_action = Action::Update(update.clone());
        let delete_hash = ActionHash::with_data_sync(&delete_action);
        let update_hash = ActionHash::with_data_sync(&update_action);

        let create_link_hash = ActionHash::with_data_sync(&create_link_action);
        delete_link.link_add_address = create_link_hash.clone();
        let delete_link_action = Action::DeleteLink(delete_link.clone());

        let signature = fixt!(Signature);
        let store_entry_op = ChainOpHashed::from_content_sync(ChainOp::StoreEntry(
            signature.clone(),
            NewEntryAction::Create(create.clone()),
            entry.clone(),
        ));

        let wire_create = Judged::valid(
            SignedAction::new(create_action, signature)
                .try_into()
                .unwrap(),
        );

        let signature = fixt!(Signature);
        let delete_entry_action_op = ChainOpHashed::from_content_sync(
            ChainOp::RegisterDeletedEntryAction(signature.clone(), delete),
        );

        let wire_delete = Judged::valid(
            SignedAction::new(delete_action, signature)
                .try_into()
                .unwrap(),
        );

        let signature = fixt!(Signature);
        let update_content_op = ChainOpHashed::from_content_sync(ChainOp::RegisterUpdatedContent(
            signature.clone(),
            update,
            update_entry.into(),
        ));
        let wire_update = Judged::valid(
            SignedAction::new(update_action, signature)
                .try_into()
                .unwrap(),
        );

        let signature = fixt!(Signature);
        let create_link_op = ChainOpHashed::from_content_sync(ChainOp::RegisterAddLink(
            signature.clone(),
            create_link.clone(),
        ));
        let wire_create_link = WireCreateLink::condense(
            create_link_action.clone().try_into().unwrap(),
            signature.clone(),
            ValidationStatus::Valid,
        );
        let wire_create_link_base = WireCreateLink::condense(
            create_link_action.try_into().unwrap(),
            signature.clone(),
            ValidationStatus::Valid,
        );

        let create_link_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::CreateLink(create_link.clone())),
            signature,
        );

        let signature = fixt!(Signature);
        let delete_link_op = ChainOpHashed::from_content_sync(ChainOp::RegisterRemoveLink(
            signature.clone(),
            delete_link.clone(),
        ));
        let wire_delete_link = WireDeleteLink::condense(
            delete_link_action.try_into().unwrap(),
            signature.clone(),
            ValidationStatus::Valid,
        );
        let delete_link_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::DeleteLink(delete_link)),
            signature,
        );

        let link_key = WireLinkKey {
            base: create_link.base_address.clone(),
            type_query: LinkTypeFilter::single_dep(0.into()),
            tag: None,
            after: None,
            before: None,
            author: None,
        };
        let link_key_tag = WireLinkKey {
            base: create_link.base_address.clone(),
            type_query: LinkTypeFilter::single_dep(0.into()),
            tag: Some(create_link.tag.clone()),
            after: None,
            before: None,
            author: None,
        };

        let link = Link {
            author: create_link.author,
            base: create_link.base_address.clone(),
            target: create_link.target_address.clone(),
            timestamp: create_link.timestamp,
            zome_index: create_link.zome_index,
            link_type: create_link.link_type,
            tag: create_link.tag,
            create_link_hash,
        };

        let link_query = WireLinkQuery {
            base: create_link.base_address,
            link_type: LinkTypeFilter::single_dep(0.into()),
            tag_prefix: None,
            before: None,
            after: None,
            author: None,
        };

        let entry = EntryData {
            entry,
            entry_type: create.entry_type,
        };

        Self {
            store_entry_op,
            delete_entry_action_op,
            update_content_op,
            hash: entry_hash,
            entry,
            wire_create,
            wire_delete,
            wire_update,
            create_hash,
            delete_hash,
            update_hash,
            create_link_op,
            delete_link_op,
            wire_create_link,
            wire_delete_link,
            link_key,
            link_key_tag,
            links: vec![link],
            create_link_action,
            delete_link_action,
            wire_create_link_base,
            link_query,
        }
    }
}



================================================
File: crates/holochain_cascade/src/test_utils/record_test_data.rs
================================================
use holo_hash::ActionHash;
use holo_hash::EntryHash;
use holochain_serialized_bytes::SerializedBytes;
use holochain_serialized_bytes::UnsafeBytes;
use holochain_state::prelude::*;
use std::convert::TryInto;

use ::fixt::prelude::*;

/// A collection of fixtures for use in Cascade tests
#[derive(Debug)]
#[allow(missing_docs)]
pub struct RecordTestData {
    pub store_record_op: ChainOpHashed,
    pub wire_create: Judged<SignedAction>,
    pub create_hash: ActionHash,
    pub deleted_by_op: ChainOpHashed,
    pub wire_delete: Judged<WireDelete>,
    pub delete_hash: ActionHash,
    pub update_record_op: ChainOpHashed,
    pub wire_update: Judged<WireUpdateRelationship>,
    pub update_hash: ActionHash,
    pub hash: EntryHash,
    pub entry: Entry,
    /// An Op from any_record
    pub any_store_record_op: ChainOpHashed,
    /// An arbitrary Action
    pub any_action: Judged<SignedAction>,
    /// The hash of any_action
    pub any_action_hash: ActionHash,
    /// The entry to go with any_action
    pub any_entry: Option<Entry>,
    /// The hash of any_entry
    pub any_entry_hash: Option<EntryHash>,
    /// A Record constructed from any_action
    pub any_record: Record,
}

impl RecordTestData {
    /// Constructor
    pub fn create() -> Self {
        let mut create = fixt!(Create);
        let mut update = fixt!(Update);
        let mut delete = fixt!(Delete);
        let mut any_action = fixt!(Action);
        let entry: AppEntryBytes = SerializedBytes::from(UnsafeBytes::from(vec![0u8]))
            .try_into()
            .unwrap();
        let entry = Entry::App(entry);
        let entry_hash = EntryHash::with_data_sync(&entry);
        let update_entry = fixt!(AppEntryBytes);
        let update_entry = Entry::App(update_entry);
        let update_entry_hash = EntryHash::with_data_sync(&update_entry);

        let mut entry_type_fixt =
            AppEntryDefFixturator::new(EntryVisibility::Public).map(EntryType::App);

        create.entry_hash = entry_hash.clone();
        create.entry_type = entry_type_fixt.next().unwrap();
        update.entry_hash = update_entry_hash;
        update.entry_type = entry_type_fixt.next().unwrap();

        let create_action = Action::Create(create);
        let create_hash = ActionHash::with_data_sync(&create_action);

        delete.deletes_address = create_hash.clone();
        delete.deletes_entry_address = entry_hash.clone();

        update.original_entry_address = entry_hash.clone();
        update.original_action_address = create_hash.clone();

        let delete_action = Action::Delete(delete.clone());
        let update_action = Action::Update(update.clone());
        let delete_hash = ActionHash::with_data_sync(&delete_action);
        let update_hash = ActionHash::with_data_sync(&update_action);

        let signature = fixt!(Signature);
        let store_record_op = ChainOpHashed::from_content_sync(ChainOp::StoreRecord(
            signature.clone(),
            create_action.clone(),
            entry.clone().into(),
        ));

        let wire_create = Judged::valid(SignedAction::new(create_action, signature));

        let signature = fixt!(Signature);
        let deleted_by_op =
            ChainOpHashed::from_content_sync(ChainOp::RegisterDeletedBy(signature.clone(), delete));

        let wire_delete = Judged::valid(
            SignedAction::new(delete_action, signature)
                .try_into()
                .unwrap(),
        );

        let signature = fixt!(Signature);
        let update_record_op = ChainOpHashed::from_content_sync(ChainOp::RegisterUpdatedRecord(
            signature.clone(),
            update,
            update_entry.into(),
        ));
        let wire_update = Judged::valid(
            SignedAction::new(update_action, signature)
                .try_into()
                .unwrap(),
        );

        let mut any_entry = None;
        let mut any_entry_hash = None;
        if any_action.entry_hash().is_some() {
            match &mut any_action {
                Action::Create(Create {
                    entry_hash: eh,
                    entry_type,
                    ..
                })
                | Action::Update(Update {
                    entry_hash: eh,
                    entry_type,
                    ..
                }) => {
                    let entry: AppEntryBytes = SerializedBytes::from(UnsafeBytes::from(vec![1u8]))
                        .try_into()
                        .unwrap();
                    let entry = Entry::App(entry);
                    *entry_type = entry_type_fixt.next().unwrap();
                    *eh = EntryHash::with_data_sync(&entry);
                    any_entry_hash = Some(eh.clone());
                    any_entry = Some(entry);
                }
                _ => unreachable!(),
            }
        }

        let any_action_hash = ActionHash::with_data_sync(&any_action);

        let signature = fixt!(Signature);
        let any_store_record_op = ChainOpHashed::from_content_sync(ChainOp::StoreRecord(
            signature.clone(),
            any_action.clone(),
            RecordEntry::new(any_action.entry_visibility(), any_entry.clone()),
        ));

        let any_record = Record::new(
            SignedActionHashed::with_presigned(
                ActionHashed::from_content_sync(any_action.clone()),
                signature.clone(),
            ),
            any_entry.clone(),
        );

        let any_action = Judged::valid(SignedAction::new(any_action, signature));

        Self {
            store_record_op,
            deleted_by_op,
            update_record_op,
            hash: entry_hash,
            entry,
            wire_create,
            wire_delete,
            wire_update,
            create_hash,
            delete_hash,
            update_hash,
            any_store_record_op,
            any_action,
            any_action_hash,
            any_entry,
            any_entry_hash,
            any_record,
        }
    }
}



================================================
File: crates/holochain_cascade/tests/integration.rs
================================================
mod tests;



================================================
File: crates/holochain_cascade/tests/tests/count_links.rs
================================================
use std::sync::Arc;

use holochain_cascade::test_utils::*;
use holochain_cascade::CascadeImpl;
use holochain_p2p::MockHolochainP2pDnaT;
use holochain_state::prelude::*;
use holochain_types::test_utils::chain::action_hash;

// Checks that links can be counted by asking a remote peer who is an authority on the base for the count
#[tokio::test(flavor = "multi_thread")]
async fn count_links_not_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td = EntryTestData::create();
    fill_db(&authority.to_db(), td.store_entry_op.clone()).await;
    fill_db(&authority.to_db(), td.create_link_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    assert_eq!(count, td.links.len());

    fill_db(&authority.to_db(), td.delete_link_op.clone()).await;

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    assert_eq!(count, 0);
}

// Checks that network access is not required for an authority, the agent can count links locally
#[tokio::test(flavor = "multi_thread")]
async fn count_links_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let vault = test_authored_db();

    // Data
    let td = EntryTestData::create();
    fill_db(&vault.to_db(), td.store_entry_op.clone()).await;
    fill_db(&vault.to_db(), td.create_link_op.clone()).await;

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(true));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_network(mock, cache.to_db())
        .with_authored(vault.to_db().into());

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    assert_eq!(count, td.links.len());

    fill_db(&vault.to_db(), td.delete_link_op.clone()).await;

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    assert_eq!(count, 0);
}

// Checks that locally authored data that hasn't yet been published to the network is included in the link count
// seen by the agent doing the publish
#[tokio::test(flavor = "multi_thread")]
async fn count_links_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let mut scratch = Scratch::new();

    // Data
    let td = EntryTestData::create();
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td.create_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // Network
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    mock.expect_count_links()
        .returning(|_| Ok(CountLinksResponse::new(vec![action_hash(&[1, 2, 3])])));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_network(mock.clone(), cache.to_db())
        .with_scratch(scratch.clone().into_sync());

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    // plus 1 to account for the remote link that we aren't storing
    assert_eq!(count, td.links.len() + 1);

    insert_op_scratch(
        &mut scratch,
        td.delete_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    let cascade = CascadeImpl::empty()
        .with_network(mock, cache.to_db())
        .with_scratch(scratch.into_sync());

    let count = cascade
        .dht_count_links(td.link_query.clone())
        .await
        .unwrap();

    // Our link has been deleted but the other link that the remote knows about remains
    assert_eq!(count, 1);
}

#[tokio::test(flavor = "multi_thread")]
async fn count_links_with_filters() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td = EntryTestData::create();
    fill_db(&authority.to_db(), td.store_entry_op.clone()).await;
    fill_db(&authority.to_db(), td.create_link_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    // Negative check for `after`
    let mut query = td.link_query.clone();
    query.after = Some(Timestamp::now());
    assert_eq!(0, execute_query(&cascade, query).await);

    // Positive check for `after`
    let mut query = td.link_query.clone();
    query.after = Some(Timestamp::MIN);
    assert_eq!(td.links.len(), execute_query(&cascade, query).await);

    // Negative check for `before`
    let mut query = td.link_query.clone();
    query.before = Some(Timestamp::MIN);
    assert_eq!(0, execute_query(&cascade, query).await);

    // Positive check for `before`
    let mut query = td.link_query.clone();
    query.before = Some(Timestamp::now());
    assert_eq!(td.links.len(), execute_query(&cascade, query).await);

    // Negative check for `author`
    let mut query = td.link_query.clone();
    query.author = Some(fake_agent_pub_key(2));
    assert_eq!(0, execute_query(&cascade, query).await);

    // Positive check for `author`
    let mut query = td.link_query.clone();
    query.author = td.links.first().map(|l| l.author.clone());
    assert_eq!(td.links.len(), execute_query(&cascade, query).await);
}

async fn execute_query(cascade: &CascadeImpl, query: WireLinkQuery) -> usize {
    cascade.dht_count_links(query).await.unwrap()
}



================================================
File: crates/holochain_cascade/tests/tests/get_activity.rs
================================================
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_cascade::error::CascadeResult;
use holochain_cascade::test_utils::*;
use holochain_cascade::CascadeImpl;
use holochain_p2p::actor::GetActivityOptions;
use holochain_sqlite::db::DbKindAuthored;
use holochain_sqlite::db::DbKindCache;
use holochain_sqlite::db::DbKindDht;
use holochain_state::integrate::authored_ops_to_dht_db_without_check;
use holochain_state::prelude::*;
use holochain_types::test_utils::chain::*;
use std::sync::Arc;
use test_case::test_case;

macro_rules! assert_agent_activity_responses_eq {
    ($expected:expr, $actual:expr) => {
        assert_eq!($expected.agent, $actual.agent);
        match (&$expected.valid_activity, &$actual.valid_activity) {
            (ChainItems::Full(e), ChainItems::Full(a)) => {
                assert_eq!(e.len(), a.len());
                for (i, (e, a)) in e.iter().zip(a.iter()).enumerate() {
                    assert_eq!(e, a, "Not equal at index {}", i);
                }
            }
            (ChainItems::Hashes(e), ChainItems::Hashes(a)) => {
                assert_eq!(e.len(), a.len());
                for (i, (e, a)) in e.iter().zip(a.iter()).enumerate() {
                    assert_eq!(e, a, "Not equal at index {}", i);
                }
            }
            (ChainItems::NotRequested, ChainItems::NotRequested) => {}
            (l, r) => panic!("Not equal: {:?} != {:?}", l, r),
        }
        assert_eq!($expected.valid_activity, $actual.valid_activity);
        assert_eq!($expected.rejected_activity, $actual.rejected_activity);
        assert_eq!($expected.warrants, $actual.warrants);
        assert_eq!($expected.status, $actual.status);
        assert_eq!($expected.highest_observed, $actual.highest_observed);
    };
}

/// A simple test that we can get the activity for an agent
#[tokio::test(flavor = "multi_thread")]
async fn get_activity() {
    holochain_trace::test_run();

    let test_data = ActivityTestData::valid_chain_scenario(false);

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_cache_db()
        .await;

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        ..Default::default()
    };

    let r = scenario.query_authority(options).await.unwrap();

    let expected = AgentActivityResponse {
        agent: test_data.agent.clone(),
        valid_activity: test_data.valid_hashes.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        status: ChainStatus::Valid(test_data.chain_head.clone()),
        highest_observed: Some(test_data.highest_observed.clone()),
    };

    assert_agent_activity_responses_eq!(expected, r);
}

/// Check that the different options for getting chain items will return the same records
#[tokio::test(flavor = "multi_thread")]
async fn get_activity_chain_items_parity() {
    holochain_trace::test_run();

    let test_data = ActivityTestData::valid_chain_scenario(false);

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await
        .with_chain_filter(ChainQueryFilter::new().include_entries(true));

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        ..Default::default()
    };

    let with_hashes = scenario.query_authority(options.clone()).await.unwrap();

    assert_agent_activity_responses_eq!(
        AgentActivityResponse {
            agent: test_data.agent.clone(),
            valid_activity: test_data.valid_hashes.clone(),
            rejected_activity: ChainItems::NotRequested,
            warrants: vec![],
            status: ChainStatus::Valid(test_data.chain_head.clone()),
            highest_observed: Some(test_data.highest_observed.clone()),
        },
        with_hashes
    );

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        ..Default::default()
    };

    let with_records = scenario.query_authority(options.clone()).await.unwrap();

    assert_agent_activity_responses_eq!(
        AgentActivityResponse {
            agent: test_data.agent.clone(),
            valid_activity: test_data.valid_records.clone(),
            rejected_activity: ChainItems::NotRequested,
            warrants: vec![],
            status: ChainStatus::Valid(test_data.chain_head.clone()),
            highest_observed: Some(test_data.highest_observed.clone()),
        },
        with_records
    );

    let hashes_from_hashes: Vec<ActionHash> = match with_hashes.valid_activity {
        ChainItems::Hashes(h) => h.into_iter().map(|(_, h)| h).collect(),
        _ => unreachable!(),
    };
    let hashes_from_records: Vec<ActionHash> = match with_records.valid_activity {
        ChainItems::Full(r) => r.into_iter().map(|r| r.action_hash().clone()).collect(),
        _ => unreachable!(),
    };

    assert_eq!(hashes_from_hashes, hashes_from_records);
}

/// When an AAA doesn't have the entries for the requested records, then the cascade should try to
/// retrieve the entries from either the cache or the network. In this test we check that it's
/// possible to retrieve entries from the cache.
#[tokio::test(flavor = "multi_thread")]
async fn fill_records_entries() {
    holochain_trace::test_run();

    let test_data = ActivityTestData::valid_chain_scenario(false);

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_cache_db()
        .await
        .with_chain_filter(ChainQueryFilter::new().include_entries(true));

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        get_options: GetOptions::local(),
        ..Default::default()
    };

    let r = scenario.query_authority(options).await.unwrap();

    let expected = AgentActivityResponse {
        agent: test_data.agent.clone(),
        valid_activity: test_data.valid_records.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        status: ChainStatus::Valid(test_data.chain_head.clone()),
        highest_observed: Some(test_data.highest_observed.clone()),
    };
    assert_agent_activity_responses_eq!(expected, r);
}

/// Try fetching locally and remotely, and forced remotely to check that all routes return the same
/// result
#[tokio::test(flavor = "multi_thread")]
async fn fetch_routes_parity() {
    holochain_trace::test_run();

    let test_data = ActivityTestData::valid_chain_scenario(false);

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_cache_db()
        .await;

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        get_options: GetOptions::local(),
        ..Default::default()
    };

    // Note that we're actually sharing one dht+cache database here so the local and remote
    // aren't quite accurate here. But the point is to exercise the code paths and not to test
    // that local and remote data are actually merged correctly.

    let local_with_remote_authority = scenario.query_authority(options.clone()).await.unwrap();

    let local_as_self_authority = scenario.query_self(options.clone()).await.unwrap();

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        get_options: GetOptions::network(),
        ..Default::default()
    };

    let remote_with_remote_authority = scenario.query_authority(options.clone()).await.unwrap();

    let remote_as_self_authority = scenario.query_self(options.clone()).await.unwrap();

    assert_agent_activity_responses_eq!(local_with_remote_authority, local_as_self_authority);
    assert_agent_activity_responses_eq!(local_with_remote_authority, remote_with_remote_authority);
    assert_agent_activity_responses_eq!(local_with_remote_authority, remote_as_self_authority);
}

/// Check that getting activity with records will not serve private entries
#[tokio::test(flavor = "multi_thread")]
async fn record_activity_does_not_serve_private_entries() {
    holochain_trace::test_run();

    let mut test_data = ActivityTestData::valid_chain_scenario(true);

    // Wipe out the private entries in the expected response
    match &mut test_data.valid_records {
        ChainItems::Full(records) => {
            // Skip 1 to leave the DNA entry as `NA` rather than `Hidden`
            records.iter_mut().skip(1).for_each(|r| {
                r.entry = RecordEntry::Hidden;
            })
        }
        _ => unreachable!(),
    }

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await;

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        get_options: GetOptions::local(),
        ..Default::default()
    };

    let r = scenario.query_authority(options).await.unwrap();

    let expected = AgentActivityResponse {
        agent: test_data.agent.clone(),
        valid_activity: test_data.valid_records.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        status: ChainStatus::Valid(test_data.chain_head.clone()),
        highest_observed: Some(test_data.highest_observed.clone()),
    };

    assert_agent_activity_responses_eq!(expected, r);
}

/// Checks that actions can be requested with a chain query filter that drops entries
#[tokio::test(flavor = "multi_thread")]
async fn filter_out_entries_with_chain_query() {
    holochain_trace::test_run();

    let mut test_data = ActivityTestData::valid_chain_scenario(false);

    // Wipe out the entries in the expected response
    match &mut test_data.valid_records {
        ChainItems::Full(records) => {
            // Skip 1 to leave the DNA entry as `NA` rather than `NotStored`
            records.iter_mut().skip(1).for_each(|r| {
                r.entry = RecordEntry::NotStored;
            })
        }
        _ => unreachable!(),
    }

    let filter = ChainQueryFilter::new().include_entries(false);

    let scenario = GetActivityTestScenario::new(test_data.clone())
        .with_chain_filter(filter)
        .include_agent_activity_ops_in_dht_db()
        .await
        .include_store_entry_ops_in_dht_db()
        .await
        .include_agent_activity_noise_ops_in_dht_db()
        .await;

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        get_options: GetOptions::local(),
        ..Default::default()
    };

    let r = scenario.query_authority(options).await.unwrap();

    let expected = AgentActivityResponse {
        agent: test_data.agent.clone(),
        valid_activity: test_data.valid_records.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        status: ChainStatus::Valid(test_data.chain_head.clone()),
        highest_observed: Some(test_data.highest_observed.clone()),
    };

    assert_agent_activity_responses_eq!(expected, r);
}

#[cfg(feature = "unstable-warrants")]
#[tokio::test(flavor = "multi_thread")]
async fn get_activity_with_warrants() {
    holochain_trace::test_run();

    // DBs
    let cache = test_cache_db();
    let dht = test_dht_db();

    // Data
    let td = ActivityTestData::valid_chain_scenario(false);

    for hash_op in td.agent_activity_ops.iter().cloned() {
        fill_db(&dht.to_db(), hash_op).await;
    }
    for hash_op in td.noise_agent_activity_ops.iter().cloned() {
        fill_db(&dht.to_db(), hash_op).await;
    }
    for hash_op in td.store_entry_ops.iter().cloned() {
        fill_db(&cache.to_db(), hash_op).await;
    }

    let warrant = {
        let action_pair = (
            (
                td.agent_activity_ops[0].action().to_hash(),
                ::fixt::fixt!(Signature),
            ),
            (
                td.agent_activity_ops[1].action().to_hash(),
                ::fixt::fixt!(Signature),
            ),
        );
        let p = WarrantProof::ChainIntegrity(ChainIntegrityWarrant::ChainFork {
            chain_author: td.agent.clone(),
            action_pair,
        });
        let warrant = Warrant::new(p, AgentPubKey::from_raw_36(vec![255; 36]), Timestamp::now());
        WarrantOp::from(SignedWarrant::new(warrant, ::fixt::fixt!(Signature)))
    };

    // Insert unvalidated warrant op
    dht.test_write({
        let op = DhtOp::from(warrant.clone()).into_hashed();
        move |txn| {
            insert_op_dht(txn, &op, None).unwrap();
        }
    });

    let options = GetActivityOptions {
        include_valid_activity: true,
        include_rejected_activity: false,
        include_full_records: true,
        ..Default::default()
    };

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![dht.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    let mut expected = AgentActivityResponse {
        agent: td.agent.clone(),
        valid_activity: td.valid_records.clone(),
        rejected_activity: ChainItems::NotRequested,
        warrants: vec![],
        status: ChainStatus::Valid(td.chain_head.clone()),
        highest_observed: Some(td.highest_observed.clone()),
    };

    let r1 = cascade
        .get_agent_activity(
            td.agent.clone(),
            ChainQueryFilter::new().include_entries(true),
            options.clone(),
        )
        .await
        .unwrap();

    assert_agent_activity_responses_eq!(expected, r1);

    // If the warrant is validated, it will be returned
    dht.test_write({
        let op = DhtOp::from(warrant.clone()).into_hashed();
        let op_hash = op.as_hash().clone();
        move |txn| {
            holochain_state::mutations::set_validation_status(
                txn,
                &op_hash,
                ValidationStatus::Valid,
            )
            .unwrap();
            holochain_state::mutations::set_when_integrated(txn, &op_hash, Timestamp::now())
                .unwrap();
        }
    });

    let r2 = cascade
        .get_agent_activity(
            td.agent.clone(),
            ChainQueryFilter::new().include_entries(true),
            options,
        )
        .await
        .unwrap();

    expected.warrants = vec![warrant.into_warrant()];

    assert_eq!(r2, expected);
}

#[derive(Default)]
struct Data {
    scratch: Option<Vec<(AgentPubKey, Vec<TestChainItem>)>>,
    authored: Vec<(AgentPubKey, Vec<TestChainItem>)>,
    cache: Vec<(AgentPubKey, Vec<TestChainItem>)>,
    dht: Vec<(AgentPubKey, Vec<TestChainItem>)>,
    warrants: Vec<WarrantOp>,
}

#[cfg(feature = "unstable-warrants")]
fn warrant(author: u8, action: u8) -> WarrantOp {
    let p = WarrantProof::ChainIntegrity(ChainIntegrityWarrant::InvalidChainOp {
        action_author: AgentPubKey::from_raw_36(vec![author; 36]),
        action: (
            ActionHash::from_raw_36(vec![action; 36]),
            ::fixt::fixt!(Signature),
        ),
        validation_type: ValidationType::Sys,
    });
    let warrant = Warrant::new(p, AgentPubKey::from_raw_36(vec![255; 36]), Timestamp::now());
    WarrantOp::from(SignedWarrant::new(warrant, ::fixt::fixt!(Signature)))
}

struct GetActivityTestScenario {
    dht: TestDb<DbKindDht>,
    cache: TestDb<DbKindCache>,
    test_data: ActivityTestData,
    chain_filter: ChainQueryFilter,
}

impl GetActivityTestScenario {
    fn new(test_data: ActivityTestData) -> Self {
        let dht = test_dht_db();
        let cache = test_cache_db();

        Self {
            dht,
            cache,
            test_data,
            chain_filter: ChainQueryFilter::new(),
        }
    }

    async fn include_agent_activity_ops_in_dht_db(self) -> Self {
        for hash_op in self.test_data.agent_activity_ops.iter().cloned() {
            fill_db(&self.dht.to_db(), hash_op).await;
        }

        self
    }

    async fn include_agent_activity_noise_ops_in_dht_db(self) -> Self {
        for hash_op in self.test_data.noise_agent_activity_ops.iter().cloned() {
            fill_db(&self.dht.to_db(), hash_op).await;
        }

        self
    }

    async fn include_store_entry_ops_in_dht_db(self) -> Self {
        for hash_op in self.test_data.store_entry_ops.iter().cloned() {
            fill_db(&self.dht.to_db(), hash_op).await;
        }

        self
    }

    async fn include_store_entry_ops_in_cache_db(self) -> Self {
        for hash_op in self.test_data.store_entry_ops.iter().cloned() {
            fill_db(&self.cache.to_db(), hash_op).await;
        }

        self
    }

    fn with_chain_filter(mut self, chain_filter: ChainQueryFilter) -> Self {
        self.chain_filter = chain_filter;
        self
    }

    async fn query_authority(
        &self,
        options: GetActivityOptions,
    ) -> CascadeResult<AgentActivityResponse> {
        let network =
            PassThroughNetwork::authority_for_nothing(vec![self.dht.to_db().clone().into()]);

        let cascade = CascadeImpl::empty().with_network(network, self.cache.to_db());

        self.query(cascade, options).await
    }

    async fn query_self(
        &self,
        options: GetActivityOptions,
    ) -> CascadeResult<AgentActivityResponse> {
        let network = PassThroughNetwork::authority_for_all(vec![self.dht.to_db().clone().into()]);

        let cascade = CascadeImpl::empty()
            .with_network(network, self.cache.to_db())
            .with_dht(self.dht.clone().into());

        self.query(cascade, options).await
    }

    async fn query(
        &self,
        cascade: CascadeImpl,
        options: GetActivityOptions,
    ) -> CascadeResult<AgentActivityResponse> {
        cascade
            .get_agent_activity(
                self.test_data.agent.clone(),
                self.chain_filter.clone(),
                options,
            )
            .await
    }
}

#[test_case(
    Data { dht: agent_chain(&[(0, 0..3)]), ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[1]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 2; "1 to genesis with dht 0 till 2")]
#[test_case(
    Data { cache: agent_chain(&[(0, 0..3)]), ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[1]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 2; "1 to genesis with cache 0 till 2")]
#[test_case(
    Data { scratch: Some(agent_chain(&[(0, 0..3)])), ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[1]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 2; "1 to genesis with scratch 0 till 2")]
#[test_case(
    Data { authored: agent_chain(&[(0, 0..3)]), scratch: Some(agent_chain(&[(0, 3..6)])), ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[4])).take(4).until(action_hash(&[0]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 4; "4 take 4 until 0 with authored 0 till 2 and scratch 3 till 5")]
#[test_case(
    Data { authored: agent_chain(&[(0, 0..6)]), ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[4])).take(4).until(action_hash(&[0]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 4; "4 take 4 until 0 with authored 0 till 5")]
#[tokio::test(flavor = "multi_thread")]
async fn test_must_get_agent_activity(
    data: Data,
    author: AgentPubKey,
    filter: ChainFilter,
) -> MustGetAgentActivityResponse {
    test_must_get_agent_activity_inner(data, author, filter).await
}

#[cfg(feature = "unstable-warrants")]
#[test_case(
    Data { dht: agent_chain(&[(0, 0..3)]), warrants: vec![warrant(1, 1)], ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[1]))
    => matches MustGetAgentActivityResponse::Activity { activity, .. } if activity.len() == 2; "1 to genesis with dht 0 till 2 with 1 unrelated chain warrant")]
#[test_case(
    Data { dht: agent_chain(&[(0, 0..3)]), warrants: vec![warrant(0, 0)], ..Default::default() },
    agent_hash(&[0]), ChainFilter::new(action_hash(&[1]))
    => matches MustGetAgentActivityResponse::Activity { warrants, .. } if warrants.len() == 1; "1 to genesis with dht 0 till 2 with 1 chain warrant")]
#[tokio::test(flavor = "multi_thread")]
async fn test_must_get_agent_activity_with_warrants(
    data: Data,
    author: AgentPubKey,
    filter: ChainFilter,
) -> MustGetAgentActivityResponse {
    test_must_get_agent_activity_inner(data, author, filter).await
}

async fn test_must_get_agent_activity_inner(
    data: Data,
    author: AgentPubKey,
    filter: ChainFilter,
) -> MustGetAgentActivityResponse {
    let Data {
        scratch,
        authored,
        cache,
        dht,
        warrants,
    } = data;
    let dht = commit_chain(DbKindDht(Arc::new(DnaHash::from_raw_36(vec![0; 36]))), dht);
    let network = PassThroughNetwork::authority_for_nothing(vec![dht.clone().into()]);
    let cache = commit_chain(
        DbKindCache(Arc::new(DnaHash::from_raw_36(vec![0; 36]))),
        cache,
    );
    let authored = commit_chain(
        DbKindAuthored(Arc::new(CellId::new(
            DnaHash::from_raw_36(vec![0; 36]),
            AgentPubKey::from_raw_36(vec![0; 36]),
        ))),
        authored,
    );
    let hashes = warrants.iter().map(|w| w.to_hash()).collect();
    authored.test_write(|txn| {
        for w in warrants {
            let w = DhtOp::from(w).into_hashed();
            insert_op_authored(txn, &w).unwrap();
        }
    });

    let dht_cache = DhtDbQueryCache::new(dht.clone().into());
    authored_ops_to_dht_db_without_check(hashes, authored.clone().into(), dht, &dht_cache)
        .await
        .unwrap();

    let sync_scratch = match scratch {
        Some(scratch) => {
            let sync_scratch = Scratch::new().into_sync();
            commit_scratch(sync_scratch.clone(), scratch);
            Some(sync_scratch)
        }
        None => None,
    };
    let mut cascade = CascadeImpl::empty()
        .with_authored(authored.into())
        .with_network(network, cache);
    if let Some(sync_scratch) = sync_scratch {
        cascade = cascade.with_scratch(sync_scratch);
    }
    cascade
        .must_get_agent_activity(author, filter)
        .await
        .unwrap()
}



================================================
File: crates/holochain_cascade/tests/tests/get_entry.rs
================================================
use std::sync::Arc;

use holo_hash::HasHash;
use holochain_cascade::test_utils::*;
use holochain_cascade::{Cascade, CascadeImpl};
use holochain_p2p::MockHolochainP2pDnaT;
use holochain_state::mutations::insert_op_scratch;
use holochain_state::prelude::*;

async fn assert_can_get(
    td_entry: &EntryTestData,
    td_record: &RecordTestData,
    cascade: &CascadeImpl,
    options: GetOptions,
) {
    // - Get via entry hash
    let r = cascade
        .dht_get(td_entry.hash.clone().into(), options.clone())
        .await
        .unwrap()
        .expect("Failed to get entry");

    assert_eq!(*r.action_address(), td_entry.create_hash);
    assert_eq!(r.action().entry_hash(), Some(&td_entry.hash));

    // - Get via action hash
    let r = cascade
        .dht_get(td_record.any_action_hash.clone().into(), options.clone())
        .await
        .unwrap()
        .expect("Failed to get record");

    assert_eq!(*r.action_address(), td_record.any_action_hash);
    assert_eq!(r.action().entry_hash(), td_record.any_entry_hash.as_ref());

    // - Get details via entry hash
    let r = cascade
        .get_details(td_entry.hash.clone().into(), options.clone())
        .await
        .unwrap()
        .expect("Failed to get entry");

    let expected = Details::Entry(EntryDetails {
        entry: td_entry.entry.entry.clone(),
        actions: vec![td_entry
            .wire_create
            .data
            .clone()
            .into_action(td_entry.entry.entry_type.clone(), td_entry.hash.clone())],
        rejected_actions: vec![],
        deletes: vec![],
        updates: vec![],
        entry_dht_status: EntryDhtStatus::Live,
    });

    assert_eq!(r, expected);

    // - Get details via action hash
    let r = cascade
        .get_details(td_record.any_action_hash.clone().into(), options.clone())
        .await
        .unwrap()
        .expect("Failed to get record details");

    let expected = Details::Record(RecordDetails {
        record: td_record.any_record.clone(),
        validation_status: ValidationStatus::Valid,
        deletes: vec![],
        updates: vec![],
    });
    assert_eq!(r, expected);
}

async fn assert_is_none(
    td_entry: &EntryTestData,
    td_record: &RecordTestData,
    cascade: &CascadeImpl,
    options: GetOptions,
) {
    // - Get via entry hash
    let r = cascade
        .dht_get(td_entry.hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());

    // - Get via action hash
    let r = cascade
        .dht_get(td_record.any_action_hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());

    // - Get details via entry hash
    let r = cascade
        .get_details(td_entry.hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());

    // - Get details via action hash
    let r = cascade
        .get_details(td_record.any_action_hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());
}

async fn assert_rejected(
    td_entry: &EntryTestData,
    td_record: &RecordTestData,
    cascade: &CascadeImpl,
    options: GetOptions,
) {
    // - Get via entry hash
    let r = cascade
        .dht_get(td_entry.hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());

    // - Get via action hash
    let r = cascade
        .dht_get(td_record.any_action_hash.clone().into(), options.clone())
        .await
        .unwrap();

    assert!(r.is_none());

    let r = cascade
        .get_details(td_entry.hash.clone().into(), Default::default())
        .await
        .unwrap()
        .expect("Failed to get entry");

    let expected = Details::Entry(EntryDetails {
        entry: td_entry.entry.entry.clone(),
        actions: vec![],
        rejected_actions: vec![td_entry
            .wire_create
            .data
            .clone()
            .into_action(td_entry.entry.entry_type.clone(), td_entry.hash.clone())],
        deletes: vec![],
        updates: vec![],
        entry_dht_status: EntryDhtStatus::Dead,
    });

    assert_eq!(r, expected);

    let r = cascade
        .get_details(td_record.any_action_hash.clone().into(), Default::default())
        .await
        .unwrap()
        .expect("Failed to get entry");

    let expected = Details::Record(RecordDetails {
        record: td_record.any_record.clone(),
        validation_status: ValidationStatus::Rejected,
        deletes: vec![],
        updates: vec![],
    });

    assert_eq!(r, expected);
}

async fn assert_can_retrieve(td_entry: &EntryTestData, cascade: &CascadeImpl, options: GetOptions) {
    // - Retrieve via entry hash
    let (r, _) = cascade
        .retrieve(td_entry.hash.clone().into(), options.clone().into())
        .await
        .unwrap()
        .expect("Failed to retrieve record");

    assert_eq!(*r.action_address(), td_entry.create_hash);
    assert_eq!(r.action().entry_hash(), Some(&td_entry.hash));

    // - Retrieve via action hash
    let (r, _) = cascade
        .retrieve(td_entry.create_hash.clone().into(), options.clone().into())
        .await
        .unwrap()
        .expect("Failed to retrieve record");

    assert_eq!(*r.action_address(), td_entry.create_hash);
    assert_eq!(r.action().entry_hash(), Some(&td_entry.hash));

    // - Retrieve entry
    let (r, _) = cascade
        .retrieve_entry(td_entry.hash.clone(), options.clone().into())
        .await
        .unwrap()
        .expect("Failed to retrieve entry");

    assert_eq!(*r.as_hash(), td_entry.hash);

    // - Retrieve action
    let (r, _) = cascade
        .retrieve_action(td_entry.create_hash.clone(), options.clone().into())
        .await
        .unwrap()
        .expect("Failed to retrieve action");

    assert_eq!(*r.as_hash(), td_entry.create_hash);
}

#[tokio::test(flavor = "multi_thread")]
async fn entry_not_authority_or_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db(&authority.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db(&authority.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    assert_can_get(&td_entry, &td_record, &cascade, GetOptions::network()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn entry_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let mut scratch = Scratch::new();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    insert_op_scratch(
        &mut scratch,
        td_entry.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td_record.any_store_record_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_scratch(scratch.into_sync())
        .with_network(mock, cache.to_db());

    assert_can_get(&td_entry, &td_record, &cascade, GetOptions::network()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn entry_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let vault = test_authored_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db(&vault.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db(&vault.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(true));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_authored(vault.to_db().into())
        .with_network(mock, cache.to_db());

    assert_can_get(&td_entry, &td_record, &cascade, GetOptions::network()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn content_not_authority_or_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let vault = test_authored_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db(&vault.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db(&vault.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_authored(vault.to_db().into())
        .with_network(mock, cache.to_db());

    assert_can_get(&td_entry, &td_record, &cascade, GetOptions::local()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn content_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let mut scratch = Scratch::new();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    insert_op_scratch(
        &mut scratch,
        td_entry.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td_record.any_store_record_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_scratch(scratch.into_sync())
        .with_network(mock, cache.to_db());

    assert_can_get(&td_entry, &td_record, &cascade, GetOptions::local()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn content_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let vault = test_authored_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(true));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_authored(vault.to_db().into())
        .with_network(mock, cache.to_db());

    assert_is_none(&td_entry, &td_record, &cascade, GetOptions::local()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn rejected_ops() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db_rejected(&authority.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db_rejected(&authority.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());
    assert_rejected(&td_entry, &td_record, &cascade, GetOptions::network()).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn check_can_handle_rejected_ops_in_cache() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db_rejected(&cache.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db_rejected(&cache.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());
    assert_rejected(&td_entry, &td_record, &cascade, GetOptions::network()).await;
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "todo"]
async fn check_all_queries_still_work() {
    // TODO: Come up with a list of different states the authority could
    // have data in (updates, rejected, abandoned, nothing etc.)
    // then create an iterator that can put databases in these states and
    // run all the above queries on them.
    todo!()
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "todo"]
async fn check_all_queries_still_work_with_cache() {
    todo!()
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "todo"]
async fn check_all_queries_still_work_with_scratch() {
    todo!()
}

#[tokio::test(flavor = "multi_thread")]
async fn test_pending_data_isnt_returned() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();
    let vault = test_authored_db();

    // Data
    let td_entry = EntryTestData::create();
    let td_record = RecordTestData::create();
    fill_db_pending(&authority.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db_pending(&authority.to_db(), td_record.any_store_record_op.clone()).await;
    fill_db_pending(&vault.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db_pending(&vault.to_db(), td_record.any_store_record_op.clone()).await;
    fill_db_pending(&cache.to_db(), td_entry.store_entry_op.clone()).await;
    fill_db_pending(&cache.to_db(), td_record.any_store_record_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    assert_is_none(&td_entry, &td_record, &cascade, GetOptions::network()).await;

    assert_can_retrieve(&td_entry, &cascade, GetOptions::network()).await;

    let network = PassThroughNetwork::authority_for_all(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    assert_is_none(&td_entry, &td_record, &cascade, GetOptions::network()).await;

    assert_can_retrieve(&td_entry, &cascade, GetOptions::network()).await;
}



================================================
File: crates/holochain_cascade/tests/tests/get_links.rs
================================================
use holochain_cascade::test_utils::*;
use holochain_cascade::CascadeImpl;
use holochain_p2p::MockHolochainP2pDnaT;
use holochain_state::prelude::*;
use std::sync::Arc;

#[tokio::test(flavor = "multi_thread")]
async fn links_not_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let authority = test_dht_db();

    // Data
    let td = EntryTestData::create();
    fill_db(&authority.to_db(), td.store_entry_op.clone()).await;
    fill_db(&authority.to_db(), td.create_link_op.clone()).await;

    // Network
    let network = PassThroughNetwork::authority_for_nothing(vec![authority.to_db().clone().into()]);

    // Cascade
    let cascade = CascadeImpl::empty().with_network(network, cache.to_db());

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(r, td.links);

    let r = cascade
        .get_link_details(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(r, vec![(td.create_link_action.clone(), vec![]),]);

    fill_db(&authority.to_db(), td.delete_link_op.clone()).await;

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert!(r.is_empty());

    let r = cascade
        .get_link_details(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(
        r,
        vec![(
            td.create_link_action.clone(),
            vec![td.delete_link_action.clone()]
        ),]
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn links_authority() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let vault = test_authored_db();

    // Data
    let td = EntryTestData::create();
    fill_db(&vault.to_db(), td.store_entry_op.clone()).await;
    fill_db(&vault.to_db(), td.create_link_op.clone()).await;

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(true));
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_network(mock, cache.to_db())
        .with_authored(vault.to_db().into());

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(r, td.links);

    fill_db(&vault.to_db(), td.delete_link_op.clone()).await;

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert!(r.is_empty());
}

#[tokio::test(flavor = "multi_thread")]
async fn links_authoring() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let mut scratch = Scratch::new();

    // Data
    let td = EntryTestData::create();
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td.create_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    mock.expect_get_links().returning(|_, _| {
        Ok(vec![WireLinkOps {
            creates: vec![],
            deletes: vec![],
        }])
    });
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_network(mock.clone(), cache.to_db())
        .with_scratch(scratch.clone().into_sync());

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(r, td.links);

    insert_op_scratch(
        &mut scratch,
        td.delete_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    let cascade = CascadeImpl::empty()
        .with_network(mock, cache.to_db())
        .with_scratch(scratch.into_sync());

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert!(r.is_empty());
}

#[tokio::test(flavor = "multi_thread")]
async fn test_links_can_match_a_partial_tag() {
    holochain_trace::test_run();

    // Environments
    let cache = test_cache_db();
    let mut scratch = Scratch::new();

    // Data
    let td = EntryTestData::create();
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td.create_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // Network
    // - Not expecting any calls to the network.
    let mut mock = MockHolochainP2pDnaT::new();
    mock.expect_authority_for_hash().returning(|_| Ok(false));
    mock.expect_get_links().returning(|_, _| {
        Ok(vec![WireLinkOps {
            creates: vec![],
            deletes: vec![],
        }])
    });
    let mock = Arc::new(mock);

    // Cascade
    let cascade = CascadeImpl::empty()
        .with_network(mock.clone(), cache.to_db())
        .with_scratch(scratch.clone().into_sync());

    let mut query = td.link_key_tag.clone();
    // Take the first 10 bytes of the tag
    query.tag = Some(LinkTag::new(
        query
            .tag
            .unwrap()
            .0
            .into_iter()
            .take(10)
            .collect::<Vec<u8>>(),
    ));

    let r = cascade
        .dht_get_links(td.link_key_tag.clone(), Default::default())
        .await
        .unwrap();

    assert_eq!(1, r.len());
}



================================================
File: crates/holochain_cascade/tests/tests/mod.rs
================================================
mod count_links;
mod get_activity;
mod get_entry;
mod get_links;



================================================
File: crates/holochain_chc/README.md
================================================
# holochain_chc

License: Apache-2.0



================================================
File: crates/holochain_chc/Cargo.toml
================================================
[package]
name = "holochain_chc"
version = "0.2.0-dev.21"
description = "Defines the Chain Head Coordinator (CHC) API for Holochain and provides an HTTP client implementation."
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_chc"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
async-trait = "0.1"
derive_more = "0.99"
futures = "0.3"
getrandom = "0.2.7"
holochain_keystore = { version = "^0.5.0-dev.20", path = "../holochain_keystore", default-features = false }
holochain_nonce = { version = "^0.5.0-dev.2", path = "../holochain_nonce" }
holochain_types = { path = "../holochain_types", version = "^0.5.0-dev.21" }
one_err = "0.0.8"
must_future = "0.1.1"
parking_lot = "0.12"
serde = { version = "1.0", features = ["derive"] }
serde_bytes = "0.11.12"
serde_json = { version = "1.0.51", features = ["preserve_order"] }
thiserror = "1.0.22"
tracing = "0.1"
url = "2.4"

holochain_serialized_bytes = { version = "=0.0.55", optional = true }
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash", optional = true }
reqwest = { version = "0.12", default-features = false, features = [
  "json",
  "rustls-tls",
], optional = true }

[dev-dependencies]
holochain_chc = { path = ".", features = ["test_utils"] }

fixt = { version = "^0.5.0-dev.1", path = "../fixt" }
pretty_assertions = "1.4"
tokio = { version = "1.36.0", features = ["full"] }

[lints]
workspace = true

[features]
default = ["http"]

test_utils = ["holochain_types/test_utils", "holo_hash/test_utils"]

http = ["holochain_serialized_bytes", "reqwest"]

instrument = []



================================================
File: crates/holochain_chc/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.2.0-dev.21

## 0.2.0-dev.20

## 0.2.0-dev.19

## 0.2.0-dev.18

## 0.2.0-dev.17

## 0.2.0-dev.16

## 0.2.0-dev.15

## 0.2.0-dev.14

## 0.2.0-dev.13

## 0.2.0-dev.12

## 0.2.0-dev.11

## 0.2.0-dev.10

## 0.2.0-dev.9

## 0.2.0-dev.8

## 0.2.0-dev.7

## 0.2.0-dev.6

## 0.2.0-dev.5

## 0.2.0-dev.4

## 0.2.0-dev.3

## 0.2.0-dev.2

## 0.2.0-dev.1

## 0.2.0-dev.0

## 0.1.0

## 0.1.0-dev.8

## 0.1.0-dev.7

## 0.1.0-dev.6

## 0.1.0-dev.5

## 0.1.0-dev.4

## 0.1.0-dev.3

## 0.1.0-dev.2

## 0.1.0-dev.1



================================================
File: crates/holochain_chc/src/chc_http.rs
================================================
//! Defines a client for use with a remote HTTP-based CHC.
//!
//! The server must implement the following endpoints:
//!
//! ## `POST /add_records/{dna_hash}/{agent_pubkey}`
//!
//! Adds a list of records to the CHC.
//!
//! The CHC state will only be altered if a 200 status code is returned, which requires that:
//! - the new records are valid
//! - the signature matches the agent pubkey
//!
//! If the new records would cause a fork of the CHC chain but are otherwise valid, a 409 status code is returned
//! along with the sequence number and hash of the fork point. This code indicates to the client that the local
//! state should be synchronized with the CHC state before attempting to add the records again.
//! (by calling `get_record_data` and then "grafting" the records onto the local chain).
//!
//! If there is some other problem with the input record data which prevents it from being added to the CHC state,
//! e.g. the new records themselves do not constitute a valid hash chain, or the signature does not match,
//! a 498 status code may be returned to indicate that the input is bad and must be fixed.
//!
//! Any other error code can be returned to indicate a server error.
//!
//! Body: msgpack-encoded [`AddRecordsRequest`]
//! Response:
//! - 200: (no data)
//! - 409: msgpack-encoded `(u32, ActionHash)` (seq number and hash of fork point)
//! - 498: msgpack-encoded `u32` (seq number of last record in the CHC chain)
//! - other: error message as plaintext string
//!
//! ## `POST /get_record_data/{dna_hash}/{agent_pubkey}`
//!
//! Returns CHC data starting from the record *after* the given hash.
//!
//! If the given hash is not present in CHC state, Error code 498 should be returned with no data.
//!
//! A nonce must be provided in the request body to prevent replay attacks. The nonce need
//! not be truly random, just unique.
//!
//! **NOTE**: the `EncryptedEntry` data is not currently encrypted. Encryption is a TODO!
//!
//! Body: msgpack-encoded [`GetRecordsRequest`]
//! Response:
//! - 200: msgpack-encoded `Vec<(SignedActionHashed, Option<(Arc<EncryptedEntry>, Signature)>)>`
//! - 498: (no data)
//! - other: error message as plaintext string
//!
//! ## Notes (for both endpoints)
//!
//! The `{dna_hash}` and `{agent_pubkey}` in the URL are base64-encoded in the standard way.
//! (See the `Display` impl for `DnaHash` and `AgentPubKey`.)
//!
//! The request body is serialized using [`holochain_serialized_bytes::encode`] and can be deserialized using any
//! msgpack decoder.
//!
//! Any msgpack-encoded response must be encoded in a way that can deserialized by [`holochain_serialized_bytes::decode`].
//! Most standard msgpack encoders should work just fine for the return types being used here.
//!

use std::sync::Arc;

use super::ChainHeadCoordinatorExt;
use super::*;
use holochain_keystore::MetaLairClient;
use url::Url;

/// An HTTP client which can talk to a remote CHC implementation
pub struct ChcHttp {
    client: ChcHttpClient,
    keystore: MetaLairClient,
    agent: AgentPubKey,
}

#[async_trait::async_trait]
impl ChainHeadCoordinator for ChcHttp {
    type Item = SignedActionHashed;

    async fn add_records_request(&self, request: AddRecordsRequest) -> ChcResult<()> {
        let response: reqwest::Response = self.client.post("add_records", &request).await?;
        let status = response.status().as_u16();
        match status {
            200 => Ok(()),
            409 => {
                let bytes = response.bytes().await.map_err(extract_string)?;
                let (seq, hash): (u32, ActionHash) = holochain_serialized_bytes::decode(&bytes)?;
                Err(ChcError::InvalidChain(seq, hash))
            }
            498 => {
                let bytes = response.bytes().await.map_err(extract_string)?;
                let seq: u32 = holochain_serialized_bytes::decode(&bytes)?;
                Err(ChcError::NoRecordsAdded(seq))
            }
            code => {
                let msg = response.text().await.map_err(extract_string)?;
                Err(ChcError::Other(format!("code: {code}, msg: {msg}")))
            }
        }
    }

    async fn get_record_data_request(
        &self,
        request: GetRecordsRequest,
    ) -> ChcResult<Vec<(SignedActionHashed, Option<(Arc<EncryptedEntry>, Signature)>)>> {
        let response = self.client.post("get_record_data", &request).await?;
        let status = response.status().as_u16();
        match status {
            200 => {
                let bytes = response.bytes().await.map_err(extract_string)?;
                Ok(holochain_serialized_bytes::decode(&bytes)?)
            }
            498 => {
                // The since_hash was not found in the CHC,
                // so we can interpret this as an empty list of records.
                Ok(vec![])
            }
            code => {
                let msg = response.text().await.map_err(extract_string)?;
                Err(ChcError::Other(format!("code: {code}, msg: {msg}")))
            }
        }
    }
}

impl ChainHeadCoordinatorExt for ChcHttp {
    fn signing_info(&self) -> (MetaLairClient, AgentPubKey) {
        (self.keystore.clone(), self.agent.clone())
    }
}

impl ChcHttp {
    /// Constructor
    pub fn new(base_url: Url, keystore: MetaLairClient, cell_id: &CellId) -> Self {
        let client = ChcHttpClient {
            base_url: base_url
                .join(&format!(
                    "{}/{}/",
                    cell_id.dna_hash(),
                    cell_id.agent_pubkey()
                ))
                .expect("invalid URL"),
            client: reqwest::Client::builder().use_rustls_tls().build().unwrap(),
        };
        Self {
            client,
            keystore,
            agent: cell_id.agent_pubkey().clone(),
        }
    }
}

/// Client for a single CHC server
pub struct ChcHttpClient {
    base_url: url::Url,
    client: reqwest::Client,
}

impl ChcHttpClient {
    fn url(&self, path: &str) -> String {
        assert!(!path.starts_with('/'));
        self.base_url.join(path).expect("invalid URL").to_string()
    }

    async fn post<T>(&self, path: &str, body: &T) -> ChcResult<reqwest::Response>
    where
        T: serde::Serialize + std::fmt::Debug,
    {
        let url = self.url(path);
        let body = holochain_serialized_bytes::encode(body)?;
        let res: reqwest::Response = self
            .client
            .post(url.clone())
            .body(body)
            .send()
            .await
            .map_err(extract_string)?;
        Ok(res)
    }
}

fn extract_string(e: reqwest::Error) -> ChcError {
    ChcError::ServiceUnreachable(e.to_string())
}

#[cfg(test)]
mod tests {

    use super::*;
    use holo_hash::fixt::*;
    use holochain_types::test_utils::valid_arbitrary_chain;
    use pretty_assertions::assert_eq;

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "this test requires a remote service, so it should only be run manually"]
    async fn test_add_records_remote() {
        let keystore = holochain_keystore::test_keystore();
        let agent = fake_agent_pubkey_1();
        let cell_id = CellId::new(::fixt::fixt!(DnaHash), agent.clone());
        let chc = Arc::new(ChcHttp::new(
            url::Url::parse("http://127.0.0.1:40845/").unwrap(),
            // url::Url::parse("https://chc.dev.holotest.net/v1/").unwrap(),
            keystore.clone(),
            &cell_id,
        ));

        let chain = valid_arbitrary_chain(&keystore, agent, 20).await;

        let t0 = &chain[0..3];
        let t1 = &chain[3..6];
        let t2 = &chain[6..9];
        let t11 = &chain[11..=11];

        let hash = |i: usize| chain[i].action_address().clone();

        // dbg!(t0
        //     .iter()
        //     .map(|r| (r.action_address(), r.action().prev_action()))
        //     .collect::<Vec<_>>());

        // dbg!(&t0, &t1, &t2);

        chc.clone()
            .add_records(t0.to_vec())
            .await
            .map_err(|e| e.to_string()[..1024.min(e.to_string().len())].to_string())
            .unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(2));

        chc.clone().add_records(t1.to_vec()).await.unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(5));

        // last_hash doesn't match
        assert!(chc.clone().add_records(t0.to_vec()).await.is_err());
        assert!(chc.clone().add_records(t1.to_vec()).await.is_err());
        assert!(chc.clone().add_records(t11.to_vec()).await.is_err());
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(5));

        chc.clone().add_records(t2.to_vec()).await.unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(8));

        assert_eq!(
            chc.clone().get_record_data(None).await.unwrap(),
            &chain[0..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(0))).await.unwrap(),
            &chain[1..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(3))).await.unwrap(),
            &chain[4..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(7))).await.unwrap(),
            &chain[8..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(8))).await.unwrap(),
            &[]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(9))).await.unwrap(),
            &[]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(13))).await.unwrap(),
            &[]
        );
    }
}



================================================
File: crates/holochain_chc/src/chc_local.rs
================================================
use std::sync::Arc;

use crate::*;
use holochain_keystore::MetaLairClient;

/// Mutable wrapper around local CHC
pub struct ChcLocal {
    inner: parking_lot::Mutex<ChcLocalInner>,
    keystore: MetaLairClient,
    agent: AgentPubKey,
}

impl ChcLocal {
    /// Constructor
    pub fn new(keystore: MetaLairClient, agent: AgentPubKey) -> Self {
        Self {
            inner: parking_lot::Mutex::new(Default::default()),
            keystore,
            agent,
        }
    }
}

/// A local Rust implementation of a CHC, for testing purposes only.
#[derive(Default)]
pub struct ChcLocalInner {
    records: Vec<RecordItem>,
}

#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
struct RecordItem {
    /// The action
    action: SignedActionHashed,

    /// The entry, encrypted (TODO: by which key?), with the signature
    /// of the encrypted bytes
    pub encrypted_entry: Option<(Arc<EncryptedEntry>, Signature)>,
}

#[async_trait::async_trait]
impl ChainHeadCoordinator for ChcLocal {
    type Item = SignedActionHashed;

    async fn add_records_request(&self, request: AddRecordsRequest) -> ChcResult<()> {
        let mut m = self.inner.lock();
        let head = m
            .records
            .last()
            .map(|r| (r.action.get_hash().clone(), r.action.seq()));
        let records: Vec<_> = request
            .into_iter()
            .map(|r| {
                let signed_action: SignedActionHashed =
                    holochain_serialized_bytes::decode(&r.signed_action_msgpack).unwrap();

                RecordItem {
                    action: signed_action,
                    encrypted_entry: r.encrypted_entry,
                }
            })
            .collect();
        let actions = records.iter().map(|r| &r.action);
        validate_chain(actions, &head).map_err(|_| {
            let (hash, seq) = head.unwrap();
            ChcError::InvalidChain(seq, hash)
        })?;
        m.records.extend(records);
        Ok(())
    }

    async fn get_record_data_request(
        &self,
        request: GetRecordsRequest,
    ) -> ChcResult<Vec<(SignedActionHashed, Option<(Arc<EncryptedEntry>, Signature)>)>> {
        let m = self.inner.lock();
        let records = if let Some(hash) = request.payload.since_hash.as_ref() {
            m.records
                .iter()
                .skip_while(|r| hash != r.action.get_hash())
                .skip(1)
                .cloned()
                .collect()
        } else {
            m.records.clone()
        };
        Ok(records
            .into_iter()
            .map(
                |RecordItem {
                     action,
                     encrypted_entry,
                 }| (action, encrypted_entry),
            )
            .collect())
    }
}

impl ChainHeadCoordinatorExt for ChcLocal {
    fn signing_info(&self) -> (MetaLairClient, AgentPubKey) {
        (self.keystore.clone(), self.agent.clone())
    }
}

#[cfg(test)]
mod tests {

    use super::*;
    use holochain_types::test_utils::valid_arbitrary_chain;
    use pretty_assertions::assert_eq;
    use ChainHeadCoordinatorExt;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_add_records_local() {
        let keystore = holochain_keystore::test_keystore();
        let agent = fake_agent_pubkey_1();
        let chc = Arc::new(ChcLocal::new(keystore.clone(), agent.clone()));

        assert_eq!(chc.clone().head().await.unwrap(), None);

        let chain = valid_arbitrary_chain(&keystore, agent, 20).await;
        let hash = |i: usize| chain[i].action_address().clone();

        let t0 = &chain[0..3];
        let t1 = &chain[3..6];
        let t2 = &chain[6..9];
        let t11 = &chain[11..=11];

        chc.clone().add_records(t0.to_vec()).await.unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(2));
        chc.clone().add_records(t1.to_vec()).await.unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(5));

        // last_hash doesn't match
        assert!(chc.clone().add_records(t0.to_vec()).await.is_err());
        assert!(chc.clone().add_records(t1.to_vec()).await.is_err());
        assert!(chc.clone().add_records(t11.to_vec()).await.is_err());
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(5));

        chc.clone().add_records(t2.to_vec()).await.unwrap();
        assert_eq!(chc.clone().head().await.unwrap().unwrap(), hash(8));

        assert_eq!(
            chc.clone().get_record_data(None).await.unwrap(),
            &chain[0..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(0))).await.unwrap(),
            &chain[1..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(3))).await.unwrap(),
            &chain[4..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(7))).await.unwrap(),
            &chain[8..9]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(8))).await.unwrap(),
            &[]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(9))).await.unwrap(),
            &[]
        );
        assert_eq!(
            chc.clone().get_record_data(Some(hash(13))).await.unwrap(),
            &[]
        );
    }
}



================================================
File: crates/holochain_chc/src/lib.rs
================================================
//! Defines the Chain Head Coordination API and an HTTP client for talking to a remote CHC server.
//!
//! A Chain Head Coordinator (CHC) is an external service which Holochain can communicate with.
//! A CHC is used in situations where it is desirable to run the same Holochain Cell (source chain)
//! on multiple different conductors. The CHC ensures that these multiple devices don't inadvertently
//! create a fork of the source chain by doing simultaneous uncoordinated writes.
//!
//! This crate introduces a [`ChainHeadCoordinator`] trait which defines the interface that Holochain
//! weaves into its logic to make use of a CHC service. Currently, the only Holochain-supported implementation
//! of this trait is [`ChcHttp`][chc_http::ChcHttp], which makes HTTP requests to a remote server and returns the responses.
//! There is also a [`ChcLocal`][chc_local::ChcLocal] reference implementation which is used for testing.
//! Other implementations can be written as needed, including alternate implementations of a HTTP client,
//! or also other kinds of clients using other protocols.
//!
//! Holochain specifies an optional `chc_url` field in its configuration which can point to an HTTP
//! server that implements the CHC interface.
//! See the [`chc_http`] module docs for specs on how to set up a remote CHC HTTP server that
//! Holochain can talk to using the provided [chc_http::ChcHttp] implementation.
//!
//! The CHC trait contains two methods, and Holochain actually only uses one of them:
//! Every time Holochain is about to commit some records to a source chain, it first calls
//! [add_records_request][`ChainHeadCoordinator::add_records_request`]
//! to request that the CHC store those new records.
//! The CHC must be written so that it recognizes that the new records being added are a valid
//! extension of the existing chain of stored records, i.e. that the first record being pushed has a
//! [`Action::prev_action`] field which matches the hash of the last record already stored,
//! and additionally that the new records also form a valid hash chain (via `prev_action`).
//! If the new records are valid, they are added to CHC's chain, and an Ok result is returned.
//!
//! If these criteria for valid new records are not met, the CHC must return [`ChcError::InvalidChain`],
//! which lets the client know that the local chain is out of sync with CHC's version,
//! due to some other conductor having updated its own chain. In this case, the user (the driver
//! of the conductor) should call the other CHC method,
//! [get_record_data_request][`ChainHeadCoordinator::get_record_data_request`],
//! which will return the diff of records that CHC has that the local conductor does not.
//! Then, the Holochain admin method `GraftRecords` can be called to "stitch" these records onto the
//! existing chain, removing any fork if necessary.
//!
//! Note that currently, [get_record_data_request][`ChainHeadCoordinator::get_record_data_request`]
//! is never called directly by Holochain, but it might be in the future.
//!
//! Note also that when a CHC is used, the CHC is always considered the authoritative source of truth.
//! If a local conductor's state for whatever reason contradicts the CHC in any way, whether the local
//! chain contains different data altogether or is strictly ahead of the CHC, the CHC's version is always
//! considered correct and the local chain should always be modified to match the CHC's state.
//!

use std::{collections::HashMap, fmt::Debug, sync::Arc};

use futures::FutureExt;
use holochain_keystore::{AgentPubKeyExt, MetaLairClient};
use holochain_nonce::Nonce256Bits;
use holochain_serialized_bytes::SerializedBytesError;
use holochain_types::prelude::*;
use must_future::MustBoxFuture;

use holochain_types::chain::ChainItem;

pub mod chc_local;

#[cfg(feature = "http")]
pub mod chc_http;

/// The API which a Chain Head Coordinator service must implement.
#[async_trait::async_trait]
pub trait ChainHeadCoordinator {
    /// The item which the chain is made of.
    type Item: ChainItem;

    /// Request that the CHC append these records to its chain.
    ///
    /// Whenever Holochain is about to commit something, this function will first be called.
    /// The CHC will do some integrity checks, which may fail.
    /// All signatures and hashes need to line up properly.
    /// If the records added would result in a fork, then a [`ChcError::InvalidChain`] will be returned
    /// along with the current chain top.
    // If there is an out-of-sync error, it will return a hash, designating the point of fork.
    async fn add_records_request(&self, request: AddRecordsRequest) -> ChcResult<()>;

    /// Get actions after (not including) the given hash.
    async fn get_record_data_request(
        &self,
        request: GetRecordsRequest,
    ) -> ChcResult<Vec<(SignedActionHashed, Option<(Arc<EncryptedEntry>, Signature)>)>>;
}

/// Add some convenience methods to the CHC trait
pub trait ChainHeadCoordinatorExt:
    'static + Send + Sync + ChainHeadCoordinator<Item = SignedActionHashed>
{
    /// Get info necessary for signing
    fn signing_info(&self) -> (MetaLairClient, AgentPubKey);

    /// More convenient way to call `add_records_request`
    fn add_records(self: Arc<Self>, records: Vec<Record>) -> MustBoxFuture<'static, ChcResult<()>> {
        let (keystore, agent) = self.signing_info();
        async move {
            let payload = AddRecordPayload::from_records(keystore, agent, records).await?;
            self.add_records_request(payload).await
        }
        .boxed()
        .into()
    }

    /// More convenient way to call the low-level CHC API method `get_record_data_request`.
    /// This method actually decodes and assembles those results into a list of `Record`s.
    fn get_record_data(
        self: Arc<Self>,
        since_hash: Option<ActionHash>,
    ) -> MustBoxFuture<'static, ChcResult<Vec<Record>>> {
        let (keystore, agent) = self.signing_info();
        async move {
            let mut bytes = [0; 32];
            getrandom::getrandom(&mut bytes).map_err(|e| ChcError::Other(e.to_string()))?;
            let nonce = Nonce256Bits::from(bytes);
            let payload = GetRecordsPayload { since_hash, nonce };
            let signature = agent.sign(&keystore, &payload).await?;
            self.get_record_data_request(GetRecordsRequest { payload, signature })
                .await?
                .into_iter()
                .map(|(a, me)| {
                    Ok(Record::new(
                        a,
                        me.map(|(e, _s)| holochain_serialized_bytes::decode(&e.0))
                            .transpose()?,
                    ))
                })
                .collect()
        }
        .boxed()
        .into()
    }

    /// Just a convenience for testing. Should not be used otherwise.
    #[cfg(feature = "test_utils")]
    fn head(self: Arc<Self>) -> MustBoxFuture<'static, ChcResult<Option<ActionHash>>> {
        async move {
            Ok(self
                .get_record_data(None)
                .await?
                .pop()
                .map(|r| r.action_address().clone()))
        }
        .boxed()
        .into()
    }
}

/// A CHC implementation
pub type ChcImpl = Arc<dyn 'static + Send + Sync + ChainHeadCoordinatorExt>;

/// A Record to be added to the CHC.
///
/// The SignedActionHashed is constructed as usual.
/// The Entry data is encrypted (TODO: by which key?), and the encrypted data
/// is signed by the agent. This ensures that only the correct agent is adding
/// records to its CHC. This EncryptedEntry signature is not used anywhere
/// outside the context of the CHC.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct AddRecordPayload {
    /// The msgpack-encoded SignedActionHashed for the Record. This is encoded as such because the CHC
    /// needs to verify the signature, and these are the exact bytes which are signed, so
    /// this removes the need to deserialize and then re-serialize.
    ///
    /// This must be deserialized as `SignedActionHashed`.
    #[serde(with = "serde_bytes")]
    pub signed_action_msgpack: Vec<u8>,

    /// The signature of the SignedActionHashed
    /// (NOTE: usually signatures are of just the Action, but in this case we want to
    /// include the entire struct in the signature so we don't have to recalculate that on the CHC)
    pub signed_action_signature: Signature,

    /// The entry, encrypted (TODO: by which key?), with the signature of
    /// of the encrypted bytes
    pub encrypted_entry: Option<(Arc<EncryptedEntry>, Signature)>,
}

impl AddRecordPayload {
    /// Create a payload from a list of records.
    /// This performs the necessary signing and encryption the CHC requires.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(keystore, records)))]
    pub async fn from_records(
        keystore: MetaLairClient,
        agent_pubkey: AgentPubKey,
        records: Vec<Record>,
    ) -> ChcResult<Vec<Self>> {
        futures::future::join_all(records.into_iter().map(
            |Record {
                 signed_action,
                 entry,
             }| {
                let keystore = keystore.clone();
                let agent_pubkey = agent_pubkey.clone();

                async move {
                    let encrypted_entry_bytes = entry
                        .into_option()
                        .map(|entry| {
                            let entry = holochain_serialized_bytes::encode(&entry)?;
                            tracing::warn!(
                                "CHC is using unencrypted entry data. TODO: add encryption"
                            );

                            ChcResult::Ok(entry)
                        })
                        .transpose()?;
                    let encrypted_entry = if let Some(bytes) = encrypted_entry_bytes {
                        let signature = keystore
                            .sign(agent_pubkey.clone(), bytes.clone().into())
                            .await?;
                        Some((Arc::new(bytes.into()), signature))
                    } else {
                        None
                    };
                    let signed_action_msgpack = holochain_serialized_bytes::encode(&signed_action)?;
                    let author = signed_action.action().author();

                    let signed_action_signature = author
                        .sign_raw(&keystore, signed_action_msgpack.clone().into())
                        .await?;

                    assert!(author
                        .verify_signature_raw(
                            &signed_action_signature,
                            signed_action_msgpack.clone().into()
                        )
                        .await
                        .unwrap());
                    ChcResult::Ok(AddRecordPayload {
                        signed_action_msgpack,
                        signed_action_signature,
                        encrypted_entry,
                    })
                }
            },
        ))
        .await
        .into_iter()
        .collect::<Result<Vec<_>, _>>()
    }
}

/// The request type for `add_records`
pub type AddRecordsRequest = Vec<AddRecordPayload>;

/// The request to retrieve records from the CHC.
///
/// If a `since_hash` is specified, all records with sequence numbers at and
/// above the one at the given hash will be returned. If no `since_hash` is
/// given, then all records will be returned.
///
/// Since this payload is signed, including a unique nonce helps prevent replay
/// attacks.
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub struct GetRecordsPayload {
    /// Only records beyond and including this hash are returned
    pub since_hash: Option<ActionHash>,
    /// Randomly selected nonce to prevent replay attacks
    pub nonce: Nonce256Bits,
}

/// The full request for get_record_data
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub struct GetRecordsRequest {
    /// The payload
    pub payload: GetRecordsPayload,
    /// The signature of the payload
    pub signature: Signature,
}

/// Encrypted bytes of an Entry
#[derive(Debug, serde::Serialize, serde::Deserialize, derive_more::From)]
pub struct EncryptedEntry(#[serde(with = "serde_bytes")] pub Vec<u8>);

/// Assemble records from a list of Actions and a map of Entries
pub fn records_from_actions_and_entries(
    actions: Vec<SignedActionHashed>,
    mut entries: HashMap<EntryHash, Entry>,
) -> ChcResult<Vec<Record>> {
    let mut records = vec![];
    for action in actions {
        let entry = if let Some(hash) = action.hashed.entry_hash() {
            Some(
                entries
                    .remove(hash)
                    .ok_or_else(|| ChcError::MissingEntryForAction(action.as_hash().clone()))?,
            )
        } else {
            None
        };
        let record = Record::new(action, entry);
        records.push(record);
    }
    Ok(records)
}

#[allow(missing_docs)]
#[derive(Debug, thiserror::Error)]
pub enum ChcError {
    #[error(transparent)]
    SerializationError(#[from] SerializedBytesError),

    #[error(transparent)]
    JsonSerializationError(#[from] serde_json::Error),

    #[error(transparent)]
    LairError(#[from] one_err::OneErr),

    /// The out of sync error only happens when you attempt to add actions
    /// that would cause a fork with respect to the CHC. This can be remedied
    /// by syncing.
    #[error("Local chain is out of sync with the CHC. The CHC head has advanced beyond the first action provided in the `add_records` request. Try calling `get_record_data` from hash {1} (sequence #{0}).")]
    InvalidChain(u32, ActionHash),

    /// All other errors are due to an invalid request, which is a mistake
    /// that can't be remedied other than by fixing the programming mistake
    /// (which would be on the Holochain side)
    /// Examples include:
    /// - `Vec<AddRecordPayload>` must be sorted by `seq_number`
    /// - There is a gap between the first action and the current CHC head
    /// - The `Vec<AddRecordPayload>` does not constitute a valid chain (prev_action must be correct)
    #[error("Invalid `add_records` payload. Seq number: {0}")]
    NoRecordsAdded(u32),

    /// An Action which has an entry was returned without the Entry
    #[error("Missing Entry for ActionHash: {0}")]
    MissingEntryForAction(ActionHash),

    #[error("The CHC service is unreachable: {0}")]
    ServiceUnreachable(String),

    /// Unexpected error
    #[error("Unexpected error: {0}")]
    Other(String),
}

#[allow(missing_docs)]
pub type ChcResult<T> = Result<T, ChcError>;



================================================
File: crates/holochain_conductor_api/README.md
================================================
# holochain_conductor_api

License: Apache-2.0



================================================
File: crates/holochain_conductor_api/Cargo.toml
================================================
[package]
name = "holochain_conductor_api"
version = "0.5.0-dev.21"
description = "Message types for Holochain admin and app interface protocols"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_conductor_api"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
cfg-if = "1.0"
derive_more = "0.99"
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../kitsune_p2p/types" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../kitsune_p2p/bin_data" }
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash", features = [
  "full",
] }
holochain_state_types = { version = "^0.5.0-dev.12", path = "../holochain_state_types" }
holochain_serialized_bytes = "=0.0.55"
holochain_types = { version = "^0.5.0-dev.21", path = "../holochain_types" }
holochain_zome_types = { version = "^0.5.0-dev.17", path = "../holochain_zome_types" }
holochain_util = { version = "^0.5.0-dev.1", default-features = false, path = "../holochain_util", features = [
  "jsonschema",
] }
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
tracing = "0.1.26"
thiserror = "1.0.22"
url2 = "0.0.6"
holochain_keystore = { version = "^0.5.0-dev.20", path = "../holochain_keystore" }
shrinkwraprs = "0.3.0"
indexmap = { version = "2.6.0", features = ["serde"] }
schemars = "0.8.21"

[dev-dependencies]
serde_json = "1.0"
rmp-serde = "1.3"
matches = { version = "0.1.8" }
holochain_trace = { version = "^0.5.0-dev.1", path = "../holochain_trace" }
kitsune_p2p = { version = "^0.5.0-dev.13", path = "../kitsune_p2p/kitsune_p2p" }
pretty_assertions = "1.4"


[lints]
workspace = true

[features]
chc = []
unstable-dpki = []
unstable-sharding = ["kitsune_p2p_types/unstable-sharding"]
unstable-countersigning = []
sqlite-encrypted = [
  "holo_hash/sqlite-encrypted",
  "holochain_types/sqlite-encrypted",
  "holochain_zome_types/sqlite-encrypted",
  "holochain_keystore/sqlite-encrypted",
]
sqlite = [
  "holo_hash/sqlite",
  "holochain_types/sqlite",
  "holochain_zome_types/sqlite",
  "holochain_keystore/sqlite",
]



================================================
File: crates/holochain_conductor_api/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

- Remove unused public type `ScottyPanel`.

## 0.5.0-dev.21

## 0.5.0-dev.20

## 0.5.0-dev.19

## 0.5.0-dev.18

## 0.5.0-dev.17

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

- AppInfo field `cell_info` is now an `IndexMap` to ensure consistent ordering.

## 0.5.0-dev.2

## 0.5.0-dev.1

- AppInfo now includes a field `installed_at` with the timestamp that the app was installed

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.28

## 0.4.0-dev.27

## 0.4.0-dev.26

## 0.4.0-dev.25

## 0.4.0-dev.24

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

- BREAKING: In the `InstallApp` request, the `agent_key` is now optional. When not specified, an agent key will be provided. This is especially applicable when using DPKI, which requires that all keys are generated rather than externally provided.

## 0.4.0-dev.18

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

- *BREAKING* Updates holochain to use the new SBD server architecture for WebRTC signaling. If you were previously running your own tx5-signal-srv server for signaling, you will need to switch to [sbd-server](https://crates.io/crates/sbd-server). If you were using the holo-provided `wss://signal.holo.host`, you must switch to `wss://sbd-0.main.infra.holo.host`. See the module level documentation at [/crates/holochain\_conductor\_api/src/config/conductor.rs](/crates/holochain_conductor_api/src/config/conductor.rs) for a commented example holochain conductor configuration file. [\#3842](https://github.com/holochain/holochain/pull/3842)

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.47

## 0.3.0-beta-dev.46

## 0.3.0-beta-dev.45

## 0.3.0-beta-dev.44

## 0.3.0-beta-dev.43

## 0.3.0-beta-dev.42

## 0.3.0-beta-dev.41

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

- Added `DumpConductorState` admin method

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

- Test: Add tests to App and Admin API to prevent unnoticed changes in serialization from breaking these interfaces.

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

Adds `ignore_genesis_failure` field to InstallApp arguments. The default is `false`, and can only use this with the CHC feature. [2612](https://github.com/holochain/holochain/pull/2612)

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

- Add links to concepts documentation to the conductor API module.

## 0.2.0

## 0.2.0-beta-rc.7

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

- `StorageBlob` is an enum that serialized to camel case named variants. Renames all variants to snake case now.

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

- Adds new functionality to the conductor admin API which returns disk storage information. The storage used by apps is broken down into blobs which are being used by one or more app.

## 0.2.0-beta-rc.2

- `AppInfo` now includes a copy of the `AppManifest` which was used to install the app. This can be used to reinstall the same app under a different agent in the same conductor without needing to supply the original DNA files. [\#2157](https://github.com/holochain/holochain/pull/2157)

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

- Reject creation of duplicate clone cells. It was possible to create a clone cell with a DNA hash identical to an already existing DNA. [\#1997](https://github.com/holochain/holochain/pull/1997)
- Adds doc comments for `StemCell`, `ProvisionedCell` and `CloneCell` structs
- Various methods may return a `CellMissing` error if an operation is performed on a disabled cell. Now such calls will return `CellDisabled` to differentiate between a truly missing cell and one thats just disabled. [\#2092](https://github.com/holochain/holochain/pull/2092)
- Enabling a clone cell thats already enabled or disabling a clone cell thats already disabled would previously return a `CloneCellNotFound` error. Now, in those cases, nothing happens and a successful result is returned. [\#2093](https://github.com/holochain/holochain/pull/2093)
- Extend `NetworkInfo` call with several data points related to peer network size and activity. [\#2183](https://github.com/holochain/holochain/pull/2183)

## 0.1.0

## 0.1.0-beta-rc.4

- **BREAKING CHANGE**: `CreateCloneCell` returns `ClonedCell` instead of `InstalledCell`.
- **BREAKING CHANGE**: `EnableCloneCell` returns `ClonedCell` instead of `InstalledCell`.
- **BREAKING CHANGE**: Remove unused call `AdminRequest::StartApp`.
- **BREAKING CHANGE**: `Cell` is split up into `ProvisionedCell` and `ClonedCell`.
- **BREAKING CHANGE**: `CellInfo` variants are renamed to snake case during serde.
- Return additional field `agent_pub_key` in `AppInfo`.

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

- Fix error while installing app and return app info of newly installed app. [\#1725](https://github.com/holochain/holochain/pull/1725)

## 0.1.0-beta-rc.0

- **BREAKING CHANGE**: Remove deprecated Admin and App API calls.

- **BREAKING CHANGE**: Remove call `InstallApp`.

- **BREAKING CHANGE**: Rename `InstallAppBundle` to `InstallApp`.

- **BREAKING CHANGE**: Rename `ZomeCall` to `CallZome`. [\#1707](https://github.com/holochain/holochain/pull/1707)

- **BREAKING CHANGE**: Rename ArchiveCloneCell to DisableCloneCell.

- **BREAKING CHANGE**: Rename RestoreArchivedCloneCell to EnableCloneCell.

- **BREAKING CHANGE**: Move EnableCloneCell to App API.

- **BREAKING CHANGE**: Refactor DeleteCloneCell to delete a single disabled clone cell. [\#1704](https://github.com/holochain/holochain/pull/1704)

- **BREAKING CHANGE**: Refactor `AppInfo` to return all cells and DNA modifiers.

- **BREAKING CHANGE**: Rename `RequestAgentInfo` to `AgentInfo`. [\#1719](https://github.com/holochain/holochain/pull/1719)

## 0.0.72

## 0.0.71

## 0.0.70

## 0.0.69

## 0.0.68

## 0.0.67

## 0.0.66

## 0.0.65

## 0.0.64

## 0.0.63

## 0.0.62

## 0.0.61

## 0.0.60

## 0.0.59

- Include cloned cells in App API call `AppInfo`. [\#1547](https://github.com/holochain/holochain/pull/1547)
- **BREAKING CHANGE:** The `AddRecords` admin api method has been changed to `GraftRecords`, and the functionality has changed accordingly. See the docs for that method to understand the changes.
  - In short, the `truncate` parameter has been removed. If you desire that functionality, simply pass a fully valid chain in for grafting, which will have the effect of removing all existing records. If you just want to append records to the existing chain, just pass in a collection of new records, with the first one pointing to the last existing record.

## 0.0.58

## 0.0.57

## 0.0.56

## 0.0.55

## 0.0.54

## 0.0.53

## 0.0.52

## 0.0.51

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

## 0.0.43

## 0.0.42

## 0.0.41

- Docs: Unify and clean up docs for admin and app interface and conductor config. [\#1391](https://github.com/holochain/holochain/pull/1391)

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

- Docs: Fix intra-doc links in crates `holochain_conductor_api` and `holochain_state` [\#1323](https://github.com/holochain/holochain/pull/1323)

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

- Adds the ability to manually insert elements into a source chain using the `AdminRequest::AddElements` command. Please check the docs and PR for more details / warnings on proper usage. [\#1166](https://github.com/holochain/holochain/pull/1166)

## 0.0.21

## 0.0.20

## 0.0.19

## 0.0.18

## 0.0.17

- **BREAKING CHANGES**: db\_sync\_level changes to db\_sync\_strategy. Options are now `Fast` and `Resilient`. Default is `Fast` and should be the standard choice for most use cases. [\#1130](https://github.com/holochain/holochain/pull/1130)

## 0.0.16

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

- BREAKING: CONDUCTOR CONFIG CHANGErelated to update to lair 0.0.3
  - `passphrase_service` is now required
    - The only implemented option is `danger_insecure_from_config`

#### Example

``` yaml
passphrase_service:
  type: danger_insecure_from_config
  passphrase: "foobar"
```

## 0.0.2

## 0.0.1



================================================
File: crates/holochain_conductor_api/src/admin_interface.rs
================================================
use std::collections::BTreeSet;

use holo_hash::*;
use holochain_types::prelude::*;
use holochain_types::websocket::AllowedOrigins;
use holochain_zome_types::cell::CellId;
use kitsune_p2p_types::agent_info::AgentInfoSigned;

use crate::{AppInfo, FullStateDump, RevokeAgentKeyPayload, StorageInfo};

/// Represents the available conductor functions to call over an admin interface.
///
/// Enum variants follow a general convention of `verb_noun` as opposed to
/// the `noun_verb` of responses.
///
/// # Errors
///
/// Returns an [`AdminResponse::Error`] with a reason why the request failed.
// Expects a serialized object with any contents of the enum on a key `data`
// and the enum variant on a key `type`, e.g.
// `{ type: 'enable_app', data: { installed_app_id: 'test_app' } }`
#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum AdminRequest {
    /// Set up and register one or more new admin interfaces
    /// as specified by a list of configurations.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AdminInterfacesAdded`]
    AddAdminInterfaces(Vec<crate::config::AdminInterfaceConfig>),

    /// Register a DNA for later app installation.
    ///
    /// Stores the given DNA into the Holochain DNA database and returns the hash of it.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::DnaRegistered`]
    RegisterDna(Box<RegisterDnaPayload>),

    /// Get the definition of a DNA.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::DnaDefinitionReturned`]
    GetDnaDefinition(Box<DnaHash>),

    /// Update coordinator zomes for an already installed DNA.
    ///
    /// Replaces any installed coordinator zomes with the same zome name.
    /// If the zome name doesn't exist then the coordinator zome is appended
    /// to the current list of coordinator zomes.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::CoordinatorsUpdated`]
    UpdateCoordinators(Box<UpdateCoordinatorsPayload>),

    /// Install an app using an [`AppBundle`].
    ///
    /// Triggers genesis to be run on all Cells and to be stored.
    /// An app is intended for use by
    /// one and only one Agent and for that reason it takes an `AgentPubKey` and
    /// installs all the DNAs with that `AgentPubKey`, forming new cells.
    /// See [`InstallAppPayload`] for full details on the configuration.
    ///
    /// Note that the new app will not be enabled automatically after installation
    /// and can be enabled by calling [`EnableApp`].
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppInstalled`]
    ///
    /// [`EnableApp`]: AdminRequest::EnableApp
    InstallApp(Box<InstallAppPayload>),

    /// Uninstalls the app specified by argument `installed_app_id` from the conductor.
    ///
    /// The app will be removed from the list of installed apps, and any cells
    /// which were referenced only by this app will be disabled and removed, clearing up
    /// any persisted data.
    /// Cells which are still referenced by other installed apps will not be removed.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppUninstalled`]
    UninstallApp {
        /// The app ID to uninstall
        installed_app_id: InstalledAppId,

        /// If anything would prevent this app from being uninstalled, such as the existence
        /// of protected dependency to one of its cells, this flag will force the uninstallation.
        /// This will generally lead to bad outcomes, and should not be used unless you're
        /// aware of the consequences.
        #[serde(default)]
        force: bool,
    },

    /// List the hashes of all installed DNAs.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::DnasListed`]
    ListDnas,

    /// Generate a new [`AgentPubKey`].
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AgentPubKeyGenerated`]
    GenerateAgentPubKey,

    /// Revoke an agent key for an app.
    ///
    /// When an agent key is revoked, it becomes invalid and can no longer be used to author
    /// actions for that app. The key is revoked in the Deepkey service if installed and deleted on
    /// the source chains of all cells of the app, making them read-only. Cloning a cell of this app will fail.
    ///
    /// If the key could not be deleted from all cells, this call can be re-attempted to delete the key from the remaining cells.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AgentKeyRevoked`]
    RevokeAgentKey(Box<RevokeAgentKeyPayload>),

    /// List the IDs of all live cells currently running in the conductor.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::CellIdsListed`]
    ListCellIds,

    /// List the apps and their information that are installed in the conductor.
    ///
    /// Results are sorted by the `installed_at` timestamp of the app, in descending order.
    ///
    /// If `status_filter` is `Some(_)`, it will return only the apps with the specified status.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppsListed`]
    ListApps {
        /// An optional status to filter the list of apps by
        status_filter: Option<AppStatusFilter>,
    },

    /// Changes the specified app from a disabled to an enabled state in the conductor.
    ///
    /// It is likely to want to call this after calling [`AdminRequest::InstallApp`], since a freshly
    /// installed app is not enabled automatically. Once the app is enabled,
    /// zomes can be immediately called and it will also be loaded and enabled automatically on any reboot of the conductor.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppEnabled`]
    EnableApp {
        /// The app ID to enable
        installed_app_id: InstalledAppId,
    },

    /// Changes the specified app from an enabled to a disabled state in the conductor.
    ///
    /// When an app is disabled, zome calls can no longer be made, and the app will not be
    /// loaded on a reboot of the conductor.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppDisabled`]
    DisableApp {
        /// The app ID to disable
        installed_app_id: InstalledAppId,
    },

    /// Open up a new websocket for processing [`AppRequest`]s. Any active app will be
    /// callable via the attached app interface.
    ///
    /// **NB:** App interfaces are persisted when shutting down the conductor and are
    /// restored when restarting the conductor. Unused app interfaces are _not_ cleaned
    /// up. It is therefore recommended to reuse existing interfaces. They can be queried
    /// with the call [`AdminRequest::ListAppInterfaces`].
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppInterfaceAttached`]
    ///
    /// # Arguments
    ///
    /// Optionally a `port` parameter can be passed to this request. If it is `None`,
    /// a free port is chosen by the conductor.
    ///
    /// An `allowed_origins` parameter to control which origins are allowed to connect
    /// to the app interface.
    ///
    /// [`AppRequest`]: super::AppRequest
    AttachAppInterface {
        /// Optional port number
        port: Option<u16>,

        /// Allowed origins for this app interface.
        ///
        /// This should be one of:
        /// - A comma separated list of origins - `http://localhost:3000,http://localhost:3001`,
        /// - A single origin - `http://localhost:3000`,
        /// - Any origin - `*`
        ///
        /// Connections from any origin which is not permitted by this config will be rejected.
        allowed_origins: AllowedOrigins,

        /// Optionally bind this app interface to a specific installed app.
        ///
        /// If this is `None` then the interface can be used to establish a connection for any app.
        ///
        /// If this is `Some` then the interface will only accept connections for the specified app.
        /// Those connections will only be able to make calls to and receive signals from that app.
        installed_app_id: Option<InstalledAppId>,
    },

    /// List all the app interfaces currently attached with [`AttachAppInterface`].
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppInterfacesListed`], a list of websocket ports that can
    /// process [`AppRequest`]s.
    ///
    /// [`AttachAppInterface`]: AdminRequest::AttachAppInterface
    /// [`AppRequest`]: super::AppRequest
    ListAppInterfaces,

    /// Dump the state of the cell specified by argument `cell_id`,
    /// including its chain, as a string containing JSON.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::StateDumped`]
    DumpState {
        /// The cell ID for which to dump state
        cell_id: Box<CellId>,
    },

    /// Dump the state of the conductor, including the in-memory representation
    /// and the persisted ConductorState, as JSON.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::ConductorStateDumped`]
    DumpConductorState,

    /// Dump the full state of the Cell specified by argument `cell_id`,
    /// including its chain and DHT shard, as a string containing JSON.
    ///
    /// **Warning**: this API call is subject to change, and will not be available to hApps.
    /// This is meant to be used by introspection tooling.
    ///
    /// Note that the response to this call can be very big, as it's requesting for
    /// the full database of the cell.
    ///
    /// Also note that while DHT ops about private entries will be returned (like `StoreRecord`),
    /// the entry in itself will be missing, as it's not actually stored publicly in the DHT shard.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::FullStateDumped`]
    DumpFullState {
        /// The cell ID for which to dump the state
        cell_id: Box<CellId>,
        /// The last seen DhtOp RowId, returned in the full dump state.
        /// Only DhtOps with RowId greater than the cursor will be returned.
        dht_ops_cursor: Option<u64>,
    },

    /// Dump the network metrics tracked by kitsune.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::NetworkMetricsDumped`]
    DumpNetworkMetrics {
        /// If set, limits the metrics dumped to a single DNA hash space.
        dna_hash: Option<DnaHash>,
    },

    /// Dump raw json network statistics from the backend networking lib.
    DumpNetworkStats,

    /// Add a list of agents to this conductor's peer store.
    ///
    /// This is a way of shortcutting peer discovery and is useful for testing.
    ///
    /// It is also helpful if you know other
    /// agents on the network and they can send you
    /// their agent info.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AgentInfoAdded`]
    AddAgentInfo {
        /// list of signed agent info to add to peer store
        agent_infos: Vec<AgentInfoSigned>,
    },

    /// Request the [`AgentInfoSigned`] stored in this conductor's
    /// peer store.
    ///
    /// You can:
    /// - Get all agent info by leaving `cell_id` to `None`.
    /// - Get a specific agent info by setting the `cell_id`.
    ///
    /// This is how you can send your agent info to another agent.
    /// It is also useful for testing across networks.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AgentInfo`]
    AgentInfo {
        /// Optionally choose the agent info of a specific cell.
        cell_id: Option<CellId>,
    },

    /// "Graft" [`Record`]s onto the source chain of the specified [`CellId`].
    ///
    /// The records must form a valid chain segment (ascending sequence numbers,
    /// and valid `prev_action` references). If the first record contains a `prev_action`
    /// which matches the existing records, then the new records will be "grafted" onto
    /// the existing chain at that point, and any other records following that point which do
    /// not match the new records will be removed.
    ///
    /// If this operation is called when there are no forks, the final state will also have
    /// no forks.
    ///
    /// **BEWARE** that this may result in the deletion of data! Any existing records which form
    /// a fork with respect to the new records will be deleted.
    ///
    /// All records must be authored and signed by the same agent.
    /// The [`DnaFile`] (but not necessarily the cell) must already be installed
    /// on this conductor.
    ///
    /// Care is needed when using this command as it can result in
    /// an invalid chain.
    /// Additionally, if conflicting source chain records are
    /// inserted on different nodes, then the chain will be forked.
    ///
    /// If an invalid or forked chain is inserted
    /// and then pushed to the DHT, it can't be undone.
    ///
    /// Note that the cell does not need to exist to run this command.
    /// It is possible to insert records into a source chain before
    /// the cell is created. This can be used to restore from backup.
    ///
    /// If the cell is installed, it is best to call [`AdminRequest::DisableApp`]
    /// before running this command, as otherwise the chain head may move.
    /// If `truncate` is true, the chain head is not checked and any new
    /// records will be lost.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::RecordsGrafted`]
    GraftRecords {
        /// The cell that the records are being inserted into.
        cell_id: CellId,
        /// If this is `true`, then the records will be validated before insertion.
        /// This is much slower but is useful for verifying the chain is valid.
        ///
        /// If this is `false`, then records will be inserted as is.
        /// This could lead to an invalid chain.
        validate: bool,
        /// The records to be inserted into the source chain.
        records: Vec<Record>,
    },

    /// Request capability grant for making zome calls.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::ZomeCallCapabilityGranted`]
    GrantZomeCallCapability(Box<GrantZomeCallCapabilityPayload>),

    /// Request capability grant info for all cells in the app.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::CapabilityGrantsInfo`]
    ListCapabilityGrants {
        installed_app_id: String,
        include_revoked: bool,
    },

    /// Delete a clone cell that was previously disabled.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::CloneCellDeleted`]
    DeleteCloneCell(Box<DeleteCloneCellPayload>),

    /// Info about storage used by apps
    ///
    /// # Returns
    ///
    /// [`AdminResponse::StorageInfo`]
    StorageInfo,

    /// Connecting to an app over an app websocket requires an authentication token. This endpoint
    /// is used to issue those tokens for use by app clients.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppAuthenticationTokenIssued`]
    IssueAppAuthenticationToken(IssueAppAuthenticationTokenPayload),

    /// Revoke an issued app authentication token.
    ///
    /// # Returns
    ///
    /// [`AdminResponse::AppAuthenticationTokenRevoked`]
    RevokeAppAuthenticationToken(AppAuthenticationToken),

    /// Find installed cells which use a DNA that's forward-compatible with the given DNA hash.
    /// Namely, this finds cells with DNAs whose manifest lists the given DNA hash in its `lineage` field.
    GetCompatibleCells(DnaHash),
}

/// Represents the possible responses to an [`AdminRequest`]
/// and follows a general convention of `noun_verb` as opposed to
/// the `verb_noun` of `AdminRequest`.
///
/// Will serialize as an object with any contents of the enum on a key `data`
/// and the enum variant on a key `type`, e.g.
/// `{ type: 'app_interface_attached', data: { port: 4000 } }`
#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[cfg_attr(test, derive(Clone))]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum AdminResponse {
    /// Can occur in response to any [`AdminRequest`].
    ///
    /// There has been an error during the handling of the request.
    Error(ExternalApiWireError),

    /// The successful response to an [`AdminRequest::RegisterDna`]
    DnaRegistered(DnaHash),

    /// The successful response to an [`AdminRequest::GetDnaDefinition`]
    DnaDefinitionReturned(DnaDef),

    /// The successful response to an [`AdminRequest::UpdateCoordinators`]
    CoordinatorsUpdated,

    /// The successful response to an [`AdminRequest::InstallApp`].
    ///
    /// The resulting [`AppInfo`] contains the app ID,
    /// the [`RoleName`]s and, most usefully, [`CellInfo`](crate::CellInfo)s
    /// of the newly installed DNAs.
    AppInstalled(AppInfo),

    /// The successful response to an [`AdminRequest::UninstallApp`].
    ///
    /// It means the app was uninstalled successfully.
    AppUninstalled,

    /// The successful response to an [`AdminRequest::AddAdminInterfaces`].
    ///
    /// It means the `AdminInterface`s have successfully been added.
    AdminInterfacesAdded,

    /// The successful response to an [`AdminRequest::GenerateAgentPubKey`].
    ///
    /// Contains a new [`AgentPubKey`] generated by the keystore.
    AgentPubKeyGenerated(AgentPubKey),

    /// The successful response to an [`AdminRequest::RevokeAgentKey`].
    ///
    /// Contains a list of errors of the cells where deletion was unsuccessful.
    ///
    /// If the key could not be deleted from all cells, the call
    /// [`AdminRequest::RevokeAgentKey`] can be re-attempted to delete the key from the remaining cells.
    AgentKeyRevoked(Vec<(CellId, String)>),

    /// The successful response to an [`AdminRequest::ListDnas`].
    ///
    /// Contains a list of the hashes of all installed DNAs.
    DnasListed(Vec<DnaHash>),

    /// The successful response to an [`AdminRequest::ListCellIds`].
    ///
    /// Contains a list of all the cell IDs in the conductor.
    CellIdsListed(Vec<CellId>),

    /// The successful response to an [`AdminRequest::ListApps`].
    ///
    /// Contains a list of the `InstalledAppInfo` of the installed apps in the conductor.
    AppsListed(Vec<AppInfo>),

    /// The successful response to an [`AdminRequest::AttachAppInterface`].
    ///
    /// Contains the port number of the attached app interface.
    AppInterfaceAttached {
        /// Networking port of the new `AppInterfaceApi`
        port: u16,
    },

    /// The list of attached app interfaces.
    AppInterfacesListed(Vec<AppInterfaceInfo>),

    /// The successful response to an [`AdminRequest::EnableApp`].
    ///
    /// It means the app was enabled successfully. If it was possible to
    /// put the app in a running state, it will be running, otherwise it will
    /// be paused.
    ///
    /// Contains the app info and list of errors for cells that could not be enabled.
    AppEnabled {
        app: AppInfo,
        errors: Vec<(CellId, String)>,
    },

    /// The successful response to an [`AdminRequest::DisableApp`].
    ///
    /// It means the app was disabled successfully.
    AppDisabled,

    /// The successful response to an [`AdminRequest::DumpState`].
    ///
    /// The result contains a string of serialized JSON data which can be deserialized to access the
    /// full state dump and inspect the source chain.
    StateDumped(String),

    /// The successful response to an [`AdminRequest::DumpFullState`].
    ///
    /// The result contains a string of serialized JSON data which can be deserialized to access the
    /// full state dump and inspect the source chain.
    ///
    /// Note that this result can be very big, as it's requesting the full database of the cell.
    FullStateDumped(FullStateDump),

    /// The successful response to an [`AdminRequest::DumpConductorState`].
    ///
    /// Simply a JSON serialized snapshot of `Conductor` and `ConductorState` from the `holochain` crate.
    ConductorStateDumped(String),

    /// The successful result of a call to [`AdminRequest::DumpNetworkMetrics`].
    ///
    /// The string is a JSON blob of the metrics results.
    NetworkMetricsDumped(String),

    /// The successful result of a call to [`AdminRequest::DumpNetworkStats`].
    ///
    /// The string is a raw JSON blob returned directly from the backend
    /// networking library.
    NetworkStatsDumped(String),

    /// The successful response to an [`AdminRequest::AddAgentInfo`].
    ///
    /// This means the agent info was successfully added to the peer store.
    AgentInfoAdded,

    /// The successful response to an [`AdminRequest::AgentInfo`].
    ///
    /// This is all the agent info that was found for the request.
    AgentInfo(Vec<AgentInfoSigned>),

    /// The successful response to an [`AdminRequest::GraftRecords`].
    RecordsGrafted,

    /// The successful response to an [`AdminRequest::GrantZomeCallCapability`].
    ZomeCallCapabilityGranted,

    /// The successful response to an [`AdminRequest::ListCapabilityGrants`].
    CapabilityGrantsInfo(AppCapGrantInfo),

    /// The successful response to an [`AdminRequest::DeleteCloneCell`].
    CloneCellDeleted,

    /// The successful response to an [`AdminRequest::StorageInfo`].
    StorageInfo(StorageInfo),

    /// The successful response to an [`AdminRequest::IssueAppAuthenticationToken`].
    AppAuthenticationTokenIssued(AppAuthenticationTokenIssued),

    /// The successful response to an [`AdminRequest::RevokeAppAuthenticationToken`].
    AppAuthenticationTokenRevoked,

    /// The successful response to an [`AdminRequest::GetCompatibleCells`].
    CompatibleCells(CompatibleCells),
}

pub type CompatibleCells = BTreeSet<(InstalledAppId, BTreeSet<CellId>)>;

/// Error type that goes over the websocket wire.
/// This intends to be application developer facing
/// so it should be readable and relevant
#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes, Clone)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum ExternalApiWireError {
    // TODO: B-01506 Constrain these errors so they are relevant to
    // application developers and what they would need
    // to react to using code (i.e. not just print)
    /// Any internal error
    InternalError(String),
    /// The input to the API failed to deseralize.
    Deserialization(String),
    /// The DNA path provided was invalid.
    DnaReadError(String),
    /// There was an error in the ribosome.
    RibosomeError(String),
    /// Error activating app.
    ActivateApp(String),
    /// The zome call failed authentication.
    ///
    /// [How to sign zome calls.](crate::AppRequest::CallZome)
    ZomeCallAuthenticationFailed(String),
    /// The zome call is unauthorized.
    ZomeCallUnauthorized(String),
    /// A countersigning session has failed.
    CountersigningSessionError(String),
}

impl ExternalApiWireError {
    /// Convert the error from the display.
    pub fn internal<T: std::fmt::Display>(e: T) -> Self {
        // Display format is used because
        // this version intended for users.
        ExternalApiWireError::InternalError(e.to_string())
    }
}

#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes, Clone)]
#[serde(rename_all = "snake_case")]
/// Filter for [`AdminRequest::ListApps`].
///
/// App Status is a combination of two pieces of independent state:
/// - Enabled/Disabled, which is a designation set by the user via the conductor interface.
/// - Running/Stopped, which is a fact about the reality of the app in the course of its operation.
pub enum AppStatusFilter {
    /// Filter on apps which are Enabled, which can include both Running and Paused apps.
    Enabled,
    /// Filter only on apps which are Disabled.
    Disabled,
    /// Filter on apps which are currently Running (meaning they are also Enabled).
    Running,
    /// Filter on apps which are Stopped, i.e. not Running.
    /// This includes apps in the Disabled status, as well as the Paused status.
    Stopped,
    /// Filter only on Paused apps.
    Paused,
}

/// Informational response for listing app interfaces.
#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes, Clone)]
pub struct AppInterfaceInfo {
    /// The port that the app interface is listening on.
    pub port: u16,

    /// The allowed origins for this app interface.
    pub allowed_origins: AllowedOrigins,

    /// The optional association with a specific installed app.
    pub installed_app_id: Option<InstalledAppId>,
}

/// Request payload for [AdminRequest::IssueAppAuthenticationToken].
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub struct IssueAppAuthenticationTokenPayload {
    /// The app ID to issue a connection token for.
    pub installed_app_id: InstalledAppId,

    /// The number of seconds that the token should be valid for. After this number of seconds, the
    /// token will no longer be accepted by the conductor.
    ///
    /// This is 30s by default which is reasonably high but with [IssueAppAuthenticationTokenPayload::single_use]
    /// set to `true`, the token will be invalidated after the first use anyway.
    ///
    /// Set this to 0 to create a token that does not expire.
    #[serde(default = "default_expiry_seconds")]
    pub expiry_seconds: u64,

    /// Whether the token should be single-use. This is `true` by default and will cause the token
    /// to be invalidated after the first use, irrespective of connection success.
    ///
    /// Set this to `false` to allow the token to be used multiple times.
    #[serde(default = "default_single_use")]
    pub single_use: bool,
}

fn default_expiry_seconds() -> u64 {
    30
}

fn default_single_use() -> bool {
    true
}

impl IssueAppAuthenticationTokenPayload {
    /// Create a new payload for issuing a connection token for the specified app.
    ///
    /// The token will be valid for 30 seconds and for a single use.
    pub fn for_installed_app_id(installed_app_id: InstalledAppId) -> Self {
        installed_app_id.into()
    }

    /// Set the expiry time for the token.
    pub fn expiry_seconds(mut self, expiry_seconds: u64) -> Self {
        self.expiry_seconds = expiry_seconds;
        self
    }

    /// Set whether the token should be single-use.
    pub fn single_use(mut self, single_use: bool) -> Self {
        self.single_use = single_use;
        self
    }
}

impl From<InstalledAppId> for IssueAppAuthenticationTokenPayload {
    fn from(installed_app_id: InstalledAppId) -> Self {
        // Defaults here should match the serde defaults in the struct definition
        Self {
            installed_app_id,
            expiry_seconds: 30,
            single_use: true,
        }
    }
}

/// A token issued by the conductor that can be used to authenticate a connection to an app interface.
pub type AppAuthenticationToken = Vec<u8>;

/// Response payload for [AdminResponse::AppAuthenticationTokenIssued].
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct AppAuthenticationTokenIssued {
    /// A token issued by the conductor that can be used to authenticate a connection to an app interface.
    /// This is expected to be passed from the caller of the admin interface to the client that will
    /// use the app interface. It should be treated as secret and kept from other parties.
    pub token: AppAuthenticationToken,

    /// The timestamp after which Holochain will consider the token invalid. This should be
    /// considered informational only and the client should not rely on the token being accepted
    /// following a client-side check that the token has not yet expired.
    ///
    /// If the token was created with [IssueAppAuthenticationTokenPayload::expiry_seconds] set to `0`
    /// then this field will be `None`.
    // TODO Kitsune type used in the conductor interface
    pub expires_at: Option<Timestamp>,
}

#[cfg(test)]
mod tests {
    use serde::Deserialize;

    use crate::{AdminRequest, AdminResponse, ExternalApiWireError};

    #[test]
    fn admin_request_serialization() {
        use rmp_serde::Deserializer;

        // make sure requests are serialized as expected
        let request = AdminRequest::DisableApp {
            installed_app_id: "some_id".to_string(),
        };
        let serialized_request = holochain_serialized_bytes::encode(&request).unwrap();
        assert_eq!(
            serialized_request,
            vec![
                130, 164, 116, 121, 112, 101, 171, 100, 105, 115, 97, 98, 108, 101, 95, 97, 112,
                112, 165, 118, 97, 108, 117, 101, 129, 176, 105, 110, 115, 116, 97, 108, 108, 101,
                100, 95, 97, 112, 112, 95, 105, 100, 167, 115, 111, 109, 101, 95, 105, 100
            ]
        );

        let json_expected = r#"{"type":"disable_app","value":{"installed_app_id":"some_id"}}"#;
        let mut deserializer = Deserializer::new(&*serialized_request);
        let json_value: serde_json::Value = Deserialize::deserialize(&mut deserializer).unwrap();
        let json_actual = serde_json::to_string(&json_value).unwrap();

        assert_eq!(json_actual, json_expected);

        // make sure responses are serialized as expected
        let response = AdminResponse::Error(ExternalApiWireError::RibosomeError(
            "error_text".to_string(),
        ));
        let serialized_response = holochain_serialized_bytes::encode(&response).unwrap();
        assert_eq!(
            serialized_response,
            vec![
                130, 164, 116, 121, 112, 101, 165, 101, 114, 114, 111, 114, 165, 118, 97, 108, 117,
                101, 130, 164, 116, 121, 112, 101, 174, 114, 105, 98, 111, 115, 111, 109, 101, 95,
                101, 114, 114, 111, 114, 165, 118, 97, 108, 117, 101, 170, 101, 114, 114, 111, 114,
                95, 116, 101, 120, 116
            ]
        );

        let json_expected =
            r#"{"type":"error","value":{"type":"ribosome_error","value":"error_text"}}"#;
        let mut deserializer = Deserializer::new(&*serialized_response);
        let json_value: serde_json::Value = Deserialize::deserialize(&mut deserializer).unwrap();
        let json_actual = serde_json::to_string(&json_value).unwrap();

        assert_eq!(json_actual, json_expected);
    }
}



================================================
File: crates/holochain_conductor_api/src/app_interface.rs
================================================
use crate::{AppAuthenticationToken, ExternalApiWireError};
use holo_hash::AgentPubKey;
use holochain_keystore::LairResult;
use holochain_keystore::MetaLairClient;
use holochain_types::prelude::*;
use indexmap::IndexMap;
use kitsune_p2p_types::fetch_pool::FetchPoolInfo;

/// Represents the available conductor functions to call over an app interface
/// and will result in a corresponding [`AppResponse`] message being sent back over the
/// interface connection.
///
/// # Errors
///
/// Returns an [`AppResponse::Error`] with a reason why the request failed.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum AppRequest {
    /// Get info about the app that you are connected to, including info about each cell installed
    /// by this app.
    ///
    /// # Returns
    ///
    /// [`AppResponse::AppInfo`]
    AppInfo,

    /// Call a zome function.
    ///
    /// The payload to this call is composed of the serialized [`ZomeCallParams`] as bytes
    /// and the provenance's signature.
    ///
    /// Serialization must be performed with MessagePack. The resulting bytes are hashed using the
    /// SHA2 512-bit algorithm, and the hash is signed with the provenance's private ed25519 key.
    /// The hash is not included in the call's payload.
    ///
    /// # Returns
    ///
    /// [`AppResponse::ZomeCalled`] Indicates the zome call was deserialized successfully. If the
    /// call was authorized, the response yields the return value of the zome function as MessagePack
    /// encoded bytes. The bytes can be deserialized to the expected return type.
    ///
    /// This response is also returned when authorization of the zome call failed because of an
    /// invalid signature, capability grant or nonce.
    ///
    /// # Errors
    ///
    /// [`SerializedBytesError`] is returned when the serialized bytes could not be deserialized
    /// to the expected [`ZomeCallParams`].
    CallZome(Box<ZomeCallParamsSigned>),

    /// Get the state of a countersigning session.
    ///
    /// # Returns
    ///
    /// [`AppResponse::CountersigningSessionState`]
    ///
    /// # Errors
    ///
    /// [`CountersigningError::WorkspaceDoesNotExist`] likely indicates that an invalid cell id was
    /// passed in to the call.
    #[cfg(feature = "unstable-countersigning")]
    GetCountersigningSessionState(Box<CellId>),

    /// Abandon an unresolved countersigning session.
    ///
    /// If the current session has not been resolved automatically, it can be forcefully abandoned.
    /// A condition for this call to succeed is that at least one attempt has been made to resolve
    /// it automatically.
    ///
    /// # Returns
    ///
    /// [`AppResponse::CountersigningSessionAbandoned`]
    ///
    /// The session is marked for abandoning and the countersigning workflow was triggered. The session
    /// has not been abandoned yet.
    ///
    /// Upon successful abandoning the system signal [`SystemSignal::AbandonedCountersigning`] will
    /// be emitted and the session removed from state, so that [`AppRequest::GetCountersigningSessionState`]
    /// would return `None`.
    ///
    /// In the countersigning workflow it will first be attempted to resolve the session with incoming
    /// signatures of the countersigned entries, before force-abandoning the session. In a very rare event
    /// it could happen that in just the moment where the [`AppRequest::AbandonCountersigningSession`]
    /// is made, signatures for this session come in. If they are valid, the session will be resolved and
    /// published as usual. Should they be invalid, however, the flag to abandon the session is erased.
    /// In such cases this request can be retried until the session has been abandoned successfully.
    ///
    /// # Errors
    ///
    /// [`CountersigningError::WorkspaceDoesNotExist`] likely indicates that an invalid cell id was
    /// passed in to the call.
    ///
    /// [`CountersigningError::SessionNotFound`] when no ongoing session could be found for the provided
    /// cell id.
    ///
    /// [`CountersigningError::SessionNotUnresolved`] when an attempt to resolve the session
    /// automatically has not been made.
    #[cfg(feature = "unstable-countersigning")]
    AbandonCountersigningSession(Box<CellId>),

    /// Publish an unresolved countersigning session.
    ///
    /// If the current session has not been resolved automatically, it can be forcefully published.
    /// A condition for this call to succeed is that at least one attempt has been made to resolve
    /// it automatically.
    ///
    /// # Returns
    ///
    /// [`AppResponse::PublishCountersigningSessionTriggered`]
    ///
    /// The session is marked for publishing and the countersigning workflow was triggered. The session
    /// has not been published yet.
    ///
    /// Upon successful publishing the system signal [`SystemSignal::SuccessfulCountersigning`] will
    /// be emitted and the session removed from state, so that [`AppRequest::GetCountersigningSessionState`]
    /// would return `None`.
    ///
    /// In the countersigning workflow it will first be attempted to resolve the session with incoming
    /// signatures of the countersigned entries, before force-publishing the session. In a very rare event
    /// it could happen that in just the moment where the [`AppRequest::PublishCountersigningSession`]
    /// is made, signatures for this session come in. If they are valid, the session will be resolved and
    /// published as usual. Should they be invalid, however, the flag to publish the session is erased.
    /// In such cases this request can be retried until the session has been published successfully.
    ///
    /// # Errors
    ///
    /// [`CountersigningError::WorkspaceDoesNotExist`] likely indicates that an invalid cell id was
    /// passed in to the call.
    ///
    /// [`CountersigningError::SessionNotFound`] when no ongoing session could be found for the provided
    /// cell id.
    ///
    /// [`CountersigningError::SessionNotUnresolved`] when an attempt to resolve the session
    /// automatically has not been made.
    #[cfg(feature = "unstable-countersigning")]
    PublishCountersigningSession(Box<CellId>),

    /// Clone a DNA (in the biological sense), thus creating a new `Cell`.
    ///
    /// Using the provided, already-registered DNA, create a new DNA with a unique
    /// ID and the specified properties, create a new cell from this cloned DNA,
    /// and add the cell to the specified app.
    ///
    /// # Returns
    ///
    /// [`AppResponse::CloneCellCreated`]
    CreateCloneCell(Box<CreateCloneCellPayload>),

    /// Disable a clone cell.
    ///
    /// Providing a [`CloneId`] or [`CellId`], disable an existing clone cell.
    /// When the clone cell exists, it is disabled and can not be called any
    /// longer. If it doesn't exist, the call is a no-op.
    ///
    /// # Returns
    ///
    /// [`AppResponse::CloneCellDisabled`] if the clone cell existed
    /// and has been disabled.
    DisableCloneCell(Box<DisableCloneCellPayload>),

    /// Enable a clone cell that was previously disabled.
    ///
    /// # Returns
    ///
    /// [`AppResponse::CloneCellEnabled`]
    EnableCloneCell(Box<EnableCloneCellPayload>),

    /// Info about networking processes
    ///
    /// # Returns
    ///
    /// [`AppResponse::NetworkInfo`]
    NetworkInfo(Box<NetworkInfoRequestPayload>),

    /// List all host functions available to wasm on this conductor.
    ///
    /// # Returns
    ///
    /// [`AppResponse::ListWasmHostFunctions`]
    ListWasmHostFunctions,

    /// Provide the membrane proofs for this app, if this app was installed
    /// using `allow_deferred_memproofs` and memproofs were not provided at
    /// installation time.
    ///
    /// # Returns
    ///
    /// [`AppResponse::Ok`]
    ProvideMemproofs(MemproofMap),

    /// Enable the app, only in special circumstances.
    /// Can only be called while the app is in the `Disabled(NotStartedAfterProvidingMemproofs)` state.
    /// Cannot be used to enable the app if it's in any other state, or Disabled for any other reason.
    ///
    /// # Returns
    ///
    /// [`AppResponse::Ok`]
    EnableApp,
    //
    // TODO: implement after DPKI lands
    // /// Replace the agent key associated with this app with a new one.
    // /// The new key will be created using the same method which is used
    // /// when installing an app with no agent key provided.
    // ///
    // /// This method is only available if this app was installed using `allow_deferred_memproofs`,
    // /// and can only be called before [`AppRequest::ProvideMemproofs`] has been called.
    // /// Until then, it can be called as many times as needed.
    // ///
    // /// # Returns
    // ///
    // /// [`AppResponse::AppAgentKeyRotated`]
    // RotateAppAgentKey,
}

/// Represents the possible responses to an [`AppRequest`].
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum AppResponse {
    /// Can occur in response to any [`AppRequest`].
    ///
    /// There has been an error during the handling of the request.
    Error(ExternalApiWireError),

    /// The successful response to an [`AppRequest::AppInfo`].
    ///
    /// Option will be `None` if there is no installed app with the given `installed_app_id`.
    AppInfo(Option<AppInfo>),

    /// The successful response to an [`AppRequest::CallZome`].
    ///
    /// Note that [`ExternIO`] is simply a structure of [`struct@SerializedBytes`], so the client will have
    /// to decode this response back into the data provided by the zome using a [msgpack] library to utilize it.
    ///
    /// [msgpack]: https://msgpack.org/
    ZomeCalled(Box<ExternIO>),

    /// The successful response to an [`AppRequest::GetCountersigningSessionState`].
    #[cfg(feature = "unstable-countersigning")]
    CountersigningSessionState(Box<Option<CountersigningSessionState>>),

    /// The successful response to an [`AppRequest::AbandonCountersigningSession`].
    #[cfg(feature = "unstable-countersigning")]
    CountersigningSessionAbandoned,

    /// The successful response to an [`AppRequest::PublishCountersigningSession`].
    #[cfg(feature = "unstable-countersigning")]
    PublishCountersigningSessionTriggered,

    /// The successful response to an [`AppRequest::CreateCloneCell`].
    ///
    /// The response contains the created clone [`ClonedCell`].
    CloneCellCreated(ClonedCell),

    /// The successful response to an [`AppRequest::DisableCloneCell`].
    ///
    /// An existing clone cell has been disabled.
    CloneCellDisabled,

    /// The successful response to an [`AppRequest::EnableCloneCell`].
    ///
    /// A previously disabled clone cell has been enabled. The [`ClonedCell`]
    /// is returned.
    CloneCellEnabled(ClonedCell),

    /// NetworkInfo is returned
    NetworkInfo(Vec<NetworkInfo>),

    /// All the wasm host functions supported by this conductor.
    ListWasmHostFunctions(Vec<String>),

    /// The app agent key as been rotated, and the new key is returned.
    AppAgentKeyRotated(AgentPubKey),

    /// Operation successful, no payload.
    Ok,
}

/// The data provided over an app interface in order to make a zome call.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct ZomeCallParamsSigned {
    /// Bytes of the serialized zome call payload that consists of all fields of the
    /// [`ZomeCallParams`].
    pub bytes: ExternIO,
    /// Signature by the provenance of the call, signing the bytes of the zome call payload.
    pub signature: Signature,
}

impl ZomeCallParamsSigned {
    pub fn new(bytes: Vec<u8>, signature: Signature) -> Self {
        Self {
            bytes: ExternIO::from(bytes),
            signature,
        }
    }

    pub async fn try_from_params(
        keystore: &MetaLairClient,
        params: ZomeCallParams,
    ) -> LairResult<Self> {
        let (bytes, bytes_hash) = params.serialize_and_hash().map_err(|e| e.to_string())?;
        let signature = params
            .provenance
            .sign_raw(keystore, bytes_hash.into())
            .await?;
        Ok(Self::new(bytes, signature))
    }
}

#[derive(Clone, Debug, Eq, PartialEq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum CellInfo {
    /// Cells provisioned at app installation as defined in the bundle.
    Provisioned(ProvisionedCell),

    // Cells created at runtime by cloning provisioned cells.
    Cloned(ClonedCell),

    /// Potential cells with deferred installation as defined in the bundle.
    /// Not yet implemented.
    Stem(StemCell),
}

impl CellInfo {
    pub fn new_provisioned(cell_id: CellId, dna_modifiers: DnaModifiers, name: String) -> Self {
        Self::Provisioned(ProvisionedCell {
            cell_id,
            dna_modifiers,
            name,
        })
    }

    pub fn new_cloned(
        cell_id: CellId,
        clone_id: CloneId,
        original_dna_hash: DnaHash,
        dna_modifiers: DnaModifiers,
        name: String,
        enabled: bool,
    ) -> Self {
        Self::Cloned(ClonedCell {
            cell_id,
            clone_id,
            original_dna_hash,
            dna_modifiers,
            name,
            enabled,
        })
    }
}

/// Cell whose instantiation has been deferred.
/// Not yet implemented.
#[derive(Clone, Debug, Eq, PartialEq, serde::Serialize, serde::Deserialize)]
pub struct StemCell {
    /// The hash of the DNA that this cell would be instantiated from
    pub original_dna_hash: DnaHash,
    /// The DNA modifiers that will be used when instantiating the cell
    pub dna_modifiers: DnaModifiers,
    /// An optional name to override the cell's bundle name when instantiating
    pub name: Option<String>,
}

/// Provisioned cell, a cell instantiated from a DNA on app installation.
#[derive(Clone, Debug, Eq, PartialEq, serde::Serialize, serde::Deserialize)]
pub struct ProvisionedCell {
    /// The cell's identifying data
    pub cell_id: CellId,
    /// The DNA modifiers that were used to instantiate the cell
    pub dna_modifiers: DnaModifiers,
    /// The name the cell was instantiated with
    pub name: String,
}

/// Info about an installed app, returned as part of [`AppResponse::AppInfo`]
#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct AppInfo {
    /// The unique identifier for an installed app in this conductor
    pub installed_app_id: InstalledAppId,
    /// Info about the cells installed in this app. Lists of cells are ordered
    /// and contain first the provisioned cell, then enabled clone cells and
    /// finally disabled clone cells.
    pub cell_info: IndexMap<RoleName, Vec<CellInfo>>,
    /// The app's current status, in an API-friendly format
    pub status: AppInfoStatus,
    /// The app's agent pub key.
    pub agent_pub_key: AgentPubKey,
    /// The original AppManifest used to install the app, which can also be used to
    /// install the app again under a new agent.
    pub manifest: AppManifest,
    /// The timestamp when this app was installed.
    pub installed_at: Timestamp,
}

impl AppInfo {
    pub fn from_installed_app(
        app: &InstalledApp,
        dna_definitions: &IndexMap<CellId, DnaDefHashed>,
    ) -> Self {
        let installed_app_id = app.id().clone();
        let status = app.status().clone().into();
        let agent_pub_key = app.agent_key().to_owned();
        let mut manifest = app.manifest().clone();
        let installed_at = *app.installed_at();

        let mut cell_info: IndexMap<RoleName, Vec<CellInfo>> = IndexMap::new();
        app.roles().iter().for_each(|(role_name, role_assignment)| {
            // create a vector with info of all cells for this role
            let mut cell_info_for_role: Vec<CellInfo> = Vec::new();

            // push the base cell to the vector of cell infos
            if let Some(provisioned_dna_hash) = role_assignment.provisioned_dna_hash() {
                let provisioned_cell_id =
                    CellId::new(provisioned_dna_hash.clone(), agent_pub_key.clone());
                if let Some(dna_def) = dna_definitions.get(&provisioned_cell_id) {
                    // TODO: populate `enabled` with cell state once it is implemented for a base cell
                    let cell_info = CellInfo::new_provisioned(
                        provisioned_cell_id.clone(),
                        dna_def.modifiers.to_owned(),
                        dna_def.name.to_owned(),
                    );
                    cell_info_for_role.push(cell_info);

                    // Update the manifest with the installed hash
                    match &mut manifest {
                        AppManifest::V1(manifest) => {
                            if let Some(role) =
                                manifest.roles.iter_mut().find(|r| r.name == *role_name)
                            {
                                role.dna.installed_hash = Some(dna_def.hash.clone().into());
                            }
                        }
                    }
                } else {
                    tracing::error!(
                        "no DNA definition found for cell id {}",
                        provisioned_cell_id
                    );
                }
            } else {
                // no provisioned cell, thus there must be a deferred cell
                // this is not implemented as of now
                unimplemented!()
            };

            // push enabled clone cells to the vector of cell infos
            if let Some(clone_cells) = app.clone_cells_for_role_name(role_name) {
                clone_cells.for_each(|(clone_id, cell_id)| {
                    if let Some(dna_def) = dna_definitions.get(&cell_id) {
                        let cell_info = CellInfo::new_cloned(
                            cell_id,
                            clone_id.to_owned(),
                            dna_def.hash.to_owned(),
                            dna_def.modifiers.to_owned(),
                            dna_def.name.to_owned(),
                            true,
                        );
                        cell_info_for_role.push(cell_info);
                    } else {
                        tracing::error!("no DNA definition found for cell id {}", cell_id);
                    }
                });
            }

            // push disabled clone cells to the vector of cell infos
            if let Some(clone_cells) = app.disabled_clone_cells_for_role_name(role_name) {
                clone_cells.for_each(|(clone_id, cell_id)| {
                    if let Some(dna_def) = dna_definitions.get(&cell_id) {
                        let cell_info = CellInfo::new_cloned(
                            cell_id,
                            clone_id.to_owned(),
                            dna_def.hash.to_owned(),
                            dna_def.modifiers.to_owned(),
                            dna_def.name.to_owned(),
                            false,
                        );
                        cell_info_for_role.push(cell_info);
                    } else {
                        tracing::error!("no DNA definition found for cell id {}", cell_id);
                    }
                });
            }

            cell_info.insert(role_name.clone(), cell_info_for_role);
        });

        Self {
            installed_app_id,
            cell_info,
            status,
            agent_pub_key,
            manifest,
            installed_at,
        }
    }
}

#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
/// The parameters to revoke an agent for an app.
pub struct RevokeAgentKeyPayload {
    pub agent_key: AgentPubKey,
    pub app_id: InstalledAppId,
}

/// A flat, slightly more API-friendly representation of [`AppInfo`]
#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum AppInfoStatus {
    Paused { reason: PausedAppReason },
    Disabled { reason: DisabledAppReason },
    Running,
    AwaitingMemproofs,
}

impl From<AppStatus> for AppInfoStatus {
    fn from(i: AppStatus) -> Self {
        match i {
            AppStatus::Running => AppInfoStatus::Running,
            AppStatus::Disabled(reason) => AppInfoStatus::Disabled { reason },
            AppStatus::Paused(reason) => AppInfoStatus::Paused { reason },
            AppStatus::AwaitingMemproofs => AppInfoStatus::AwaitingMemproofs,
        }
    }
}

impl From<AppInfoStatus> for AppStatus {
    fn from(i: AppInfoStatus) -> Self {
        match i {
            AppInfoStatus::Running => AppStatus::Running,
            AppInfoStatus::Disabled { reason } => AppStatus::Disabled(reason),
            AppInfoStatus::Paused { reason } => AppStatus::Paused(reason),
            AppInfoStatus::AwaitingMemproofs => AppStatus::AwaitingMemproofs,
        }
    }
}

#[derive(Clone, Debug, PartialEq, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct NetworkInfo {
    pub fetch_pool_info: FetchPoolInfo,
    pub current_number_of_peers: u32,
    pub arc_size: f64,
    pub total_network_peers: u32,
    pub bytes_since_last_time_queried: u64,
    pub completed_rounds_since_last_time_queried: u32,
}

/// The request payload sent on a Holochain app websocket to authenticate the connection.
#[derive(Clone, Debug, PartialEq, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct AppAuthenticationRequest {
    /// The authentication token that was provided by the conductor when [`crate::admin_interface::AdminRequest::IssueAppAuthenticationToken`] was called.
    pub token: AppAuthenticationToken,
}

#[cfg(test)]
mod tests {
    use crate::{AppInfoStatus, AppRequest, AppResponse};
    use holochain_types::app::{AppStatus, DisabledAppReason, PausedAppReason};
    use serde::Deserialize;

    #[test]
    fn app_request_serialization() {
        use rmp_serde::Deserializer;

        // make sure requests are serialized as expected
        let request = AppRequest::AppInfo;
        let serialized_request = holochain_serialized_bytes::encode(&request).unwrap();
        assert_eq!(
            serialized_request,
            vec![129, 164, 116, 121, 112, 101, 168, 97, 112, 112, 95, 105, 110, 102, 111]
        );

        let json_expected = r#"{"type":"app_info"}"#;
        let mut deserializer = Deserializer::new(&*serialized_request);
        let json_value: serde_json::Value = Deserialize::deserialize(&mut deserializer).unwrap();
        let json_actual = serde_json::to_string(&json_value).unwrap();

        assert_eq!(json_actual, json_expected);

        // make sure responses are serialized as expected
        let response = AppResponse::ListWasmHostFunctions(vec![
            "host_fn_1".to_string(),
            "host_fn_2".to_string(),
        ]);
        let serialized_response = holochain_serialized_bytes::encode(&response).unwrap();
        assert_eq!(
            serialized_response,
            vec![
                130, 164, 116, 121, 112, 101, 184, 108, 105, 115, 116, 95, 119, 97, 115, 109, 95,
                104, 111, 115, 116, 95, 102, 117, 110, 99, 116, 105, 111, 110, 115, 165, 118, 97,
                108, 117, 101, 146, 169, 104, 111, 115, 116, 95, 102, 110, 95, 49, 169, 104, 111,
                115, 116, 95, 102, 110, 95, 50
            ]
        );

        let json_expected =
            r#"{"type":"list_wasm_host_functions","value":["host_fn_1","host_fn_2"]}"#;
        let mut deserializer = Deserializer::new(&*serialized_response);
        let json_value: serde_json::Value = Deserialize::deserialize(&mut deserializer).unwrap();
        let json_actual = serde_json::to_string(&json_value).unwrap();

        assert_eq!(json_actual, json_expected);
    }

    #[test]
    fn status_serialization() {
        use serde_json;

        let status: AppInfoStatus =
            AppStatus::Disabled(DisabledAppReason::Error("because".into())).into();

        assert_eq!(
            serde_json::to_string(&status).unwrap(),
            "{\"type\":\"disabled\",\"value\":{\"reason\":{\"type\":\"error\",\"value\":\"because\"}}}"
        );

        let status: AppInfoStatus =
            AppStatus::Paused(PausedAppReason::Error("because".into())).into();

        assert_eq!(
            serde_json::to_string(&status).unwrap(),
            "{\"type\":\"paused\",\"value\":{\"reason\":{\"type\":\"error\",\"value\":\"because\"}}}",
        );

        let status: AppInfoStatus = AppStatus::Disabled(DisabledAppReason::User).into();

        assert_eq!(
            serde_json::to_string(&status).unwrap(),
            "{\"type\":\"disabled\",\"value\":{\"reason\":{\"type\":\"user\"}}}",
        );
    }
}



================================================
File: crates/holochain_conductor_api/src/config.rs
================================================
pub mod conductor;
mod interface;
pub use interface::*;



================================================
File: crates/holochain_conductor_api/src/docs.rs
================================================
//! # Additional Documentation
//!
//! ## Enum Serialization Convention
//!
//! Serialization of enums exposed on the conductor API in most cases follow the following
//! convention:
//!
//! 1. Enums with **only unit-like variants** have their variant names converted to `snake_case`
//! using the `#[serde(rename_all = "snake_case")]` attribute.
//!
//! For example:
//!
//! ```
//! #[serde(rename_all = "snake_case")]
//! pub enum AppStatusFilter {
//!     Enabled,
//!     Disabled,
//!     Running,
//!     Stopped,
//!     Paused,
//! }
//! ```
//!
//! would lead to the following associated TypeScript type
//!
//! ```
//! type AppStatusFilter = "enabled" | "disabled" | "running" | "stopped" | "paused";
//! ```
//!
//! 2. Enums that **include tuple-like and/or struct-like variants** are serialized using
//! the `#[serde(tag = "type", content = "value", rename_all = "snake_case")]` attributes.
//!
//! For example:
//!
//! ```
//! #[serde(tag = "type", content = "value", rename_all = "snake_case")]
//! pub enum Signal {
//!     App {
//!         cell_id: CellId,
//!         zome_name: ZomeName,
//!         signal: AppSignal,
//!     },
//!     System(SystemSignal),
//! }
//! ```
//!
//! would lead to the following associated TypeScript type
//!
//! ```
//! type Signal =
//!   | {
//!       type: "app",
//!       value: {
//!         cell_id: CellId,
//!         zome_name: ZomeName,
//!         signal: AppSignal
//!       }
//!     }
//!   | {
//!       type: "system_signal"
//!       value: SystemSignal
//!     };
//! ```



================================================
File: crates/holochain_conductor_api/src/lib.rs
================================================
//! Interfaces to manage Holochain applications (hApps) and call their functions.
//!
//! The Conductor is the central component of Holochain. It exposes WebSockets for clients to
//! connect to, processes incoming requests and orchestrates data flow and persistence.
//!
//! Refer to [Holochain's architecture](https://developer.holochain.org/concepts/2_application_architecture)
//! for more info. Read about hApp development in the
//! [Happ Development Kit (HDK) documentation](https://docs.rs/hdk/latest/hdk).
//!
//! There is a [Holochain client for JavaScript](https://github.com/holochain/holochain-client-js)
//! and a [Rust client](https://github.com/holochain/holochain-client-rust)
//! to connect to the Conductor.
//!
//! The Conductor API is split into Admin and App requests and responses. Each has
//! an associated enum [`AdminRequest`] and [`AppRequest`] that define and
//! document available calls.
//!
//! The admin interface generally manages the conductor itself, such as installing apps,
//! listing dnas, cells and apps, accessing state and metric information dumps and managing
//! agents.
//!
//! The app interface is smaller and focussed on interfacing with an app directly.
//! Notably the app interface allows calling functions exposed by the hApps'
//! modules, called DNAs. To discover a particular hApp's structure, its app
//! info can be requested.
//!
//! Additional information can be found in the [docs module][`crate::docs`].
//!

#[cfg(doc)]
pub mod docs;

mod admin_interface;
mod app_interface;
pub mod config;
pub mod signal_subscription;
pub mod state_dump;
pub mod storage_info;

pub use admin_interface::*;
pub use app_interface::*;
pub use config::*;
pub use state_dump::*;
pub use storage_info::*;



================================================
File: crates/holochain_conductor_api/src/signal_subscription.rs
================================================
use holochain_serialized_bytes::prelude::*;
use holochain_types::app::InstalledAppId;
use holochain_zome_types::cell::CellId;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Declares updated Signal subscription settings for an App.
/// This message is part of the AppInterfaceApi
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, SerializedBytes)]
pub struct SignalSubscription {
    /// The app for which to manage subscription
    installed_app_id: InstalledAppId,
    /// Fine-grained per-cell filters
    filters: SignalFilterSet,
}

/// Associate a SignalFilter with each Cell in an App.
/// The filtering can be interpreted as inclusive or exclusive,
/// depending on the use case.
///
/// An empty Exclude filter means "allow all signals" (subscribe to all).
/// An empty Include filter means "block all signals" (unsubscribe from all).
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, SerializedBytes)]
pub enum SignalFilterSet {
    /// Only allow signals from the specified Cells with the specified filters,
    /// block everything else
    Include(HashMap<CellId, SignalFilter>),
    /// Only block signals from the specified Cells with the specified filters
    /// allow everything else
    Exclude(HashMap<CellId, SignalFilter>),
}

impl Default for SignalFilterSet {
    fn default() -> Self {
        // The default is no filter
        Self::allow_all()
    }
}

impl SignalFilterSet {
    /// Allow all signals to come through (subscribe to all)
    pub fn allow_all() -> Self {
        SignalFilterSet::Exclude(HashMap::new())
    }

    /// Block all signals (unsubscribe from all)
    pub fn block_all() -> Self {
        SignalFilterSet::Include(HashMap::new())
    }
}

/// Specifies fine-grained filter controls for the signals
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, SerializedBytes)]
pub struct SignalFilter;

impl Default for SignalFilter {
    fn default() -> Self {
        // The default is no filter
        Self::empty()
    }
}

impl SignalFilter {
    /// A passthrough filter which filters nothing
    pub fn empty() -> Self {
        SignalFilter
    }
}



================================================
File: crates/holochain_conductor_api/src/state_dump.rs
================================================
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_state_types::SourceChainDump;
use holochain_types::dht_op::DhtOp;
use kitsune_p2p_bin_data::{KitsuneAgent, KitsuneSpace};
use serde::Deserialize;
use serde::Serialize;
use std::sync::Arc;

#[derive(Serialize, Deserialize)]
pub struct JsonDump {
    pub peer_dump: P2pAgentsDump,
    pub source_chain_dump: SourceChainDump,
    pub integration_dump: IntegrationStateDump,
}

#[derive(Serialize, Clone, Debug, Deserialize, PartialEq, Eq)]
pub struct FullStateDump {
    pub peer_dump: P2pAgentsDump,
    pub source_chain_dump: SourceChainDump,
    pub integration_dump: FullIntegrationStateDump,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
/// A collection of many cells dumps for easy viewing.
/// Use display to see a nice printout.
pub struct IntegrationStateDumps(pub Vec<IntegrationStateDump>);

#[derive(Serialize, Deserialize, Debug, Clone)]
/// A high level view of the incoming ops and where
/// they are currently.
/// Ops start in the validation limbo then proceed
/// to the integration limbo then finally are integrated.
pub struct IntegrationStateDump {
    /// Ops in validation limbo awaiting sys
    /// or app validation.
    pub validation_limbo: usize,
    /// Ops waiting to be integrated.
    pub integration_limbo: usize,
    /// Ops that are integrated.
    /// This includes rejected.
    pub integrated: usize,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
/// A full view of the DHT shard of the Cell.
/// Ops start in the validation limbo then proceed
/// to the integration limbo then finally are integrated.
pub struct FullIntegrationStateDump {
    /// Ops in validation limbo awaiting sys
    /// or app validation.
    pub validation_limbo: Vec<DhtOp>,

    /// Ops waiting to be integrated.
    pub integration_limbo: Vec<DhtOp>,

    /// Ops that are integrated.
    /// This includes rejected.
    pub integrated: Vec<DhtOp>,

    /// RowId for the latest DhtOp that we have seen
    /// Useful for subsequent calls to `FullStateDump`
    /// to return only what they haven't seen
    pub dht_ops_cursor: u64,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
/// State dump of all the peer info
pub struct P2pAgentsDump {
    /// The info of this agents cell.
    pub this_agent_info: Option<AgentInfoDump>,
    /// The dna as a [`DnaHash`] and [`KitsuneSpace`].
    pub this_dna: Option<(DnaHash, KitsuneSpace)>,
    /// The agent as [`AgentPubKey`] and [`KitsuneAgent`].
    pub this_agent: Option<(AgentPubKey, KitsuneAgent)>,
    /// All other agent info.
    pub peers: Vec<AgentInfoDump>,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
/// Agent info dump with the agent,
/// space, signed time, expires in and
/// urls printed in a pretty way.
pub struct AgentInfoDump {
    pub kitsune_agent: Arc<KitsuneAgent>,
    pub kitsune_space: Arc<KitsuneSpace>,
    pub dump: String,
}

impl std::fmt::Display for JsonDump {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let num_other_peers = self.peer_dump.peers.len();
        let s = &self.source_chain_dump;
        writeln!(f, "--- Cell State Dump Summary ---")?;
        writeln!(
            f,
            "Number of other peers in p2p store: {},",
            num_other_peers
        )?;
        writeln!(
            f,
            "Records authored: {}, Ops published: {}",
            s.records.len(),
            s.published_ops_count
        )
    }
}

impl std::fmt::Display for IntegrationStateDumps {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "[")?;
        for i in &self.0 {
            write!(f, "{},", i)?;
        }
        writeln!(f, "]")
    }
}

impl std::fmt::Display for IntegrationStateDump {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        writeln!(
            f,
            "({:?},{:?},{:?})",
            self.validation_limbo, self.integration_limbo, self.integrated
        )
    }
}

impl std::fmt::Display for AgentInfoDump {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        writeln!(f, "space: {:?}", self.kitsune_space)?;
        writeln!(f, "agent: {:?}", self.kitsune_agent)?;
        writeln!(f, "{}", self.dump)
    }
}
impl std::fmt::Display for P2pAgentsDump {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if let Some(this_agent) = &self.this_agent {
            writeln!(f, "This Agent {:?} is {:?}", this_agent.0, this_agent.1)?;
        }
        if let Some(this_dna) = &self.this_dna {
            writeln!(f, "This DNA {:?} is {:?}", this_dna.0, this_dna.1)?;
        }
        if let Some(this_agent_info) = &self.this_agent_info {
            writeln!(f, "This agents info: {}", this_agent_info)?;
        }
        for peer in &self.peers {
            writeln!(f, "{}", peer)?;
        }
        Ok(())
    }
}



================================================
File: crates/holochain_conductor_api/src/storage_info.rs
================================================
use holochain_types::prelude::*;

/// Storage info for DNA used by one or more hApps.
#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct DnaStorageInfo {
    pub authored_data_size: usize,
    pub authored_data_size_on_disk: usize,
    pub dht_data_size: usize,
    pub dht_data_size_on_disk: usize,
    pub cache_data_size: usize,
    pub cache_data_size_on_disk: usize,
    pub dna_hash: DnaHash,
    pub used_by: Vec<InstalledAppId>,
}

/// The type of storage blob
#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum StorageBlob {
    /// Storage blob used by hApps to store data
    Dna(DnaStorageInfo),
}

/// Response type for storage used by holochain and applications
#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct StorageInfo {
    pub blobs: Vec<StorageBlob>,
}



================================================
File: crates/holochain_conductor_api/src/config/conductor.rs
================================================
#![deny(missing_docs)]
//! This module is used to configure the conductor.
//!
//! #### Example minimum conductor config:
//!
//! ```rust
//! let yaml = r#"---
//!
//! ## Configure the keystore to be used.
//! keystore:
//!
//!   ## Use an in-process keystore with default database location.
//!   type: lair_server_in_proc
//!
//! ## Configure an admin WebSocket interface at a specific port.
//! admin_interfaces:
//!   - driver:
//!       type: websocket
//!       port: 1234
//!       allowed_origins: "*"
//!
//! ## Configure the network.
//! network:
//!
//!   ## Use the Holo-provided default production bootstrap server.
//!   bootstrap_service: https://bootstrap.holo.host
//!
//!   ## This currently has no effect on functionality but is required. Please just include as-is for now.
//!   network_type: quic_bootstrap
//!
//!   ## Setup a specific network configuration.
//!   transport_pool:
//!     ## Use WebRTC, which is the only option for now.
//!     - type: webrtc
//!
//!       ## Use the Holo-provided default production sbd (signal) server.
//!       ## `signal_url` is REQUIRED.
//!       signal_url: wss://sbd-0.main.infra.holo.host
//!
//!       ## Override the default WebRTC STUN configuration.
//!       ## This is OPTIONAL. If this is not specified, it will default
//!       ## to what you can see here:
//!       webrtc_config: {
//!         "iceServers": [
//!           { "urls": ["stun:stun-0.main.infra.holo.host:443"] },
//!           { "urls": ["stun:stun-1.main.infra.holo.host:443"] }
//!         ]
//!       }
//! "#;
//!
//!use holochain_conductor_api::conductor::ConductorConfig;
//!
//!let _: ConductorConfig = serde_yaml::from_str(yaml).unwrap();
//! ```

use crate::conductor::process::ERROR_CODE;
use holochain_types::prelude::DbSyncStrategy;
use kitsune_p2p_types::config::{KitsuneP2pConfig, KitsuneP2pTuningParams};
use schemars::JsonSchema;
use serde::de::DeserializeOwned;
use serde::Deserialize;
use serde::Serialize;

mod admin_interface_config;
mod dpki_config;
#[allow(missing_docs)]
mod error;
mod keystore_config;
/// Defines subdirectories of the config directory.
pub mod paths;
pub mod process;
//mod logger_config;
//mod signal_config;

pub use super::*;
pub use dpki_config::DpkiConfig;
//pub use logger_config::LoggerConfig;
pub use error::*;
pub use keystore_config::KeystoreConfig;
//pub use signal_config::SignalConfig;
use std::path::Path;

use crate::config::conductor::paths::DataRootPath;

// TODO change types from "stringly typed" to Url2
/// All the config information for the conductor
#[derive(Clone, Deserialize, Serialize, Debug, PartialEq, Default, JsonSchema)]
pub struct ConductorConfig {
    /// Override the environment specified tracing config.
    #[serde(default)]
    pub tracing_override: Option<String>,

    /// The path to the data root for this conductor;
    /// This can be `None` while building up the config programatically but MUST
    /// be set by the time the config is used to build a conductor.
    /// The database and compiled wasm directories are derived from this path.
    pub data_root_path: Option<DataRootPath>,

    /// The lair tag used to refer to the "device seed" which was used to generate
    /// the AgentPubKey for the DPKI cell.
    ///
    /// This must not be changed once the conductor has been started for the first time.
    pub device_seed_lair_tag: Option<String>,

    /// If set, and if there is no seed in lair at the tag specified in `device_seed_lair_tag`,
    /// the conductor will create a random seed and store it in lair at the specified tag.
    /// This should only be used for test or throwaway environments, because this device seed
    /// can never be regenerated, which defeats the purpose of having a device seed in the first place.
    ///
    /// If `device_seed_lair_tag` is not set, this setting has no effect.
    #[serde(default)]
    pub danger_generate_throwaway_device_seed: bool,

    /// Define how Holochain conductor will connect to a keystore.
    #[serde(default)]
    pub keystore: KeystoreConfig,

    /// DPKI config for this conductor. This setting must not change once the conductor has been
    /// started for the first time.
    ///
    /// If `dna_path` is present, the DNA file at this path will be used to install the DPKI service upon first conductor startup.
    /// If not present, the Deepkey DNA specified by the `holochain_deepkey_dna` crate and built into Holochain, will be used instead.
    #[serde(default)]
    pub dpki: DpkiConfig,

    /// Setup admin interfaces to control this conductor through a websocket connection.
    pub admin_interfaces: Option<Vec<AdminInterfaceConfig>>,

    /// Optional config for the network module.
    #[serde(default)]
    pub network: KitsuneP2pConfig,

    /// Optional specification of Chain Head Coordination service URL.
    /// If set, each cell's commit workflow will include synchronizing with the specified CHC service.
    /// If you don't know what this means, leave this setting alone (as `None`)
    #[schemars(default, schema_with = "holochain_util::jsonschema::url2_schema")]
    #[cfg(feature = "chc")]
    pub chc_url: Option<url2::Url2>,

    /// Override the default database synchronous strategy.
    ///
    /// See [sqlite documentation] for information about database sync levels.
    /// See [`DbSyncStrategy`] for details.
    /// This is best left at its default value unless you know what you
    /// are doing.
    ///
    /// [sqlite documentation]: https://www.sqlite.org/pragma.html#pragma_synchronous
    #[serde(default)]
    pub db_sync_strategy: DbSyncStrategy,

    /// Tuning parameters to adjust the behaviour of the conductor.
    #[serde(default)]
    pub tuning_params: Option<ConductorTuningParams>,
}

/// Helper function to load a config from a YAML string.
fn config_from_yaml<T>(yaml: &str) -> ConductorConfigResult<T>
where
    T: DeserializeOwned,
{
    serde_yaml::from_str(yaml).map_err(ConductorConfigError::SerializationError)
}

impl ConductorConfig {
    /// Create a conductor config from a YAML file path.
    pub fn load_yaml(path: &Path) -> ConductorConfigResult<ConductorConfig> {
        let config_yaml = std::fs::read_to_string(path).map_err(|err| match err {
            e @ std::io::Error { .. } if e.kind() == std::io::ErrorKind::NotFound => {
                ConductorConfigError::ConfigMissing(path.into())
            }
            _ => err.into(),
        })?;
        config_from_yaml(&config_yaml)
    }

    /// Get tuning params for this config (default if not set)
    pub fn kitsune_tuning_params(&self) -> KitsuneP2pTuningParams {
        self.network.tuning_params.clone()
    }

    /// Get the tracing scope from the network config
    pub fn tracing_scope(&self) -> Option<String> {
        self.network.tracing_scope.clone()
    }

    /// Get the data directory for this config or say something nice and die.
    pub fn data_root_path_or_die(&self) -> DataRootPath {
        match &self.data_root_path {
            Some(path) => path.clone(),
            None => {
                println!(
                    "
                    The conductor config does not contain a data_root_path. Please check and fix the
                    config file. Details:

                        Missing field `data_root_path`",
                );
                std::process::exit(ERROR_CODE);
            }
        }
    }

    /// Get the conductor tuning params for this config (default if not set)
    pub fn conductor_tuning_params(&self) -> ConductorTuningParams {
        self.tuning_params.clone().unwrap_or_default()
    }

    /// Check if the config is set to use a rendezvous bootstrap server
    pub fn has_rendezvous_bootstrap(&self) -> bool {
        self.network.bootstrap_service == Some(url2::url2!("rendezvous:"))
    }
}

/// Tuning parameters to adjust the behaviour of the conductor.
#[derive(Debug, Clone, PartialEq, Deserialize, Serialize, JsonSchema)]
pub struct ConductorTuningParams {
    /// The delay between retries of sys validation when there are missing dependencies waiting to be found on the DHT.
    ///
    /// Default: 10 seconds
    pub sys_validation_retry_delay: Option<std::time::Duration>,
    /// The delay between retries attempts at resolving failed countersigning sessions.
    ///
    /// This is potentially a very heavy operation because it has to gather information from the network,
    /// so it is recommended not to set this too low.
    ///
    /// Default: 5 minutes
    pub countersigning_resolution_retry_delay: Option<std::time::Duration>,
    /// The maximum number of times that Holochain should attempt to resolve a failed countersigning session.
    ///
    /// Note that this *only* applies to sessions that fail through a timeout. Sessions that fail because
    /// of a conductor crash or otherwise will not be limited by this value. This is a safety measure to
    /// make it less likely that timeout leads to a wrong decision because of a temporary network issue.
    ///
    /// Holochain will always try once, whatever value you set. The possible values for this setting are:
    /// - `None`: Not set, then Holochain will just make a single attempt and then consider the session failed
    ///    if it can't make a decision.
    /// - `Some(0)`: Holochain will treat this the same as a session that failed after a crash. It will retry
    ///   until it can make a decision or until the user forces a decision.
    /// - `Some(n)`, n > 0: Holochain will retry `n` times, including the required first attempt. If
    ///   it can't make a decision after `n` retries, it will consider the session failed.
    pub countersigning_resolution_retry_limit: Option<usize>,
    /// Only publish a DhtOp once during this interval. This allows for triggering the publish workflow
    /// frequently without flooding the network with spurious publishes.
    ///
    /// Default: 5 minutes
    pub min_publish_interval: Option<std::time::Duration>,
}

impl ConductorTuningParams {
    /// Create a new [`ConductorTuningParams`] with all values missing, which will cause the defaults to be used.
    pub fn new() -> Self {
        Self {
            sys_validation_retry_delay: None,
            countersigning_resolution_retry_delay: None,
            countersigning_resolution_retry_limit: None,
            min_publish_interval: None,
        }
    }

    /// Get the current value of `sys_validation_retry_delay` or its default value.
    pub fn sys_validation_retry_delay(&self) -> std::time::Duration {
        self.sys_validation_retry_delay
            .unwrap_or_else(|| std::time::Duration::from_secs(10))
    }

    /// Get the current value of `countersigning_resolution_retry_delay` or its default value.
    pub fn countersigning_resolution_retry_delay(&self) -> std::time::Duration {
        self.countersigning_resolution_retry_delay
            .unwrap_or_else(|| std::time::Duration::from_secs(60 * 5))
    }

    /// Get the current value of `min_publish_interval` or its default value.
    pub fn min_publish_interval(&self) -> std::time::Duration {
        self.min_publish_interval
            .unwrap_or_else(|| std::time::Duration::from_secs(60 * 5))
    }
}

impl Default for ConductorTuningParams {
    fn default() -> Self {
        let empty = Self::new();
        Self {
            sys_validation_retry_delay: Some(empty.sys_validation_retry_delay()),
            countersigning_resolution_retry_delay: Some(
                empty.countersigning_resolution_retry_delay(),
            ),
            countersigning_resolution_retry_limit: None,
            min_publish_interval: None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use holochain_types::websocket::AllowedOrigins;
    use kitsune_p2p_types::config::TransportConfig;
    use matches::assert_matches;
    use std::path::Path;
    use std::path::PathBuf;

    #[test]
    fn test_config_load_yaml() {
        let bad_path = Path::new("fake");
        let result = ConductorConfig::load_yaml(bad_path);
        assert_eq!(
            "Err(ConfigMissing(\"fake\"))".to_string(),
            format!("{:?}", result)
        );

        // successful load test in conductor/interactive
    }

    #[test]
    fn test_config_bad_yaml() {
        let result: ConductorConfigResult<ConductorConfig> = config_from_yaml("this isn't yaml");
        assert_matches!(result, Err(ConductorConfigError::SerializationError(_)));
    }

    #[test]
    fn test_config_complete_minimal_config() {
        let yaml = r#"---
    data_root_path: /path/to/env
    network:
      transport_pool:
        - type: mem
    keystore:
      type: danger_test_keystore
    "#;
        let result: ConductorConfig = config_from_yaml(yaml).unwrap();
        pretty_assertions::assert_eq!(
            result,
            ConductorConfig {
                tracing_override: None,
                data_root_path: Some(PathBuf::from("/path/to/env").into()),
                device_seed_lair_tag: None,
                danger_generate_throwaway_device_seed: false,
                network: KitsuneP2pConfig::mem(),
                dpki: DpkiConfig::default(),
                keystore: KeystoreConfig::DangerTestKeystore,
                admin_interfaces: None,
                db_sync_strategy: DbSyncStrategy::default(),
                #[cfg(feature = "chc")]
                chc_url: None,
                tuning_params: None,
            }
        );
    }

    #[cfg(not(feature = "unstable-dpki"))]
    #[test]
    #[allow(clippy::field_reassign_with_default)]
    fn test_config_complete_config() {
        holochain_trace::test_run();

        let yaml = r#"---
    data_root_path: /path/to/env
    signing_service_uri: ws://localhost:9001
    encryption_service_uri: ws://localhost:9002
    decryption_service_uri: ws://localhost:9003

    keystore:
      type: lair_server_in_proc

    dpki:
      dna_path: ~
      network_seed: ""
      no_dpki: true

    admin_interfaces:
      - driver:
          type: websocket
          port: 1234
          allowed_origins: "*"

    network:
      bootstrap_service: https://bootstrap-staging.holo.host
      transport_pool:
        - type: webrtc
          signal_url: wss://sbd-0.main.infra.holo.host
          webrtc_config: {
            "iceServers": [
              { "urls": ["stun:stun-0.main.infra.holo.host:443"] },
              { "urls": ["stun:stun-1.main.infra.holo.host:443"] }
            ]
          }
      tuning_params:
        gossip_loop_iteration_delay_ms: 42
        default_rpc_single_timeout_ms: 42
        default_rpc_multi_remote_agent_count: 42
        default_rpc_multi_remote_request_grace_ms: 42
        agent_info_expires_after_ms: 42
        tls_in_mem_session_storage: 42
        proxy_keepalive_ms: 42
        proxy_to_expire_ms: 42
        tx5_min_ephemeral_udp_port: 40000
        tx5_max_ephemeral_udp_port: 40255
      network_type: quic_bootstrap

    db_sync_strategy: Fast
    "#;
        let result: ConductorConfigResult<ConductorConfig> = config_from_yaml(yaml);
        let mut network_config = KitsuneP2pConfig::mem();
        network_config.bootstrap_service = Some(url2::url2!("https://bootstrap-staging.holo.host"));
        network_config.transport_pool = vec![TransportConfig::WebRTC {
            signal_url: "wss://sbd-0.main.infra.holo.host".into(),
            webrtc_config: Some(serde_json::json!({
              "iceServers": [
                { "urls": ["stun:stun-0.main.infra.holo.host:443"] },
                { "urls": ["stun:stun-1.main.infra.holo.host:443"] }
              ]
            })),
        }];
        let mut tuning_params =
            kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
        tuning_params.gossip_loop_iteration_delay_ms = 42;
        tuning_params.default_rpc_single_timeout_ms = 42;
        tuning_params.default_rpc_multi_remote_agent_count = 42;
        tuning_params.default_rpc_multi_remote_request_grace_ms = 42;
        tuning_params.agent_info_expires_after_ms = 42;
        tuning_params.tls_in_mem_session_storage = 42;
        tuning_params.proxy_keepalive_ms = 42;
        tuning_params.proxy_to_expire_ms = 42;
        tuning_params.tx5_min_ephemeral_udp_port = 40000;
        tuning_params.tx5_max_ephemeral_udp_port = 40255;
