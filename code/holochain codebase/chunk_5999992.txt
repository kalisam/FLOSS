        assert_eq!(parsed["records"][2]["action"]["entry_type"], "AgentPubKey");
        assert_eq!(parsed["records"][2]["entry"]["entry_type"], "Agent");
        assert_ne!(
            parsed["records"][2]["entry"]["entry"],
            serde_json::Value::Null
        );

        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn source_chain_query() {
        let test_db = test_authored_db();
        let dht_db = test_dht_db();
        let dht_db_cache = DhtDbQueryCache::new(dht_db.to_db().into());
        let keystore = test_keystore();
        let vault = test_db.to_db();
        let alice = keystore.new_sign_keypair_random().await.unwrap();
        let bob = keystore.new_sign_keypair_random().await.unwrap();
        let dna_hash = fixt!(DnaHash);

        genesis(
            vault.clone(),
            dht_db.to_db(),
            &dht_db_cache,
            keystore.clone(),
            dna_hash.clone(),
            alice.clone(),
            None,
            None,
        )
        .await
        .unwrap();

        genesis(
            vault.clone(),
            dht_db.to_db(),
            &dht_db_cache,
            keystore.clone(),
            dna_hash.clone(),
            bob.clone(),
            None,
            None,
        )
        .await
        .unwrap();

        // test_db.dump_tmp();

        let chain = SourceChain::new(vault, dht_db.to_db(), dht_db_cache, keystore, alice.clone())
            .await
            .unwrap();

        let records = chain.query(ChainQueryFilter::default()).await.unwrap();

        // All of the range queries which should return a full set of records
        let full_ranges = [
            ChainQueryFilterRange::Unbounded,
            ChainQueryFilterRange::ActionSeqRange(0, 2),
            ChainQueryFilterRange::ActionHashRange(
                records[0].action_address().clone(),
                records[2].action_address().clone(),
            ),
            ChainQueryFilterRange::ActionHashTerminated(records[2].action_address().clone(), 2),
        ];

        // A variety of combinations of query parameters
        let cases = [
            ((None, None, vec![], false), 3),
            ((None, None, vec![], true), 3),
            ((Some(vec![ActionType::Dna]), None, vec![], false), 1),
            ((None, Some(vec![EntryType::AgentPubKey]), vec![], false), 1),
            ((None, Some(vec![EntryType::AgentPubKey]), vec![], true), 1),
            ((Some(vec![ActionType::Create]), None, vec![], false), 1),
            ((Some(vec![ActionType::Create]), None, vec![], true), 1),
            (
                (
                    Some(vec![ActionType::Create]),
                    Some(vec![EntryType::AgentPubKey]),
                    vec![],
                    false,
                ),
                1,
            ),
            (
                (
                    Some(vec![ActionType::Create]),
                    Some(vec![EntryType::AgentPubKey]),
                    vec![records[2].action().entry_hash().unwrap().clone()],
                    true,
                ),
                1,
            ),
            (
                (
                    Some(vec![ActionType::Create, ActionType::Dna]),
                    None,
                    vec![],
                    true,
                ),
                2,
            ),
            (
                (
                    None,
                    // Redundant but covers the code that constructs the IN query
                    Some(vec![EntryType::AgentPubKey, EntryType::AgentPubKey]),
                    vec![],
                    true,
                ),
                1,
            ),
        ];

        // Test all permutations of cases defined with all full range queries,
        // and both boolean values of `include_entries`.
        for ((action_type, entry_type, entry_hashes, include_entries), num_expected) in cases {
            let entry_hashes = if entry_hashes.is_empty() {
                None
            } else {
                Some(entry_hashes.into_iter().collect())
            };
            for sequence_range in full_ranges.clone() {
                let query = ChainQueryFilter {
                    sequence_range: sequence_range.clone(),
                    action_type: action_type.clone(),
                    entry_type: entry_type.clone(),
                    entry_hashes: entry_hashes.clone(),
                    include_entries,
                    order_descending: false,
                };
                if sequence_range != ChainQueryFilterRange::Unbounded
                    && (action_type.is_some()
                        || entry_type.is_some()
                        || entry_hashes.is_some()
                        || include_entries)
                {
                    assert!(matches!(
                        chain.query(query.clone()).await,
                        Err(SourceChainError::UnsupportedQuery(_))
                    ));
                } else {
                    let queried = chain.query(query.clone()).await.unwrap();
                    let actual = queried.len();
                    assert!(queried.iter().all(|e| e.action().author() == &alice));
                    assert_eq!(
                        num_expected, actual,
                        "Expected {} items but got {} with filter {:?}",
                        num_expected, actual, query
                    );
                }
            }
        }
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn source_chain_query_ordering() {
        let test_db = test_authored_db();
        let dht_db = test_dht_db();
        let dht_db_cache = DhtDbQueryCache::new(dht_db.to_db().into());
        let keystore = test_keystore();
        let vault = test_db.to_db();
        let alice = keystore.new_sign_keypair_random().await.unwrap();
        let dna_hash = fixt!(DnaHash);

        genesis(
            vault.clone(),
            dht_db.to_db(),
            &dht_db_cache,
            keystore.clone(),
            dna_hash.clone(),
            alice.clone(),
            None,
            None,
        )
        .await
        .unwrap();

        let chain = SourceChain::new(vault, dht_db.to_db(), dht_db_cache, keystore, alice.clone())
            .await
            .unwrap();

        let asc = chain.query(ChainQueryFilter::default()).await.unwrap();
        let desc = chain
            .query(ChainQueryFilter::default().descending())
            .await
            .unwrap();

        assert_eq!(asc.len(), 3);
        assert_ne!(asc, desc);

        let mut desc_sorted = desc;
        desc_sorted.sort_by_key(|r| r.signed_action.action().action_seq());
        assert_eq!(asc, desc_sorted);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn init_zomes_complete() {
        let test_db = test_authored_db();
        let dht_db = test_dht_db();
        let dht_db_cache = DhtDbQueryCache::new(dht_db.to_db().into());
        let keystore = test_keystore();
        let vault = test_db.to_db();
        let alice = keystore.new_sign_keypair_random().await.unwrap();
        let dna_hash = fixt!(DnaHash);

        genesis(
            vault.clone(),
            dht_db.to_db(),
            &dht_db_cache,
            keystore.clone(),
            dna_hash.clone(),
            alice.clone(),
            None,
            None,
        )
        .await
        .unwrap();

        let chain = SourceChain::new(vault, dht_db.to_db(), dht_db_cache, keystore, alice.clone())
            .await
            .unwrap();

        // zomes initialized should be false after genesis
        let zomes_initialized = chain.zomes_initialized().await.unwrap();
        assert!(!zomes_initialized);

        // insert init marker into source chain
        let result = chain
            .put(
                builder::InitZomesComplete {},
                None,
                ChainTopOrdering::Strict,
            )
            .await;
        assert!(result.is_ok());

        chain.flush(vec![DhtArc::Empty], None).await.unwrap();

        // zomes initialized should be true after init zomes has run
        let zomes_initialized = chain.zomes_initialized().await.unwrap();
        assert!(zomes_initialized);
    }
}



================================================
File: crates/holochain_state/src/test_utils.rs
================================================
//! Helpers for unit tests

use base64::engine::general_purpose::URL_SAFE_NO_PAD;
use base64::Engine;
use holochain_keystore::MetaLairClient;
use holochain_sqlite::prelude::*;
use holochain_sqlite::rusqlite::Statement;
use holochain_sqlite::rusqlite::Transaction;
use holochain_types::prelude::*;
use holochain_zome_types::test_utils::fake_cell_id;
use kitsune_p2p::KitsuneSpace;
use shrinkwraprs::Shrinkwrap;
use std::collections::HashMap;
use std::path::Path;
use std::path::PathBuf;
use std::sync::Arc;
use tempfile::TempDir;

pub mod mutations_helpers;

#[cfg(test)]
mod tests {
    use holochain_sqlite::error::DatabaseResult;
    use holochain_sqlite::rusqlite::Transaction;

    fn _dbg_db_schema(db_name: &str, conn: &Transaction) {
        #[allow(dead_code)]
        #[derive(Debug)]
        pub struct Schema {
            pub ty: String,
            pub name: String,
            pub tbl_name: String,
            pub rootpage: u64,
            pub sql: Option<String>,
        }

        let mut statement = conn.prepare("select * from sqlite_schema").unwrap();
        let iter = statement
            .query_map([], |row| {
                Ok(Schema {
                    ty: row.get(0)?,
                    name: row.get(1)?,
                    tbl_name: row.get(2)?,
                    rootpage: row.get(3)?,
                    sql: row.get(4)?,
                })
            })
            .unwrap();

        println!("~~~ {} START ~~~", &db_name);
        for i in iter {
            dbg!(&i);
        }
        println!("~~~ {} END ~~~", &db_name);
    }

    #[tokio::test(flavor = "multi_thread")]
    pub async fn dbg_db_schema() {
        super::test_conductor_db()
            .db
            .read_async(move |txn| -> DatabaseResult<()> {
                _dbg_db_schema("conductor", txn);
                Ok(())
            })
            .await
            .unwrap();

        super::test_p2p_agents_db()
            .db
            .read_async(move |txn| -> DatabaseResult<()> {
                _dbg_db_schema("p2p_agents", txn);
                Ok(())
            })
            .await
            .unwrap();
    }
}

/// Create a [`TestDb`] of [`DbKindAuthored`], backed by a temp directory.
pub fn test_authored_db() -> TestDb<DbKindAuthored> {
    test_authored_db_with_id(1)
}

pub fn test_authored_db_with_id(id: u8) -> TestDb<DbKindAuthored> {
    test_db(DbKindAuthored(Arc::new(CellId::new(
        fake_dna_hash(id),
        fake_agent_pub_key(id),
    ))))
}

/// Create a [`TestDb`] of [`DbKindDht`], backed by a temp directory.
pub fn test_dht_db() -> TestDb<DbKindDht> {
    test_dht_db_with_id(1)
}

pub fn test_dht_db_with_id(id: u8) -> TestDb<DbKindDht> {
    test_db(DbKindDht(Arc::new(fake_dna_hash(id))))
}

pub fn test_dht_db_with_dna_hash(hash: DnaHash) -> TestDb<DbKindDht> {
    test_db(DbKindDht(Arc::new(hash)))
}

/// Create a [`TestDb`] of [`DbKindCache`], backed by a temp directory.
pub fn test_cache_db() -> TestDb<DbKindCache> {
    test_cache_db_with_id(1)
}

pub fn test_cache_db_with_id(id: u8) -> TestDb<DbKindCache> {
    test_db(DbKindCache(Arc::new(fake_cell_id(id).dna_hash().clone())))
}

pub fn test_cache_db_with_dna_hash(hash: DnaHash) -> TestDb<DbKindCache> {
    test_db(DbKindCache(Arc::new(hash)))
}

/// Create a [`TestDb`] of [DbKindConductor], backed by a temp directory.
pub fn test_conductor_db() -> TestDb<DbKindConductor> {
    test_db(DbKindConductor)
}

/// Create a [`TestDb`] of [DbKindWasm], backed by a temp directory.
pub fn test_wasm_db() -> TestDb<DbKindWasm> {
    test_db(DbKindWasm)
}

/// Create a [`TestDb`] of [`DbKindP2pAgents`], backed by a temp directory.
pub fn test_p2p_agents_db() -> TestDb<DbKindP2pAgents> {
    test_db(DbKindP2pAgents(Arc::new(KitsuneSpace(vec![0; 36]))))
}

/// Create a [`TestDb`] of [DbKindP2pMetrics], backed by a temp directory.
pub fn test_p2p_metrics_db() -> TestDb<DbKindP2pMetrics> {
    test_db(DbKindP2pMetrics(Arc::new(KitsuneSpace(vec![0; 36]))))
}

fn test_db<Kind: DbKindT>(kind: Kind) -> TestDb<Kind> {
    let tmpdir = tempfile::Builder::new()
        .prefix("holochain-test-environments-")
        .suffix(&nanoid::nanoid!())
        .tempdir()
        .unwrap();
    TestDb {
        db: DbWrite::test(tmpdir.path(), kind).expect("Couldn't create test database"),
        dir: tmpdir.into(),
    }
}

/// Create a [`DbWrite`] of [`DbKindT`] in memory.
pub fn test_in_mem_db<Kind: DbKindT>(kind: Kind) -> DbWrite<Kind> {
    DbWrite::test_in_mem(kind).expect("Couldn't create test database")
}

/// Create a fresh set of test environments with a new TempDir
pub fn test_db_dir() -> TempDir {
    tempfile::Builder::new()
        .prefix("holochain-test-environments")
        .suffix(&nanoid::nanoid!())
        .tempdir()
        .unwrap()
}

/// Create a fresh set of test environments with a new TempDir in a given directory.
pub fn test_dbs_in(path: impl AsRef<Path>) -> TestDbs {
    let tempdir = tempfile::Builder::new()
        .prefix("holochain-test-environments")
        .suffix(&nanoid::nanoid!())
        .tempdir_in(path)
        .unwrap();
    TestDbs::new(tempdir)
}

/// A test database in a temp directory
#[derive(Shrinkwrap)]
pub struct TestDb<Kind: DbKindT> {
    #[shrinkwrap(main_field)]
    /// sqlite database
    db: DbWrite<Kind>,
    /// temp directory for this environment
    dir: TestDir,
}

impl<Kind: DbKindT> TestDb<Kind> {
    /// Accessor
    pub fn to_db(&self) -> DbWrite<Kind> {
        self.db.clone()
    }

    /// Accessor
    pub fn persist(&mut self) {
        self.dir.persist()
    }

    /// Dump db to a location.
    pub fn dump(&self, out: &Path) -> std::io::Result<()> {
        std::fs::create_dir(out).ok();
        for entry in std::fs::read_dir(&self.dir)? {
            let entry = entry?;
            let path = entry.path();
            if path.is_file() {
                let mut out = out.to_owned();
                out.push(format!(
                    "backup.{}",
                    path.extension().unwrap().to_string_lossy()
                ));
                std::fs::copy(path, out)?;
            }
        }
        Ok(())
    }

    /// Dump db into `/tmp/test_dbs`.
    pub async fn dump_tmp(&self) {
        dump_tmp(&self.db).await;
    }

    pub fn dna_hash(&self) -> Option<Arc<DnaHash>> {
        match self.db.kind().kind() {
            DbKind::Cache(hash) | DbKind::Dht(hash) => Some(hash),
            DbKind::Authored(cell_id) => Some(Arc::new(cell_id.dna_hash().clone())),
            _ => None,
        }
    }
}

/// Dump db into `/tmp/test_dbs`.
pub async fn dump_tmp<Kind: DbKindT>(env: &DbWrite<Kind>) {
    let mut tmp = std::env::temp_dir();
    tmp.push("test_dbs");
    std::fs::create_dir(&tmp).ok();
    tmp.push("backup.sqlite");
    println!("dumping db to {}", tmp.display());
    std::fs::write(&tmp, b"").unwrap();
    env.read_async(move |txn| -> DatabaseResult<usize> {
        Ok(txn.execute("VACUUM main into ?", [tmp.to_string_lossy()])?)
    })
    .await
    .unwrap();
}

/// A container for all three non-cell environments
pub struct TestDbs {
    /// A test conductor environment
    conductor: DbWrite<DbKindConductor>,
    /// A test wasm environment
    wasm: DbWrite<DbKindWasm>,
    /// A test p2p environment
    p2p: Arc<parking_lot::Mutex<HashMap<Arc<KitsuneSpace>, DbWrite<DbKindP2pAgents>>>>,
    /// A test p2p environment
    p2p_metrics: Arc<parking_lot::Mutex<HashMap<Arc<KitsuneSpace>, DbWrite<DbKindP2pMetrics>>>>,
    /// The shared root temp dir for these environments
    dir: TestDir,
    /// The keystore sender for these environments
    keystore: MetaLairClient,
}

#[derive(Debug)]
pub enum TestDir {
    Temp(TempDir),
    Perm(PathBuf),
    Blank,
}

impl AsRef<Path> for TestDir {
    fn as_ref(&self) -> &Path {
        match self {
            Self::Temp(d) => d.path(),
            Self::Perm(d) => d.as_path(),
            Self::Blank => unreachable!(),
        }
    }
}

impl std::ops::Deref for TestDir {
    type Target = Path;

    fn deref(&self) -> &Path {
        match self {
            Self::Temp(d) => d.path(),
            Self::Perm(d) => d.as_path(),
            Self::Blank => unreachable!(),
        }
    }
}

impl From<TempDir> for TestDir {
    fn from(d: TempDir) -> Self {
        Self::new(d)
    }
}

impl TestDir {
    pub fn new(d: TempDir) -> Self {
        Self::Temp(d)
    }

    pub fn persist(&mut self) {
        let old = std::mem::replace(self, Self::Blank);
        match old {
            Self::Temp(d) => {
                println!("Made temp dir permanent at {:?}", d);
                tracing::info!("Made temp dir permanent at {:?}", d);
                *self = Self::Perm(d.into_path());
            }
            old => *self = old,
        }
    }
}

#[allow(missing_docs)]
impl TestDbs {
    /// Create all four non-cell environments at once with a custom keystore
    pub fn with_keystore(tempdir: TempDir, keystore: MetaLairClient) -> Self {
        let conductor = DbWrite::test(tempdir.path(), DbKindConductor).unwrap();
        let wasm = DbWrite::test(tempdir.path(), DbKindWasm).unwrap();
        let p2p = Arc::new(parking_lot::Mutex::new(HashMap::new()));
        let p2p_metrics = Arc::new(parking_lot::Mutex::new(HashMap::new()));
        Self {
            conductor,
            wasm,
            p2p,
            p2p_metrics,
            dir: TestDir::new(tempdir),
            keystore,
        }
    }

    /// Create all three non-cell environments at once with a test keystore
    pub fn new(tempdir: TempDir) -> Self {
        Self::with_keystore(tempdir, holochain_keystore::test_keystore())
    }

    pub fn conductor(&self) -> DbWrite<DbKindConductor> {
        self.conductor.clone()
    }

    pub fn wasm(&self) -> DbWrite<DbKindWasm> {
        self.wasm.clone()
    }

    pub fn p2p(
        &self,
    ) -> Arc<parking_lot::Mutex<HashMap<Arc<KitsuneSpace>, DbWrite<DbKindP2pAgents>>>> {
        self.p2p.clone()
    }

    pub fn p2p_metrics(
        &self,
    ) -> Arc<parking_lot::Mutex<HashMap<Arc<KitsuneSpace>, DbWrite<DbKindP2pMetrics>>>> {
        self.p2p_metrics.clone()
    }

    /// Get the root path for these environments
    pub fn path(&self) -> &Path {
        &self.dir
    }

    pub fn keystore(&self) -> &MetaLairClient {
        &self.keystore
    }
}

/// Produce file and line number info at compile-time
#[macro_export]
macro_rules! here {
    ($test: expr) => {
        concat!($test, " !!!_LOOK HERE:---> ", file!(), ":", line!())
    };
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(txn)))]
pub fn dump_db(txn: &Transaction) {
    let dump = |mut stmt: Statement| {
        let mut rows = stmt.query([]).unwrap();
        while let Some(row) = rows.next().unwrap() {
            for column in row.as_ref().column_names() {
                let row = row.get_ref_unwrap(column);
                match row {
                    holochain_sqlite::rusqlite::types::ValueRef::Null
                    | holochain_sqlite::rusqlite::types::ValueRef::Integer(_)
                    | holochain_sqlite::rusqlite::types::ValueRef::Real(_) => {
                        tracing::debug!(?column, ?row);
                    }
                    holochain_sqlite::rusqlite::types::ValueRef::Text(text) => {
                        tracing::debug!(?column, row = ?String::from_utf8_lossy(text));
                    }
                    holochain_sqlite::rusqlite::types::ValueRef::Blob(blob) => {
                        let blob = URL_SAFE_NO_PAD.encode(blob);
                        tracing::debug!("column: {:?} row:{}", column, blob);
                    }
                }
            }
        }
    };
    tracing::debug!("Actions:");
    let stmt = txn.prepare("SELECT * FROM Action").unwrap();
    dump(stmt);

    tracing::debug!("Entries:");
    let stmt = txn.prepare("SELECT * FROM Entry").unwrap();
    dump(stmt);

    tracing::debug!("DhtOps:");
    let stmt = txn.prepare("SELECT * FROM DhtOp").unwrap();
    dump(stmt);
}



================================================
File: crates/holochain_state/src/validation_db.rs
================================================
//! # Validation Database Types

use holochain_serialized_bytes::prelude::*;
use holochain_sqlite::rusqlite::ToSql;

/// The status of a [`DhtOp`](holochain_types::dht_op::DhtOp) in limbo
#[derive(Clone, Debug, Serialize, Deserialize, Eq, PartialEq)]
pub enum ValidationStage {
    /// Is awaiting to be system validated
    Pending,
    /// Is waiting for dependencies so the op can proceed to system validation
    AwaitingSysDeps,
    /// Is awaiting to be app validated
    SysValidated,
    /// Is waiting for dependencies so the op can proceed to app validation
    AwaitingAppDeps,
    /// Is awaiting to be integrated.
    AwaitingIntegration,
}

impl ToSql for ValidationStage {
    fn to_sql(
        &self,
    ) -> holochain_sqlite::rusqlite::Result<holochain_sqlite::rusqlite::types::ToSqlOutput> {
        let stage = match self {
            ValidationStage::Pending => None,
            ValidationStage::AwaitingSysDeps => Some(0),
            ValidationStage::SysValidated => Some(1),
            ValidationStage::AwaitingAppDeps => Some(2),
            ValidationStage::AwaitingIntegration => Some(3),
        };
        Ok(holochain_sqlite::rusqlite::types::ToSqlOutput::Owned(
            stage.into(),
        ))
    }
}



================================================
File: crates/holochain_state/src/validation_receipts.rs
================================================
//! Module for items related to aggregating validation_receipts

use holo_hash::DhtOpHash;
use holo_hash::{ActionHash, AgentPubKey};
use holochain_sqlite::prelude::*;
use holochain_sqlite::rusqlite::OptionalExtension;
use holochain_sqlite::rusqlite::Transaction;
use holochain_sqlite::rusqlite::{named_params, Params, Statement};
use holochain_types::dht_op::DhtOpType;
use holochain_types::prelude::{SignedValidationReceipt, ValidationReceipt};
use holochain_zome_types::prelude::{ValidationReceiptInfo, ValidationReceiptSet};
use mutations::StateMutationResult;
use std::collections::HashMap;

use crate::mutations;
use crate::prelude::*;

pub fn list_receipts(
    txn: &Transaction,
    op_hash: &DhtOpHash,
) -> StateQueryResult<Vec<SignedValidationReceipt>> {
    let mut stmt = txn.prepare(
        "
        SELECT blob FROM ValidationReceipt WHERE op_hash = :op_hash
        ",
    )?;
    let iter = stmt.query_and_then(
        named_params! {
            ":op_hash": op_hash
        },
        |row| from_blob::<SignedValidationReceipt>(row.get("blob")?),
    )?;
    iter.collect()
}

pub fn count_valid(txn: &Transaction, op_hash: &DhtOpHash) -> DatabaseResult<usize> {
    let count: usize = txn
        .query_row(
            "SELECT COUNT(hash) FROM ValidationReceipt WHERE op_hash = :op_hash",
            named_params! {
                ":op_hash": op_hash
            },
            |row| row.get(0),
        )
        .optional()?
        .unwrap_or(0);
    Ok(count)
}

pub fn add_if_unique(
    txn: &mut Transaction,
    receipt: SignedValidationReceipt,
) -> StateMutationResult<()> {
    mutations::insert_validation_receipt(txn, receipt)
}

pub fn get_pending_validation_receipts(
    txn: &Transaction,
    validators: Vec<AgentPubKey>,
) -> StateQueryResult<Vec<(ValidationReceipt, AgentPubKey)>> {
    let mut stmt = txn.prepare(
        "
            SELECT Action.author, DhtOp.hash, DhtOp.validation_status,
            DhtOp.when_integrated
            From DhtOp
            JOIN Action ON DhtOp.action_hash = Action.hash
            WHERE
            DhtOp.require_receipt = 1
            AND
            DhtOp.when_integrated IS NOT NULL
            AND
            DhtOp.validation_status IS NOT NULL
            ",
    )?;

    let ops = stmt
        .query_and_then([], |r| {
            let author: AgentPubKey = r.get("author")?;
            let dht_op_hash: DhtOpHash = r.get("hash")?;
            let validation_status = r.get("validation_status")?;
            // NB: timestamp will never be null, so this is OK
            let when_integrated = r.get("when_integrated")?;
            Ok((
                ValidationReceipt {
                    dht_op_hash,
                    validation_status,
                    validators: validators.clone(),
                    when_integrated,
                },
                author,
            ))
        })?
        .collect::<StateQueryResult<Vec<_>>>()?;

    Ok(ops)
}

/// Finds [DhtOp]s for the given [ActionHash] and returns the associated [ValidationReceiptSet]s.
///
/// Each [ValidationReceiptSet] contains the validation receipts we have received for a single [DhtOp].
/// If we have received enough validation receipts for an op, then its validation receipt set will
/// have the `receipts_complete` field set to `true`.
pub fn validation_receipts_for_action(
    txn: &Transaction,
    action_hash: ActionHash,
) -> StateQueryResult<Vec<ValidationReceiptSet>> {
    let stmt = txn.prepare(
        "
            SELECT
              ValidationReceipt.blob as receipt,
              DhtOp.hash as op_hash,
              DhtOp.type as op_type,
              DhtOp.receipts_complete as op_receipts_complete
            FROM
              Action
              INNER JOIN DhtOp ON DhtOp.action_hash = Action.hash
              INNER JOIN ValidationReceipt ON DhtOp.hash = ValidationReceipt.op_hash
            WHERE
              Action.hash = :action_hash
            ",
    )?;

    query_validation_receipts(
        stmt,
        named_params! {
            ":action_hash": action_hash
        },
    )
}

fn query_validation_receipts<P: Params>(
    mut stmt: Statement,
    params: P,
) -> StateQueryResult<Vec<ValidationReceiptSet>> {
    let db_result = stmt
        .query_and_then(params, |row| {
            let receipt = from_blob::<SignedValidationReceipt>(row.get("receipt")?)?;
            let op_hash: DhtOpHash = row.get("op_hash")?;
            let op_type: DhtOpType = row.get("op_type")?;
            let receipts_complete: Option<bool> = row.get("op_receipts_complete")?;

            Ok((
                receipt,
                op_hash,
                op_type,
                receipts_complete.unwrap_or(false),
            ))
        })?
        .collect::<StateQueryResult<Vec<_>>>()?;
    Ok(db_result
        .into_iter()
        .filter_map(
            |(receipt, op_hash, op_type, receipts_complete)| match op_type {
                DhtOpType::Chain(op_type) => Some((
                    op_hash,
                    op_type.to_string(),
                    receipts_complete,
                    ValidationReceiptInfo {
                        validation_status: receipt.receipt.validation_status,
                        validators: receipt.receipt.validators,
                    },
                )),
                _ => None,
            },
        )
        .fold(HashMap::new(), |mut acc, item| {
            acc.entry(item.0.clone())
                .or_insert_with(|| ValidationReceiptSet {
                    op_hash: item.0,
                    op_type: item.1.clone(),
                    receipts_complete: item.2,
                    receipts: Vec::new(),
                })
                .receipts
                .push(item.3);
            acc
        })
        .into_values()
        .collect())
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::mutations::set_when_integrated;
    use crate::prelude::*;
    use ::fixt::prelude::*;
    use holo_hash::fixt::AgentPubKeyFixturator;
    use holo_hash::{HasHash, HoloHashOf};
    use holochain_keystore::{test_keystore, MetaLairClient};
    use std::collections::HashSet;

    async fn fake_vr(
        dht_op_hash: &DhtOpHash,
        keystore: &MetaLairClient,
    ) -> SignedValidationReceipt {
        let agent = keystore.new_sign_keypair_random().await.unwrap();
        let receipt = ValidationReceipt {
            dht_op_hash: dht_op_hash.clone(),
            validation_status: ValidationStatus::Valid,
            validators: vec![agent],
            when_integrated: Timestamp::now(),
        };
        receipt.sign(keystore).await.unwrap().unwrap()
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_validation_receipts_db_populate_and_list() -> StateMutationResult<()> {
        holochain_trace::test_run();

        let test_db = crate::test_utils::test_authored_db();
        let env = test_db.to_db();
        let keystore = test_keystore();

        let op = DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(
            fixt!(Signature),
            fixt!(Action),
        ));
        let test_op_hash = op.as_hash().clone();
        env.write_async(move |txn| mutations::insert_op_authored(txn, &op))
            .await
            .unwrap();

        let vr1 = fake_vr(&test_op_hash, &keystore).await;
        let vr2 = fake_vr(&test_op_hash, &keystore).await;

        env.write_async({
            let put_vr1 = vr1.clone();
            let put_vr2 = vr2.clone();

            move |txn| {
                add_if_unique(txn, put_vr1.clone())?;
                add_if_unique(txn, put_vr1.clone())?;
                add_if_unique(txn, put_vr2.clone())
            }
        })
        .await?;

        env.write_async({
            let put_vr1 = vr1.clone();

            move |txn| add_if_unique(txn, put_vr1)
        })
        .await?;

        env.read_async(move |txn| -> DatabaseResult<()> {
            assert_eq!(2, count_valid(txn, &test_op_hash).unwrap());

            let mut list = list_receipts(txn, &test_op_hash).unwrap();
            list.sort_by(|a, b| {
                a.receipt.validators[0]
                    .partial_cmp(&b.receipt.validators[0])
                    .unwrap()
            });

            let mut expects = vec![vr1, vr2];
            expects.sort_by(|a, b| {
                a.receipt.validators[0]
                    .partial_cmp(&b.receipt.validators[0])
                    .unwrap()
            });

            assert_eq!(expects, list);

            Ok(())
        })
        .await
        .unwrap();
        Ok(())
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn no_pending_receipts() {
        holochain_trace::test_run();

        let env = crate::test_utils::test_dht_db().to_db();

        // With no validators
        let pending = env
            .read_async(|txn| get_pending_validation_receipts(txn, vec![]))
            .await
            .unwrap();

        assert!(pending.is_empty());

        // Same result with validators
        let pending = env
            .read_async(|txn| get_pending_validation_receipts(txn, vec![fixt!(AgentPubKey)]))
            .await
            .unwrap();

        assert!(pending.is_empty());
    }

    async fn create_modified_op(
        vault: DbWrite<DbKindDht>,
        modifier: fn(
            txn: &mut Txn<DbKindDht>,
            op_hash: HoloHashOf<DhtOp>,
        ) -> StateMutationResult<()>,
    ) -> StateMutationResult<DhtOpHash> {
        // The actual op does not matter, just some of the status fields
        let op = DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(
            fixt!(Signature),
            fixt!(Action),
        ));

        let test_op_hash = op.as_hash().clone();
        vault
            .write_async({
                let test_op_hash = test_op_hash.clone();
                move |txn| -> StateMutationResult<()> {
                    mutations::insert_op_dht(txn, &op, None)?;
                    modifier(txn, test_op_hash)?;

                    Ok(())
                }
            })
            .await
            .unwrap();

        Ok(test_op_hash)
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn filter_for_pending_validation_receipts() {
        holochain_trace::test_run();

        let test_db = crate::test_utils::test_dht_db();
        let env = test_db.to_db();

        // Has not been integrated yet
        create_modified_op(env.clone(), |_txn, _hash| {
            // Do nothing
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent
        let valid_op_hash = create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, true)?;
            set_when_integrated(txn, &op_hash, Timestamp::now())?;
            set_validation_status(txn, &op_hash, ValidationStatus::Valid)?;
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent, with rejected status
        let rejected_op_hash = create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, true)?;
            set_when_integrated(txn, &op_hash, Timestamp::now())?;
            set_validation_status(txn, &op_hash, ValidationStatus::Rejected)?;
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent, with abandoned status
        let abandoned_op_hash = create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, true)?;
            set_when_integrated(txn, &op_hash, Timestamp::now())?;
            set_validation_status(txn, &op_hash, ValidationStatus::Abandoned)?;
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent, but does not require one
        create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, false)?;
            set_when_integrated(txn, &op_hash, Timestamp::now())?;
            set_validation_status(txn, &op_hash, ValidationStatus::Valid)?;
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent, but when_integrated was not set
        create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, true)?;
            set_validation_status(txn, &op_hash, ValidationStatus::Valid)?;
            Ok(())
        })
        .await
        .unwrap();

        // Is ready to have a receipt sent, but validation_status was not set
        create_modified_op(env.clone(), |txn, op_hash| {
            set_require_receipt(txn, &op_hash, true)?;
            set_when_integrated(txn, &op_hash, Timestamp::now())?;
            Ok(())
        })
        .await
        .unwrap();

        let pending = env
            .read_async(
                move |txn| -> StateQueryResult<Vec<(ValidationReceipt, AgentPubKey)>> {
                    get_pending_validation_receipts(txn, vec![])
                },
            )
            .await
            .unwrap();

        assert_eq!(3, pending.len());

        let pending_ops: HashSet<DhtOpHash> =
            pending.into_iter().map(|p| p.0.dht_op_hash).collect();
        assert!(pending_ops.contains(&valid_op_hash));
        assert!(pending_ops.contains(&rejected_op_hash));
        assert!(pending_ops.contains(&abandoned_op_hash));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn validation_receipts_from_action() {
        let test_db = test_dht_db();
        let env = test_db.to_db();

        let keystore = test_keystore();

        let action = fixt!(Action);

        let action_hash = ActionHash::with_data_sync(&action);
        let op = DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(
            fixt!(Signature),
            action,
        ));
        let test_op_hash = op.as_hash().clone();
        env.write_async(move |txn| insert_op_dht(txn, &op, None))
            .await
            .unwrap();

        let vr1 = fake_vr(&test_op_hash, &keystore).await;
        let vr2 = fake_vr(&test_op_hash, &keystore).await;

        env.write_async({
            let put_vr1 = vr1.clone();
            let put_vr2 = vr2.clone();

            move |txn| -> StateMutationResult<()> {
                add_if_unique(txn, put_vr1.clone())?;
                add_if_unique(txn, put_vr2.clone())?;
                Ok(())
            }
        })
        .await
        .unwrap();

        let receipt_sets = env
            .read_async(move |txn| validation_receipts_for_action(txn, action_hash))
            .await
            .unwrap();

        check_receipt_sets(receipt_sets, test_op_hash, vr1, vr2);
    }

    fn check_receipt_sets(
        receipt_sets: Vec<ValidationReceiptSet>,
        test_op_hash: DhtOpHash,
        vr1: SignedValidationReceipt,
        vr2: SignedValidationReceipt,
    ) {
        assert_eq!(receipt_sets.len(), 1);

        assert_eq!(test_op_hash, receipt_sets[0].op_hash);
        assert_eq!("RegisterAgentActivity", receipt_sets[0].op_type);

        let receipts_count = receipt_sets[0].receipts.len();
        assert_eq!(receipts_count, 2);

        assert_eq!(
            vr1.receipt.validation_status,
            receipt_sets[0].receipts[0].validation_status
        );
        assert_eq!(1, receipt_sets[0].receipts[0].validators.len());
        assert_eq!(
            vr2.receipt.validation_status,
            receipt_sets[0].receipts[1].validation_status
        );
        assert_eq!(1, receipt_sets[0].receipts[1].validators.len());
    }
}



================================================
File: crates/holochain_state/src/wasm.rs
================================================
use std::sync::Arc;

use holo_hash::WasmHash;
use holochain_sqlite::rusqlite::named_params;
use holochain_sqlite::rusqlite::OptionalExtension;
use holochain_sqlite::rusqlite::Transaction;
use holochain_types::prelude::*;

use crate::mutations;
use crate::prelude::StateMutationResult;
use crate::prelude::StateQueryResult;

pub fn get(txn: &Transaction<'_>, hash: &WasmHash) -> StateQueryResult<Option<DnaWasmHashed>> {
    let item = txn
        .query_row(
            "SELECT hash, blob FROM Wasm WHERE hash = :hash",
            named_params! {
                ":hash": hash
            },
            |row| {
                let hash: WasmHash = row.get("hash")?;
                let wasm: Vec<u8> = row.get("blob")?;
                Ok((hash, wasm))
            },
        )
        .optional()?;
    match item {
        Some((hash, wasm)) => Ok(Some(DnaWasmHashed::with_pre_hashed(
            DnaWasm {
                code: Arc::new(wasm.into_boxed_slice()),
            },
            hash,
        ))),
        None => Ok(None),
    }
}

pub fn contains(txn: &Transaction<'_>, hash: &WasmHash) -> StateQueryResult<bool> {
    Ok(txn.query_row(
        "SELECT EXISTS(SELECT 1 FROM Wasm WHERE hash = :hash)",
        named_params! {
            ":hash": hash
        },
        |row| row.get(0),
    )?)
}

pub fn put(txn: &mut Transaction, wasm: DnaWasmHashed) -> StateMutationResult<()> {
    mutations::insert_wasm(txn, wasm)
}

#[cfg(test)]
mod tests {
    use super::*;
    use holochain_sqlite::prelude::DatabaseResult;
    use holochain_types::dna::wasm::DnaWasm;

    #[tokio::test(flavor = "multi_thread")]
    async fn wasm_store_round_trip() -> DatabaseResult<()> {
        holochain_trace::test_run();

        // all the stuff needed to have a WasmBuf
        let db = crate::test_utils::test_wasm_db();

        // a wasm
        let wasm =
            DnaWasmHashed::from_content(DnaWasm::from(holochain_wasm_test_utils::TestWasm::Foo))
                .await;

        // Put wasm
        db.write_async({
            let put_wasm = wasm.clone();

            move |txn| put(txn, put_wasm.clone())
        })
        .await
        .unwrap();
        db.read_async(move |txn| -> DatabaseResult<()> {
            assert!(contains(txn, wasm.as_hash()).unwrap());
            // a wasm from the WasmBuf
            let ret = get(txn, wasm.as_hash()).unwrap().unwrap();

            // assert the round trip
            assert_eq!(ret, wasm);

            Ok(())
        })
        .await?;

        Ok(())
    }
}



================================================
File: crates/holochain_state/src/workspace.rs
================================================
//! Workspaces are a simple abstraction used to stage changes during Workflow
//! execution to be persisted later
//!
//! Every Workflow has an associated Workspace type.

use super::source_chain::SourceChainError;
use holochain_sqlite::error::DatabaseError;
use thiserror::Error;

#[derive(Debug, Error)]
#[allow(missing_docs)]
pub enum WorkspaceError {
    #[error(transparent)]
    DatabaseError(#[from] DatabaseError),

    #[error(transparent)]
    SourceChainError(#[from] SourceChainError),

    #[error(transparent)]
    StateQueryError(#[from] crate::query::StateQueryError),

    #[error(transparent)]
    StateMutationError(#[from] crate::mutations::StateMutationError),
}

#[allow(missing_docs)]
pub type WorkspaceResult<T> = Result<T, WorkspaceError>;



================================================
File: crates/holochain_state/src/mutations/error.rs
================================================
use holochain_types::db_cache::DbCacheError;
use thiserror::Error;

use crate::query::StateQueryError;
#[derive(Error, Debug)]
pub enum StateMutationError {
    #[error(transparent)]
    Sql(#[from] holochain_sqlite::rusqlite::Error),
    #[error(transparent)]
    Infallible(#[from] std::convert::Infallible),
    #[error(transparent)]
    DatabaseError(#[from] holochain_sqlite::error::DatabaseError),
    #[error(transparent)]
    DbCacheError(#[from] DbCacheError),
    #[error(transparent)]
    DhtOpError(#[from] holochain_types::dht_op::DhtOpError),
    #[error(transparent)]
    StateQueryError(#[from] StateQueryError),
    #[error(transparent)]
    SerializedBytesError(#[from] holochain_serialized_bytes::SerializedBytesError),
    #[error(transparent)]
    ScheduleError(#[from] holochain_zome_types::schedule::ScheduleError),
    #[error("Authors of actions must all be the same when inserting to the source chain")]
    AuthorsMustMatch,
    #[error("Cannot remove a fully published countersigning session")]
    CannotRemoveFullyPublished,
}

pub type StateMutationResult<T> = Result<T, StateMutationError>;



================================================
File: crates/holochain_state/src/query/chain_head.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::*;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use crate::prelude::HeadInfo;

use super::Params;
use super::*;

#[derive(Debug, Clone)]
pub struct ChainHeadQuery(Arc<AgentPubKey>);

impl ChainHeadQuery {
    pub fn new(agent: Arc<AgentPubKey>) -> Self {
        Self(agent)
    }
}

impl Query for ChainHeadQuery {
    type Item = Judged<SignedActionHashed>;
    type State = Option<SignedActionHashed>;
    type Output = Option<HeadInfo>;

    fn query(&self) -> String {
        "
        SELECT Action.blob, Action.hash
        FROM Action
        JOIN DhtOp ON DhtOp.action_hash = Action.hash
        WHERE Action.author = :author AND Action.hash IS NOT NULL
        ORDER BY Action.seq DESC LIMIT 1
        "
        .into()
    }

    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":author": self.0,
        };
        params.to_vec()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(None)
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let author = self.0.clone();
        // NB: it's a little redundant to filter on author, since we should never
        // be putting any actions by other authors in our scratch, but it
        // certainly doesn't hurt to be consistent.
        let f = move |action: &SignedActionHashed| *action.action().author() == *author;
        Box::new(f)
    }

    fn fold(&self, state: Self::State, sh: Self::Item) -> StateQueryResult<Self::State> {
        // We don't need the validation status from this point.
        let sh = sh.data;
        // Simple maximum finding
        Ok(Some(match state {
            None => sh,
            Some(old) => {
                if sh.action().action_seq() > old.action().action_seq() {
                    sh
                } else {
                    old
                }
            }
        }))
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        Ok(state.map(|sh| {
            let seq = sh.action().action_seq();
            let timestamp = sh.action().timestamp();
            let action = sh.hashed.hash;
            HeadInfo {
                action,
                seq,
                timestamp,
            }
        }))
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_and_hash_to_action("blob", "hash");
        // Valid because the data is authored.
        Arc::new(move |r| Ok(Judged::valid(f(r)?)))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::mutations::{insert_action, insert_op_lite};
    use ::fixt::prelude::*;
    use holo_hash::fixt::AgentPubKeyFixturator;
    use holo_hash::fixt::DhtOpHashFixturator;
    use holochain_sqlite::schema::SCHEMA_CELL;
    use holochain_types::dht_op::DhtOpLite;
    use holochain_types::dht_op::OpOrder;

    #[test]
    fn test_chain_head_query() {
        holochain_trace::test_run();
        let mut conn = Connection::open_in_memory().unwrap();
        SCHEMA_CELL.initialize(&mut conn, None).unwrap();

        let mut txn = conn
            .transaction_with_behavior(TransactionBehavior::Exclusive)
            .unwrap();

        let author = fixt!(AgentPubKey);

        // Create 5 consecutive actions for the authoring agent,
        // as well as 5 other random actions, interspersed.
        let mut actions: Vec<_> = vec![
            fixt!(ActionBuilderCommon),
            fixt!(ActionBuilderCommon),
            fixt!(ActionBuilderCommon),
            fixt!(ActionBuilderCommon),
            fixt!(ActionBuilderCommon),
        ]
        .into_iter()
        .enumerate()
        .flat_map(|(seq, random_action)| {
            let mut chain_action = random_action.clone();
            chain_action.action_seq = seq as u32;
            chain_action.author = author.clone();
            vec![chain_action, random_action]
        })
        .map(|b| {
            SignedActionHashed::with_presigned(
                // A chain made entirely of InitZomesComplete actions is totally invalid,
                // but we don't need a valid chain for this test,
                // we just need an ordered sequence of actions
                ActionHashed::from_content_sync(InitZomesComplete::from_builder(b)),
                fixt!(Signature),
            )
        })
        .collect();

        // Other actions have a different author, so the 9th action should be the head for our author's chain
        let expected_head = actions[8].clone();
        // Shuffle so the head will sometimes be in scratch and sometimes be in the database and not always the last action by our author.
        actions.shuffle(&mut thread_rng());

        for action in &actions[..6] {
            let hash = action.action_address();
            let op = DhtOpLite::from(ChainOpLite::StoreRecord(
                hash.clone(),
                None,
                hash.clone().into(),
            ));
            let op_order = OpOrder::new(op.get_type(), action.action().timestamp());
            insert_action(&mut txn, action).unwrap();
            insert_op_lite(
                &mut txn,
                &op,
                &fixt!(DhtOpHash),
                &op_order,
                &action.action().timestamp(),
                None,
            )
            .unwrap();
        }

        let mut scratch = Scratch::new();

        // It's also totally invalid for a call_zome scratch to contain actions
        // from other authors, but it doesn't matter here
        for action in &actions[6..] {
            scratch.add_action(action.clone(), ChainTopOrdering::default());
        }

        let query = ChainHeadQuery::new(Arc::new(author));

        let head = query.run(DbScratch::new(&[&mut txn], &scratch)).unwrap();
        assert_eq!(
            head.unwrap(),
            HeadInfo {
                action: expected_head.as_hash().clone(),
                seq: expected_head.action().action_seq(),
                timestamp: expected_head.action().timestamp()
            }
        );
    }
}



================================================
File: crates/holochain_state/src/query/entry_details.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::named_params;
use holochain_types::dht_op::ChainOpType;
use holochain_types::prelude::DhtOpError;
use holochain_types::prelude::Judged;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::*;

#[derive(Debug, Clone)]
pub struct GetEntryDetailsQuery(EntryHash, Option<Arc<AgentPubKey>>);

impl GetEntryDetailsQuery {
    pub fn new(hash: EntryHash) -> Self {
        Self(hash, None)
    }
}

pub struct State {
    actions: HashSet<SignedActionHashed>,
    rejected_actions: HashSet<SignedActionHashed>,
    deletes: HashMap<ActionHash, SignedActionHashed>,
    updates: HashSet<SignedActionHashed>,
}

impl Query for GetEntryDetailsQuery {
    type Item = Judged<SignedActionHashed>;
    type State = State;
    type Output = Option<EntryDetails>;

    fn query(&self) -> String {
        "
        SELECT Action.blob AS action_blob, DhtOp.validation_status AS status
        FROM DhtOp
        JOIN Action On DhtOp.action_hash = Action.hash
        WHERE DhtOp.type IN (:create_type, :delete_type, :update_type)
        AND DhtOp.basis_hash = :entry_hash
        AND DhtOp.when_integrated IS NOT NULL
        AND DhtOp.validation_status IS NOT NULL
        AND (Action.private_entry = 0 OR Action.private_entry IS NULL OR Action.author = :author)
        "
        .into()
    }
    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":create_type": ChainOpType::StoreEntry,
            ":delete_type": ChainOpType::RegisterDeletedEntryAction,
            ":update_type": ChainOpType::RegisterUpdatedContent,
            ":entry_hash": self.0,
            ":author": self.1,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = |row: &Row| {
            let action =
                from_blob::<SignedAction>(row.get(row.as_ref().column_index("action_blob")?)?)?;
            let (action, signature) = action.into();
            let action = ActionHashed::from_content_sync(action);
            let shh = SignedActionHashed::with_presigned(action, signature);
            let status = row.get(row.as_ref().column_index("status")?)?;
            let r = Judged::new(shh, status);
            Ok(r)
        };
        Arc::new(f)
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let entry_filter = self.0.clone();
        let f = move |action: &QueryData<Self>| {
            let action = &action;
            match action.action() {
                Action::Create(Create { entry_hash, .. })
                | Action::Update(Update { entry_hash, .. })
                    if *entry_hash == entry_filter =>
                {
                    true
                }
                Action::Update(Update {
                    original_entry_address,
                    ..
                }) => *original_entry_address == entry_filter,
                Action::Delete(Delete {
                    deletes_entry_address,
                    ..
                }) => *deletes_entry_address == entry_filter,
                _ => false,
            }
        };
        Box::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(State {
            actions: Default::default(),
            rejected_actions: Default::default(),
            deletes: Default::default(),
            updates: Default::default(),
        })
    }

    fn fold(&self, mut state: Self::State, item: Self::Item) -> StateQueryResult<Self::State> {
        let (shh, validation_status) = item.into();
        let add_action = |state: &mut State, shh| match validation_status {
            Some(ValidationStatus::Valid) => {
                state.actions.insert(shh);
            }
            Some(ValidationStatus::Rejected) => {
                state.rejected_actions.insert(shh);
            }
            _ => (),
        };
        match shh.action() {
            Action::Create(_) => add_action(&mut state, shh),
            Action::Update(update) => {
                if update.original_entry_address == self.0 && update.entry_hash == self.0 {
                    state.updates.insert(shh.clone());
                    add_action(&mut state, shh);
                } else if update.entry_hash == self.0 {
                    add_action(&mut state, shh);
                } else if update.original_entry_address == self.0 {
                    state.updates.insert(shh.clone());
                }
            }
            Action::Delete(delete) => {
                let hash = delete.deletes_address.clone();
                state.deletes.insert(hash, shh.clone());
            }
            _ => {
                return Err(StateQueryError::UnexpectedAction(
                    shh.action().action_type(),
                ))
            }
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        // Choose an arbitrary action.
        // TODO: Is it sound to us a rejected action here?
        let action = state
            .actions
            .iter()
            .chain(state.rejected_actions.iter())
            .next();
        match action {
            Some(action) => {
                let entry_hash = action
                    .action()
                    .entry_hash()
                    .ok_or_else(|| DhtOpError::ActionWithoutEntry(action.action().clone()))?;
                let author = self.1.as_ref().map(|a| a.as_ref());
                let details = stores
                    .get_public_or_authored_entry(entry_hash, author)?
                    .map(|entry| {
                        let entry_dht_status = compute_entry_status(&state);
                        EntryDetails {
                            entry,
                            actions: state.actions.into_iter().collect(),
                            rejected_actions: state.rejected_actions.into_iter().collect(),
                            deletes: state.deletes.into_values().collect(),
                            updates: state.updates.into_iter().collect(),
                            entry_dht_status,
                        }
                    });
                Ok(details)
            }
            None => Ok(None),
        }
    }
}

fn compute_entry_status(state: &State) -> EntryDhtStatus {
    let live_actions = state
        .actions
        .iter()
        .filter(|h| !state.deletes.contains_key(h.action_address()))
        .count();
    if live_actions > 0 {
        EntryDhtStatus::Live
    } else {
        EntryDhtStatus::Dead
    }
}

impl PrivateDataQuery for GetEntryDetailsQuery {
    type Hash = EntryHash;

    fn with_private_data_access(hash: Self::Hash, author: Arc<AgentPubKey>) -> Self {
        Self(hash, Some(author))
    }

    fn without_private_data_access(hash: Self::Hash) -> Self {
        Self::new(hash)
    }
}



================================================
File: crates/holochain_state/src/query/error.rs
================================================
use holochain_types::prelude::*;
use thiserror::Error;

use crate::scratch::SyncScratchError;
#[derive(Error, Debug)]
pub enum StateQueryError {
    #[error(transparent)]
    Sql(#[from] holochain_sqlite::rusqlite::Error),
    #[error(transparent)]
    Infallible(#[from] std::convert::Infallible),
    #[error(transparent)]
    DatabaseError(#[from] holochain_sqlite::error::DatabaseError),
    #[error(transparent)]
    SerializedBytesError(#[from] holochain_serialized_bytes::SerializedBytesError),
    #[error(transparent)]
    DhtOpError(#[from] holochain_types::dht_op::DhtOpError),
    #[error("Unexpected op {0:?} for query")]
    UnexpectedOp(ChainOpType),
    #[error("Unexpected action {0:?} for query")]
    UnexpectedAction(ActionType),
    #[error(transparent)]
    WrongActionError(#[from] holochain_zome_types::prelude::WrongActionError),
    #[error(transparent)]
    ActionError(#[from] holochain_zome_types::prelude::ActionError),
    #[error(transparent)]
    SyncScratchError(#[from] SyncScratchError),
    #[error("{0}")]
    Other(String),
}

pub type StateQueryResult<T> = Result<T, StateQueryError>;



================================================
File: crates/holochain_state/src/query/link.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::named_params;
use holochain_types::dht_op::ChainOpType;
use holochain_types::sql::ToSqlStatement;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::*;

#[derive(Debug, Clone)]
pub struct GetLinksQuery {
    query: LinksQuery,
}

#[derive(Debug, Clone, Default)]
pub struct GetLinksFilter {
    pub after: Option<Timestamp>,
    pub before: Option<Timestamp>,
    pub author: Option<AgentPubKey>,
}

#[derive(Debug, Clone)]
pub struct LinksQuery {
    pub base: Arc<AnyLinkableHash>,
    pub type_query: LinkTypeFilter,
    pub tag: Option<String>,
    filter: GetLinksFilter,
    query: String,
}

impl LinksQuery {
    pub fn new(
        base: AnyLinkableHash,
        type_query: LinkTypeFilter,
        tag: Option<LinkTag>,
        filter: GetLinksFilter,
    ) -> Self {
        let tag = tag.map(|tag| Self::tag_to_hex(&tag));
        let create_string = Self::create_query_string(&type_query, tag.clone(), &filter);
        let delete_string = Self::delete_query_string(&type_query, tag.clone());
        Self {
            base: Arc::new(base),
            type_query,
            tag,
            filter,
            query: Self::create_query(create_string, delete_string),
        }
    }

    pub fn tag_to_hex(tag: &LinkTag) -> String {
        use std::fmt::Write;
        let mut s = String::with_capacity(tag.0.len());
        for b in &tag.0 {
            write!(&mut s, "{:02X}", b).ok();
        }
        s
    }

    pub fn base(base: AnyLinkableHash, dependencies: Vec<ZomeIndex>) -> Self {
        Self::new(
            base,
            LinkTypeFilter::Dependencies(dependencies),
            None,
            GetLinksFilter::default(),
        )
    }

    fn create_query(create: String, delete: String) -> String {
        format!("{} UNION ALL {}", create, delete)
    }

    pub fn query(&self) -> String {
        self.query.clone()
    }

    fn common_query_string() -> &'static str {
        "
            JOIN Action On DhtOp.action_hash = Action.hash
            WHERE DhtOp.type = :create
            AND
            Action.base_hash = :base_hash
            AND
            DhtOp.validation_status = :status
            AND DhtOp.when_integrated IS NOT NULL
        "
    }

    fn create_query_string(
        type_query: &LinkTypeFilter,
        tag: Option<String>,
        filter: &GetLinksFilter,
    ) -> String {
        let mut s = format!(
            "
            SELECT Action.blob AS action_blob FROM DhtOp
            {}
            ",
            Self::common_query_string()
        );
        s = Self::add_type_query(s, type_query);
        s = Self::add_tag(s, tag);
        s = Self::add_after(s, filter.after);
        s = Self::add_before(s, filter.before);
        s = Self::add_author(s, filter.author.as_ref());

        s
    }

    fn add_type_query(q: String, type_query: &LinkTypeFilter) -> String {
        format!("{} {} ", q, type_query.to_sql_statement())
    }

    fn add_tag(q: String, tag: Option<String>) -> String {
        match tag {
            Some(tag) => {
                format!(
                    "{}
                    AND
                    HEX(Action.tag) like '{}%'",
                    q, tag
                )
            }
            None => q,
        }
    }

    fn add_after(q: String, after: Option<Timestamp>) -> String {
        match after {
            Some(_) => format!("{} AND DhtOp.authored_timestamp >= :after", q),
            None => format!("{} AND :after IS NULL", q),
        }
    }

    fn add_before(q: String, before: Option<Timestamp>) -> String {
        match before {
            Some(_) => format!("{} AND DhtOp.authored_timestamp <= :before", q),
            None => format!("{} AND :before IS NULL", q),
        }
    }

    fn add_author(q: String, author: Option<&AgentPubKey>) -> String {
        match author {
            Some(_) => format!("{} AND Action.author = :author", q),
            None => format!("{} AND :author IS NULL", q),
        }
    }

    fn delete_query_string(type_query: &LinkTypeFilter, tag: Option<String>) -> String {
        let mut sub_create_query = format!(
            "
            SELECT Action.hash FROM DhtOp
            {}
            ",
            Self::common_query_string()
        );
        sub_create_query = Self::add_type_query(sub_create_query, type_query);
        sub_create_query = Self::add_tag(sub_create_query, tag);
        let delete_query = format!(
            "
            SELECT Action.blob AS action_blob FROM DhtOp
            JOIN Action On DhtOp.action_hash = Action.hash
            WHERE DhtOp.type = :delete
            AND
            Action.create_link_hash IN ({})
            AND
            DhtOp.validation_status = :status
            AND
            DhtOp.when_integrated IS NOT NULL
            ",
            sub_create_query
        );
        delete_query
    }

    pub fn params(&self) -> Vec<Params> {
        {
            named_params! {
                ":create": ChainOpType::RegisterAddLink,
                ":delete": ChainOpType::RegisterRemoveLink,
                ":status": ValidationStatus::Valid,
                ":base_hash": self.base,
                ":after": self.filter.after,
                ":before": self.filter.before,
                ":author": self.filter.author,
            }
        }
        .to_vec()
    }
}

impl GetLinksQuery {
    pub fn new(
        base: AnyLinkableHash,
        type_query: LinkTypeFilter,
        tag: Option<LinkTag>,
        filter: GetLinksFilter,
    ) -> Self {
        Self {
            query: LinksQuery::new(base, type_query, tag, filter),
        }
    }

    pub fn base(base: AnyLinkableHash, dependencies: Vec<ZomeIndex>) -> Self {
        Self {
            query: LinksQuery::base(base, dependencies),
        }
    }
}

impl Query for GetLinksQuery {
    type Item = Judged<SignedActionHashed>;
    type State = Maps<Link>;
    type Output = Vec<Link>;
    fn query(&self) -> String {
        self.query.query()
    }

    fn params(&self) -> Vec<Params> {
        self.query.params()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(Maps::new())
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_to_action("action_blob");
        // Data is valid because it is filtered in the sql query.
        Arc::new(move |row| Ok(Judged::valid(f(row)?)))
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let query = &self.query;
        let base_filter = query.base.clone();
        let type_query_filter = query.type_query.clone();
        let tag_filter = query.tag.clone();
        let f = move |action: &QueryData<Self>| match action.action() {
            Action::CreateLink(CreateLink {
                base_address,
                tag,
                zome_index,
                link_type,
                ..
            }) => {
                *base_address == *base_filter
                    && type_query_filter.contains(zome_index, link_type)
                    && tag_filter
                        .as_ref()
                        .map_or(true, |t| LinksQuery::tag_to_hex(tag).starts_with(&(**t)))
            }
            Action::DeleteLink(DeleteLink { base_address, .. }) => *base_address == *base_filter,
            _ => false,
        };
        Box::new(f)
    }

    fn fold(&self, mut state: Self::State, data: Self::Item) -> StateQueryResult<Self::State> {
        let shh = data.data;
        let (action, _) = shh.into_inner();
        let (action, hash) = action.into_inner();
        match action {
            Action::CreateLink(create_link) => {
                if !state.deletes.contains(&hash) {
                    state
                        .creates
                        .insert(hash, link_from_action(Action::CreateLink(create_link))?);
                }
            }
            Action::DeleteLink(delete_link) => {
                state.creates.remove(&delete_link.link_add_address);
                state.deletes.insert(delete_link.link_add_address);
            }
            _ => return Err(StateQueryError::UnexpectedAction(action.action_type())),
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        let mut links: Self::Output = state.creates.into_values().collect();
        links.sort_by_key(|l| l.timestamp);
        Ok(links)
    }
}

fn link_from_action(action: Action) -> StateQueryResult<Link> {
    let hash = ActionHash::with_data_sync(&action);
    match action {
        Action::CreateLink(action) => Ok(Link {
            author: action.author,
            base: action.base_address,
            target: action.target_address,
            timestamp: action.timestamp,
            zome_index: action.zome_index,
            link_type: action.link_type,
            tag: action.tag,
            create_link_hash: hash,
        }),
        _ => Err(StateQueryError::UnexpectedAction(action.action_type())),
    }
}



================================================
File: crates/holochain_state/src/query/link_count.rs
================================================
use crate::query::link::GetLinksFilter;
use holochain_types::link::WireLinkQuery;

// Note that link_count uses `GetLinksQuery`, so there is no query implemented here

impl From<WireLinkQuery> for GetLinksFilter {
    fn from(value: WireLinkQuery) -> Self {
        Self {
            before: value.before,
            after: value.after,
            author: value.author,
        }
    }
}



================================================
File: crates/holochain_state/src/query/link_details.rs
================================================
use crate::query::link::GetLinksFilter;
use holo_hash::*;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::link::LinksQuery;
use super::*;

#[derive(Debug, Clone)]
pub struct GetLinkDetailsQuery {
    query: LinksQuery,
}

impl GetLinkDetailsQuery {
    pub fn new(base: AnyLinkableHash, type_query: LinkTypeFilter, tag: Option<LinkTag>) -> Self {
        Self {
            query: LinksQuery::new(base, type_query, tag, GetLinksFilter::default()),
        }
    }
}

impl Query for GetLinkDetailsQuery {
    type Item = Judged<SignedActionHashed>;
    type State = HashMap<ActionHash, (Option<SignedActionHashed>, HashSet<SignedActionHashed>)>;
    type Output = Vec<(SignedActionHashed, Vec<SignedActionHashed>)>;
    fn query(&self) -> String {
        self.query.query()
    }

    fn params(&self) -> Vec<Params> {
        self.query.params()
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(HashMap::new())
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_to_action("action_blob");
        // Data is valid because it is filtered in the sql query.
        Arc::new(move |row| Ok(Judged::valid(f(row)?)))
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let query = &self.query;
        let base_filter = query.base.clone();
        let type_query_filter = query.type_query.clone();
        let tag_filter = query.tag.clone();
        let f = move |action: &QueryData<Self>| match action.action() {
            Action::CreateLink(CreateLink {
                base_address,
                tag,
                zome_index,
                link_type,
                ..
            }) => {
                *base_address == *base_filter
                    && type_query_filter.contains(zome_index, link_type)
                    && tag_filter
                        .as_ref()
                        .map_or(true, |t| LinksQuery::tag_to_hex(tag).starts_with(&(**t)))
            }
            Action::DeleteLink(DeleteLink { base_address, .. }) => *base_address == *base_filter,
            _ => false,
        };
        Box::new(f)
    }

    fn fold(&self, mut state: Self::State, data: Self::Item) -> StateQueryResult<Self::State> {
        let shh = data.data;
        let action = shh.action();
        match action {
            Action::CreateLink(_) => {
                state
                    .entry(shh.as_hash().clone())
                    .or_insert((Some(shh), HashSet::new()));
            }
            Action::DeleteLink(delete_link) => {
                let entry = state
                    .entry(delete_link.link_add_address.clone())
                    .or_insert((None, HashSet::new()));
                entry.1.insert(shh);
            }
            _ => {
                return Err(StateQueryError::UnexpectedAction(
                    shh.action().action_type(),
                ))
            }
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, _stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        // TODO: This could be done above by using BTMaps but deferring this optimization
        // because it's simpler .
        // Order by timestamp.
        let mut r = state
            .into_iter()
            .filter_map(|(_, (create, deletes))| {
                create.map(|create| {
                    let mut deletes = deletes.into_iter().collect::<Vec<_>>();
                    deletes.sort_by_key(|l| l.action().timestamp());
                    (create, deletes.into_iter().collect())
                })
            })
            .collect::<Vec<_>>();
        r.sort_by_key(|l| l.0.action().timestamp());
        Ok(r)
    }
}



================================================
File: crates/holochain_state/src/query/live_entry.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::named_params;
use holochain_types::dht_op::ChainOpType;
use holochain_types::prelude::DhtOpError;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::*;

#[cfg(test)]
mod test;

#[derive(Debug, Clone)]
pub struct GetLiveEntryQuery(EntryHash, Option<Arc<AgentPubKey>>);

impl GetLiveEntryQuery {
    pub fn new(hash: EntryHash) -> Self {
        Self(hash, None)
    }
}

impl Query for GetLiveEntryQuery {
    type Item = Judged<SignedActionHashed>;
    type State = Maps<SignedActionHashed>;
    type Output = Option<Record>;

    fn query(&self) -> String {
        "
        SELECT Action.blob AS action_blob
        FROM DhtOp
        JOIN Action On DhtOp.action_hash = Action.hash
        WHERE DhtOp.type IN (:create_type, :delete_type, :update_type)
        AND DhtOp.basis_hash = :entry_hash
        AND DhtOp.validation_status = :status
        AND DhtOp.when_integrated IS NOT NULL
        AND (Action.private_entry = 0 OR Action.private_entry IS NULL OR Action.author = :author)
        "
        .into()
    }

    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":create_type": ChainOpType::StoreEntry,
            ":delete_type": ChainOpType::RegisterDeletedEntryAction,
            ":update_type": ChainOpType::RegisterUpdatedContent,
            ":status": ValidationStatus::Valid,
            ":entry_hash": self.0,
            ":author": self.1,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_to_action("action_blob");
        // Data is valid because it is filtered in the sql query.
        Arc::new(move |row| Ok(Judged::valid(f(row)?)))
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let entry_filter = self.0.clone();
        let f = move |action: &QueryData<Self>| match action.action() {
            Action::Create(Create { entry_hash, .. })
            | Action::Update(Update { entry_hash, .. }) => *entry_hash == entry_filter,
            Action::Delete(Delete {
                deletes_entry_address,
                ..
            }) => *deletes_entry_address == entry_filter,
            _ => false,
        };
        Box::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(Maps::new())
    }

    fn fold(&self, mut state: Self::State, data: Self::Item) -> StateQueryResult<Self::State> {
        let shh = data.data;
        let hash = shh.as_hash().clone();
        match shh.action() {
            Action::Create(_) => {
                if !state.deletes.contains(&hash) {
                    state.creates.insert(hash, shh);
                }
            }
            Action::Update(update) => {
                if update.original_entry_address == self.0 && update.entry_hash == self.0 {
                    follow_update_chain(&state, &shh);
                    if !state.deletes.contains(&hash) {
                        state.creates.insert(hash, shh);
                    }
                } else if update.entry_hash == self.0 {
                    if !state.deletes.contains(&hash) {
                        state.creates.insert(hash, shh);
                    }
                } else if update.original_entry_address == self.0 {
                    follow_update_chain(&state, &shh);
                }
            }
            Action::Delete(delete) => {
                state.creates.remove(&delete.deletes_address);
                state.deletes.insert(delete.deletes_address.clone());
            }
            _ => {
                return Err(StateQueryError::UnexpectedAction(
                    shh.action().action_type(),
                ))
            }
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        // If we have author authority then find an action from this author.
        let authored_action = self.1.as_ref().map(|a| a.as_ref()).and_then(|a| {
            state
                .creates
                .values()
                .find(|h| *h.action().author() == *a)
                .cloned()
        });
        let is_authored = authored_action.is_some();
        // If there is no authored action, choose an arbitrary action.
        let action = authored_action.or_else(|| {
            // The line below was added when migrating to rust edition 2021, per
            // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
            let _ = &state;
            state.creates.into_values().next()
        });
        match action {
            Some(action) => {
                let entry_hash = action
                    .action()
                    .entry_hash()
                    .ok_or_else(|| DhtOpError::ActionWithoutEntry(action.action().clone()))?;
                // If this action is authored then we can get an authored entry.
                let author = is_authored.then(|| action.action().author());
                let record = stores
                    .get_public_or_authored_entry(entry_hash, author)?
                    .map(|entry| Record::new(action, Some(entry)));
                Ok(record)
            }
            None => Ok(None),
        }
    }
}

fn follow_update_chain(_state: &Maps<SignedActionHashed>, _shh: &SignedActionHashed) {
    // TODO: This is where update chains will be followed
    // when we add that functionality.
}

impl PrivateDataQuery for GetLiveEntryQuery {
    type Hash = EntryHash;

    fn with_private_data_access(hash: Self::Hash, author: Arc<AgentPubKey>) -> Self {
        Self(hash, Some(author))
    }

    fn without_private_data_access(hash: Self::Hash) -> Self {
        Self::new(hash)
    }
}



================================================
File: crates/holochain_state/src/query/live_record.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::named_params;
use holochain_types::dht_op::ChainOpType;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::*;

#[cfg(test)]
mod test;

#[derive(Debug, Clone)]
pub struct GetLiveRecordQuery(ActionHash, Option<Arc<AgentPubKey>>);

impl GetLiveRecordQuery {
    pub fn new(hash: ActionHash) -> Self {
        Self(hash, None)
    }
}

impl Query for GetLiveRecordQuery {
    type Item = Judged<SignedActionHashed>;
    type State = (Option<SignedActionHashed>, HashSet<ActionHash>);
    type Output = Option<Record>;

    fn query(&self) -> String {
        "
        SELECT Action.blob AS action_blob
        FROM DhtOp
        JOIN Action On DhtOp.action_hash = Action.hash
        WHERE DhtOp.type IN (:create_type, :delete_type, :update_type)
        AND DhtOp.basis_hash = :action_hash
        AND DhtOp.validation_status = :status
        AND DhtOp.when_integrated IS NOT NULL
        "
        .into()
    }
    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":create_type": ChainOpType::StoreRecord,
            ":delete_type": ChainOpType::RegisterDeletedBy,
            ":update_type": ChainOpType::RegisterUpdatedRecord,
            ":status": ValidationStatus::Valid,
            ":action_hash": self.0,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = row_blob_to_action("action_blob");
        // Data is valid because it is filtered in the sql query.
        Arc::new(move |row| Ok(Judged::valid(f(row)?)))
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let action_filter = self.0.clone();
        let f = move |action: &QueryData<Self>| {
            if *action.action_address() == action_filter {
                true
            } else if let Action::Delete(Delete {
                deletes_address, ..
            }) = action.action()
            {
                *deletes_address == action_filter
            } else {
                false
            }
        };
        Box::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok((None, HashSet::new()))
    }

    fn fold(&self, mut state: Self::State, data: Self::Item) -> StateQueryResult<Self::State> {
        let shh = data.data;
        let hash = shh.as_hash();
        if *hash == self.0 && state.0.is_none() {
            if !state.1.contains(hash) {
                state.0 = Some(shh);
            }
        } else if let Action::Delete(delete) = shh.action() {
            let action = state.0.take();
            if let Some(h) = action {
                if *h.as_hash() != delete.deletes_address {
                    state.0 = Some(h);
                }
            }
            state.1.insert(delete.deletes_address.clone());
        }
        Ok(state)
    }

    fn render<S>(&self, state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        match state.0 {
            Some(action) => {
                let mut entry = None;
                if let Some(entry_hash) = action.action().entry_hash() {
                    let author = self
                        .1
                        .as_ref()
                        .map(|a| a.as_ref())
                        .filter(|a| *a == action.action().author());
                    entry = stores.get_public_or_authored_entry(entry_hash, author)?;
                }
                Ok(Some(Record::new(action, entry)))
            }
            None => Ok(None),
        }
    }
}

impl PrivateDataQuery for GetLiveRecordQuery {
    type Hash = ActionHash;

    fn with_private_data_access(hash: Self::Hash, author: Arc<AgentPubKey>) -> Self {
        Self(hash, Some(author))
    }

    fn without_private_data_access(hash: Self::Hash) -> Self {
        Self::new(hash)
    }
}



================================================
File: crates/holochain_state/src/query/record_details.rs
================================================
use holo_hash::*;
use holochain_sqlite::rusqlite::named_params;
use holochain_types::dht_op::ChainOpType;
use holochain_types::prelude::Judged;
use holochain_zome_types::prelude::*;
use std::fmt::Debug;

use super::*;

#[derive(Debug, Clone)]
pub struct GetRecordDetailsQuery(ActionHash, Option<Arc<AgentPubKey>>);

impl GetRecordDetailsQuery {
    pub fn new(hash: ActionHash) -> Self {
        Self(hash, None)
    }
}

#[derive(Debug)]
pub struct State {
    action: Option<SignedActionHashed>,
    rejected_action: Option<SignedActionHashed>,
    deletes: HashSet<SignedActionHashed>,
    updates: HashSet<SignedActionHashed>,
}

impl Query for GetRecordDetailsQuery {
    type Item = Judged<SignedActionHashed>;
    type State = State;
    type Output = Option<RecordDetails>;

    fn query(&self) -> String {
        "
        SELECT Action.blob AS action_blob, DhtOp.validation_status AS status
        FROM DhtOp
        JOIN Action On DhtOp.action_hash = Action.hash
        WHERE DhtOp.type IN (:create_type, :delete_type, :update_type)
        AND DhtOp.basis_hash = :action_hash
        AND DhtOp.when_integrated IS NOT NULL
        AND DhtOp.validation_status IS NOT NULL
        "
        .into()
    }
    fn params(&self) -> Vec<Params> {
        let params = named_params! {
            ":create_type": ChainOpType::StoreRecord,
            ":delete_type": ChainOpType::RegisterDeletedBy,
            ":update_type": ChainOpType::RegisterUpdatedRecord,
            ":action_hash": self.0,
        };
        params.to_vec()
    }

    fn as_map(&self) -> Arc<dyn Fn(&Row) -> StateQueryResult<Self::Item>> {
        let f = |row: &Row| {
            let action =
                from_blob::<SignedAction>(row.get(row.as_ref().column_index("action_blob")?)?)?;
            let (action, signature) = action.into();
            let action = ActionHashed::from_content_sync(action);
            let shh = SignedActionHashed::with_presigned(action, signature);
            let status = row.get(row.as_ref().column_index("status")?)?;
            let r = Judged::new(shh, status);
            Ok(r)
        };
        Arc::new(f)
    }

    fn as_filter(&self) -> Box<dyn Fn(&QueryData<Self>) -> bool> {
        let action_filter = self.0.clone();
        let f = move |action: &QueryData<Self>| {
            let action = &action;
            if *action.action_address() == action_filter {
                true
            } else {
                match action.action() {
                    Action::Delete(Delete {
                        deletes_address, ..
                    }) => *deletes_address == action_filter,
                    Action::Update(Update {
                        original_action_address,
                        ..
                    }) => *original_action_address == action_filter,
                    _ => false,
                }
            }
        };
        Box::new(f)
    }

    fn init_fold(&self) -> StateQueryResult<Self::State> {
        Ok(State {
            action: Default::default(),
            rejected_action: Default::default(),
            deletes: Default::default(),
            updates: Default::default(),
        })
    }

    fn fold(&self, mut state: Self::State, item: Self::Item) -> StateQueryResult<Self::State> {
        let (shh, validation_status) = item.into();
        if *shh.as_hash() == self.0 {
            if state.action.is_none() && state.rejected_action.is_none() {
                match validation_status {
                    Some(ValidationStatus::Valid) => {
                        state.action = Some(shh);
                    }
                    Some(ValidationStatus::Rejected) => {
                        state.rejected_action = Some(shh);
                    }
                    _ => (),
                }
            }
        } else {
            match shh.action() {
                Action::Update(Update {
                    original_action_address,
                    ..
                }) if *original_action_address == self.0 => {
                    state.updates.insert(shh);
                }
                Action::Delete(Delete {
                    deletes_address, ..
                }) if *deletes_address == self.0 => {
                    state.deletes.insert(shh);
                }
                _ => (),
            }
        }

        Ok(state)
    }

    fn render<S>(&self, state: Self::State, stores: S) -> StateQueryResult<Self::Output>
    where
        S: Store,
    {
        let State {
            action,
            rejected_action,
            deletes,
            updates,
        } = state;

        let (action, validation_status) = match (action, rejected_action) {
            (None, None) => return Ok(None),
            (None, Some(h)) => (h, ValidationStatus::Rejected),
            (Some(h), None) => (h, ValidationStatus::Valid),
            (Some(_), Some(h)) => {
                // TODO: this is a conflict between multiple sources and
                // needs to be handled.
                (h, ValidationStatus::Rejected)
            }
        };

        let mut entry = None;
        if let Some(entry_hash) = action.action().entry_hash() {
            let author = self
                .1
                .as_ref()
                .map(|a| a.as_ref())
                .filter(|a| *a == action.action().author());
            entry = stores.get_public_or_authored_entry(entry_hash, author)?;
        }
        let record = Record::new(action, entry);
        let details = RecordDetails {
            record,
            validation_status,
            deletes: deletes.into_iter().collect(),
            updates: updates.into_iter().collect(),
        };
        Ok(Some(details))
    }
}

impl PrivateDataQuery for GetRecordDetailsQuery {
    type Hash = ActionHash;

    fn with_private_data_access(hash: Self::Hash, author: Arc<AgentPubKey>) -> Self {
        Self(hash, Some(author))
    }

    fn without_private_data_access(hash: Self::Hash) -> Self {
        Self::new(hash)
    }
}



================================================
File: crates/holochain_state/src/query/test_data.rs
================================================
#![allow(clippy::redundant_clone)]
use ::fixt::prelude::*;
use holo_hash::*;
use holochain_types::action::NewEntryAction;
use holochain_types::dht_op::{ChainOp, ChainOpHashed};
use holochain_zome_types::prelude::*;

use super::link::*;
use super::link_details::GetLinkDetailsQuery;
use super::live_entry::*;

pub struct LinkTestData {
    pub create_link_op: ChainOpHashed,
    pub later_create_link_op: ChainOpHashed,
    pub delete_link_op: ChainOpHashed,
    pub link: Link,
    pub later_link: Link,
    pub base_op: ChainOpHashed,
    pub target_op: ChainOpHashed,
    pub base_query: GetLinksQuery,
    pub tag_query: GetLinksQuery,
    pub details_tag_query: GetLinkDetailsQuery,
    pub create_link_action: SignedActionHashed,
    pub later_create_link_action: SignedActionHashed,
}

pub struct EntryTestData {
    pub store_entry_op: ChainOpHashed,
    pub update_store_entry_op: ChainOpHashed,
    pub delete_entry_action_op: ChainOpHashed,
    pub entry: Entry,
    pub hash: EntryHash,
    pub query: GetLiveEntryQuery,
    pub action: SignedActionHashed,
    pub update_action: SignedActionHashed,
}

pub struct RecordTestData {
    pub store_record_op: ChainOpHashed,
    pub update_store_record_op: ChainOpHashed,
    pub entry: Entry,
    pub action: SignedActionHashed,
    pub update_action: SignedActionHashed,
    pub update_hash: ActionHash,
}

impl LinkTestData {
    pub fn new() -> Self {
        let mut create_link = fixt!(CreateLink);
        create_link.zome_index = 0.into();
        let mut later_create_link = create_link.clone();
        let mut delete_link = fixt!(DeleteLink);

        let now = holochain_zome_types::prelude::Timestamp::now();
        let before = (now - std::time::Duration::from_secs(10)).unwrap();

        create_link.timestamp = before;
        later_create_link.timestamp = now;

        let mut create_base = fixt!(Create);
        let base = Entry::App(fixt!(AppEntryBytes));
        let base_hash = EntryHash::with_data_sync(&base);
        create_base.entry_hash = base_hash.clone();

        let mut create_target = fixt!(Create);
        let target = Entry::App(fixt!(AppEntryBytes));
        let target_hash = EntryHash::with_data_sync(&target);
        create_target.entry_hash = target_hash.clone();

        create_link.base_address = base_hash.clone().into();
        later_create_link.base_address = base_hash.clone().into();
        create_link.target_address = target_hash.clone().into();
        later_create_link.target_address = target_hash.clone().into();

        let create_link_sig = fixt!(Signature);
        let create_link_op = ChainOp::RegisterAddLink(create_link_sig.clone(), create_link.clone());
        let create_link_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::CreateLink(create_link.clone())),
            create_link_sig,
        );
        let later_create_link_sig = fixt!(Signature);
        let later_create_link_op =
            ChainOp::RegisterAddLink(later_create_link_sig.clone(), later_create_link.clone());

        let later_create_link_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::CreateLink(later_create_link.clone())),
            later_create_link_sig,
        );

        let create_link_hash = ActionHash::with_data_sync(&Action::CreateLink(create_link.clone()));
        let later_create_link_hash =
            ActionHash::with_data_sync(&Action::CreateLink(later_create_link.clone()));

        delete_link.link_add_address = create_link_hash.clone();
        delete_link.base_address = base_hash.clone().into();

        let delete_link_op = ChainOp::RegisterRemoveLink(fixt!(Signature), delete_link.clone());

        let base_op = ChainOp::StoreEntry(
            fixt!(Signature),
            NewEntryAction::Create(create_base.clone()),
            base.clone(),
        );

        let target_op = ChainOp::StoreEntry(
            fixt!(Signature),
            NewEntryAction::Create(create_target.clone()),
            target.clone(),
        );

        let link = Link {
            author: create_link.author,
            base: create_link.base_address,
            target: target_hash.clone().into(),
            timestamp: create_link.timestamp,
            tag: create_link.tag.clone(),
            zome_index: create_link.zome_index,
            link_type: create_link.link_type,
            create_link_hash: create_link_hash.clone(),
        };

        let later_link = Link {
            author: later_create_link.author,
            base: later_create_link.base_address,
            target: target_hash.clone().into(),
            timestamp: later_create_link.timestamp,
            tag: later_create_link.tag.clone(),
            zome_index: later_create_link.zome_index,
            link_type: later_create_link.link_type,
            create_link_hash: later_create_link_hash.clone(),
        };

        let base_query = GetLinksQuery::base(base_hash.clone().into(), vec![ZomeIndex(0)]);
        let tag_query = GetLinksQuery::new(
            base_hash.clone().into(),
            LinkTypeFilter::single_dep(0.into()),
            Some(create_link.tag.clone()),
            GetLinksFilter::default(),
        );
        let details_tag_query = GetLinkDetailsQuery::new(
            base_hash.clone().into(),
            LinkTypeFilter::single_dep(0.into()),
            Some(create_link.tag.clone()),
        );

        Self {
            create_link_op: ChainOpHashed::from_content_sync(create_link_op),
            later_create_link_op: ChainOpHashed::from_content_sync(later_create_link_op),
            delete_link_op: ChainOpHashed::from_content_sync(delete_link_op),
            link,
            base_op: ChainOpHashed::from_content_sync(base_op),
            target_op: ChainOpHashed::from_content_sync(target_op),
            base_query,
            tag_query,
            later_link,
            details_tag_query,
            create_link_action,
            later_create_link_action,
        }
    }
}

impl EntryTestData {
    pub fn new() -> Self {
        let mut create = fixt!(Create);
        let mut update = fixt!(Update);
        let mut delete = fixt!(Delete);
        let entry = Entry::App(fixt!(AppEntryBytes));
        let entry_hash = EntryHash::with_data_sync(&entry);
        create.entry_hash = entry_hash.clone();
        update.entry_hash = entry_hash.clone();
        create.entry_type = EntryType::App(AppEntryDef::new(
            0.into(),
            0.into(),
            EntryVisibility::Public,
        ));
        update.entry_type = EntryType::App(AppEntryDef::new(
            0.into(),
            0.into(),
            EntryVisibility::Public,
        ));

        let create_hash = ActionHash::with_data_sync(&Action::Create(create.clone()));

        delete.deletes_entry_address = entry_hash.clone();
        delete.deletes_address = create_hash.clone();

        let signature = fixt!(Signature);
        let store_entry_op = ChainOpHashed::from_content_sync(ChainOp::StoreEntry(
            signature.clone(),
            NewEntryAction::Create(create.clone()),
            entry.clone(),
        ));

        let action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::Create(create.clone())),
            signature.clone(),
        );

        let signature = fixt!(Signature);
        let delete_entry_action_op = ChainOpHashed::from_content_sync(
            ChainOp::RegisterDeletedEntryAction(signature.clone(), delete.clone()),
        );

        let signature = fixt!(Signature);
        let update_store_entry_op = ChainOpHashed::from_content_sync(ChainOp::StoreEntry(
            signature.clone(),
            NewEntryAction::Update(update.clone()),
            entry.clone(),
        ));

        let update_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::Update(update.clone())),
            signature.clone(),
        );
        let query = GetLiveEntryQuery::new(entry_hash.clone());

        Self {
            store_entry_op,
            action,
            update_store_entry_op,
            update_action,
            entry,
            query,
            delete_entry_action_op,
            hash: entry_hash,
        }
    }
}

impl RecordTestData {
    pub fn new() -> Self {
        let mut create = fixt!(Create);
        let mut update = fixt!(Update);
        let mut delete = fixt!(Delete);
        let entry = fixt!(Entry);
        let entry_hash = EntryHash::with_data_sync(&entry);
        create.entry_hash = entry_hash.clone();
        update.entry_hash = entry_hash.clone();

        let create_hash = ActionHash::with_data_sync(&Action::Create(create.clone()));
        let update_hash = ActionHash::with_data_sync(&Action::Update(update.clone()));

        delete.deletes_entry_address = entry_hash.clone();
        delete.deletes_address = create_hash.clone();

        let signature = fixt!(Signature);
        let store_record_op = ChainOpHashed::from_content_sync(ChainOp::StoreRecord(
            signature.clone(),
            Action::Create(create.clone()),
            entry.clone().into(),
        ));

        let action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::Create(create.clone())),
            signature.clone(),
        );

        let signature = fixt!(Signature);
        let update_store_record_op = ChainOpHashed::from_content_sync(ChainOp::StoreRecord(
            signature.clone(),
            Action::Update(update.clone()),
            entry.clone().into(),
        ));

        let update_action = SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::Update(update.clone())),
            signature.clone(),
        );

        Self {
            store_record_op,
            action,
            update_store_record_op,
            update_action,
            entry,
            update_hash,
        }
    }
}



================================================
File: crates/holochain_state/src/query/tests.rs
================================================
use crate::prelude::mutations_helpers::insert_valid_integrated_op;
use crate::scratch::Scratch;
use ::fixt::prelude::*;
use holo_hash::*;
use holochain_sqlite::rusqlite::Transaction;
use holochain_sqlite::rusqlite::TransactionBehavior;
use holochain_sqlite::{rusqlite::Connection, schema::SCHEMA_CELL};
use holochain_types::action::NewEntryAction;
use holochain_types::dht_op::DhtOpHashed;
use holochain_types::dht_op::OpOrder;
use holochain_zome_types::entry::EntryHashed;
use holochain_zome_types::prelude::*;

use super::link::*;
use super::live_entry::*;
use super::test_data::*;
use super::*;
use crate::mutations::*;

mod details;
mod links;
mod links_test;
mod store;

#[tokio::test(flavor = "multi_thread")]
async fn get_links() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut cache = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut cache, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let mut cache_txn = cache
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = LinkTestData::new();

    // - Add link to db.
    insert_valid_integrated_op(&mut txn, &td.base_op.downcast()).unwrap();
    insert_valid_integrated_op(&mut txn, &td.target_op.downcast()).unwrap();
    insert_valid_integrated_op(&mut txn, &td.create_link_op.downcast()).unwrap();

    // - Check we can get the link query back.
    let r = get_link_query(&[&txn], None, td.tag_query.clone());
    assert_eq!(r[0], td.link);

    // - Add the same link to the cache.
    insert_valid_integrated_op(&mut cache_txn, &td.base_op.downcast()).unwrap();
    insert_valid_integrated_op(&mut cache_txn, &td.target_op.downcast()).unwrap();
    insert_valid_integrated_op(&mut cache_txn, &td.create_link_op.downcast()).unwrap();

    // - Check duplicates don't cause issues.
    insert_valid_integrated_op(&mut cache_txn, &td.create_link_op.downcast()).unwrap();

    // - Add to the scratch
    insert_op_scratch(
        &mut scratch,
        td.base_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td.target_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_scratch(
        &mut scratch,
        td.create_link_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // - Check we can resolve this to a single link.
    let r = get_link_query(&[&cache_txn], Some(&scratch), td.base_query.clone());
    assert_eq!(r[0], td.link);
    assert_eq!(r.len(), 1);
    let r = get_link_query(&[&cache_txn, &txn], Some(&scratch), td.tag_query.clone());
    assert_eq!(r[0], td.link);
    assert_eq!(r.len(), 1);

    // - Insert a delete op.
    insert_valid_integrated_op(&mut txn, &td.delete_link_op.downcast()).unwrap();

    let r = get_link_query(&[&cache_txn, &txn], Some(&scratch), td.tag_query.clone());
    // - We should not have any links now.
    assert!(r.is_empty())
}

#[tokio::test(flavor = "multi_thread")]
async fn get_entry() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut cache = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut cache, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let mut cache_txn = cache
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = EntryTestData::new();

    // - Create an entry on main db.
    insert_valid_integrated_op(&mut txn, &td.store_entry_op.downcast()).unwrap();

    // - Check we get that action back.
    let r = get_entry_query(&[&txn], None, td.query.clone()).unwrap();
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);

    // - Create the same entry in the cache.
    insert_valid_integrated_op(&mut cache_txn, &td.store_entry_op.downcast()).unwrap();
    // - Check duplicates is ok.
    insert_valid_integrated_op(&mut cache_txn, &td.store_entry_op.downcast()).unwrap();

    // - Add to the scratch
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();

    // - Get the entry from both stores and union the query results.
    let r = get_entry_query(&[&txn, &cache_txn], Some(&scratch), td.query.clone());
    // - Check it's the correct entry and action.
    let r = r.unwrap();
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);
    assert_eq!(*r.action(), *td.action.action());

    // - Delete the entry in the cache.
    insert_valid_integrated_op(&mut cache_txn, &td.delete_entry_action_op.downcast()).unwrap();

    // - Get the entry from both stores and union the queries.
    let r = get_entry_query(&[&txn, &cache_txn], Some(&scratch), td.query.clone());
    // - There should be no live actions so resolving
    // returns no record.
    assert!(r.is_none());
}

/// Test that `insert_op` also inserts an action and potentially an entry
#[tokio::test(flavor = "multi_thread")]
async fn insert_op_equivalence() {
    holochain_trace::test_run();
    let mut conn1 = Connection::open_in_memory().unwrap();
    let mut conn2 = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn1, None).unwrap();
    SCHEMA_CELL.initialize(&mut conn2, None).unwrap();

    let mut create_action = fixt!(Create);
    let create_entry = fixt!(Entry);
    let create_entry_hash = EntryHash::with_data_sync(&create_entry);
    create_action.entry_hash = create_entry_hash.clone();

    let sig = fixt!(Signature);
    let op = ChainOp::StoreEntry(
        sig.clone(),
        NewEntryAction::Create(create_action.clone()),
        create_entry.clone(),
    );
    let op = DhtOpHashed::from_content_sync(op);

    // Insert the op in 3 steps on conn1
    let mut txn1 = conn1
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let mut txn2 = conn2
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();
    let e = EntryHashed::from_content_sync(create_entry);
    insert_entry(&mut txn1, e.as_hash(), e.as_content()).unwrap();
    let op_order = OpOrder::new(op.get_type(), create_action.timestamp);
    insert_action(
        &mut txn1,
        &SignedActionHashed::with_presigned(
            ActionHashed::from_content_sync(Action::Create(create_action.clone())),
            fixt!(Signature),
        ),
    )
    .unwrap();
    insert_op_lite(
        &mut txn1,
        &op.to_lite(),
        op.as_hash(),
        &op_order,
        &create_action.timestamp,
        None,
    )
    .unwrap();

    // Insert the op in a single step on conn2
    insert_valid_integrated_op(&mut txn2, &op).unwrap();

    txn1.commit().unwrap();
    txn2.commit().unwrap();

    // Query the DB on conn1
    let entries1: Vec<u8> = conn1
        .query_row("SELECT * FROM Entry", [], |row| row.get("hash"))
        .unwrap();
    let actions1: Vec<u8> = conn1
        .query_row("SELECT * FROM Action", [], |row| row.get("hash"))
        .unwrap();
    let ops1: Vec<u8> = conn1
        .query_row("SELECT * FROM DhtOp", [], |row| row.get("hash"))
        .unwrap();

    // Query the DB on conn2
    let entries2: Vec<u8> = conn2
        .query_row("SELECT * FROM Entry", [], |row| row.get("hash"))
        .unwrap();
    let actions2: Vec<u8> = conn2
        .query_row("SELECT * FROM Action", [], |row| row.get("hash"))
        .unwrap();
    let ops2: Vec<u8> = conn2
        .query_row("SELECT * FROM DhtOp", [], |row| row.get("hash"))
        .unwrap();

    assert_eq!(entries1, entries2);
    assert_eq!(actions1, actions2);
    assert_eq!(ops1, ops2);
}

fn get_link_query<'a, 'b: 'a>(
    txns: &[&'a Transaction<'b>],
    scratch: Option<&Scratch>,
    query: GetLinksQuery,
) -> Vec<Link> {
    match scratch {
        Some(scratch) => {
            let stores = DbScratch::new(txns, scratch);
            query.run(stores).unwrap()
        }
        None => query.run(Txns::from(txns)).unwrap(),
    }
}

fn get_entry_query<'a, 'b: 'a>(
    txns: &[&'a Transaction<'b>],
    scratch: Option<&Scratch>,
    query: GetLiveEntryQuery,
) -> Option<Record> {
    match scratch {
        Some(scratch) => {
            let stores = DbScratch::new(txns, scratch);
            query.run(stores).unwrap()
        }
        None => query.run(Txns::from(txns)).unwrap(),
    }
}



================================================
File: crates/holochain_state/src/query/live_entry/test.rs
================================================
use holochain_sqlite::rusqlite::Connection;
use holochain_sqlite::rusqlite::TransactionBehavior;
use holochain_sqlite::schema::SCHEMA_CELL;

use crate::mutations::insert_op_scratch;
use crate::prelude::mutations_helpers::insert_valid_integrated_op;
use crate::query::test_data::EntryTestData;

use super::*;

#[tokio::test(flavor = "multi_thread")]
async fn can_handle_update_in_scratch() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = EntryTestData::new();

    // - Create an entry on main db.
    insert_valid_integrated_op(&mut txn, &td.update_store_entry_op.downcast()).unwrap();
    let r = td
        .query
        .run(CascadeTxnWrapper::from(&txn))
        .unwrap()
        .expect("Record not found");
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);
    assert_eq!(*r.action(), *td.update_action.action());

    // - Add to the scratch
    insert_op_scratch(
        &mut scratch,
        td.update_store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    let r = td
        .query
        .run(scratch.clone())
        .unwrap()
        .expect("Record not found");
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);
    assert_eq!(*r.action(), *td.update_action.action());
}



================================================
File: crates/holochain_state/src/query/live_record/test.rs
================================================
use holochain_sqlite::rusqlite::Connection;
use holochain_sqlite::rusqlite::TransactionBehavior;
use holochain_sqlite::schema::SCHEMA_CELL;
use test_data::RecordTestData;

use crate::mutations::insert_op_scratch;
use crate::prelude::mutations_helpers::insert_valid_integrated_op;

use super::*;

#[tokio::test(flavor = "multi_thread")]
async fn can_handle_update_in_scratch() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = RecordTestData::new();
    let query = GetLiveRecordQuery::with_private_data_access(
        td.update_hash,
        Arc::new(td.update_store_record_op.action().author().clone()),
    );

    // - Create an entry on main db.
    insert_valid_integrated_op(&mut txn, &td.update_store_record_op.downcast()).unwrap();
    let r = query
        .run(CascadeTxnWrapper::from(&txn))
        .unwrap()
        .expect("Record not found");
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);
    assert_eq!(*r.action(), *td.update_action.action());

    // - Add to the scratch
    insert_op_scratch(
        &mut scratch,
        td.update_store_record_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    let r = query
        .run(scratch.clone())
        .unwrap()
        .expect("Record not found");
    assert_eq!(*r.entry().as_option().unwrap(), td.entry);
    assert_eq!(*r.action(), *td.update_action.action());
}



================================================
File: crates/holochain_state/src/query/tests/details.rs
================================================
use record_details::GetRecordDetailsQuery;

use crate::{
    prelude::mutations_helpers::insert_valid_integrated_op,
    query::entry_details::GetEntryDetailsQuery,
};

use super::*;

#[tokio::test(flavor = "multi_thread")]
async fn entry_scratch_same_as_sql() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = EntryTestData::new();
    let query = GetEntryDetailsQuery::with_private_data_access(
        td.hash.clone(),
        Arc::new(td.store_entry_op.action().author().clone()),
    );
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_valid_integrated_op(&mut txn, &td.store_entry_op.downcast()).unwrap();
    let r1 = query
        .run(CascadeTxnWrapper::from(&txn))
        .unwrap()
        .expect("Record not found");
    let r2 = query
        .run(scratch.clone())
        .unwrap()
        .expect("Record not found");
    assert_eq!(r1, r2);
}

#[tokio::test(flavor = "multi_thread")]
async fn record_scratch_same_as_sql() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = RecordTestData::new();
    let query = GetRecordDetailsQuery::with_private_data_access(
        td.action.as_hash().clone(),
        Arc::new(td.store_record_op.action().author().clone()),
    );
    insert_op_scratch(
        &mut scratch,
        td.store_record_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_valid_integrated_op(&mut txn, &td.store_record_op.downcast()).unwrap();
    let r1 = query
        .run(CascadeTxnWrapper::from(&txn))
        .unwrap()
        .expect("Record not found");
    let r2 = query
        .run(scratch.clone())
        .unwrap()
        .expect("Record not found");
    assert_eq!(r1, r2);
}



================================================
File: crates/holochain_state/src/query/tests/links.rs
================================================
use crate::prelude::mutations_helpers::insert_valid_integrated_op;

use super::*;

#[tokio::test(flavor = "multi_thread")]
async fn link_queries_are_ordered_by_timestamp() {
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = LinkTestData::new();
    insert_valid_integrated_op(&mut txn, &td.create_link_op.downcast()).unwrap();
    insert_valid_integrated_op(&mut txn, &td.later_create_link_op.downcast()).unwrap();
    let links = td.tag_query.run(CascadeTxnWrapper::from(&txn)).unwrap();
    assert_eq!(links, vec![td.link.clone(), td.later_link.clone()]);
    let links = td
        .details_tag_query
        .run(CascadeTxnWrapper::from(&txn))
        .unwrap();
    assert_eq!(
        links,
        vec![
            (td.create_link_action.clone(), vec![]),
            (td.later_create_link_action.clone(), vec![])
        ]
    );
}



================================================
File: crates/holochain_state/src/query/tests/links_test.rs
================================================
use super::*;
use crate::here;
use crate::prelude::mutations_helpers::insert_valid_integrated_op;
use crate::prelude::*;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::EntryHashFixturator;
use holochain_trace;
use holochain_types::db::DbWrite;
use holochain_types::record::SignedActionHashedExt;
use std::vec::IntoIter;

#[derive(Clone)]
struct TestData {
    link_add: CreateLink,
    link_remove: DeleteLink,
    base_hash: EntryHash,
    zome_index: ZomeIndex,
    link_type: LinkType,
    tag: LinkTag,
    expected_link: Link,
    env: DbWrite<DbKindDht>,
    scratch: Scratch,
    query: GetLinksQuery,
    query_no_tag: GetLinksQuery,
}

fn fixtures(env: DbWrite<DbKindDht>, n: usize) -> Vec<TestData> {
    let mut tag_fix = BytesFixturator::new(Predictable);
    let mut data = Vec::new();
    let mut agent_pub_key_fixt = AgentPubKeyFixturator::new(Predictable);
    let mut base_hash_fixt = EntryHashFixturator::new(Predictable);
    let mut target_hash_fixt = EntryHashFixturator::new(Unpredictable);
    for i in 0..n {
        // Create a known link add
        let base_address = base_hash_fixt.next().unwrap();
        let target_address = target_hash_fixt.next().unwrap();
        let agent_pub_key = agent_pub_key_fixt.next().unwrap();

        let tag = LinkTag::new(tag_fix.next().unwrap());
        let zome_index = ZomeIndex(i as u8);
        let link_type = LinkType(i as u8);

        let link_add = KnownCreateLink {
            author: agent_pub_key.clone(),
            base_address: base_address.clone().into(),
            target_address: target_address.clone().into(),
            zome_index,
            link_type,
            tag: tag.clone(),
        };

        let link_add = CreateLinkFixturator::new(link_add).next().unwrap();

        // Create the expected link result
        let (_, link_add_hash): (_, ActionHash) =
            ActionHashed::from_content_sync(Action::CreateLink(link_add.clone())).into();

        let expected_link = Link {
            author: agent_pub_key,
            create_link_hash: link_add_hash.clone(),
            base: link_add.base_address.clone(),
            target: target_address.clone().into(),
            zome_index,
            link_type,
            timestamp: link_add.timestamp,
            tag: tag.clone(),
        };

        let link_remove = KnownDeleteLink {
            link_add_address: link_add_hash,
            base_address: link_add.base_address.clone(),
        };
        let link_remove = DeleteLinkFixturator::new(link_remove).next().unwrap();
        let query = GetLinksQuery::new(
            link_add.base_address.clone(),
            LinkTypeFilter::single_dep(zome_index),
            Some(link_add.tag.clone()),
            GetLinksFilter::default(),
        );
        let query_no_tag = GetLinksQuery::base(link_add.base_address.clone(), vec![zome_index]);

        let td = TestData {
            link_add,
            link_remove,
            base_hash: base_address.clone(),
            zome_index,
            link_type,
            tag,
            expected_link,
            env: env.clone(),
            scratch: Scratch::new(),
            query,
            query_no_tag,
        };
        data.push(td);
    }
    data
}

impl TestData {
    /// Create the same test data with a new timestamp
    fn with_same_keys(mut td: Self) -> Self {
        td.link_add.timestamp = holochain_zome_types::prelude::Timestamp::now();
        let link_add_hash =
            ActionHashed::from_content_sync(Action::CreateLink(td.link_add.clone())).into_hash();
        td.link_remove.link_add_address = link_add_hash.clone();
        td.expected_link.timestamp = td.link_add.timestamp;
        td.expected_link.create_link_hash = link_add_hash;
        td
    }

    async fn empty<'a>(&'a self, test: &'static str) {
        let val = self
            .env
            .read_async({
                let query = self.query.clone();
                let scratch = self.scratch.clone();

                move |txn| -> DatabaseResult<bool> {
                    Ok(query
                        .run(DbScratch::new(&[txn], &scratch))
                        .unwrap()
                        .is_empty())
                }
            })
            .await
            .unwrap();
        assert!(val, "{}", test);
    }

    async fn only_on_full_key<'a>(&'a self, test: &'static str) {
        let val = self
            .env
            .read_async({
                let query = self.query_no_tag.clone();
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap();
        assert_eq!(val, &[self.expected_link.clone()], "{}", test);
    }

    async fn not_on_full_key<'a>(&'a self, test: &'static str) {
        let val = self
            .env
            .read_async({
                let query = self.query.clone();
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap()
            .contains(&self.expected_link);
        assert!(
            !val,
            "LinkMetaVal: {:?} should not be present {}",
            self.expected_link, test
        );
    }

    async fn only_on_base<'a>(&'a self, test: &'static str) {
        let val = self
            .env
            .read_async({
                let query_no_tag = self.query_no_tag.clone();
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query_no_tag.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap();
        assert_eq!(val, &[self.expected_link.clone()], "{}", test);
    }

    async fn is_on_type<'a>(&'a self, test: &'static str) {
        let query = GetLinksQuery::new(
            self.base_hash.clone().into(),
            LinkTypeFilter::single_type(self.zome_index, self.link_type),
            None,
            GetLinksFilter::default(),
        );

        let val = self
            .env
            .read_async({
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap()
            .contains(&self.expected_link);
        assert!(
            val,
            "Results should contain link: {:?} in test: {}",
            self.expected_link, test
        );
    }

    async fn is_on_type_query<'a>(&'a self, type_query: LinkTypeFilter, test: &'static str) {
        let query = GetLinksQuery::new(
            self.base_hash.clone().into(),
            type_query,
            None,
            GetLinksFilter::default(),
        );

        let val = self
            .env
            .read_async({
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap()
            .contains(&self.expected_link);
        assert!(
            val,
            "Results should contain link: {:?} in test: {}",
            self.expected_link, test
        );
    }

    async fn only_on_half_tag<'a>(&'a self, test: &'static str) {
        let tag_len = self.tag.0.len();
        // Make sure there is at least some tag
        let half_tag = if tag_len > 1 { tag_len / 2 } else { tag_len };
        let half_tag = LinkTag::new(&self.tag.0[..half_tag]);
        let query = GetLinksQuery::new(
            self.base_hash.clone().into(),
            LinkTypeFilter::single_type(self.zome_index, self.link_type),
            Some(half_tag),
            GetLinksFilter::default(),
        );

        let val = self
            .env
            .read_async({
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap();
        assert_eq!(val, &[self.expected_link.clone()], "{}", test);
    }

    async fn is_on_half_tag<'a>(&'a self, test: &'static str) {
        let tag_len = self.tag.0.len();
        // Make sure there is at least some tag
        let half_tag = if tag_len > 1 { tag_len / 2 } else { tag_len };
        let half_tag = LinkTag::new(&self.tag.0[..half_tag]);
        let query = GetLinksQuery::new(
            self.base_hash.clone().into(),
            LinkTypeFilter::single_type(self.zome_index, self.link_type),
            Some(half_tag),
            GetLinksFilter::default(),
        );

        let val = self
            .env
            .read_async({
                let scratch = self.scratch.clone();

                move |txn| -> StateQueryResult<Vec<Link>> {
                    query.run(DbScratch::new(&[txn], &scratch))
                }
            })
            .await
            .unwrap()
            .contains(&self.expected_link);
        assert!(
            val,
            "Results should contain LinkMetaVal: {:?} in test: {}",
            self.expected_link, test
        );
    }

    async fn add_link(&self) {
        let op = DhtOpHashed::from_content_sync(ChainOp::RegisterAddLink(
            fixt!(Signature),
            self.link_add.clone(),
        ));
        self.env
            .write_async(move |txn| -> StateMutationResult<()> {
                insert_valid_integrated_op(txn, &op)
            })
            .await
            .unwrap();
    }

    fn add_link_scratch(&mut self) {
        let action = SignedActionHashed::from_content_sync(SignedAction::new(
            Action::CreateLink(self.link_add.clone()),
            fixt!(Signature),
        ));
        self.scratch.add_action(action, ChainTopOrdering::default());
    }

    fn add_link_given_scratch(&mut self, scratch: &mut Scratch) {
        let action = SignedActionHashed::from_content_sync(SignedAction::new(
            Action::CreateLink(self.link_add.clone()),
            fixt!(Signature),
        ));
        scratch.add_action(action, ChainTopOrdering::default());
    }

    async fn delete_link(&self) {
        let op = DhtOpHashed::from_content_sync(ChainOp::RegisterRemoveLink(
            fixt!(Signature),
            self.link_remove.clone(),
        ));
        self.env
            .write_async(move |txn| -> StateMutationResult<()> {
                insert_valid_integrated_op(txn, &op)
            })
            .await
            .unwrap();
    }

    fn delete_link_scratch(&mut self) {
        let action = SignedActionHashed::from_content_sync(SignedAction::new(
            Action::DeleteLink(self.link_remove.clone()),
            fixt!(Signature),
        ));
        self.scratch.add_action(action, ChainTopOrdering::default());
    }
    fn clear_scratch(&mut self) {
        self.scratch.drain_actions().for_each(|_| ());
    }

    async fn only_these_on_base<'a>(td: &'a [Self], test: &'static str) {
        // Check all base hash are the same
        for d in td {
            assert_eq!(d.base_hash, td[0].base_hash, "{}", test);
        }
        let base_hash = td[0].base_hash.clone();
        let expected = td
            .iter()
            .map(|d| d.expected_link.clone())
            .collect::<Vec<_>>();
        let mut val = Vec::new();
        for d in td {
            let query = GetLinksQuery::new(
                base_hash.clone().into(),
                LinkTypeFilter::single_type(d.zome_index, d.link_type),
                None,
                GetLinksFilter::default(),
            );

            val.extend(
                d.env
                    .read_async({
                        let scratch = d.scratch.clone();

                        move |txn| -> DatabaseResult<IntoIter<Link>> {
                            Ok(query
                                .run(DbScratch::new(&[txn], &scratch))
                                .unwrap()
                                .into_iter())
                        }
                    })
                    .await
                    .unwrap(),
            );
        }
        assert_eq!(val, expected, "{}", test);
    }

    async fn only_these_on_query<'a>(
        td: &'a [Self],
        scratch: &Scratch,
        query: impl Into<LinkTypeFilter>,
        test: &str,
    ) {
        // Check all base hash are the same
        for d in td {
            assert_eq!(d.base_hash, td[0].base_hash, "{}", test);
        }
        let base_hash = td[0].base_hash.clone();
        let expected = td
            .iter()
            .map(|d| d.expected_link.clone())
            .collect::<HashSet<_>>();
        let query = GetLinksQuery::new(
            base_hash.clone().into(),
            query.into(),
            None,
            GetLinksFilter::default(),
        );

        let val: HashSet<_> = td[0]
            .env
            .clone()
            .read_async({
                let scratch = scratch.clone();

                move |txn| -> DatabaseResult<IntoIter<Link>> {
                    Ok(query
                        .run(DbScratch::new(&[txn], &scratch))
                        .unwrap()
                        .into_iter())
                }
            })
            .await
            .unwrap()
            .collect();
        assert_eq!(val, expected, "{}", test);
    }

    async fn only_these_on_full_key<'a>(td: &'a [Self], test: &'static str) {
        // Check all base hash, link type, tag are the same
        for d in td {
            assert_eq!(d.base_hash, td[0].base_hash, "{}", test);
            assert_eq!(d.link_type, td[0].link_type, "{}", test);
            assert_eq!(d.tag, td[0].tag, "{}", test);
        }
        let base_hash = td[0].base_hash.clone();
        let zome_index = td[0].zome_index;
        let tag = td[0].tag.clone();
        let expected = td
            .iter()
            .map(|d| d.expected_link.clone())
            .collect::<Vec<_>>();
        let query = GetLinksQuery::new(
            base_hash.into(),
            LinkTypeFilter::single_dep(zome_index),
            Some(tag),
            GetLinksFilter::default(),
        );
        let mut val = Vec::new();
        for d in td {
            val.extend(
                d.env
                    .read_async({
                        let my_query = query.clone();
                        let scratch = d.scratch.clone();

                        move |txn| -> DatabaseResult<IntoIter<Link>> {
                            Ok(my_query
                                .run(DbScratch::new(&[txn], &scratch))
                                .unwrap()
                                .into_iter())
                        }
                    })
                    .await
                    .unwrap(),
            );
        }
        assert_eq!(val, expected, "{}", test);
    }

    async fn only_these_on_half_key<'a>(td: &'a [Self], test: &'static str) {
        let tag_len = td[0].tag.0.len();
        // Make sure there is at least some tag
        let tag_len = if tag_len > 1 { tag_len / 2 } else { tag_len };
        let half_tag = LinkTag::new(&td[0].tag.0[..tag_len]);
        // Check all base hash, zome_index, half tag are the same
        for d in td {
            assert_eq!(d.base_hash, td[0].base_hash, "{}", test);
            assert_eq!(d.link_type, td[0].link_type, "{}", test);
            assert_eq!(&d.tag.0[..tag_len], &half_tag.0[..], "{}", test);
        }
        let base_hash = td[0].base_hash.clone();
        let zome_index = td[0].zome_index;
        let expected = td
            .iter()
            .map(|d| d.expected_link.clone())
            .collect::<Vec<_>>();
        let query = GetLinksQuery::new(
            base_hash.into(),
            LinkTypeFilter::single_dep(zome_index),
            Some(half_tag),
            GetLinksFilter::default(),
        );
        let mut val = Vec::new();
        for d in td {
            val.extend(
                d.env
                    .read_async({
                        let my_query = query.clone();
                        let scratch = d.scratch.clone();

                        move |txn| -> DatabaseResult<IntoIter<Link>> {
                            Ok(my_query
                                .run(DbScratch::new(&[txn], &scratch))
                                .unwrap()
                                .into_iter())
                        }
                    })
                    .await
                    .unwrap(),
            );
        }
        assert_eq!(val, expected, "{}", test);
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn can_add_and_delete_link() {
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 1).into_iter().next().unwrap();

    // Check it's empty
    td.empty(here!("empty at start")).await;

    // Add a link
    // Add
    td.add_link_scratch();
    // Is in scratch
    td.only_on_full_key(here!("add link in scratch")).await;

    // Remove from scratch
    td.delete_link_scratch();

    // Is empty
    td.empty(here!("empty after remove")).await;

    let new_td = TestData::with_same_keys(td.clone());
    td = new_td;

    // Add again
    td.add_link_scratch();

    // Is in scratch again
    td.only_on_full_key(here!("Is still in the scratch")).await;

    // Remove from scratch
    td.delete_link_scratch();

    // Is empty
    td.empty(here!("empty after remove")).await;

    // Check it's in db
    td.clear_scratch();
    td.add_link().await;

    td.only_on_full_key(here!("It's in the db")).await;

    // Remove the link
    td.delete_link().await;
    // Is empty

    td.empty(here!("empty after remove in db")).await;

    // Add a link
    let new_td = TestData::with_same_keys(td.clone());
    td = new_td;
    // Add
    td.add_link_scratch();
    // Is in scratch
    td.only_on_full_key(here!("add link in scratch")).await;
    // No zome, no tag
    td.only_on_base(here!("scratch")).await;
    // Half the tag
    td.only_on_half_tag(here!("scratch")).await;

    td.delete_link_scratch();
    td.empty(here!("empty after remove in db")).await;

    // Partial matching
    td.clear_scratch();
    td.add_link().await;

    td.only_on_full_key(here!("db")).await;
    // No zome, no tag
    td.only_on_base(here!("db")).await;
    // Half the tag
    td.only_on_half_tag(here!("db")).await;
}

#[tokio::test(flavor = "multi_thread")]
async fn multiple_links() {
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 10);

    // Add links
    {
        // Add
        for d in &mut td {
            d.add_link_scratch();
        }
        // Is in scratch
        for d in &mut td {
            d.only_on_full_key(here!("add link in scratch")).await;
        }

        // Remove from scratch
        td[5].delete_link_scratch();

        td[5].not_on_full_key(here!("removed in scratch")).await;

        for d in td[0..5].iter().chain(&td[6..]) {
            d.only_on_full_key(here!("all except 5 scratch")).await;
        }
        // Can't add back the same action because removes are tombstones
        // so add one with the same key
        let new_td = TestData::with_same_keys(td[5].clone());
        td[5] = new_td;
        // Add again
        td[5].add_link_scratch();

        // Is in scratch again
        td[5]
            .only_on_full_key(here!("Is back in the scratch"))
            .await;

        for d in &mut td {
            d.only_on_full_key(here!("add link in scratch")).await;
        }
        for d in &mut td {
            d.clear_scratch();
        }
    }

    {
        for d in &mut td {
            d.add_link().await;
        }
        for d in &mut td {
            d.only_on_full_key(here!("all in db")).await;
        }
        td[0].delete_link().await;

        for d in &td[1..] {
            d.only_on_full_key(here!("all except 0 scratch")).await;
        }

        td[0].not_on_full_key(here!("removed in scratch")).await;
    }

    for d in &td[1..] {
        d.only_on_full_key(here!("all except 0")).await;
    }
    td[0].not_on_full_key(here!("removed in db")).await;
}
#[tokio::test(flavor = "multi_thread")]
async fn duplicate_links() {
    holochain_trace::test_run();
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 10);
    // Add to db then the same to scratch and expect on one result
    {
        // Add
        for d in &mut td {
            d.add_link_scratch();
        }
        // Is in scratch
        for d in &mut td {
            d.only_on_full_key(here!("re add")).await;
            // No zome, no tag
            d.only_on_base(here!("re add")).await;
            // Half the tag
            d.is_on_half_tag(here!("re add")).await;
        }
        // Add Again
        for d in &mut td {
            d.add_link_scratch();
        }
        // Is in scratch
        for d in &mut td {
            d.only_on_full_key(here!("re add")).await;
            // No zome, no tag
            d.only_on_base(here!("re add")).await;
            // Half the tag
            d.is_on_half_tag(here!("re add")).await;
        }
    }
    {
        // Add
        for d in &mut td {
            d.add_link().await;
        }
        // Is in scratch
        for d in &mut td {
            d.only_on_full_key(here!("re add")).await;
            // No zome, no tag
            d.only_on_base(here!("re add")).await;
            // Half the tag
            d.is_on_half_tag(here!("re add")).await;
        }
    }

    for d in &mut td {
        d.clear_scratch();
    }
    // Is in db
    for d in &mut td {
        d.only_on_full_key(here!("re add")).await;
        // No zome, no tag
        d.only_on_base(here!("re add")).await;
        // Half the tag
        d.is_on_half_tag(here!("re add")).await;
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn links_on_same_base() {
    holochain_trace::test_run();
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 2);
    let base_hash = td[0].base_hash.clone();
    let base_hash = &base_hash;
    for d in td.iter_mut() {
        d.base_hash = base_hash.clone();
        d.link_add.base_address = base_hash.clone().into();
        // Create the new hash
        let (_, link_add_hash): (_, ActionHash) =
            ActionHashed::from_content_sync(Action::CreateLink(d.link_add.clone())).into();
        d.expected_link.create_link_hash = link_add_hash.clone();
        d.link_remove.link_add_address = link_add_hash;
        d.link_remove.base_address = base_hash.clone().into();
        d.query = GetLinksQuery::new(
            base_hash.clone().into(),
            LinkTypeFilter::single_dep(d.zome_index),
            Some(d.tag.clone()),
            GetLinksFilter::default(),
        );
        d.query_no_tag = GetLinksQuery::base(base_hash.clone().into(), vec![d.zome_index]);
        d.expected_link.base = d.link_add.base_address.clone();
    }
    {
        // Add
        for d in &mut td {
            d.add_link_scratch();
        }
        // Is in scratch
        for d in &mut td {
            d.only_on_full_key(here!("same base")).await;
            // Half the tag
            d.is_on_half_tag(here!("same base")).await;
        }
        TestData::only_these_on_base(&td, here!("check all return on same base")).await;
    }
    {
        for d in &mut td {
            d.add_link().await;
        }
        // In db
        for d in &mut td {
            d.only_on_full_key(here!("same base")).await;
            // Half the tag
            d.is_on_half_tag(here!("same base")).await;
        }
        TestData::only_these_on_base(&td, here!("check all return on same base")).await;
    }
    {
        for d in &mut td {
            d.clear_scratch();
        }
        // In db
        for d in &mut td {
            d.only_on_full_key(here!("same base")).await;
            // Half the tag
            d.is_on_half_tag(here!("same base")).await;
        }
        TestData::only_these_on_base(&td, here!("check all return on same base")).await;
    }
    // Check removes etc.
    {
        for d in &mut td {
            d.add_link_scratch();
        }
        td[0].delete_link_scratch();
        for d in &td[1..] {
            d.only_on_full_key(here!("same base")).await;
            // Half the tag
            d.is_on_half_tag(here!("same base")).await;
        }
        TestData::only_these_on_base(&td[1..], here!("check all return on same base")).await;
        td[0].not_on_full_key(here!("removed in scratch")).await;
    }
    {
        for d in &mut td {
            d.clear_scratch();
        }
        td[0].delete_link().await;
        for d in &td[1..] {
            d.only_on_full_key(here!("same base")).await;
            d.is_on_half_tag(here!("same base")).await;
        }
        TestData::only_these_on_base(&td[1..], here!("check all return on same base")).await;
        td[0].not_on_full_key(here!("removed in scratch")).await;
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn links_on_same_tag() {
    holochain_trace::test_run();
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 10);
    let base_hash = td[0].base_hash.clone();
    let link_type = td[0].link_type;
    let zome_index = td[0].zome_index;
    let tag = td[0].tag.clone();

    for d in td.iter_mut() {
        d.base_hash = base_hash.clone();
        d.zome_index = zome_index;
        d.link_type = link_type;
        d.tag = tag.clone();
        d.link_add.base_address = base_hash.clone().into();
        d.link_add.zome_index = zome_index;
        d.link_add.link_type = link_type;
        d.link_add.tag = tag.clone();
        d.link_remove.base_address = base_hash.clone().into();

        // Create the new hash
        let (_, link_add_hash): (_, ActionHash) =
            ActionHashed::from_content_sync(Action::CreateLink(d.link_add.clone())).into();
        d.expected_link.create_link_hash = link_add_hash.clone();
        d.expected_link.base = d.link_add.base_address.clone();
        d.expected_link.tag = tag.clone();
        d.expected_link.zome_index = zome_index;
        d.expected_link.link_type = link_type;
        d.link_remove.link_add_address = link_add_hash;

        d.query = GetLinksQuery::new(
            base_hash.clone().into(),
            LinkTypeFilter::single_dep(d.zome_index),
            Some(tag.clone()),
            GetLinksFilter::default(),
        );
        d.query_no_tag = GetLinksQuery::base(base_hash.clone().into(), vec![d.zome_index]);
    }
    {
        // Add
        for d in &mut td {
            d.add_link_scratch();
        }
        TestData::only_these_on_base(&td[..], here!("check all return on same base")).await;
        TestData::only_these_on_full_key(&td[..], here!("check all return on same base")).await;
        TestData::only_these_on_half_key(&td[..], here!("check all return on same base")).await;
    }
    {
        // In db
        TestData::only_these_on_base(&td[..], here!("check all return on same base")).await;
        TestData::only_these_on_full_key(&td[..], here!("check all return on same base")).await;
        TestData::only_these_on_half_key(&td[..], here!("check all return on same base")).await;
    }
    // Check removes etc.
    {
        td[5].delete_link().await;
        td[6].delete_link().await;
        let partial_td = &td[..5].iter().chain(&td[7..]).cloned().collect::<Vec<_>>();
        TestData::only_these_on_base(&partial_td[..], here!("check all return on same base")).await;
        TestData::only_these_on_full_key(&partial_td[..], here!("check all return on same base"))
            .await;
        TestData::only_these_on_half_key(&partial_td[..], here!("check all return on same base"))
            .await;
    }
    {
        let partial_td = &td[..5].iter().chain(&td[7..]).cloned().collect::<Vec<_>>();
        TestData::only_these_on_base(&partial_td[..], here!("check all return on same base")).await;
        TestData::only_these_on_full_key(&partial_td[..], here!("check all return on same base"))
            .await;
        TestData::only_these_on_half_key(&partial_td[..], here!("check all return on same base"))
            .await;
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn links_on_same_type() {
    holochain_trace::test_run();
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 10);
    let base_hash = td[0].base_hash.clone();
    let link_type = td[0].link_type;

    for d in td.iter_mut() {
        d.base_hash = base_hash.clone();
        d.link_type = link_type;
        d.link_add.base_address = base_hash.clone().into();
        d.link_add.link_type = link_type;

        // Create the new hash
        let (_, link_add_hash): (_, ActionHash) =
            ActionHashed::from_content_sync(Action::CreateLink(d.link_add.clone())).into();
        d.expected_link.create_link_hash = link_add_hash.clone();
        d.expected_link.base = d.link_add.base_address.clone();
        d.expected_link.link_type = link_type;
    }

    for d in &mut td {
        d.add_link_scratch();
    }
    for d in &td {
        d.is_on_type(here!("Each link is returned for a type"))
            .await;
        d.is_on_type_query(
            LinkTypeFilter::Dependencies(td.iter().map(|d| d.zome_index).collect()),
            here!("Each link is returned for a type"),
        )
        .await;
        d.is_on_type_query(
            LinkTypeFilter::single_type(d.zome_index, d.link_type),
            here!("Each link is returned for a type"),
        )
        .await;
    }
    for d in &mut td {
        d.add_link().await;
    }
    for d in &td {
        d.is_on_type(here!("Each link is returned for a type"))
            .await;
        d.is_on_type_query(
            LinkTypeFilter::Dependencies(td.iter().map(|d| d.zome_index).collect()),
            here!("Each link is returned for a type"),
        )
        .await;
        d.is_on_type_query(
            LinkTypeFilter::single_type(d.zome_index, d.link_type),
            here!("Each link is returned for a type"),
        )
        .await;
        d.is_on_type_query(
            LinkTypeFilter::Types(vec![(d.zome_index, vec![d.link_type])]),
            here!("Each link is returned for a type"),
        )
        .await;
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn link_type_ranges() {
    holochain_trace::test_run();
    let test_db = test_dht_db();
    let arc = test_db.to_db();

    let mut td = fixtures(arc.clone(), 10);
    let base_hash = td[0].base_hash.clone();
    let mut scratch = Scratch::new();

    for (i, d) in td.iter_mut().enumerate() {
        d.base_hash = base_hash.clone();
        d.link_type = LinkType(i as u8);
        d.link_add.base_address = base_hash.clone().into();
        d.link_add.link_type = LinkType(i as u8);

        // Create the new hash
        let link_add_hash = ActionHash::with_data_sync(&Action::CreateLink(d.link_add.clone()));
        d.expected_link.create_link_hash = link_add_hash.clone();
        d.expected_link.base = d.link_add.base_address.clone();
    }

    // Add
    for d in &mut td {
        d.add_link_given_scratch(&mut scratch);
    }
    TestData::only_these_on_query(
        &td,
        &scratch,
        LinkTypeFilter::Dependencies(td.iter().map(|d| d.zome_index).collect()),
        here!("all return on full range"),
    )
    .await;
    TestData::only_these_on_query(
        &td[0..=0],
        &scratch,
        LinkTypeFilter::single_type(0.into(), 0.into()),
        here!("only single on single range"),
    )
    .await;
    TestData::only_these_on_query(
        &td[4..=9],
        &scratch,
        LinkTypeFilter::Types(vec![
            (4.into(), vec![4.into()]),
            (5.into(), vec![5.into()]),
            (6.into(), vec![6.into()]),
            (7.into(), vec![7.into()]),
            (8.into(), vec![8.into()]),
            (9.into(), vec![9.into()]),
        ]),
        here!("range matches"),
    )
    .await;
    let partial_td = &td[2..5]
        .iter()
        .chain(&td[7..9])
        .cloned()
        .collect::<Vec<_>>();
    TestData::only_these_on_query(
        &partial_td[..],
        &scratch,
        LinkTypeFilter::Types(vec![
            (2.into(), vec![2.into()]),
            (3.into(), vec![3.into()]),
            (8.into(), vec![8.into()]),
            (7.into(), vec![7.into()]),
            (4.into(), vec![4.into()]),
        ]),
        here!("individual types"),
    )
    .await;
    let partial_td = &td[2..5]
        .iter()
        .chain(&td[7..9])
        .cloned()
        .collect::<Vec<_>>();
    TestData::only_these_on_query(
        &partial_td[..],
        &scratch,
        LinkTypeFilter::Types(vec![
            (7.into(), vec![7.into()]),
            (8.into(), vec![8.into()]),
            (2.into(), vec![2.into()]),
            (3.into(), vec![3.into()]),
            (4.into(), vec![4.into()]),
        ]),
        here!("individual types"),
    )
    .await;
    for d in &mut td {
        d.add_link().await;
    }
    TestData::only_these_on_query(
        &td,
        &Scratch::new(),
        LinkTypeFilter::Dependencies(td.iter().map(|d| d.zome_index).collect()),
        here!("all return on full range"),
    )
    .await;
    TestData::only_these_on_query(
        &td[0..=0],
        &Scratch::new(),
        LinkTypeFilter::single_type(0.into(), 0.into()),
        here!("all return on full range"),
    )
    .await;
    let partial_td = &td[2..5]
        .iter()
        .chain(&td[7..9])
        .cloned()
        .collect::<Vec<_>>();
    TestData::only_these_on_query(
        &partial_td[..],
        &scratch,
        LinkTypeFilter::Types(vec![
            (7.into(), vec![7.into()]),
            (8.into(), vec![8.into()]),
            (2.into(), vec![2.into()]),
            (3.into(), vec![3.into()]),
            (4.into(), vec![4.into()]),
        ]),
        here!("individual types"),
    )
    .await;
}



================================================
File: crates/holochain_state/src/query/tests/store.rs
================================================
use super::*;

#[tokio::test(flavor = "multi_thread")]
async fn exists() {
    holochain_trace::test_run();
    let mut scratch = Scratch::new();
    let mut conn = Connection::open_in_memory().unwrap();
    SCHEMA_CELL.initialize(&mut conn, None).unwrap();

    let mut txn = conn
        .transaction_with_behavior(TransactionBehavior::Exclusive)
        .unwrap();

    let td = EntryTestData::new();
    insert_op_scratch(
        &mut scratch,
        td.store_entry_op.clone(),
        ChainTopOrdering::default(),
    )
    .unwrap();
    insert_op_untyped(&mut txn, &td.store_entry_op.downcast()).unwrap();
    assert!(CascadeTxnWrapper::from(&txn)
        .contains_hash(&td.hash.clone().into())
        .unwrap());
    assert!(CascadeTxnWrapper::from(&txn)
        .contains_hash(&td.action.as_hash().clone().into())
        .unwrap());
    assert!(scratch.contains_hash(&td.hash.clone().into()).unwrap());
    assert!(scratch
        .contains_hash(&td.action.as_hash().clone().into())
        .unwrap());
}



================================================
File: crates/holochain_state/src/source_chain/error.rs
================================================
// use crate::holochain::core::workflow::produce_dht_ops_workflow::dht_op_light::error::DhtOpConvertError;
use holo_hash::ActionHash;
use holo_hash::EntryHash;
use holochain_chc::ChcError;
use holochain_serialized_bytes::prelude::*;
use holochain_sqlite::error::DatabaseError;
use holochain_types::prelude::*;
use thiserror::Error;

use crate::prelude::StateMutationError;
use crate::query::StateQueryError;
use crate::scratch::ScratchError;
use crate::scratch::SyncScratchError;

use super::HeadInfo;

#[derive(Error, Debug)]
pub enum SourceChainError {
    #[error("The source chain is empty, but is expected to have been initialized")]
    ChainEmpty,

    #[error("Agent key {0} invalid in cell {1}")]
    InvalidAgentKey(AgentPubKey, CellId),

    #[error(
        "Attempted to commit a bundle to the source chain, but the source chain head has moved since the bundle began. Bundle head: {2:?}, Current head: {3:?}"
    )]
    HeadMoved(
        Vec<SignedActionHashed>,
        Vec<EntryHashed>,
        Option<ActionHash>,
        Option<HeadInfo>,
    ),

    #[error(
        "Attempted to commit a bundle to the source chain, but the CHC's head has moved since the bundle began. \
        The source chain needs to be synced with the CHC before proceeding. \
        Context: {0}, Original error: {1:?}"
    )]
    ChcHeadMoved(String, ChcError),

    #[error(transparent)]
    TimestampError(#[from] holochain_zome_types::prelude::TimestampError),

    #[error(transparent)]
    ScratchError(#[from] ScratchError),

    #[error("Attempted to write anything other than the countersigning session entry while the chain was locked for a countersigning session.")]
    ChainLocked,

    #[error("Attempted to write a countersigning session that has already expired")]
    LockExpired,

    #[error("Attempted to write anything other than the countersigning session entry at the same time as the session entry.")]
    DirtyCounterSigningWrite,

    #[error("Attempted to write a countersigning session when there is no active session.")]
    CountersigningWriteWithoutSession,

    #[error(
        "The source chain's structure is invalid. This error is not recoverable. Detail:\n{0}"
    )]
    InvalidStructure(ChainInvalidReason),

    #[error("The source chain's head is pointing to an address which has no content.")]
    MissingHead,

    #[error("The content at address {0} is malformed and can't be deserialized.")]
    MalformedEntry(EntryHash),

    #[error("Serialization error: {0}")]
    SerializationError(#[from] SerializedBytesError),

    #[error("Workspace error: {0}")]
    DatabaseError(#[from] DatabaseError),

    #[error("SerdeJson Error: {0}")]
    SerdeJsonError(String),

    /// Record signature doesn't validate against the action
    #[error("Record signature is invalid")]
    InvalidSignature,

    /// Record previous action reference is invalid
    #[error("Record previous action reference is invalid: {0}")]
    InvalidPreviousAction(String),

    #[error("InvalidCommit error: {0}")]
    InvalidCommit(String),

    #[error("The commit could not be completed but may be retried: {0:?}")]
    IncompleteCommit(IncompleteCommitReason),

    #[error("InvalidLink error: {0}")]
    InvalidLink(String),

    #[error("KeystoreError: {0}")]
    KeystoreError(#[from] holochain_keystore::KeystoreError),

    #[error(transparent)]
    DhtOpError(#[from] DhtOpError),

    #[error("Required the scratch space to be empty but contained values")]
    ScratchNotFresh,

    /// Record signature doesn't validate against the action
    #[error("Record associated with action {0} was not found on the source chain")]
    RecordMissing(String),

    #[error(transparent)]
    RecordGroupError(#[from] RecordGroupError),

    #[error(transparent)]
    StateMutationError(#[from] StateMutationError),

    #[error(transparent)]
    StateQueryError(#[from] StateQueryError),

    #[error(transparent)]
    SyncScratchError(#[from] SyncScratchError),

    #[error(transparent)]
    CounterSigningError(#[from] CounterSigningError),

    #[error("The source chain was missing for a host call that requires it.")]
    SourceChainMissing,

    #[error("The supplied query parameters contains filters that are mutually incompatible.
             In particular, `sequence_range` cannot currently be used with any other filter.
             In the future, all filters will be compatible with each other and this will not be an error.")]
    UnsupportedQuery(ChainQueryFilter),

    /// Other
    #[error("Other: {0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

impl SourceChainError {
    /// promote a custom error type to a SourceChainError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self::Other(e.into())
    }
}

// serde_json::Error does not implement PartialEq - why is that a requirement??
impl From<serde_json::Error> for SourceChainError {
    fn from(e: serde_json::Error) -> Self {
        Self::SerdeJsonError(format!("{:?}", e))
    }
}

impl From<one_err::OneErr> for SourceChainError {
    fn from(e: one_err::OneErr) -> Self {
        Self::other(e)
    }
}

#[derive(Error, Debug, PartialEq, Eq)]
pub enum ChainInvalidReason {
    #[error("A valid chain always begins with a Dna entry, followed by an Agent entry.")]
    GenesisDataMissing,

    #[error("A genesis record contains incorrect data.")]
    MalformedGenesisData,

    #[error("A chain action and its corresponding entry have a discrepancy. Entry address: {0}")]
    ActionAndEntryMismatch(EntryHash),

    #[error("Content was expected to definitely exist at this address, but didn't: {0}")]
    MissingData(EntryHash),
}

/// The reason that a commit could not be completed.
///
/// These errors are required to be retryable. They may not succeed in the future, so it is up to
/// the caller to decide whether to retry, but they are not fatal errors.
#[derive(Debug)]
pub enum IncompleteCommitReason {
    /// Inline validation failed because of missing dependencies.
    ///
    /// This may happen if you depend on something that has been created but not widely published
    /// yet, so that doing a `get` for it might miss it.
    DepMissingFromDht(Vec<AnyDhtHash>),
}

pub type SourceChainResult<T> = Result<T, SourceChainError>;



================================================
File: crates/holochain_state/src/test_utils/mutations_helpers.rs
================================================
use crate::mutations::*;
use crate::prelude::*;
use holo_hash::HasHash;
use holochain_sqlite::rusqlite::Transaction;

pub fn insert_valid_integrated_op(
    txn: &mut Transaction,
    op: &DhtOpHashed,
) -> StateMutationResult<()> {
    let hash = op.as_hash();
    insert_op_dht(&mut txn.into(), op, None)?;
    set_validation_status(txn, hash, ValidationStatus::Valid)?;
    set_when_integrated(txn, hash, Timestamp::now())?;

    Ok(())
}



================================================
File: crates/holochain_state/tests/integration.rs
================================================
pub mod cache_tests;
pub mod corrupt_db;



================================================
File: crates/holochain_state/tests/cache_tests/mod.rs
================================================
use ::fixt::*;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::DhtOpHashFixturator;
use holo_hash::fixt::DnaHashFixturator;
use holo_hash::fixt::EntryHashFixturator;
use holo_hash::*;
use holochain_sqlite::prelude::*;
use holochain_state::mutations;
use holochain_state::prelude::*;
use pretty_assertions::assert_eq;
use std::collections::HashMap;
use std::ops::RangeInclusive;
use std::sync::Arc;
use test_case::test_case;

fn insert_action_and_op(txn: &mut Txn<DbKindDht>, action: &Action) -> DhtOpHash {
    let timestamp = Timestamp::now();
    let op_order = OpOrder::new(ChainOpType::RegisterAgentActivity, timestamp);
    let basis_hash: OpBasis = fixt!(EntryHash).into();
    let action = SignedActionHashed::with_presigned(
        ActionHashed::from_content_sync(action.clone()),
        Signature(vec![1; 64].try_into().unwrap()),
    );
    let hash = action.as_hash().clone();
    let op_hash = fixt!(DhtOpHash);
    mutations::insert_action(txn, &action).unwrap();
    mutations::insert_op_lite(
        txn,
        &ChainOpLite::RegisterAgentActivity(hash, basis_hash.clone()).into(),
        &op_hash,
        &op_order,
        &timestamp,
        None,
    )
    .unwrap();

    op_hash
}

fn set_integrated(db: &DbWrite<DbKindDht>, op_hash: DhtOpHash) {
    db.test_write({
        let op_hash = op_hash.clone();
        move |txn| {
            mutations::set_validation_stage(txn, &op_hash, ValidationStage::Pending).unwrap();
            mutations::set_when_integrated(txn, &op_hash, Timestamp::now()).unwrap();
        }
    });
}

fn set_ready_to_integrate(db: &DbWrite<DbKindDht>, op_hash: DhtOpHash) {
    db.test_write(move |txn| {
        mutations::set_validation_stage(txn, &op_hash, ValidationStage::AwaitingIntegration)
            .unwrap();
        mutations::set_validation_status(txn, &op_hash, ValidationStatus::Valid).unwrap();
    });
}

async fn check_state(
    cache: &DhtDbQueryCache,
    f: impl FnOnce(&HashMap<Arc<AgentPubKey>, ActivityState>),
) {
    cache.get_state().await.share_ref(|activity| f(activity));
}

#[tokio::test(flavor = "multi_thread")]
async fn cache_inits_correctly() {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| assert!(activity.is_empty())).await;

    let action = Action::Dna(Dna {
        author: fixt!(AgentPubKey),
        timestamp: Timestamp::now(),
        hash: fixt!(DnaHash),
    });
    let author = action.author().clone();
    let hash = ActionHash::with_data_sync(&action);
    let op_hash = db.test_write({
        let action = action.clone();
        move |txn| insert_action_and_op(txn, &action)
    });

    let cache = DhtDbQueryCache::new(db.clone().into());

    check_state(&cache, |activity| assert!(activity.is_empty())).await;

    set_ready_to_integrate(&db, op_hash.clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, None);
        assert_eq!(b.ready_to_integrate, Some(0));
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, *action.author());
    assert_eq!(to_integrate[0].1, 0..=0);

    set_integrated(&db, op_hash.clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 0);

    let mut action = fixt!(Create);
    action.prev_action = hash.clone();
    action.action_seq = 1;
    action.author = author.clone();
    let action: Action = action.into();
    let op_hash = db.test_write({
        let action = action.clone();
        move |txn| insert_action_and_op(txn, &action)
    });

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 0);

    set_ready_to_integrate(&db, op_hash.clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, Some(1));
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, *action.author());
    assert_eq!(to_integrate[0].1, 1..=1);

    set_integrated(&db, op_hash.clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(1));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 0);
}

#[tokio::test(flavor = "multi_thread")]
async fn cache_init_catches_gaps() {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));

    let action = Action::Dna(Dna {
        author: fixt!(AgentPubKey),
        timestamp: Timestamp::now(),
        hash: fixt!(DnaHash),
    });
    let hash = ActionHash::with_data_sync(&action);
    let author = action.author().clone();

    // Create the missing action so we can get the hash.
    let mut missing_action = fixt!(Create);
    missing_action.prev_action = hash;
    missing_action.action_seq = 1;
    missing_action.author = author.clone();
    let missing_action: Action = missing_action.into();
    let missing_hash = ActionHash::with_data_sync(&missing_action);

    let mut op_hashes = db.test_write({
        let action = action.clone();
        move |txn| {
            let mut op_hashes = Vec::new();
            op_hashes.push(insert_action_and_op(txn, &action));

            let mut action = fixt!(Create);
            action.prev_action = missing_hash;
            action.action_seq = 2;
            action.author = author.clone();
            let action: Action = action.into();
            op_hashes.push(insert_action_and_op(txn, &action));
            op_hashes
        }
    });

    set_ready_to_integrate(&db, op_hashes[0].clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, {
        let author = action.author().clone();
        move |activity| {
            let b = activity.get(&author).unwrap();
            assert_eq!(b.integrated, None);
            assert_eq!(b.ready_to_integrate, Some(0));
        }
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, *action.author());
    assert_eq!(to_integrate[0].1, 0..=0);

    set_integrated(&db, op_hashes[0].clone());
    set_ready_to_integrate(&db, op_hashes[1].clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 0);

    op_hashes.push(db.test_write(move |txn| insert_action_and_op(txn, &missing_action)));

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 0);

    set_ready_to_integrate(&db, op_hashes[2].clone());

    let cache = DhtDbQueryCache::new(db.clone().into());
    check_state(&cache, |activity| {
        let b = activity.get(action.author()).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, Some(2));
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, *action.author());
    assert_eq!(to_integrate[0].1, 1..=2);
}

#[tokio::test(flavor = "multi_thread")]
async fn cache_set_integrated() {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));

    let action = Action::Dna(fixt!(Dna));
    let author = action.author().clone();
    db.test_write(move |txn| insert_action_and_op(txn, &action));

    let cache = DhtDbQueryCache::new(db.clone().into());

    cache
        .set_activity_ready_to_integrate(&author, Some(0))
        .await
        .unwrap();

    check_state(&cache, |activity| {
        let b = activity.get(&author).unwrap();
        assert_eq!(b.integrated, None);
        assert_eq!(b.ready_to_integrate, Some(0));
    })
    .await;

    check_state(&cache, |activity| {
        dbg!(activity);
    })
    .await;
    cache
        .set_activity_to_integrated(&author, Some(0))
        .await
        .unwrap();

    check_state(&cache, |activity| {
        dbg!(activity);
        let b = activity.get(&author).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;

    cache
        .set_activity_ready_to_integrate(&author, Some(1))
        .await
        .unwrap();
    cache
        .set_activity_ready_to_integrate(&author, Some(2))
        .await
        .unwrap();

    check_state(&cache, |activity| {
        let b = activity.get(&author).unwrap();
        assert_eq!(b.integrated, Some(0));
        assert_eq!(b.ready_to_integrate, Some(2));
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, author);
    assert_eq!(to_integrate[0].1, 1..=2);

    cache
        .set_activity_to_integrated(&author, Some(1))
        .await
        .unwrap();

    check_state(&cache, |activity| {
        let b = activity.get(&author).unwrap();
        assert_eq!(b.integrated, Some(1));
        assert_eq!(b.ready_to_integrate, Some(2));
    })
    .await;

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert_eq!(to_integrate.len(), 1);
    assert_eq!(*to_integrate[0].0, author);
    assert_eq!(to_integrate[0].1, 2..=2);

    cache
        .set_activity_to_integrated(&author, Some(2))
        .await
        .unwrap();

    check_state(&cache, |activity| {
        let b = activity.get(&author).unwrap();
        assert_eq!(b.integrated, Some(2));
        assert_eq!(b.ready_to_integrate, None);
    })
    .await;
}

#[tokio::test(flavor = "multi_thread")]
async fn cache_set_all_integrated() {
    let test_activity: Vec<_> = std::iter::repeat_with(|| (Arc::new(fixt!(AgentPubKey)), 0..=100))
        .take(1000)
        .collect();
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());
    cache
        .set_all_activity_to_integrated(test_activity.clone())
        .await
        .unwrap();
    check_state(&cache, |activity| {
        for (author, seq_range) in &test_activity {
            let b = activity.get(author.as_ref()).unwrap();
            assert_eq!(b.integrated, Some(*seq_range.end()));
            assert_eq!(b.ready_to_integrate, None);
        }
    })
    .await;
    let test_activity: HashMap<_, _> = test_activity.into_iter().collect();
    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    for (author, seq_range) in to_integrate {
        let range = test_activity.get(&author).unwrap();
        assert_eq!(*range, seq_range);
    }
}

#[tokio::test(flavor = "multi_thread")]
async fn check_none_integrated_with_awaiting_deps() {
    let author = Arc::new(fixt!(AgentPubKey));
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());
    cache
        .set_activity_ready_to_integrate(author.as_ref(), Some(3))
        .await
        .unwrap();
    check_state(&cache, |activity| {
        let b = activity.get(author.as_ref()).unwrap();
        assert_eq!(b.integrated, None);
        assert_eq!(b.ready_to_integrate, None);
        assert_eq!(b.awaiting_deps, vec![3]);
    })
    .await;
    let to_integrate = cache.get_activity_to_integrate().await.unwrap();
    assert!(to_integrate.is_empty());
}

#[tokio::test(flavor = "multi_thread")]
async fn no_activities_to_integrate_when_nothing_waiting() {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();

    assert!(to_integrate.is_empty());
}

#[tokio::test(flavor = "multi_thread")]
async fn no_activities_to_integrate_when_everything_already_integrated() {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());
    let agent_key = Arc::new(fixt!(AgentPubKey));

    // Set some actions to be integrated
    for i in 0..10 {
        cache
            .set_activity_to_integrated(&agent_key, Some(i))
            .await
            .unwrap();
    }

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();

    assert!(to_integrate.is_empty());
}

#[test_case(0, 1, 0..=0 ; "only first action at index 0")]
#[test_case(0, 8, 0..=7 ; "starting from index 0")]
#[test_case(13, 8, 13..=20 ; "starting from later index")]
#[test_case(20, 1, 20..=20 ; "more integrated actions than ready to integrate ones")]
#[tokio::test(flavor = "multi_thread")]
// Pass expected as parameter so can use `pretty_assertions::assert_eq`
async fn range_of_activities_to_integrate_for_single_agent(
    start: u32,
    action_count: u32,
    expected_range: RangeInclusive<u32>,
) {
    let db = test_in_mem_db(DbKindDht(Arc::new(DnaHash::from_raw_32(vec![0; 32]))));
    let cache = DhtDbQueryCache::new(db.clone().into());
    let agent_key = Arc::new(fixt!(AgentPubKey));

    // Set previous actions to integrated
    for i in 0..start {
        cache
            .set_activity_to_integrated(&agent_key, Some(i))
            .await
            .unwrap();
    }

    for i in start..start + action_count {
        cache
            .set_activity_ready_to_integrate(&agent_key, Some(i))
            .await
            .unwrap();
    }

    let to_integrate = cache.get_activity_to_integrate().await.unwrap();

    assert_eq!(to_integrate.len(), 1);
    assert_eq!(to_integrate[0].0, agent_key);
    assert_eq!(to_integrate[0].1, expected_range);
}



================================================
File: crates/holochain_state/tests/corrupt_db/mod.rs
================================================
use ::fixt::*;
use holo_hash::fixt::DnaHashFixturator;
use holochain_sqlite::prelude::DatabaseError;
use holochain_sqlite::rusqlite::Connection;
use holochain_state::prelude::{mutations_helpers, *};
use std::{path::Path, sync::Arc};
use tempfile::TempDir;

#[tokio::test(flavor = "multi_thread")]
/// Checks a corrupt cache will be wiped on load.
async fn corrupt_cache_creates_new_db() {
    holochain_trace::test_run();

    let kind = DbKindCache(Arc::new(fixt!(DnaHash)));

    // - Create a corrupt cache db.
    let testdir = create_corrupt_db(kind.clone());

    // - Try to open it.
    let db = DbWrite::test(testdir.path(), kind).unwrap();

    // - It opens successfully but the data is wiped.
    let n: usize = db
        .read_async(move |txn| {
            txn.query_row("SELECT COUNT(rowid) FROM DhtOp", [], |row| row.get(0))
                .map_err(DatabaseError::from)
        })
        .await
        .unwrap();
    assert_eq!(n, 0);
}

#[tokio::test(flavor = "multi_thread")]
async fn corrupt_source_chain_panics() {
    holochain_trace::test_run();

    let kind = DbKindAuthored(Arc::new(fixt!(CellId)));

    // - Create a corrupt cell db.
    let testdir = create_corrupt_db(kind.clone());

    // - Try to open it.
    let result = DbWrite::test(testdir.path(), kind);

    // - It cannot open.
    assert!(result.is_err());
}

/// Corrupts some bytes of the db.
fn corrupt_db(path: &Path) {
    let mut file = std::fs::read(path).unwrap();

    for (i, b) in file.iter_mut().take(200).enumerate() {
        if i % 2 == 0 {
            *b = 0;
        }
    }
    std::fs::write(path, file).unwrap();
}

/// Creates a db with some data in it then corrupts the db.
fn create_corrupt_db<Kind: DbKindT>(kind: Kind) -> TempDir {
    let testdir = tempfile::Builder::new()
        .prefix("corrupt_source_chain")
        .tempdir()
        .unwrap();
    let path = testdir.path().join(kind.filename());
    std::fs::create_dir_all(path.parent().unwrap()).unwrap();
    let mut conn = Connection::open(&path).unwrap();
    holochain_sqlite::schema::SCHEMA_CELL
        .initialize(&mut conn, Some(kind.kind()))
        .unwrap();
    let op = DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(
        Signature(vec![1; 64].try_into().unwrap()),
        Action::Create(fixt!(Create)),
    ));
    let mut txn = conn
        .transaction_with_behavior(holochain_sqlite::rusqlite::TransactionBehavior::Exclusive)
        .unwrap();
    mutations_helpers::insert_valid_integrated_op(&mut txn, &op).unwrap();
    txn.commit().unwrap();
    conn.close().unwrap();
    corrupt_db(path.as_ref());
    testdir
}



================================================
File: crates/holochain_state_types/README.md
================================================
# holochain_state_types

License: CAL-1.0



================================================
File: crates/holochain_state_types/Cargo.toml
================================================
[package]
name = "holochain_state_types"
version = "0.5.0-dev.12"
description = "Types for the holochain_state crate"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_state_types"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
serde = { version = "1.0", features = ["derive"] }
holochain_integrity_types = { version = "^0.5.0-dev.12", path = "../holochain_integrity_types", default-features = false }
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash" }

[lints]
workspace = true



================================================
File: crates/holochain_state_types/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

- Change the license from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

- New package to extract types from `holochain_state`



================================================
File: crates/holochain_state_types/src/lib.rs
================================================
use holo_hash::ActionHash;
use holochain_integrity_types::{Action, Entry, Signature};
use serde::{Deserialize, Serialize};

// TODO fix this.  We shouldn't really have nil values but this would
// show if the database is corrupted and doesn't have a record
#[derive(Serialize, Debug, Clone, Deserialize, PartialEq, Eq)]
pub struct SourceChainDump {
    pub records: Vec<SourceChainDumpRecord>,
    pub published_ops_count: usize,
}

#[derive(Serialize, Debug, Clone, Deserialize, PartialEq, Eq)]
pub struct SourceChainDumpRecord {
    pub signature: Signature,
    pub action_address: ActionHash,
    pub action: Action,
    pub entry: Option<Entry>,
}

pub mod prelude {
    pub use crate::*;
}



================================================
File: crates/holochain_terminal/README.md
================================================
# hcterm

A terminal for viewing information about a running conductor and other Holochain-adjacent services.



================================================
File: crates/holochain_terminal/Cargo.toml
================================================
[package]
name = "hcterm"
version = "0.5.0-dev.21"
description = "A terminal for Holochain"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_terminal"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"
default-run = "hcterm"

[[bin]]
name = "hcterm"
path = "src/main.rs"

# reminder - do not use workspace deps
[dependencies]
anyhow = "1.0"
crossterm = "0.28.0"
ratatui = "0.28"
clap = { version = "4", features = ["derive"] }
url = "2"
once_cell = "1"
chrono = "0.4"
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash", features = [
  "encoding",
] }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../kitsune_p2p/types" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../kitsune_p2p/bin_data" }
kitsune_p2p_bootstrap_client = { version = "^0.5.0-dev.11", path = "../kitsune_p2p/bootstrap_client" }
holochain_util = { version = "^0.5.0-dev.1", path = "../holochain_util" }
holochain_conductor_api = { version = "^0.5.0-dev.21", path = "../holochain_conductor_api" }
holochain_websocket = { version = "^0.5.0-dev.21", path = "../holochain_websocket" }
holochain_types = { version = "^0.5.0-dev.21", path = "../holochain_types" }
tokio = { version = "1.36.0", features = ["full"] }

[lints]
workspace = true



================================================
File: crates/holochain_terminal/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## Unreleased

## 0.5.0-dev.21

## 0.5.0-dev.20

## 0.5.0-dev.19

## 0.5.0-dev.18

## 0.5.0-dev.17

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.28

## 0.4.0-dev.27

## 0.4.0-dev.26

## 0.4.0-dev.25

## 0.4.0-dev.24

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

- Adds a new `hcterm` landing screen to show what args have been provided and how to use the app.
- Fixes the network info screen of `hcterm` to not fetch network info on every render. Instead, it uses a 10s automatic refresh to fetch the network info.

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

- Change the license from MIT to Apache-2.0.

## 0.3.0-beta-dev.0

## 0.2.0

## 0.1.0

- New crate



================================================
File: crates/holochain_terminal/src/app.rs
================================================
use crate::cli::Args;
use crate::client::{AdminClient, AppClient};
use crate::event::ScreenEvent;
use kitsune_p2p_types::dependencies::tokio;
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct App {
    /// Whether the app should keep running
    running: bool,

    /// The current tab
    tab_index: usize,

    /// The number of tabs
    tab_count: usize,

    /// Events to be processed by the current screen
    pending_events: Vec<ScreenEvent>,

    /// The command line args provided to the terminal on launch
    args: Arc<Args>,

    /// An admin client if the `admin_url` flag was provided
    #[allow(dead_code)]
    admin_client: Option<Arc<Mutex<AdminClient>>>,

    /// An app client if the `admin_url` flag was provided
    app_client: Option<Arc<Mutex<AppClient>>>,
}

impl App {
    pub fn new(
        args: Args,
        admin_client: Option<AdminClient>,
        app_client: Option<AppClient>,
        tab_count: usize,
    ) -> Self {
        Self {
            running: true,
            tab_index: 0,
            tab_count,
            pending_events: vec![ScreenEvent::Refresh],
            args: Arc::new(args),
            admin_client: admin_client.map(|c| Arc::new(Mutex::new(c))),
            app_client: app_client.map(|c| Arc::new(Mutex::new(c))),
        }
    }

    pub fn is_running(&self) -> bool {
        self.running
    }

    pub fn stop(&mut self) {
        self.running = false;
    }

    pub fn push_event(&mut self, event: ScreenEvent) {
        self.pending_events.push(event);
    }

    pub fn drain_events(&mut self) -> Vec<ScreenEvent> {
        self.pending_events.drain(0..).collect()
    }

    pub fn tab_index(&self) -> usize {
        self.tab_index
    }

    pub fn incr_tab_index(&mut self) -> usize {
        self.tab_index = (self.tab_index + 1) % self.tab_count;
        self.tab_index
    }

    pub fn decr_tab_index(&mut self) -> usize {
        self.tab_index = (self.tab_index + self.tab_count - 1) % self.tab_count;
        self.tab_index
    }

    pub fn args(&self) -> Arc<Args> {
        self.args.clone()
    }

    #[allow(dead_code)]
    pub fn admin_client(&mut self) -> Option<Arc<Mutex<AdminClient>>> {
        self.admin_client.clone()
    }

    pub fn app_client(&mut self) -> Option<Arc<Mutex<AppClient>>> {
        self.app_client.clone()
    }
}



================================================
File: crates/holochain_terminal/src/cli.rs
================================================
use anyhow::anyhow;
use clap::Parser;
use holo_hash::{DnaHash, DnaHashB64};
use url::Url;

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
pub struct Args {
    /// The websocket URL to connect to the conductor admin API. For example ws://localhost:8000
    #[arg(long)]
    pub admin_url: Option<Url>,

    /// The bootstrap URL to connect to for debugging peer discovery. For example http://localhost:3000
    #[arg(long)]
    pub bootstrap_url: Option<Url>,

    /// The DNA hash in Base64 format to use for
    #[arg(long, value_parser = dna_hash_parser)]
    pub dna_hash: Option<DnaHash>,

    /// The app ID to discover information for
    #[arg(long)]
    pub app_id: Option<String>,
}

impl Args {
    pub fn validate(&self) -> anyhow::Result<()> {
        if let Some(admin_url) = &self.admin_url {
            if admin_url.scheme() != "ws" && admin_url.scheme() != "wss" {
                return Err(anyhow!("Admin URL should use the ws or wss scheme"));
            }
        }

        if let Some(bootstrap_url) = &self.bootstrap_url {
            if bootstrap_url.scheme() != "http" && bootstrap_url.scheme() != "https" {
                return Err(anyhow!("Bootstrap URL should use the http or https scheme"));
            }
        }

        Ok(())
    }
}

fn dna_hash_parser(v: &str) -> anyhow::Result<DnaHash> {
    let raw = DnaHashB64::from_b64_str(v)?;
    Ok(raw.into())
}



================================================
File: crates/holochain_terminal/src/client.rs
================================================
use anyhow::{anyhow, Context};
use holo_hash::{AgentPubKey, DnaHash};
use holochain_conductor_api::{
    AdminRequest, AdminResponse, AppAuthenticationRequest, AppAuthenticationToken, AppInfo,
    AppInterfaceInfo, AppRequest, AppResponse, CellInfo, NetworkInfo,
};
use holochain_types::prelude::{InstalledAppId, NetworkInfoRequestPayload};
use holochain_types::websocket::AllowedOrigins;
use holochain_websocket::{connect, ConnectRequest, WebsocketConfig, WebsocketSender};
use std::sync::Arc;

pub struct AppClient {
    tx: WebsocketSender,
    rx: tokio::task::JoinHandle<()>,
}

impl Drop for AppClient {
    fn drop(&mut self) {
        self.rx.abort();
    }
}

impl AppClient {
    /// Creates a App websocket client which can send messages but ignores any incoming messages
    async fn connect(
        addr: std::net::SocketAddr,
        token: AppAuthenticationToken,
    ) -> anyhow::Result<Self> {
        let (tx, mut rx) = connect(
            Arc::new(WebsocketConfig::CLIENT_DEFAULT),
            ConnectRequest::new(addr).try_set_header("origin", HC_TERM_ORIGIN)?,
        )
        .await?;

        let rx = tokio::task::spawn(async move { while rx.recv::<AppResponse>().await.is_ok() {} });

        tx.authenticate(AppAuthenticationRequest { token })
            .await
            .context("Failed to authenticate app client")?;

        Ok(AppClient { tx, rx })
    }

    pub async fn discover_network_info_params(
        &mut self,
        app_id: InstalledAppId,
    ) -> anyhow::Result<(AgentPubKey, Vec<(String, DnaHash)>)> {
        let app_info = self
            .app_info()
            .await?
            .ok_or(anyhow!("App not found {}", app_id))?;

        let agent = app_info.agent_pub_key;
        let named_dna_hashes: Vec<(String, DnaHash)> = app_info
            .cell_info
            .values()
            .flat_map(|c| {
                c.iter().filter_map(|c| match c {
                    CellInfo::Provisioned(p) => {
                        Some((p.name.clone(), p.cell_id.dna_hash().clone()))
                    }
                    _ => None,
                })
            })
            .collect();

        Ok((agent, named_dna_hashes))
    }

    pub async fn network_info(
        &mut self,
        agent: AgentPubKey,
        dna_hashes: Vec<DnaHash>,
    ) -> anyhow::Result<Vec<NetworkInfo>> {
        let r = NetworkInfoRequestPayload {
            agent_pub_key: agent,
            dnas: dna_hashes,
            last_time_queried: None,
        };
        let msg = AppRequest::NetworkInfo(Box::new(r));
        let response = self.send(msg).await?;
        match response {
            AppResponse::NetworkInfo(infos) => Ok(infos),
            _ => unreachable!("Unexpected response {:?}", response),
        }
    }

    async fn app_info(&mut self) -> anyhow::Result<Option<AppInfo>> {
        let msg = AppRequest::AppInfo;
        let response = self.send(msg).await?;
        match response {
            AppResponse::AppInfo(app_info) => Ok(app_info),
            _ => unreachable!("Unexpected response {:?}", response),
        }
    }

    async fn send(&mut self, msg: AppRequest) -> anyhow::Result<AppResponse> {
        let response = self.tx.request(msg).await?;

        match response {
            AppResponse::Error(error) => Err(anyhow!("External error: {:?}", error)),
            _ => Ok(response),
        }
    }
}

pub struct AdminClient {
    tx: WebsocketSender,
    rx: tokio::task::JoinHandle<()>,
    addr: std::net::SocketAddr,
}

impl Drop for AdminClient {
    fn drop(&mut self) {
        self.rx.abort();
    }
}

const HC_TERM_ORIGIN: &str = "hcterm";

impl AdminClient {
    /// Creates an Admin websocket client which can send messages but ignores any incoming messages
    pub async fn connect(addr: std::net::SocketAddr) -> anyhow::Result<Self> {
        let (tx, mut rx) = connect(Arc::new(WebsocketConfig::CLIENT_DEFAULT), addr).await?;

        let rx =
            tokio::task::spawn(async move { while rx.recv::<AdminResponse>().await.is_ok() {} });

        Ok(AdminClient { tx, rx, addr })
    }

    pub async fn connect_app_client(
        &mut self,
        installed_app_id: InstalledAppId,
    ) -> anyhow::Result<AppClient> {
        let app_interfaces = self.list_app_interfaces().await?;

        let app_port = if let Some(interface) =
            Self::select_usable_app_interface(app_interfaces, installed_app_id.clone())
        {
            interface.port
        } else {
            self.attach_app_interface(0).await?
        };

        let app_addr = (self.addr.ip(), app_port).into();

        let issue_token_response = self
            .tx
            .request(AdminRequest::IssueAppAuthenticationToken(
                installed_app_id.into(),
            ))
            .await?;
        let token = match issue_token_response {
            AdminResponse::AppAuthenticationTokenIssued(issued) => issued.token,
            _ => anyhow::bail!("Unexpected response {:?}", issue_token_response),
        };

        AppClient::connect(app_addr, token).await
    }

    async fn list_app_interfaces(&mut self) -> anyhow::Result<Vec<AppInterfaceInfo>> {
        let msg = AdminRequest::ListAppInterfaces;
        let response = self.send(msg).await?;
        match response {
            AdminResponse::AppInterfacesListed(interfaces) => Ok(interfaces),
            _ => unreachable!("Unexpected response {:?}", response),
        }
    }

    async fn attach_app_interface(&mut self, port: u16) -> anyhow::Result<u16> {
        let msg = AdminRequest::AttachAppInterface {
            port: Some(port),
            allowed_origins: HC_TERM_ORIGIN.to_string().into(),
            installed_app_id: None,
        };
        let response = self.send(msg).await?;
        match response {
            AdminResponse::AppInterfaceAttached { port } => Ok(port),
            _ => unreachable!("Unexpected response {:?}", response),
        }
    }

    fn select_usable_app_interface(
        interfaces: impl IntoIterator<Item = AppInterfaceInfo>,
        installed_app_id: InstalledAppId,
    ) -> Option<AppInterfaceInfo> {
        interfaces.into_iter().find(|interface| {
            let can_use_app_id = interface.installed_app_id.is_none()
                || interface.installed_app_id.clone().unwrap() == installed_app_id;

            let can_use_origin = match interface.allowed_origins {
                AllowedOrigins::Any => true,
                AllowedOrigins::Origins(ref origins) => origins.contains(HC_TERM_ORIGIN),
            };

            can_use_app_id && can_use_origin
        })
    }

    async fn send(&mut self, msg: AdminRequest) -> anyhow::Result<AdminResponse> {
        let response = self.tx.request(msg).await?;

        match response {
            AdminResponse::Error(error) => Err(anyhow!("External error: {:?}", error)),
            _ => Ok(response),
        }
    }
}



================================================
File: crates/holochain_terminal/src/components.rs
================================================
pub mod bootstrap;
pub mod common;
pub mod home;
pub mod network_info;



================================================
File: crates/holochain_terminal/src/event.rs
================================================
use crate::app::App;
use crossterm::event::{self, Event, KeyCode};

#[derive(Debug)]
pub enum ScreenEvent {
    Refresh,
    SwitchNetwork,
    NavDown,
    NavUp,
}

pub fn handle_events(app: &mut App) -> anyhow::Result<()> {
    if event::poll(std::time::Duration::from_millis(50))? {
        if let Event::Key(key) = event::read()? {
            if key.kind == event::KeyEventKind::Press {
                if key.code == KeyCode::Tab {
                    app.incr_tab_index();
                } else if key.code == KeyCode::BackTab {
                    app.decr_tab_index();
                } else if key.code == KeyCode::Char('q') || key.code == KeyCode::Esc {
                    app.stop();
                } else if key.code == KeyCode::Char('r') {
                    app.push_event(ScreenEvent::Refresh);
                } else if key.code == KeyCode::Char('n') {
                    app.push_event(ScreenEvent::SwitchNetwork);
                } else if key.code == KeyCode::Down {
                    app.push_event(ScreenEvent::NavDown)
                } else if key.code == KeyCode::Up {
                    app.push_event(ScreenEvent::NavUp)
                }
            }
        }
    }
    Ok(())
}



================================================
File: crates/holochain_terminal/src/main.rs
================================================
mod app;
mod cli;
mod client;
mod components;
mod event;
mod tui;

use crate::app::App;
use crate::cli::Args;
use crate::client::AdminClient;
use crate::event::handle_events;
use crate::tui::Tui;
use anyhow::anyhow;
use clap::Parser;
use holochain_util::tokio_helper::block_on;
use ratatui::prelude::*;
use std::io::{self};
use std::time::Duration;

fn main() -> anyhow::Result<()> {
    let args = Args::parse();
    args.validate()?;

    let (admin_client, app_client) = if let Some(admin_url) = &args.admin_url {
        let connect_clients_result = block_on(
            async {
                let addr = if let url::Origin::Tuple(_, host, port) = admin_url.origin() {
                    match tokio::net::lookup_host((host.to_string(), port)).await {
                        Ok(mut addr_list) => addr_list.next(),
                        Err(err) => return Err(anyhow!(err)),
                    }
                } else {
                    None
                };

                let addr = match addr {
                    None => return Err(anyhow!(format!("Invalid admin_url: {admin_url}"))),
                    Some(addr) => addr,
                };

                let mut admin_client = AdminClient::connect(addr).await?;
                let app_client = if let Some(app_id) = &args.app_id {
                    Some(admin_client.connect_app_client(app_id.clone()).await?)
                } else {
                    None
                };

                Ok((admin_client, app_client))
            },
            Duration::from_secs(10),
        );
        match connect_clients_result {
            Ok(Ok((admin_client, app_client))) => (Some(admin_client), app_client),
            Ok(Err(e)) => {
                return Err(e);
            }
            Err(_) => {
                return Err(anyhow!("Timed out while connecting to Holochain"));
            }
        }
    } else {
        (None, None)
    };

    let mut app = App::new(args, admin_client, app_client, 3);

    let backend = CrosstermBackend::new(io::stdout());
    let terminal = Terminal::new(backend)?;
    let mut tui = Tui::new(terminal);
    tui.init()?;

    while app.is_running() {
        tui.draw(&mut app)?;
        handle_events(&mut app)?;
    }

    tui.exit()?;
    Ok(())
}



================================================
File: crates/holochain_terminal/src/tui.rs
================================================
use crate::app::App;
use crate::components::bootstrap::BootstrapWidget;
use crate::components::home::HomeWidget;
use crate::components::network_info::NetworkInfoWidget;
use crossterm::terminal::{enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen};
use crossterm::{terminal, ExecutableCommand};
use ratatui::backend::Backend;
use ratatui::layout::{Alignment, Constraint};
use ratatui::prelude::{Color, Direction, Layout, Line, Style};
use ratatui::symbols::DOT;
use ratatui::widgets::{Block, Tabs};
use ratatui::{Frame, Terminal};
use std::io;
use std::io::stdout;
use std::panic;

#[derive(Debug)]
pub struct Tui<B: Backend> {
    /// Interface to the Terminal.
    terminal: Terminal<B>,
}

impl<B: Backend> Tui<B> {
    /// Constructs a new instance of [`Tui`].
    pub fn new(terminal: Terminal<B>) -> Self {
        Self { terminal }
    }

    pub fn init(&mut self) -> anyhow::Result<()> {
        stdout().execute(EnterAlternateScreen)?;
        enable_raw_mode()?;

        // Define a custom panic hook to reset the terminal properties.
        // This way, you won't have your terminal messed up if an unexpected error happens.
        let panic_hook = panic::take_hook();
        panic::set_hook(Box::new(move |panic| {
            Self::reset().expect("failed to reset the terminal");
            panic_hook(panic);
        }));

        self.terminal.hide_cursor()?;
        self.terminal.clear()?;
        Ok(())
    }

    /// Draw the terminal interface by [`render`]ing the widgets.
    pub fn draw(&mut self, app: &mut App) -> anyhow::Result<()> {
        self.terminal.draw(|frame| render(app, frame))?;
        Ok(())
    }

    /// Resets the terminal interface.
    fn reset() -> anyhow::Result<()> {
        terminal::disable_raw_mode()?;
        crossterm::execute!(io::stdout(), LeaveAlternateScreen)?;
        Ok(())
    }

    /// Exits the terminal interface.
    pub fn exit(&mut self) -> anyhow::Result<()> {
        Self::reset()?;
        self.terminal.show_cursor()?;
        Ok(())
    }
}

fn render(app: &mut App, frame: &mut Frame) {
    let root_layout = Layout::default()
        .direction(Direction::Vertical)
        .constraints([Constraint::Length(2), Constraint::Min(0)])
        .split(frame.area());

    let titles = ["Home", "Network", "Bootstrap"]
        .iter()
        .cloned()
        .map(Line::from);
    let tabs = Tabs::new(titles)
        .select(app.tab_index())
        .block(
            Block::default()
                .title("Holochain terminal")
                .title_alignment(Alignment::Center),
        )
        .style(Style::default().fg(Color::White))
