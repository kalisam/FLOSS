    let op =
        ChainOp::RegisterDeletedEntryAction(fixt!(Signature), delete_delete_action.clone()).into();
    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![delete_action_signed_hashed.clone()])
        .with_op(op)
        .run()
        .await
        .unwrap();
    assert_eq!(
        Outcome::Rejected(
            ValidationOutcome::NotNewEntry(delete_action_signed_hashed.action().clone())
                .to_string()
        ),
        outcome
    );

    // Validate a deleted by.
    let op = ChainOp::RegisterDeletedBy(fixt!(Signature), delete_delete_action).into();
    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![])
        .with_op(op)
        .run()
        .await
        .unwrap();
    assert_eq!(
        Outcome::Rejected(
            ValidationOutcome::NotNewEntry(delete_action_signed_hashed.action().clone())
                .to_string()
        ),
        outcome
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn validate_valid_add_link() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Op to validate
    let mut create_link_action = fixt!(CreateLink);
    create_link_action.tag = "hello".as_bytes().to_vec().into();
    create_link_action.timestamp = Timestamp::now();
    let op = ChainOp::RegisterAddLink(fixt!(Signature), create_link_action).into();

    // Note that no mocking is configured so the base and target addressed for the link aren't not going to be checked.
    // This is intentional as the validation isn't meant to check them but not very obvious from this test, hence the comment!
    let outcome = test_case.with_op(op).run().await.unwrap();

    assert_eq!(Outcome::Accepted, outcome);
}

#[tokio::test(flavor = "multi_thread")]
async fn validate_add_link_tag_too_large() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Op to validate
    let mut create_link_action = fixt!(CreateLink);
    create_link_action.tag = vec![0; 2_000].into();
    create_link_action.timestamp = Timestamp::now();
    let op = ChainOp::RegisterAddLink(fixt!(Signature), create_link_action).into();

    // Note that no mocking is configured so the base and target addressed for the link aren't not going to be checked.
    // This is intentional as the validation isn't meant to check them but not very obvious from this test, hence the comment!
    let outcome = test_case.with_op(op).run().await.unwrap();

    assert_eq!(
        Outcome::Rejected(ValidationOutcome::TagTooLarge(2_000).to_string()),
        outcome
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn validate_valid_remove_link() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Previous action
    let mut action = fixt!(CreateLink);
    action.author = test_case.agent.clone();
    action.timestamp = Timestamp::now();
    let previous_action = test_case.sign_action(Action::CreateLink(action)).await;

    // Op to validate
    let mut delete_link_action = fixt!(DeleteLink);
    delete_link_action.timestamp = Timestamp::now();
    delete_link_action.link_add_address = previous_action.as_hash().clone();
    let op = ChainOp::RegisterRemoveLink(fixt!(Signature), delete_link_action).into();

    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![previous_action])
        .with_op(op)
        .run()
        .await
        .unwrap();

    assert_eq!(Outcome::Accepted, outcome);
}

#[tokio::test(flavor = "multi_thread")]
async fn validate_remove_link_missing_link_add_ref() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Dummy action to set up the mock, won't be referenced
    let mut dummy_action = fixt!(CreateLink);
    dummy_action.author = test_case.agent.clone();
    dummy_action.timestamp = Timestamp::now();
    let dummy_action = test_case
        .sign_action(Action::CreateLink(dummy_action))
        .await;

    let mut mismatched_action_hash = fixt!(ActionHash);
    loop {
        if dummy_action.as_hash() != &mismatched_action_hash {
            break;
        }
        mismatched_action_hash = fixt!(ActionHash);
    }

    // Op to validate
    let mut delete_link_action = fixt!(DeleteLink);
    delete_link_action.timestamp = Timestamp::now();
    delete_link_action.link_add_address = mismatched_action_hash;
    let op = ChainOp::RegisterRemoveLink(fixt!(Signature), delete_link_action).into();

    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![dummy_action])
        .with_op(op)
        .run()
        .await
        .unwrap();

    assert!(
        matches!(outcome, Outcome::MissingDhtDep),
        "Expected MissingDhtDep but actual outcome was {:?}",
        outcome
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn validate_remove_link_with_wrong_target_type() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Previous action
    let mut action = fixt!(Update);
    action.author = test_case.agent.clone();
    action.timestamp = Timestamp::now();
    let previous_action = test_case.sign_action(Action::Update(action)).await;

    // Op to validate
    let mut delete_link_action = fixt!(DeleteLink);
    delete_link_action.timestamp = Timestamp::now();
    delete_link_action.link_add_address = previous_action.as_hash().clone();
    let op = ChainOp::RegisterRemoveLink(fixt!(Signature), delete_link_action).into();

    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![previous_action.clone()])
        .with_op(op)
        .run()
        .await
        .unwrap();

    assert_eq!(
        Outcome::Rejected(
            ValidationOutcome::NotCreateLink(previous_action.as_hash().clone()).to_string()
        ),
        outcome
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn action_after_close_chain() {
    holochain_trace::test_run();

    let mut test_case = TestCase::new().await;

    // Previous action
    let mut dna_action = CloseChain {
        author: test_case.agent.clone(),
        timestamp: Timestamp::now(),
        action_seq: 23,
        prev_action: fixt!(ActionHash),
        new_target: Some(fixt!(MigrationTarget)),
    };

    // If this is an agent migration, the agent keypair needs to exist
    // so the Close can be signed.
    if let Some(MigrationTarget::Agent(agent)) = dna_action.new_target.as_mut() {
        *agent = test_case.keystore.new_sign_keypair_random().await.unwrap();
    }

    let previous_action = test_case.sign_action(Action::CloseChain(dna_action)).await;

    let mut create = fixt!(Create);
    create.prev_action = previous_action.as_hash().clone();
    create.action_seq = 24;
    create.entry_type = EntryType::App(AppEntryDef {
        entry_index: 0.into(),
        zome_index: 0.into(),
        visibility: EntryVisibility::Public,
    });

    // Use agent activity so that we'll validate the previous action
    let op =
        ChainOp::RegisterAgentActivity(fixt!(Signature), Action::Create(create.clone())).into();

    let outcome = test_case
        .expect_retrieve_records_from_cascade(vec![previous_action])
        .with_op(op)
        .run()
        .await
        .unwrap();

    assert_eq!(
        Outcome::Rejected(
            ValidationOutcome::PrevActionError(
                (
                    PrevActionErrorKind::ActionAfterChainClose,
                    Action::Create(create)
                )
                    .into()
            )
            .to_string()
        ),
        outcome
    );
}

// TODO this hits code which claims to be unreachable. Clearly it isn't so investigate the code path.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "TODO fix this test"]
async fn crash_case() {
    holochain_trace::test_run();

    let keystore = holochain_keystore::test_keystore();

    let agent = keystore.new_sign_keypair_random().await.unwrap();

    // This is the previous
    let mut create_action = fixt!(AgentValidationPkg);
    create_action.author = agent.clone();
    create_action.timestamp = Timestamp::now();
    create_action.action_seq = 10;
    let action = Action::AgentValidationPkg(create_action);
    let action_hashed = ActionHashed::from_content_sync(action);
    let signed_action = SignedActionHashed::sign(&keystore, action_hashed)
        .await
        .unwrap();

    // and current which needs values from previous
    let op = test_op(&signed_action);

    let dna_def = DnaDef::unique_from_zomes(vec![], vec![]);
    let dna_def = DnaDefHashed::from_content_sync(dna_def);

    let mut cascade = MockCascade::new();

    cascade.expect_retrieve_action().times(1).returning({
        let signed_action = signed_action.clone();
        move |_, _| {
            let signed_action = signed_action.clone();
            async move { Ok(Some((signed_action, CascadeSource::Local))) }.boxed()
        }
    });

    cascade
        .expect_retrieve()
        .times(1)
        .returning(move |_hash, _options| {
            let signed_action = signed_action.clone();
            async move {
                // TODO this line creates the problem, expects a None value
                Ok(Some((
                    Record::new(signed_action, Some(Entry::Agent(fixt!(AgentPubKey)))),
                    CascadeSource::Local,
                )))
            }
            .boxed()
        });

    let validation_outcome = validate_op(&op, &dna_def, SysValDeps::default(), None)
        .await
        .unwrap();

    assert!(matches!(validation_outcome, Outcome::Accepted));
}

struct TestCase {
    op: Option<DhtOp>,
    keystore: holochain_keystore::MetaLairClient,
    cascade: MockCascade,
    current_validation_dependencies: SysValDeps,
    dna_def: DnaDef,
    agent: AgentPubKey,
}

impl TestCase {
    async fn new() -> Self {
        let dna_def = DnaDef::unique_from_zomes(vec![], vec![]);

        let keystore = holochain_keystore::test_keystore();
        let agent = keystore.new_sign_keypair_random().await.unwrap();

        TestCase {
            op: None,
            keystore,
            cascade: MockCascade::new(),
            current_validation_dependencies: SysValDeps::default(),
            dna_def,
            agent,
        }
    }

    pub fn with_op(&mut self, op: DhtOp) -> &mut Self {
        self.op = Some(op);
        self
    }

    pub fn cascade_mut(&mut self) -> &mut MockCascade {
        &mut self.cascade
    }

    pub fn dna_def_mut(&mut self) -> &mut DnaDef {
        &mut self.dna_def
    }

    pub fn dna_def_hash(&self) -> HoloHashed<DnaDef> {
        DnaDefHashed::from_content_sync(self.dna_def.clone())
    }

    pub async fn sign_action(&self, action: Action) -> SignedActionHashed {
        let action_hashed = ActionHashed::from_content_sync(action);
        SignedActionHashed::sign(&self.keystore, action_hashed)
            .await
            .unwrap()
    }

    pub fn expect_retrieve_records_from_cascade(
        &mut self,
        previous_actions: Vec<SignedActionHashed>,
    ) -> &mut Self {
        let previous_actions = previous_actions
            .into_iter()
            .map(|a| (a.as_hash().clone(), a))
            .collect::<HashMap<_, _>>();
        self.cascade
            .expect_retrieve_action()
            .times(previous_actions.len())
            .returning({
                let previous_actions = previous_actions.clone();
                move |hash, _| match previous_actions.get(&hash).cloned() {
                    Some(action) => async move { Ok(Some((action, CascadeSource::Local))) }.boxed(),
                    None => async move { Ok(None) }.boxed(),
                }
            });

        self
    }

    async fn run(&mut self) -> WorkflowResult<Outcome> {
        let dna_def = self.dna_def_hash();

        // Swap out the cascade so we can move it into the workflow
        let mut new_cascade = MockCascade::new();
        std::mem::swap(&mut new_cascade, &mut self.cascade);

        let cascade = Arc::new(new_cascade);

        retrieve_previous_actions_for_ops(
            self.current_validation_dependencies.clone(),
            cascade.clone(),
            vec![self
                .op
                .as_ref()
                .expect("No op set, invalid test case")
                .clone()]
            .into_iter()
            .map(DhtOpHashed::from_content_sync),
        )
        .await;

        validate_op(
            self.op.as_ref().expect("No op set, invalid test case"),
            &dna_def,
            self.current_validation_dependencies.clone(),
            None,
        )
        .await
    }
}

fn test_op(previous: &SignedHashed<Action>) -> DhtOp {
    let mut create_action = fixt!(Create);
    create_action.author = previous.action().author().clone();
    create_action.action_seq = previous.action().action_seq() + 1;
    create_action.prev_action = previous.as_hash().clone();
    create_action.timestamp = Timestamp::now();
    create_action.entry_type = EntryType::App(AppEntryDef {
        entry_index: 0.into(),
        zome_index: 0.into(),
        visibility: EntryVisibility::Public,
    });
    let action = Action::Create(create_action);

    ChainOp::RegisterAgentActivity(fixt!(Signature), action).into()
}



================================================
File: crates/holochain/src/core/workflow/sys_validation_workflow/validation_deps.rs
================================================
use holo_hash::HoloHash;
use holochain_cascade::CascadeSource;
use holochain_types::prelude::*;
use std::{
    collections::{HashMap, HashSet},
    sync::Arc,
};

#[derive(Clone)]
/// The sources of all dependencies needed in sys validation.
/// Currently this comprises only action hashes within the same DHT, but could
/// some day include items from other DHTs or other sources.
pub struct SysValDeps {
    /// Dependencies found in the same DHT as the dependent
    pub same_dht: Arc<parking_lot::Mutex<ValidationDependencies<SignedActionHashed>>>,
    /// Dependencies found in the DPKI service's Deepkey DNA
    /// (this is not generic for all possible DPKI implementations, only Deepkey)
    /// (this is also currently unused! It may be useful if we need to be more explicit
    /// about missing DPKI dependencies, but so far it hasn't been a problem. If it's never
    /// a problem, this can be removed.)
    pub _deepkey_dht: Arc<parking_lot::Mutex<ValidationDependencies<EntryHashed>>>,
}

impl Default for SysValDeps {
    fn default() -> Self {
        Self {
            same_dht: Arc::new(parking_lot::Mutex::new(ValidationDependencies::new())),
            _deepkey_dht: Arc::new(parking_lot::Mutex::new(ValidationDependencies::new())),
        }
    }
}

/// A collection of validation dependencies for the current set of DHT ops requiring validation.
/// This is used as an in-memory cache of dependency info, held across all validation workflow calls,
/// to minimize the amount of network and database calls needed to check if dependencies have been satisfied
pub struct ValidationDependencies<T: HasHash = SignedActionHashed> {
    /// The state of each dependency, keyed by its hash.
    states: HashMap<HoloHash<T::HashType>, ValidationDependencyState<T>>,
    /// Tracks which dependencies have been accessed during a search for dependencies. Anything which
    /// isn't in this set is no longer needed for validation and can be dropped from [`states`].
    retained_deps: HashSet<HoloHash<T::HashType>>,
}

impl<T> Default for ValidationDependencies<T>
where
    T: holo_hash::HasHash,
{
    fn default() -> Self {
        Self::new()
    }
}

impl<T> ValidationDependencies<T>
where
    T: holo_hash::HasHash,
{
    pub fn new() -> Self {
        Self {
            states: HashMap::new(),
            retained_deps: HashSet::new(),
        }
    }

    /// Check whether a given dependency is currently held.
    /// Note that we may have this dependency as a key but the state won't contain the dependency because
    /// this is how we're tracking ops we know we need to fetch from the network.
    pub fn has(&mut self, hash: &HoloHash<T::HashType>) -> bool {
        self.retained_deps.insert(hash.clone());
        self.states
            .get(hash)
            .map(|state| state.dependency.is_some())
            .unwrap_or(false)
    }

    /// Get the state of a given dependency. This should always return a value because we should know about the dependency
    /// by examining the ops that are being validated. However, the dependency may not be found on the DHT yet.
    pub fn get(&self, hash: &HoloHash<T::HashType>) -> Option<&ValidationDependencyState<T>> {
        match self.states.get(hash) {
            Some(dep) => Some(dep),
            None => {
                tracing::warn!(hash = ?hash, "Have not attempted to fetch requested dependency, this is a bug");
                None
            }
        }
    }

    pub fn get_mut(
        &mut self,
        hash: &HoloHash<T::HashType>,
    ) -> Option<&mut ValidationDependencyState<T>> {
        match self.states.get_mut(hash) {
            Some(dep) => Some(dep),
            None => {
                tracing::warn!(hash = ?hash, "Have not attempted to fetch requested dependency, this is a bug");
                None
            }
        }
    }

    /// Get the hashes of all dependencies that are currently missing from the DHT.
    pub fn get_missing_hashes(&self) -> Vec<HoloHash<T::HashType>> {
        self.states
            .iter()
            .filter_map(|(hash, state)| {
                if state.dependency.is_none() {
                    Some(hash.clone())
                } else {
                    None
                }
            })
            .collect()
    }

    /// Get the hashes of all dependencies that have been fetched from the network.
    /// We need to let the incoming dht ops workflow know about these so that it can ingest them and get them validated.
    pub fn get_network_fetched_hashes(&self) -> Vec<HoloHash<T::HashType>> {
        self.states
            .iter()
            .filter_map(|(hash, state)| match state {
                ValidationDependencyState {
                    dependency:
                        Some(ValidationDependency {
                            fetched_from: CascadeSource::Network,
                            ..
                        }),
                    ..
                } => Some(hash.clone()),
                _ => None,
            })
            .collect()
    }

    /// Insert a record which was found after this set of dependencies was created.
    pub fn insert(&mut self, action: T, source: CascadeSource) -> bool {
        let hash = action.as_hash();

        // Note that `has` is checking that the dependency is actually set, not just that we have the key!
        if self.has(hash) {
            tracing::warn!(hash = ?hash, "Attempted to insert a dependency that was already present, this is not expected");
            return false;
        }

        self.retained_deps.insert(hash.clone());

        if let Some(s) = self.states.get_mut(hash) {
            s.set_dep(action);
            s.set_source(source);
            return true;
        }

        false
    }

    /// Forget which dependencies have been accessed since this method was last called.
    /// This is intended to be used with [`Self::purge_held_deps`] to remove any dependencies that are no longer needed.
    pub fn clear_retained_deps(&mut self) {
        self.retained_deps.clear();
    }

    /// Remove any dependencies that are no longer needed for validation.
    pub fn purge_held_deps(&mut self) {
        self.states.retain(|k, _| self.retained_deps.contains(k));
    }

    /// Merge the dependencies from another set into this one.
    pub fn merge(&mut self, other: Self) {
        self.retained_deps.extend(other.states.keys().cloned());
        self.states.extend(other.states);
    }

    pub fn new_from_iter<
        I: IntoIterator<Item = (HoloHash<T::HashType>, ValidationDependencyState<T>)>,
    >(
        iter: I,
    ) -> Self {
        Self {
            states: iter.into_iter().collect(),
            retained_deps: HashSet::new(),
        }
    }
}

#[derive(Clone, Debug)]
pub struct ValidationDependencyState<T> {
    /// The dependency if we've been able to fetch it, otherwise None until we manage to find it.
    dependency: Option<ValidationDependency<T>>,
}

impl<T> ValidationDependencyState<T> {
    pub fn new(dependency: Option<ValidationDependency<T>>) -> Self {
        Self { dependency }
    }

    pub fn single(dep: T, fetched_from: CascadeSource) -> Self {
        Self {
            dependency: Some(ValidationDependency { dep, fetched_from }),
        }
    }

    pub fn set_dep(&mut self, dep: T) {
        match self.dependency {
            None => {
                self.dependency = Some(ValidationDependency {
                    dep,
                    fetched_from: CascadeSource::Network,
                });
            }
            _ => {
                tracing::warn!("Attempted to set a record on a dependency that already has a value, this is a bug")
            }
        }
    }

    pub fn set_source(&mut self, new_source: CascadeSource) {
        if let Some(ValidationDependency { fetched_from, .. }) = &mut self.dependency {
            *fetched_from = new_source;
        }
    }
}

impl ValidationDependencyState<SignedActionHashed> {
    pub fn as_action(&self) -> Option<&Action> {
        self.dependency.as_ref().map(|d| d.dep.action())
    }
}

/// A validation dependency which is either an Action or a Record, and the source of the dependency.
#[derive(Clone, Debug)]
#[allow(clippy::large_enum_variant)]
pub struct ValidationDependency<T> {
    dep: T,
    fetched_from: CascadeSource,
}



================================================
File: crates/holochain/src/core/workflow/sys_validation_workflow/validation_query.rs
================================================
use holochain_sqlite::db::DbKindDht;
use holochain_state::prelude::*;

use crate::core::workflow::WorkflowResult;

/// Get all ops that need to sys or app validated in order.
/// - Sys validated or awaiting app dependencies.
/// - Ordered by type then timestamp (See [`OpOrder`])
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn get_ops_to_app_validate(db: &DbRead<DbKindDht>) -> WorkflowResult<Vec<DhtOpHashed>> {
    get_ops_to_validate(db, false).await
}

/// Get all ops that need to sys or app validated in order.
/// - Pending or awaiting sys dependencies.
/// - Ordered by type then timestamp (See [`OpOrder`])
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn get_ops_to_sys_validate(db: &DbRead<DbKindDht>) -> WorkflowResult<Vec<DhtOpHashed>> {
    get_ops_to_validate(db, true).await
}

async fn get_ops_to_validate(
    db: &DbRead<DbKindDht>,
    system: bool,
) -> WorkflowResult<Vec<DhtOpHashed>> {
    let mut sql = "
        SELECT
        Action.blob as action_blob,
        Action.author as author,
        Entry.blob as entry_blob,
        DhtOp.type as dht_type,
        DhtOp.hash as dht_hash
        FROM DhtOp
        JOIN
        Action ON DhtOp.action_hash = Action.hash
        LEFT JOIN
        Entry ON Action.entry_hash = Entry.hash
        "
    .to_string();
    if system {
        sql.push_str(
            "
            WHERE
            DhtOp.when_integrated IS NULL
            AND DhtOp.validation_status IS NULL
            AND (
                DhtOp.validation_stage IS NULL
                OR DhtOp.validation_stage = 0
            )
            ",
        );
    } else {
        sql.push_str(
            "
            WHERE
            DhtOp.when_integrated IS NULL
            AND DhtOp.validation_status IS NULL
            AND (
                DhtOp.validation_stage = 1
                OR DhtOp.validation_stage = 2
            )
            ",
        );
    }
    // TODO: There is a very unlikely chance that 10000 ops
    // could all fail to validate and prevent validation from
    // moving on but this is not easy to overcome.
    // Once we impl abandoned this won't happen anyway.
    sql.push_str(
        "
        ORDER BY
        DhtOp.num_validation_attempts ASC,
        DhtOp.op_order ASC
        LIMIT 10000
        ",
    );
    db.read_async(move |txn| {
        let mut stmt = txn.prepare(&sql)?;
        let r = stmt.query_and_then([], |row| {
            let op = WorkflowResult::Ok(holochain_state::query::map_sql_dht_op(
                true, "dht_type", row,
            )?)?;
            let hash = row.get("dht_hash")?;
            Ok(DhtOpHashed::with_pre_hashed(op, hash))
        })?;
        let r = r.collect();
        WorkflowResult::Ok(r)
    })
    .await?
}

#[cfg(test)]
mod tests {
    use ::fixt::prelude::*;
    use holo_hash::HasHash;
    use holochain_sqlite::prelude::DatabaseResult;
    use holochain_state::prelude::*;
    use holochain_state::validation_db::ValidationStage;
    use std::collections::HashSet;

    use super::*;

    #[derive(Debug, Clone, Copy)]
    struct Facts {
        pending: bool,
        awaiting_sys_deps: bool,
        sys_validated: bool,
        awaiting_app_deps: bool,
        awaiting_integration: bool,
        has_validation_status: bool,
        num_attempts: usize,
    }

    struct Expected {
        to_sys_validate: Vec<DhtOpHashed>,
        to_app_validate: Vec<DhtOpHashed>,
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn sys_validation_query() {
        holochain_trace::test_run();
        let db = test_dht_db();
        let expected = create_test_data(&db.to_db()).await;
        let ops = get_ops_to_sys_validate(&db.to_db().into()).await.unwrap();

        assert_sorted_by_op_order(&ops).await;
        assert_sorted_by_validation_attempts(&db.to_db(), &ops).await;

        // Check all the expected ops were returned
        for op in ops {
            assert!(expected.to_sys_validate.iter().any(|i| *i == op));
        }
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn app_validation_query() {
        holochain_trace::test_run();
        let db = test_dht_db();
        let expected = create_test_data(&db.to_db()).await;
        let ops = get_ops_to_app_validate(&db.to_db().into()).await.unwrap();

        assert_sorted_by_op_order(&ops).await;
        assert_sorted_by_validation_attempts(&db.to_db(), &ops).await;

        // Check all the expected ops were returned
        for op in ops {
            assert!(expected.to_app_validate.iter().any(|i| *i == op));
        }
    }

    /// Make sure both workflows can't pull in the same ops.
    #[tokio::test(flavor = "multi_thread")]
    async fn workflows_are_exclusive() {
        holochain_trace::test_run();
        let db = test_dht_db();
        create_test_data(&db.to_db()).await;
        let app_validation_ops = get_ops_to_app_validate(&db.to_db().into()).await.unwrap();
        let sys_validation_ops = get_ops_to_sys_validate(&db.to_db().into()).await.unwrap();

        let app_hashes = app_validation_ops
            .into_iter()
            .map(|o| o.hash)
            .collect::<HashSet<_>>();
        let sys_hashes = sys_validation_ops
            .into_iter()
            .map(|o| o.hash)
            .collect::<HashSet<_>>();

        let overlap = app_hashes.intersection(&sys_hashes).collect::<HashSet<_>>();
        assert!(overlap.is_empty());
    }

    async fn create_test_data(db: &DbWrite<DbKindDht>) -> Expected {
        let mut to_sys_validate = Vec::with_capacity(40);
        let mut to_app_validate = Vec::with_capacity(40);

        // We **do** expect any of these in the sys validation results but **do not** expect them in the app validation results:
        let facts = Facts {
            pending: true, // Should appear in sys validation if no validation stage is set
            awaiting_sys_deps: false,
            sys_validated: false,
            awaiting_app_deps: false,
            awaiting_integration: false,
            has_validation_status: false,
            num_attempts: 1,
        };
        for _ in 0..20 {
            let op = create_and_insert_op(db, facts).await;
            to_sys_validate.push(op);
        }

        let facts = Facts {
            pending: false,
            awaiting_sys_deps: true, // Should appear in sys validation and be retried if awaiting deps
            sys_validated: false,
            awaiting_app_deps: false,
            awaiting_integration: false,
            has_validation_status: false,
            num_attempts: 0,
        };
        for _ in 0..20 {
            let op = create_and_insert_op(db, facts).await;
            to_sys_validate.push(op);
        }

        // We **don't** expect any of these in the sys validation results but **do** expect them in app the validation results:
        let facts = Facts {
            pending: false,
            awaiting_sys_deps: false,
            sys_validated: true, // Should appear in app validation if sys validation has already been done
            awaiting_app_deps: false,
            awaiting_integration: false,
            has_validation_status: true,
            num_attempts: 6,
        };
        for _ in 0..20 {
            let op = create_and_insert_op(db, facts).await;
            to_app_validate.push(op);
        }

        let facts = Facts {
            pending: false,
            awaiting_sys_deps: false,
            sys_validated: false,
            awaiting_app_deps: true, // Should appear in app validation and be retried if awaiting deps
            awaiting_integration: false,
            has_validation_status: true,
            num_attempts: 2,
        };
        for _ in 0..20 {
            let op = create_and_insert_op(db, facts).await;
            to_app_validate.push(op);
        }

        // We **don't** expect any of these to appear in either sys validation or app validation
        let facts = Facts {
            pending: false,
            awaiting_sys_deps: false,
            sys_validated: false,
            awaiting_app_deps: true,
            awaiting_integration: true, // Should not appear once sys and app validation has finished and waiting for integration
            has_validation_status: true,
            num_attempts: 5,
        };
        for _ in 0..20 {
            create_and_insert_op(db, facts).await;
        }

        let facts = Facts {
            pending: false,
            awaiting_sys_deps: false,
            sys_validated: false,
            awaiting_app_deps: false,
            awaiting_integration: false,
            has_validation_status: true, // Should not appear if there is already a validation outcome
            num_attempts: 10,
        };
        for _ in 0..20 {
            create_and_insert_op(db, facts).await;
        }

        Expected {
            to_sys_validate,
            to_app_validate,
        }
    }

    async fn create_and_insert_op(db: &DbWrite<DbKindDht>, facts: Facts) -> DhtOpHashed {
        let state = DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(
            fixt!(Signature),
            fixt!(Action),
        ));

        db.write_async({
            let query_state = state.clone();

            move |txn| -> DatabaseResult<()> {
                let hash = query_state.as_hash().clone();
                // XXX: This is inserted into the DHT DB, so `transfer_data` here should be Some
                insert_op_dht(txn, &query_state, None).unwrap();
                if facts.has_validation_status {
                    set_validation_status(txn, &hash, ValidationStatus::Valid).unwrap();
                }
                if facts.pending {
                    // No need to do anything because status and stage are null already.
                } else if facts.awaiting_sys_deps {
                    set_validation_stage(txn, &hash, ValidationStage::AwaitingSysDeps).unwrap();
                } else if facts.sys_validated {
                    set_validation_stage(txn, &hash, ValidationStage::SysValidated).unwrap();
                } else if facts.awaiting_app_deps {
                    set_validation_stage(txn, &hash, ValidationStage::AwaitingAppDeps).unwrap();
                } else if facts.awaiting_integration {
                    set_validation_stage(txn, &hash, ValidationStage::AwaitingIntegration).unwrap();
                }
                txn.execute(
                    "UPDATE DhtOp SET num_validation_attempts = :num_attempts",
                    named_params! {
                        ":num_attempts": facts.num_attempts,
                    },
                )?;
                Ok(())
            }
        })
        .await
        .unwrap();
        state
    }

    async fn assert_sorted_by_op_order(ops: &Vec<DhtOpHashed>) {
        let mut ops_sorted = ops.clone();
        ops_sorted.sort_by_key(|d| {
            let op_type = d.get_type();
            let timestamp = d.timestamp();
            OpOrder::new(op_type, timestamp)
        });
        assert_eq!(ops, &ops_sorted);
    }

    async fn assert_sorted_by_validation_attempts(db: &DbWrite<DbKindDht>, ops: &Vec<DhtOpHashed>) {
        assert!(
            get_num_validation_attempts(db, ops.iter().map(|op| op.hash.clone()).collect())
                .await
                .windows(2)
                .all(|w| { w[0] <= w[1] })
        );
    }

    async fn get_num_validation_attempts(
        db: &DbWrite<DbKindDht>,
        hashes: Vec<DhtOpHash>,
    ) -> Vec<usize> {
        db.read_async(|txn| -> DatabaseResult<Vec<usize>> {
            hashes
                .into_iter()
                .map(|h| -> DatabaseResult<usize> {
                    Ok(txn.query_row(
                        "SELECT num_validation_attempts FROM DhtOp WHERE hash = :op_hash",
                        named_params! {
                            ":op_hash": h
                        },
                        |r| r.get(0),
                    )?)
                })
                .collect()
        })
        .await
        .unwrap()
    }
}



================================================
File: crates/holochain/src/core/workflow/validation_receipt_workflow/tests.rs
================================================
use crate::sweettest::*;
use crate::test_utils::inline_zomes::simple_create_read_zome;
use hdk::prelude::*;
use holo_hash::DhtOpHash;
use holochain_keystore::AgentPubKeyExt;
use holochain_state::prelude::*;
#[cfg(feature = "unstable-warrants")]
use {
    crate::core::ribosome::guest_callback::validate::ValidateResult, crate::prelude::InlineZomeSet,
};

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky, doesn't take into account timing or retries"]
async fn test_validation_receipt() {
    holochain_trace::test_run();
    const NUM_CONDUCTORS: usize = 3;

    let mut conductors = SweetConductorBatch::from_standard_config(NUM_CONDUCTORS).await;

    let (dna_file, _, _) =
        SweetDnaFile::unique_from_inline_zomes(("simple", simple_create_read_zome())).await;

    let apps = conductors.setup_app("app", &[dna_file]).await.unwrap();
    conductors.exchange_peer_info().await;

    let ((alice,), (bobbo,), (carol,)) = apps.into_tuples();

    // Call the "create" zome fn on Alice's app
    let hash: ActionHash = conductors[0]
        .call(&alice.zome("simple"), "create", ())
        .await;

    await_consistency(10, [&alice, &bobbo, &carol])
        .await
        .unwrap();

    // Get op hashes
    let vault = alice.dht_db();
    let record = vault
        .read_async(move |txn| -> StateQueryResult<Record> {
            Ok(CascadeTxnWrapper::from(txn)
                .get_record(&hash.clone().into())?
                .unwrap())
        })
        .await
        .unwrap();
    let ops = produce_ops_from_record(&record)
        .unwrap()
        .into_iter()
        .map(|op| DhtOpHash::with_data_sync(&op))
        .collect::<Vec<_>>();

    // Wait for receipts to be sent
    crate::assert_eq_retry_10s!(
        {
            let mut counts = Vec::new();
            for hash in &ops {
                let count = vault
                    .read_async({
                        let query_hash = hash.clone();
                        move |txn| -> StateQueryResult<usize> {
                            Ok(list_receipts(txn, &query_hash)?.len())
                        }
                    })
                    .await
                    .unwrap();
                counts.push(count);
            }
            counts
        },
        vec![2, 2, 2],
    );

    // Check alice has receipts from both bobbo and carol
    for hash in &ops {
        let receipts: Vec<_> = vault
            .read_async({
                let query_hash = hash.clone();
                move |txn| list_receipts(txn, &query_hash)
            })
            .await
            .unwrap();
        assert_eq!(receipts.len(), 2);
        for receipt in receipts {
            let SignedValidationReceipt {
                receipt,
                validators_signatures: sigs,
            } = receipt;
            let validator = receipt.validators[0].clone();
            assert!(validator == *bobbo.agent_pubkey() || validator == *carol.agent_pubkey());
            assert!(validator.verify_signature(&sigs[0], receipt).await.unwrap());
        }
    }

    // Check alice has 2 receipts in their authored dht ops table.
    crate::assert_eq_retry_1m!(
        {
            vault
                .read_async(move |txn| -> DatabaseResult<Vec<u32>> {
                    let mut stmt = txn
                        .prepare("SELECT COUNT(hash) FROM ValidationReceipt GROUP BY op_hash")
                        .unwrap();
                    Ok(stmt
                        .query_map([], |row| row.get::<_, Option<u32>>(0))
                        .unwrap()
                        .filter_map(Result::unwrap)
                        .collect::<Vec<u32>>())
                })
                .await
                .unwrap()
        },
        vec![2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
    );
}

#[cfg(feature = "unstable-warrants")]
macro_rules! wait_until {
    ($expression:expr; $interval_ms:literal; $timeout_ms:literal; $wait_msg:literal; $timeout_msg:literal;) => {
        let timeout = (Timestamp::now() + std::time::Duration::from_millis($timeout_ms)).unwrap();
        let interval_duration = std::time::Duration::from_millis($interval_ms);
        tokio::time::sleep(interval_duration).await;
        while !$expression {
            if Timestamp::now() > timeout {
                panic!($timeout_msg);
            }
            dbg!($wait_msg);
            tokio::time::sleep(interval_duration).await;
        }
    };
}

#[cfg(feature = "unstable-warrants")]
#[tokio::test(flavor = "multi_thread")]
#[cfg_attr(target_os = "macos", ignore = "flaky")]
#[cfg_attr(target_os = "windows", ignore = "flaky")]
async fn test_block_invalid_receipt() {
    holochain_trace::test_run();
    let unit_entry_def = EntryDef::default_from_id("unit");
    let integrity_name = "integrity";
    let coordinator_name = "coordinator";
    let integrity_uuid = "a";
    let create_coordinator_uuid = "b";
    let check_coordinator_uuid = "c";
    let network_seed = "network_seed";
    let create_function_name = "create";
    let app_prefix = "app-";

    let zomes_that_create = InlineZomeSet::new_single(
        integrity_name,
        coordinator_name,
        integrity_uuid,
        create_coordinator_uuid,
        vec![unit_entry_def.clone()],
        0,
    )
    .function(coordinator_name, create_function_name, move |api, ()| {
        let entry = Entry::app(().try_into().unwrap()).unwrap();
        let hash = api.create(CreateInput::new(
            InlineZomeSet::get_entry_location(&api, EntryDefIndex(0)),
            EntryVisibility::Public,
            entry,
            ChainTopOrdering::default(),
        ))?;
        Ok(hash)
    });

    let zomes_that_check = InlineZomeSet::new_single(
        integrity_name,
        coordinator_name,
        integrity_uuid,
        check_coordinator_uuid,
        vec![unit_entry_def.clone()],
        0,
    )
    .function(integrity_name, "validate", |_api, op: Op| match op {
        Op::StoreEntry(StoreEntry { action, .. })
            if action.hashed.content.app_entry_def().is_some() =>
        {
            Ok(ValidateResult::Invalid("Entry defs are bad".into()))
        }
        _ => Ok(ValidateResult::Valid),
    });

    let config = SweetConductorConfig::rendezvous(true);
    let conductors = SweetConductorBatch::from_config_rendezvous(2, config).await;

    let mut conductors = conductors.into_inner().into_iter();

    let mut alice_conductor = conductors.next().unwrap();
    let mut bob_conductor = conductors.next().unwrap();

    let (dna_that_creates, _, _) =
        SweetDnaFile::from_inline_zomes(network_seed.into(), zomes_that_create).await;

    let (dna_that_checks, _, _) =
        SweetDnaFile::from_inline_zomes(network_seed.into(), zomes_that_check).await;

    let alice_app = alice_conductor
        .setup_app(app_prefix, &[dna_that_creates])
        .await
        .unwrap();

    let (alice_cell,) = alice_app.into_tuple();

    let bob_apps = bob_conductor
        .setup_app(app_prefix, &[dna_that_checks])
        .await
        .unwrap();

    let (bob_cell,) = bob_apps.into_tuple();

    let _action_hash: ActionHash = alice_conductor
        .call(&alice_cell.zome(coordinator_name), create_function_name, ())
        .await;

    // Don't check alice's integrated ops, since she gets blocked during gossip
    await_consistency_advanced(
        10,
        vec![(alice_cell.agent_pubkey().clone(), 1)],
        [(&alice_cell, false), (&bob_cell, true)],
    )
    .await
    .unwrap();

    let alice_block_target = BlockTargetId::Cell(alice_cell.cell_id().to_owned());
    let bob_block_target = BlockTargetId::Cell(bob_cell.cell_id().to_owned());

    for now in [Timestamp::now(), Timestamp::MIN, Timestamp::MAX] {
        assert!(!alice_conductor
            .spaces
            .is_blocked(alice_block_target.clone(), now)
            .await
            .unwrap());
        assert!(!alice_conductor
            .spaces
            .is_blocked(bob_block_target.clone(), now)
            .await
            .unwrap());
        assert!(!bob_conductor
            .spaces
            .is_blocked(bob_block_target.clone(), now)
            .await
            .unwrap());

        // It can take a little more than consistency to have the receipts
        // processed.
        wait_until!(
            bob_conductor.spaces.is_blocked(alice_block_target.clone(), now).await.unwrap();
            1_000;
            20_000;
            "waiting for block due to warrant";
            "warrant block never happened";
        );
    }
}



================================================
File: crates/holochain/src/core/workflow/validation_receipt_workflow/unit_tests.rs
================================================
use crate::core::queue_consumer::WorkComplete;
use crate::core::workflow::validation_receipt_workflow::validation_receipt_workflow;
use crate::prelude::CreateFixturator;
use crate::prelude::DhtOpHashed;
use crate::prelude::SignatureFixturator;
use ::fixt::fixt;
use futures::future::BoxFuture;
use futures::FutureExt;
use hdk::prelude::Action;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::DnaHashFixturator;
use holo_hash::HasHash;
use holo_hash::{AgentPubKey, DhtOpHash};
use holochain_p2p::MockHolochainP2pDnaT;
use holochain_sqlite::error::DatabaseResult;
use holochain_sqlite::prelude::{DbKindDht, DbWrite};
use holochain_state::prelude::*;
use parking_lot::RwLock;
use rusqlite::named_params;
use std::sync::Arc;

#[tokio::test(flavor = "multi_thread")]
async fn no_running_cells() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    let mut dna = MockHolochainP2pDnaT::new();
    dna.expect_send_validation_receipts().never(); // Verify no receipts sent

    let work_complete = validation_receipt_workflow(
        Arc::new(fixt!(DnaHash)),
        vault,
        dna,
        keystore,
        vec![].into_iter().collect(), // No running cells
        |_block| unreachable!("This test should not send a block"),
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);
}

#[tokio::test(flavor = "multi_thread")]
async fn do_not_block_or_send_to_self() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    let dna_hash = fixt!(DnaHash);
    let author = fixt!(AgentPubKey);

    // Create a valid op that would require a validation receipt except that it's created by us
    let (_, valid_op_hash) =
        create_op_with_status(vault.clone(), Some(author.clone()), ValidationStatus::Valid)
            .await
            .unwrap();

    // Create a rejected op which would usually cause a block but it's created by us
    let (_, rejected_op_hash) = create_op_with_status(
        vault.clone(),
        Some(author.clone()),
        ValidationStatus::Rejected,
    )
    .await
    .unwrap();

    let mut dna = MockHolochainP2pDnaT::new();

    dna.expect_send_validation_receipts().never(); // Verify no receipts sent

    let validator = CellId::new(dna_hash.clone(), author);

    let work_complete = validation_receipt_workflow(
        Arc::new(dna_hash),
        vault.clone(),
        dna,
        keystore,
        vec![validator].into_iter().collect(), // No running cells
        |_block| unreachable!("This test should not send a block"), // Verify no blocks sent
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);

    assert!(!get_requires_receipt(vault.clone(), valid_op_hash).await);
    assert!(!get_requires_receipt(vault.clone(), rejected_op_hash).await);
}

#[tokio::test(flavor = "multi_thread")]
async fn block_invalid_op_author() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    // Any op created by somebody else, which has been rejected by validation.
    let (author, op_hash) = create_op_with_status(vault.clone(), None, ValidationStatus::Rejected)
        .await
        .unwrap();

    // We'll still send a validation receipt, but we should also block them
    let mut dna = MockHolochainP2pDnaT::new();
    dna.expect_send_validation_receipts()
        .return_once(|_, _| Ok(()));

    let dna_hash = fixt!(DnaHash);
    let validator = CellId::new(
        dna_hash.clone(),
        keystore.new_sign_keypair_random().await.unwrap(),
    );

    let blocks = Arc::new(RwLock::new(Vec::<Block>::new()));

    let work_complete = validation_receipt_workflow(
        Arc::new(dna_hash),
        vault.clone(),
        dna,
        keystore,
        vec![validator].into_iter().collect(),
        {
            let blocks = blocks.clone();
            move |block| -> BoxFuture<DatabaseResult<()>> {
                blocks.write().push(block);
                async move { Ok(()) }.boxed()
            }
        },
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);

    {
        let read_blocks = blocks.read();
        assert_eq!(1, read_blocks.len());
        match read_blocks.first().unwrap().target() {
            BlockTarget::Cell(cell_id, reason) => {
                assert_eq!(CellBlockReason::InvalidOp(op_hash.clone()), *reason);
                assert_eq!(author, *cell_id.agent_pubkey());
            }
            _ => unreachable!("Only expect a cell block"),
        }
    }

    // The op was rejected and the sender blocked but the `require_receipt` flag should still be cleared
    // so we don't reprocess the op.
    assert!(!get_requires_receipt(vault, op_hash).await);
}

#[tokio::test(flavor = "multi_thread")]
async fn continues_if_receipt_cannot_be_signed() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    // Any op created by somebody else, which is valid
    let (_, op_hash) = create_op_with_status(vault.clone(), None, ValidationStatus::Valid)
        .await
        .unwrap();

    let mut dna = MockHolochainP2pDnaT::new();
    dna.expect_send_validation_receipts().never();

    let dna_hash = fixt!(DnaHash);

    let invalid_validator = CellId::new(
        dna_hash.clone(),
        fixt!(AgentPubKey), // Not valid because it won't be found in Lair
    );

    let work_complete = validation_receipt_workflow(
        Arc::new(dna_hash),
        vault.clone(),
        dna,
        keystore,
        vec![invalid_validator].into_iter().collect(),
        |_block| unreachable!("Should not try to block"),
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);
    assert!(!get_requires_receipt(vault, op_hash).await);
}

#[tokio::test(flavor = "multi_thread")]
async fn send_validation_receipt() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    // Any op created by somebody else, which is valid
    let (_, op_hash) = create_op_with_status(vault.clone(), None, ValidationStatus::Valid)
        .await
        .unwrap();

    let mut dna = MockHolochainP2pDnaT::new();
    dna.expect_send_validation_receipts()
        .return_once(|_, _| Ok(()));

    let dna_hash = fixt!(DnaHash);

    let validator = CellId::new(
        dna_hash.clone(),
        keystore.new_sign_keypair_random().await.unwrap(),
    );

    let work_complete = validation_receipt_workflow(
        Arc::new(dna_hash),
        vault.clone(),
        dna,
        keystore,
        vec![validator].into_iter().collect(), // No running cells
        |_block| unreachable!("Should not try to block"),
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);

    // Should no longer require a receipt
    assert!(!get_requires_receipt(vault.clone(), op_hash).await);
}

#[tokio::test(flavor = "multi_thread")]
async fn errors_for_some_ops_does_not_prevent_the_workflow_proceeding() {
    holochain_trace::test_run();

    let test_db = holochain_state::test_utils::test_dht_db();
    let vault = test_db.to_db();
    let keystore = holochain_keystore::test_keystore();

    let (author1, op_hash1) = create_op_with_status(vault.clone(), None, ValidationStatus::Valid)
        .await
        .unwrap();

    let (author2, op_hash2) = create_op_with_status(vault.clone(), None, ValidationStatus::Valid)
        .await
        .unwrap();

    let mut dna = MockHolochainP2pDnaT::new();
    let mut seq = mockall::Sequence::new();
    dna.expect_send_validation_receipts()
        .times(1)
        .withf(move |author: &AgentPubKey, _| *author == author1)
        .in_sequence(&mut seq)
        .returning(|_, _| Err("I'm a test error".into()));

    dna.expect_send_validation_receipts()
        .times(1)
        .withf(move |author: &AgentPubKey, _| *author == author2)
        .in_sequence(&mut seq)
        .returning(|_, _| Ok(()));

    let dna_hash = fixt!(DnaHash);

    let validator = CellId::new(
        dna_hash.clone(),
        keystore.new_sign_keypair_random().await.unwrap(),
    );

    let work_complete = validation_receipt_workflow(
        Arc::new(dna_hash),
        vault.clone(),
        dna,
        keystore,
        vec![validator].into_iter().collect(), // No running cells
        |_block| unreachable!("Should not try to block"),
    )
    .await
    .unwrap();

    assert_eq!(WorkComplete::Complete, work_complete);

    // Should no longer require a receipt for either
    assert!(!get_requires_receipt(vault.clone(), op_hash1).await);
    assert!(!get_requires_receipt(vault.clone(), op_hash2).await);
}

async fn create_op_with_status(
    vault: DbWrite<DbKindDht>,
    author: Option<AgentPubKey>,
    validation_status: ValidationStatus,
) -> StateMutationResult<(AgentPubKey, DhtOpHash)> {
    // The actual op does not matter, just some of the status fields
    let mut create_action = fixt!(Create);
    let author = author.unwrap_or_else(|| fixt!(AgentPubKey));
    create_action.author = author.clone();
    let action = Action::Create(create_action);

    let op =
        DhtOpHashed::from_content_sync(ChainOp::RegisterAgentActivity(fixt!(Signature), action));

    let test_op_hash = op.as_hash().clone();
    vault
        .write_async({
            let test_op_hash = test_op_hash.clone();
            move |txn| -> StateMutationResult<()> {
                holochain_state::mutations::insert_op_dht(txn, &op, None)?;
                set_require_receipt(txn, &test_op_hash, true)?;
                set_when_integrated(txn, &test_op_hash, Timestamp::now())?;
                set_validation_status(txn, &test_op_hash, validation_status)?;

                Ok(())
            }
        })
        .await
        .unwrap();

    Ok((author, test_op_hash))
}

async fn get_requires_receipt(vault: DbWrite<DbKindDht>, op_hash: DhtOpHash) -> bool {
    vault
        .read_async(move |txn| -> DatabaseResult<bool> {
            let requires = txn.query_row(
                "SELECT require_receipt FROM DhtOp WHERE hash = :hash",
                named_params! {
                    ":hash": op_hash,
                },
                |row| row.get(0),
            )?;

            Ok(requires)
        })
        .await
        .unwrap()
}



================================================
File: crates/holochain/src/fixt/curve.rs
================================================
use holochain_wasm_test_utils::TestWasm;

pub struct Zomes(pub Vec<TestWasm>);



================================================
File: crates/holochain/src/sweettest/mod.rs
================================================
//! sweettest = Streamlined Holochain test utils with lots of added sugar
//!
//! Features:
//!
//! ### SweetConductor
//! A wrapper around ConductorHandle which provides useful methods for app setup
//! and zome calling, as well as some helpful references to Cells and Zomes
//! which make zome interaction much less verbose.
//!
//! ### SweetApp
//! A handy collection of cells installed under the same app.
//! Makes it easy to destructure the result of a SweetConductor::setup_app call
//! into a collection of SweetCells which can be used for zome calls.

mod sweet_agents;
mod sweet_app;
mod sweet_app_installation;
mod sweet_cell;
mod sweet_conductor;
mod sweet_conductor_batch;
mod sweet_conductor_config;
mod sweet_conductor_config_rendezvous;
mod sweet_conductor_handle;
pub mod sweet_consistency;
mod sweet_dna;
mod sweet_zome;

pub use sweet_agents::*;
pub use sweet_app::*;
pub use sweet_app_installation::*;
pub use sweet_cell::*;
pub use sweet_conductor::*;
pub use sweet_conductor_batch::*;
pub use sweet_conductor_config::*;
pub use sweet_conductor_config_rendezvous::*;
pub use sweet_conductor_handle::*;
pub use sweet_consistency::*;
pub use sweet_dna::*;
pub use sweet_zome::*;



================================================
File: crates/holochain/src/sweettest/sweet_agents.rs
================================================
//! Simple methods for generating collections of AgentPubKeys for use in tests

use futures::StreamExt;
use holo_hash::AgentPubKey;
use holochain_keystore::MetaLairClient;
use holochain_types::prelude::*;

/// Provides simple methods for generating collections of AgentPubKeys for use in tests
pub struct SweetAgents;

impl SweetAgents {
    /// Get an infinite stream of AgentPubKeys
    pub fn stream(keystore: MetaLairClient) -> impl futures::Stream<Item = AgentPubKey> {
        futures::stream::unfold(keystore, |keystore| async {
            let key = keystore
                .new_sign_keypair_random()
                .await
                .expect("can generate AgentPubKey");
            Some((key, keystore))
        })
    }

    /// Get a Vec of AgentPubKeys
    pub async fn get(keystore: MetaLairClient, num: usize) -> Vec<AgentPubKey> {
        Self::stream(keystore).take(num).collect().await
    }

    /// Get one AgentPubKey
    pub async fn one(keystore: MetaLairClient) -> AgentPubKey {
        let mut agents = Self::get(keystore, 1).await;
        agents.pop().unwrap()
    }

    /// Get two AgentPubKeys
    pub async fn two(keystore: MetaLairClient) -> (AgentPubKey, AgentPubKey) {
        let mut agents = Self::get(keystore, 2).await;
        (agents.pop().unwrap(), agents.pop().unwrap())
    }

    /// Get three AgentPubKeys
    pub async fn three(keystore: MetaLairClient) -> (AgentPubKey, AgentPubKey, AgentPubKey) {
        let mut agents = Self::get(keystore, 3).await;
        (
            agents.pop().unwrap(),
            agents.pop().unwrap(),
            agents.pop().unwrap(),
        )
    }

    /// Get the same two AgentPubKeys every time
    pub fn alice_and_bob() -> (AgentPubKey, AgentPubKey) {
        (fake_agent_pubkey_1(), fake_agent_pubkey_2())
    }

    /// Return only alice.
    pub fn alice() -> AgentPubKey {
        fake_agent_pubkey_1()
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_app.rs
================================================
use super::SweetCell;
use holo_hash::AgentPubKey;
use holochain_types::app::InstalledAppId;
use itertools::traits::HomogeneousTuple;
use itertools::Itertools;

/// An installed app, with prebuilt SweetCells
#[derive(Clone, Debug)]
pub struct SweetApp {
    installed_app_id: InstalledAppId,
    cells: Vec<SweetCell>,
}

impl SweetApp {
    /// Constructor
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub(super) fn new(installed_app_id: InstalledAppId, cells: Vec<SweetCell>) -> Self {
        // Ensure that all Agents are the same
        assert!(
            cells.iter().map(|c| c.agent_pubkey()).dedup().count() == 1,
            "Agent key differs across Cells in this app"
        );
        Self {
            installed_app_id,
            cells,
        }
    }

    /// Accessor
    pub fn installed_app_id(&self) -> &InstalledAppId {
        &self.installed_app_id
    }

    /// Accessor
    pub fn cells(&self) -> &Vec<SweetCell> {
        &self.cells
    }

    /// Accessor
    pub fn into_cells(self) -> Vec<SweetCell> {
        self.cells
    }

    /// Returns the AgentPubKey associated with this app.
    /// All Cells in this app will have the same Agent, so we just return the first.
    pub fn agent(&self) -> &AgentPubKey {
        self.cells[0].agent_pubkey()
    }

    /// Helper to destructure into a tuple of SweetCells.
    /// Can only be used for up to 4 cells. For more, please use `into_cells`.
    pub fn into_tuple<Inner>(self) -> Inner
    where
        Inner: HomogeneousTuple<Item = SweetCell>,
        Inner::Buffer: std::convert::AsRef<[Option<SweetCell>]>,
        Inner::Buffer: std::convert::AsMut<[Option<SweetCell>]>,
    {
        self.into_cells()
            .into_iter()
            .collect_tuple::<Inner>()
            .expect(
                "Wrong number of Cells in destructuring pattern, or too many (must be 4 or less)",
            )
    }
}

/// A collection of installed apps
#[derive(
    Clone,
    derive_more::From,
    derive_more::Into,
    derive_more::AsRef,
    derive_more::IntoIterator,
    derive_more::Index,
)]
pub struct SweetAppBatch(pub(super) Vec<SweetApp>);

impl SweetAppBatch {
    /// Get the underlying data
    pub fn into_inner(self) -> Vec<SweetApp> {
        self.0
    }

    /// Helper to destructure the nested cell data as nested tuples.
    /// The outer tuple contains the apps, the inner layer contains the cells in each app.
    ///
    /// Each level of nesting can contain 1-4 items, i.e. up to 4 apps with 4 DNAs each.
    /// Beyond 4, and this will PANIC! (But it's just for tests so it's fine.)
    pub fn into_tuples<Outer, Inner>(self) -> Outer
    where
        Outer: HomogeneousTuple<Item = Inner>,
        Inner: HomogeneousTuple<Item = SweetCell>,
        Outer::Buffer: std::convert::AsRef<[Option<Inner>]>,
        Outer::Buffer: std::convert::AsMut<[Option<Inner>]>,
        Inner::Buffer: std::convert::AsRef<[Option<SweetCell>]>,
        Inner::Buffer: std::convert::AsMut<[Option<SweetCell>]>,
    {
        self.into_inner()
            .into_iter()
            .map(|a| {
                a.into_cells()
                    .into_iter()
                    .collect_tuple::<Inner>()
                    .expect("Wrong number of DNAs in destructuring pattern, or too many (must be 4 or less)")
            })
            .collect_tuple::<Outer>()
            .expect("Wrong number of Agents in destructuring pattern, or too many (must be 4 or less)")
    }

    /// Access all Cells across all Apps, with Cells from the same App being contiguous
    pub fn cells_flattened(&self) -> Vec<SweetCell> {
        self.0
            .iter()
            .flat_map(|app| app.cells().iter())
            .cloned()
            .collect()
    }

    /// Get the underlying data
    pub fn iter(&self) -> impl Iterator<Item = &SweetApp> {
        self.0.iter()
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_app_installation.rs
================================================
use std::path::PathBuf;

use holochain_types::prelude::*;

use crate::conductor::conductor::app_manifest_from_dnas;

/// Get a "standard" AppBundle from a single DNA, with Create provisioning,
/// with no modifiers, clone limit of 255, and arbitrary role names
pub async fn app_bundle_from_dnas(
    dnas_with_roles: &[impl DnaWithRole],
    memproofs_deferred: bool,
    network_seed: Option<NetworkSeed>,
) -> AppBundle {
    let (roles, resources): (Vec<_>, Vec<_>) = dnas_with_roles
        .iter()
        .map(|dr| {
            let dna = dr.dna();

            let path = PathBuf::from(format!("{}", dna.dna_hash()));
            let modifiers = DnaModifiersOpt::none();
            let manifest = AppRoleManifest {
                name: dr.role(),
                dna: AppRoleDnaManifest {
                    location: Some(DnaLocation::Bundled(path.clone())),
                    modifiers,
                    // NOTE: for testing with inline zomes, it's essential that the
                    //       installed_hash is included, so it can be used to fetch
                    //       the DNA file from the conductor's DNA store rather
                    //       than the one in the bundle which lacks inline zomes
                    //       due to serialization.
                    installed_hash: Some(dr.dna().dna_hash().clone().into()),
                    clone_limit: 255,
                },
                provisioning: Some(CellProvisioning::Create { deferred: false }),
            };
            let bundle = DnaBundle::from_dna_file(dna.clone()).unwrap();
            (manifest, (path, bundle))
        })
        .unzip();

    let manifest = AppManifestCurrentBuilder::default()
        .name("[generated]".into())
        .description(None)
        .roles(roles)
        .allow_deferred_memproofs(memproofs_deferred)
        .build()
        .unwrap()
        .into();

    debug_assert_eq!(
        manifest,
        app_manifest_from_dnas(dnas_with_roles, 255, memproofs_deferred, network_seed),
        "app_bundle_from_dnas and app_manifest_from_dnas should produce the same manifest"
    );

    AppBundle::new(manifest, resources, PathBuf::from("."))
        .await
        .unwrap()
}

/// Get a "standard" InstallAppPayload from a single DNA
pub async fn get_install_app_payload_from_dnas(
    installed_app_id: impl Into<InstalledAppId>,
    agent_key: Option<AgentPubKey>,
    data: &[(impl DnaWithRole, Option<MembraneProof>)],
    network_seed: Option<NetworkSeed>,
) -> InstallAppPayload {
    let dnas_with_roles: Vec<_> = data.iter().map(|(dr, _)| dr).cloned().collect();
    let bundle = app_bundle_from_dnas(&dnas_with_roles, false, network_seed).await;
    let roles_settings = Some(
        data.iter()
            .map(|(dr, memproof)| {
                (
                    dr.role(),
                    RoleSettings::Provisioned {
                        modifiers: Default::default(),
                        membrane_proof: memproof.clone(),
                    },
                )
            })
            .collect(),
    );

    let bytes = bundle
        .encode()
        .expect("failed to encode app bundle as bytes");
    InstallAppPayload {
        agent_key,
        source: AppBundleSource::Bytes(bytes),
        installed_app_id: Some(installed_app_id.into()),
        network_seed: None,
        roles_settings,
        ignore_genesis_failure: false,
        allow_throwaway_random_agent_key: false,
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_cell.rs
================================================
use std::sync::Arc;

use super::SweetZome;
use hdk::prelude::*;
use holo_hash::DnaHash;
use holochain_conductor_api::conductor::ConductorConfig;
use holochain_sqlite::db::{DbKindAuthored, DbKindDht};
use holochain_types::db::DbWrite;
/// A reference to a Cell created by a SweetConductor installation function.
/// It has very concise methods for calling a zome on this cell
#[derive(Clone, Debug)]
pub struct SweetCell {
    pub(super) cell_id: CellId,
    pub(super) cell_authored_db: DbWrite<DbKindAuthored>,
    pub(super) cell_dht_db: DbWrite<DbKindDht>,
    pub(super) conductor_config: Arc<ConductorConfig>,
}

impl SweetCell {
    /// Accessor for CellId
    pub fn cell_id(&self) -> &CellId {
        &self.cell_id
    }

    /// Get the authored environment for this cell
    pub fn authored_db(&self) -> &DbWrite<DbKindAuthored> {
        &self.cell_authored_db
    }

    /// Get the dht environment for this cell
    pub fn dht_db(&self) -> &DbWrite<DbKindDht> {
        &self.cell_dht_db
    }

    /// Accessor for AgentPubKey
    pub fn agent_pubkey(&self) -> &AgentPubKey {
        self.cell_id.agent_pubkey()
    }

    /// Accessor for DnaHash
    pub fn dna_hash(&self) -> &DnaHash {
        self.cell_id.dna_hash()
    }

    /// Get a SweetZome with the given name
    pub fn zome<Z: Into<ZomeName>>(&self, zome_name: Z) -> SweetZome {
        SweetZome::new(self.cell_id.clone(), zome_name.into())
    }

    /// Accessor for ConductorConfig
    pub fn conductor_config(&self) -> Arc<ConductorConfig> {
        self.conductor_config.clone()
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_conductor.rs
================================================
//! A wrapper around ConductorHandle with more convenient methods for testing
// TODO [ B-03669 ] move to own crate

use super::*;
use crate::conductor::api::error::ConductorApiError;
use crate::conductor::ConductorHandle;
use crate::conductor::{
    api::error::ConductorApiResult, config::ConductorConfig, error::ConductorResult, CellError,
    Conductor, ConductorBuilder,
};
use ::fixt::prelude::StdRng;
use hdk::prelude::*;
use holochain_conductor_api::{
    AdminRequest, AdminResponse, AppAuthenticationRequest, CellInfo, ProvisionedCell,
};
use holochain_keystore::MetaLairClient;
use holochain_p2p::AgentPubKeyExt;
use holochain_state::prelude::test_db_dir;
use holochain_state::source_chain::SourceChain;
use holochain_state::test_utils::TestDir;
use holochain_types::prelude::*;
use holochain_types::websocket::AllowedOrigins;
use holochain_websocket::*;
use kitsune_p2p_types::config::TransportConfig;
use nanoid::nanoid;
use rand::Rng;
use std::net::ToSocketAddrs;
use std::path::Path;
use std::sync::atomic::Ordering;
use std::sync::Arc;
use std::time::{Duration, Instant};

/// Standin until std::io::Error::other is stablized.
pub fn err_other<E>(error: E) -> std::io::Error
where
    E: Into<Box<dyn std::error::Error + Send + Sync>>,
{
    std::io::Error::new(std::io::ErrorKind::Other, error.into())
}

/// A stream of signals.
pub type SignalStream = Box<dyn tokio_stream::Stream<Item = Signal> + Send + Sync + Unpin>;

/// A useful Conductor abstraction for testing, allowing startup and shutdown as well
/// as easy installation of apps across multiple Conductors and Agents.
///
/// This is intentionally NOT `Clone`, because the drop handle triggers a shutdown of
/// the conductor handle, which would render all other cloned instances useless,
/// as well as the fact that the SweetConductor has some extra state which would not
/// be tracked by cloned instances.
/// If you need multiple references to a SweetConductor, put it in an Arc
#[derive(derive_more::From)]
pub struct SweetConductor {
    handle: Option<SweetConductorHandle>,
    db_dir: TestDir,
    keystore: MetaLairClient,
    config: Arc<ConductorConfig>,
    dnas: Vec<DnaFile>,
    rendezvous: Option<DynSweetRendezvous>,
}

/// ID based equality is good for SweetConductors so we can track them
/// independently no matter what kind of mutations/state might eventuate.
impl PartialEq for SweetConductor {
    fn eq(&self, other: &Self) -> bool {
        self.id() == other.id()
    }
}

impl Eq for SweetConductor {}

impl SweetConductor {
    /// Get the ID of this conductor for manual equality checks.
    pub fn id(&self) -> String {
        self.config
            .tracing_scope()
            .expect("SweetConductor must have a tracing scope set")
    }

    /// Update the config if the conductor is shut down
    pub fn update_config(&mut self, f: impl FnOnce(ConductorConfig) -> ConductorConfig) {
        if self.is_running() {
            panic!("Cannot update config while conductor is running");
        }
        self.config = Arc::from(f((*self.config).clone()));
    }

    /// Create a SweetConductor from an already-built ConductorHandle and environments
    /// RibosomeStore
    /// The conductor will be supplied with a single test AppInterface named
    /// "sweet-interface" so that signals may be emitted
    pub async fn new(
        handle: ConductorHandle,
        env_dir: TestDir,
        config: Arc<ConductorConfig>,
        rendezvous: Option<DynSweetRendezvous>,
    ) -> SweetConductor {
        let keystore = handle.keystore().clone();

        Self {
            handle: Some(SweetConductorHandle(handle)),
            db_dir: env_dir,
            keystore,
            config,
            dnas: Vec::new(),
            rendezvous,
        }
    }

    /// Create a SweetConductor with a new set of TestEnvs from the given config
    pub async fn from_config<C>(config: C) -> SweetConductor
    where
        C: Into<SweetConductorConfig>,
    {
        let config: SweetConductorConfig = config.into();
        let vous = config.get_rendezvous();
        Self::create_with_defaults(config, None, vous).await
    }

    /// Create a SweetConductor with a new set of TestEnvs from the given config
    pub async fn from_config_rendezvous<C, R>(config: C, rendezvous: R) -> SweetConductor
    where
        C: Into<SweetConductorConfig>,
        R: Into<DynSweetRendezvous> + Clone,
    {
        Self::create_with_defaults(config, None, Some(rendezvous)).await
    }

    /// Create a SweetConductor with a new set of TestEnvs from the given config
    pub async fn create_with_defaults<C, R>(
        config: C,
        keystore: Option<MetaLairClient>,
        rendezvous: Option<R>,
    ) -> SweetConductor
    where
        C: Into<SweetConductorConfig>,
        R: Into<DynSweetRendezvous> + Clone,
    {
        Self::create_with_defaults_and_metrics(config, keystore, rendezvous, false).await
    }

    /// Create a SweetConductor with a new set of TestEnvs from the given config
    /// and a metrics initialization.
    pub async fn create_with_defaults_and_metrics<C, R>(
        config: C,
        keystore: Option<MetaLairClient>,
        rendezvous: Option<R>,
        with_metrics: bool,
    ) -> SweetConductor
    where
        C: Into<SweetConductorConfig>,
        R: Into<DynSweetRendezvous> + Clone,
    {
        let rendezvous = rendezvous.map(|r| r.into());
        let dir = TestDir::new(test_db_dir());

        assert!(
            dir.read_dir().unwrap().next().is_none(),
            "Test dir not empty - {:?}",
            dir.to_path_buf()
        );

        if with_metrics {
            #[cfg(feature = "metrics_influxive")]
            holochain_metrics::HolochainMetricsConfig::new(dir.as_ref())
                .init()
                .await;
        }

        let config: SweetConductorConfig = config.into();
        let mut config: ConductorConfig = if let Some(r) = rendezvous.clone() {
            config.apply_rendezvous(&r).into()
        } else {
            if let Some(b) = config.network.bootstrap_service.as_ref() {
                if b.to_string().starts_with("rendezvous:") {
                    panic!("Must use rendezvous SweetConductor if rendezvous: is specified in config.network.bootstrap_service");
                }
            }
            if config.network.transport_pool.iter().any(|p| match p {
                TransportConfig::WebRTC { signal_url, .. } => signal_url.starts_with("rendezvous:"),
                _ => false,
            }) {
                panic!("Must use rendezvous SweetConductor if rendezvous: is specified in config.network.transport_pool[].signal_url");
            }
            config.into()
        };

        if config.tracing_scope().is_none() {
            config.network.tracing_scope = Some(format!(
                "{}.{}",
                NUM_CREATED.load(Ordering::SeqCst),
                nanoid!(5)
            ));
        }

        if config.data_root_path.is_none() {
            config.data_root_path = Some(dir.as_ref().to_path_buf().into());
        }

        let keystore = keystore.unwrap_or_else(holochain_keystore::test_keystore);

        let handle = Self::handle_from_existing(keystore, &config, &[]).await;

        Self::new(handle, dir, Arc::new(config), rendezvous).await
    }

    /// Create a SweetConductor from a partially-configured ConductorBuilder
    pub async fn from_builder(builder: ConductorBuilder) -> SweetConductor {
        let db_dir = TestDir::new(test_db_dir());
        let builder = builder.with_data_root_path(db_dir.as_ref().to_path_buf().into());
        let config = builder.config.clone();
        let handle = builder.test(&[]).await.unwrap();
        Self::new(handle, db_dir, Arc::new(config), None).await
    }

    /// Create a SweetConductor from a partially-configured ConductorBuilder
    pub async fn from_builder_rendezvous<R>(
        builder: ConductorBuilder,
        rendezvous: R,
    ) -> SweetConductor
    where
        R: Into<DynSweetRendezvous> + Clone,
    {
        let db_dir = TestDir::new(test_db_dir());
        let builder = builder.with_data_root_path(db_dir.as_ref().to_path_buf().into());
        let config = builder.config.clone();
        let handle = builder.test(&[]).await.unwrap();
        Self::new(handle, db_dir, Arc::new(config), Some(rendezvous.into())).await
    }

    /// Create a handle from an existing environment and config
    pub async fn handle_from_existing(
        keystore: MetaLairClient,
        config: &ConductorConfig,
        extra_dnas: &[DnaFile],
    ) -> ConductorHandle {
        NUM_CREATED.fetch_add(1, Ordering::SeqCst);

        Conductor::builder()
            .config(config.clone())
            .with_keystore(keystore)
            .no_print_setup()
            .test(extra_dnas)
            .await
            .unwrap()
    }

    /// Create a SweetConductor with a new set of TestEnvs from the given config
    pub async fn from_standard_config() -> SweetConductor {
        Self::from_config(SweetConductorConfig::standard()).await
    }

    /// Get the rendezvous config that this conductor is using, if any
    pub fn get_rendezvous_config(&self) -> Option<DynSweetRendezvous> {
        self.rendezvous.clone()
    }

    /// Access the database path for this conductor
    pub fn db_path(&self) -> &Path {
        &self.db_dir
    }

    /// Make the temp db dir persistent
    pub fn persist_dbs(&mut self) -> &Path {
        self.db_dir.persist();
        &self.db_dir
    }

    /// Access the MetaLairClient for this conductor
    pub fn keystore(&self) -> MetaLairClient {
        self.keystore.clone()
    }

    /// Convenience function that uses the internal handle to enable an app
    pub async fn enable_app(
        &self,
        id: InstalledAppId,
    ) -> ConductorResult<(InstalledApp, Vec<(CellId, CellError)>)> {
        self.raw_handle().enable_app(id).await
    }

    /// Convenience function that uses the internal handle to disable an app
    pub async fn disable_app(
        &self,
        id: InstalledAppId,
        reason: DisabledAppReason,
    ) -> ConductorResult<InstalledApp> {
        self.raw_handle().disable_app(id, reason).await
    }

    /// Convenience function that uses the internal handle to start an app
    pub async fn start_app(&self, id: InstalledAppId) -> ConductorResult<InstalledApp> {
        self.raw_handle().start_app(id).await
    }

    /// Convenience function that uses the internal handle to pause an app
    pub async fn pause_app(
        &self,
        id: InstalledAppId,
        reason: PausedAppReason,
    ) -> ConductorResult<InstalledApp> {
        self.raw_handle().pause_app(id, reason).await
    }

    /// Install the dna first.
    /// This allows a big speed up when
    /// installing many apps with the same dna
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn setup_app_1_register_dna(
        &mut self,
        dna_files: impl IntoIterator<Item = &DnaFile>,
    ) -> ConductorApiResult<()> {
        for dna_file in dna_files.into_iter() {
            self.register_dna(dna_file.to_owned()).await?;
            self.dnas.push(dna_file.to_owned());
        }
        Ok(())
    }

    /// Install the app and enable it
    // TODO: make this take a more flexible config for specifying things like
    // membrane proofs
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn setup_app_2_install_and_enable(
        &mut self,
        installed_app_id: &str,
        agent: Option<AgentPubKey>,
        dnas_with_roles: &[impl DnaWithRole],
    ) -> ConductorApiResult<AgentPubKey> {
        let installed_app_id = installed_app_id.to_string();

        let dnas_with_proof: Vec<_> = dnas_with_roles
            .iter()
            .cloned()
            .map(|dr| {
                let dna = dr.dna().clone().update_modifiers(Default::default());
                (dr.replace_dna(dna), None)
            })
            .collect();

        let agent = self
            .raw_handle()
            .install_app_minimal(installed_app_id.clone(), agent, &dnas_with_proof, None)
            .await?;

        self.raw_handle().enable_app(installed_app_id).await?;
        Ok(agent)
    }

    /// Build the SweetCells after `setup_cells` has been run
    /// The setup is split into two parts because the Cell environments
    /// are not available until after `setup_cells` has run, and it is
    /// better to do that once for all apps in the case of multiple apps being
    /// set up at once.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn setup_app_3_create_sweet_app(
        &self,
        installed_app_id: &str,
        agent: AgentPubKey,
        roles: &[RoleName],
    ) -> ConductorApiResult<SweetApp> {
        let info = self
            .raw_handle()
            .get_app_info(&installed_app_id.to_owned())
            .await
            .expect("Error getting AppInfo for just-installed app")
            .expect("Couldn't get AppInfo for just-installed app");

        let mut sweet_cells = Vec::new();

        for role in roles {
            if let Some(CellInfo::Provisioned(ProvisionedCell { cell_id, .. })) =
                info.cell_info[role].first()
            {
                assert_eq!(cell_id.agent_pubkey(), &agent, "Agent mismatch for cell");

                // Initialize per-space databases
                let _space = self.spaces.get_or_create_space(cell_id.dna_hash())?;

                // Create and add the SweetCell
                sweet_cells.push(self.get_sweet_cell(cell_id.clone())?);
            }
        }

        Ok(SweetApp::new(installed_app_id.into(), sweet_cells))
    }

    /// Construct a SweetCell for a cell which has already been created
    pub fn get_sweet_cell(&self, cell_id: CellId) -> ConductorApiResult<SweetCell> {
        let cell_authored_db = self
            .raw_handle()
            .get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())?;
        let cell_dht_db = self.raw_handle().get_dht_db(cell_id.dna_hash())?;
        let conductor_config = self.config.clone();
        Ok(SweetCell {
            cell_id,
            cell_authored_db,
            cell_dht_db,
            conductor_config,
        })
    }

    /// Opinionated app setup.
    /// Creates an app for the given agent, if specified, using the given DnaFiles,
    /// with no extra configuration.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    async fn setup_app_for_optional_agent<'a>(
        &mut self,
        installed_app_id: &str,
        agent: Option<AgentPubKey>,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)>,
    ) -> ConductorApiResult<SweetApp> {
        let dnas_with_roles: Vec<_> = dnas_with_roles.into_iter().cloned().collect();
        let dnas = dnas_with_roles
            .iter()
            .map(|dr| dr.dna())
            .collect::<Vec<_>>();

        self.setup_app_1_register_dna(dnas.clone()).await?;

        let agent = self
            .setup_app_2_install_and_enable(
                installed_app_id,
                agent.clone(),
                dnas_with_roles.as_slice(),
            )
            .await?;

        self.raw_handle()
            .reconcile_cell_status_with_app_status()
            .await?;
        let roles = dnas_with_roles
            .iter()
            .map(|dr| dr.role())
            .collect::<Vec<_>>();
        self.setup_app_3_create_sweet_app(installed_app_id, agent, &roles)
            .await
    }

    /// Opinionated app setup.
    /// Creates an app for the given agent, using the given DnaFiles, with no extra configuration.
    pub async fn setup_app_for_agent<'a>(
        &mut self,
        installed_app_id: &str,
        agent: AgentPubKey,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)>,
    ) -> ConductorApiResult<SweetApp> {
        self.setup_app_for_optional_agent(installed_app_id, Some(agent), dnas_with_roles)
            .await
    }

    /// Opinionated app setup.
    /// Creates an app using the given DnaFiles, with no extra configuration.
    /// An AgentPubKey will be generated, and is accessible via the returned SweetApp.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn setup_app<'a>(
        &mut self,
        installed_app_id: &str,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)> + Clone,
    ) -> ConductorApiResult<SweetApp> {
        // If DPKI is in use, we must let DPKI generate the agent key
        self.setup_app_for_optional_agent(installed_app_id, None, dnas_with_roles)
            .await
    }

    /// Opinionated app setup. Creates one app per agent, using the given DnaFiles.
    ///
    /// All InstalledAppIds and RoleNames are auto-generated. In tests driven directly
    /// by Rust, you typically won't care what these values are set to, but in case you
    /// do, they are set as so:
    /// - InstalledAppId: {app_id_prefix}-{agent_pub_key}
    /// - RoleName: {dna_hash}
    ///
    /// Returns a batch of SweetApps, sorted in the same order as Agents passed in.
    pub async fn setup_app_for_agents<'a>(
        &mut self,
        app_id_prefix: &str,
        agents: impl IntoIterator<Item = &AgentPubKey>,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)>,
    ) -> ConductorApiResult<SweetAppBatch> {
        let agents: Vec<_> = agents.into_iter().collect();
        let dnas_with_roles: Vec<_> = dnas_with_roles.into_iter().cloned().collect();
        let dnas: Vec<&DnaFile> = dnas_with_roles.iter().map(|dr| dr.dna()).collect();
        let roles: Vec<RoleName> = dnas_with_roles.iter().map(|dr| dr.role()).collect();
        self.setup_app_1_register_dna(dnas.clone()).await?;

        for &agent in agents.iter() {
            let installed_app_id = format!("{}{}", app_id_prefix, agent);
            self.setup_app_2_install_and_enable(
                &installed_app_id,
                Some(agent.to_owned()),
                &dnas_with_roles,
            )
            .await?;
        }

        self.raw_handle()
            .reconcile_cell_status_with_app_status()
            .await?;

        let mut apps = Vec::new();
        for agent in agents {
            let installed_app_id = format!("{}{}", app_id_prefix, agent);
            apps.push(
                self.setup_app_3_create_sweet_app(&installed_app_id, agent.clone(), &roles)
                    .await?,
            );
        }

        Ok(SweetAppBatch(apps))
    }

    /// Setup N apps with generated agent keys and the same set of DNAs
    pub async fn setup_apps<'a>(
        &mut self,
        app_id_prefix: &str,
        num: usize,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)>,
    ) -> ConductorApiResult<SweetAppBatch> {
        let dnas_with_roles: Vec<_> = dnas_with_roles.into_iter().cloned().collect();

        let mut apps = vec![];

        for i in 0..num {
            let app = self
                .setup_app(&format!("{}{}", app_id_prefix, i), &dnas_with_roles)
                .await?;
            apps.push(app);
        }

        Ok(SweetAppBatch(apps))
    }

    /// Install DPKI a bit more concisely
    pub async fn install_dpki(&self) {
        let dpki_config = self.config.dpki.clone();
        let (dna, _) = crate::conductor::conductor::get_dpki_dna(&dpki_config)
            .await
            .unwrap()
            .into_dna_file(Default::default())
            .await
            .unwrap();
        self.raw_handle().install_dpki(dna, true).await.unwrap()
    }

    /// Get the cell providing the DPKI service, if applicable
    pub fn dpki_cell(&self) -> Option<SweetCell> {
        let dpki = self.raw_handle().running_services().dpki?;
        let cell_id = dpki.cell_id.clone();
        Some(self.get_sweet_cell(cell_id).unwrap())
    }

    /// Call into the underlying create_clone_cell function, and register the
    /// created dna with SweetConductor so it will be reloaded on restart.
    pub async fn create_clone_cell(
        &mut self,
        installed_app_id: &InstalledAppId,
        payload: CreateCloneCellPayload,
    ) -> ConductorResult<holochain_zome_types::clone::ClonedCell> {
        let clone = self
            .raw_handle()
            .create_clone_cell(installed_app_id, payload)
            .await?;
        let dna_file = self.get_dna_file(clone.cell_id.dna_hash()).unwrap();
        self.dnas.push(dna_file);
        Ok(clone)
    }

    /// Get a new websocket client which can send requests over the admin
    /// interface. It presupposes that an admin interface has been configured.
    /// (The standard_config includes an admin interface at port 0.)
    pub async fn admin_ws_client<D>(&self) -> (WebsocketSender, WsPollRecv)
    where
        D: std::fmt::Debug,
        SerializedBytes: TryInto<D, Error = SerializedBytesError>,
    {
        let port = self
            .get_arbitrary_admin_websocket_port()
            .expect("No admin port open on conductor");
        let (tx, rx) = websocket_client_by_port(port).await.unwrap();

        (tx, WsPollRecv::new::<D>(rx))
    }

    /// Create a new app interface and get a websocket client which can send requests
    /// to it.
    pub async fn app_ws_client<D>(
        &self,
        installed_app_id: InstalledAppId,
    ) -> (WebsocketSender, WsPollRecv)
    where
        D: std::fmt::Debug,
        SerializedBytes: TryInto<D, Error = SerializedBytesError>,
    {
        let port = self
            .raw_handle()
            .add_app_interface(either::Either::Left(0), AllowedOrigins::Any, None)
            .await
            .expect("Couldn't create app interface");
        let (tx, rx) = websocket_client_by_port(port).await.unwrap();

        authenticate_app_ws_client(
            tx.clone(),
            self.get_arbitrary_admin_websocket_port()
                .expect("No admin ports on this conductor"),
            installed_app_id,
        )
        .await;

        (tx, WsPollRecv::new::<D>(rx))
    }

    /// Shutdown this conductor.
    /// This will wait for the conductor to shut down but
    /// keep the inner state to restart it.
    ///
    /// Attempting to use this conductor without starting it up again will cause a panic.
    pub async fn shutdown(&mut self) {
        self.try_shutdown().await.unwrap();
    }

    /// Shutdown this conductor.
    /// This will wait for the conductor to shutdown but
    /// keep the inner state to restart it.
    ///
    /// Attempting to use this conductor without starting it up again will cause a panic.
    pub async fn try_shutdown(&mut self) -> std::io::Result<()> {
        if let Some(handle) = self.handle.take() {
            handle
                .shutdown()
                .await
                .map_err(err_other)?
                .map_err(err_other)
        } else {
            panic!("Attempted to shutdown conductor which was already shutdown");
        }
    }

    /// Start up this conductor if it's not already running.
    pub async fn startup(&mut self) {
        if self.handle.is_none() {
            // There's a db dir in the sweet conductor and the config, that are
            // supposed to be the same. Let's assert that they are.
            assert_eq!(
                Some(self.db_dir.as_ref().to_path_buf().into()),
                self.config.data_root_path,
                "SweetConductor db_dir and config.data_root_path are not the same",
            );
            self.handle = Some(SweetConductorHandle(
                Self::handle_from_existing(
                    self.keystore.clone(),
                    &self.config,
                    self.dnas.as_slice(),
                )
                .await,
            ));
        } else {
            panic!("Attempted to start conductor which was already started");
        }
    }

    /// Check if this conductor is running
    pub fn is_running(&self) -> bool {
        self.handle.is_some()
    }

    /// Get the underlying SweetConductorHandle.
    #[allow(dead_code)]
    pub fn sweet_handle(&self) -> SweetConductorHandle {
        self.handle
            .as_ref()
            .map(|h| h.clone_privately())
            .expect("Tried to use a conductor that is offline")
    }

    /// Get the ConductorHandle within this Conductor.
    /// Be careful when using this, because this leaks out handles, which may
    /// make it harder to shut down the conductor during tests.
    pub fn raw_handle(&self) -> ConductorHandle {
        self.handle
            .as_ref()
            .map(|h| h.0.clone())
            .expect("Tried to use a conductor that is offline")
    }

    /// Force trigger all dht ops that haven't received
    /// enough validation receipts yet.
    pub async fn force_all_publish_dht_ops(&self) {
        use futures::stream::StreamExt;
        if let Some(handle) = self.handle.as_ref() {
            let iter = handle.running_cell_ids().into_iter().map(|id| async move {
                let db = self
                    .get_or_create_authored_db(id.dna_hash(), id.agent_pubkey().clone())
                    .unwrap();
                let trigger = self.get_cell_triggers(&id).await.unwrap();
                (db, trigger)
            });
            futures::stream::iter(iter)
                .then(|f| f)
                .for_each(|(db, mut triggers)| async move {
                    // The line below was added when migrating to rust edition 2021, per
                    // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
                    let _ = &triggers;
                    crate::test_utils::force_publish_dht_ops(&db, &mut triggers.publish_dht_ops)
                        .await
                        .unwrap();
                })
                .await;
        }
    }

    /// Let each conductor know about each others' agents so they can do networking
    pub async fn exchange_peer_info(conductors: impl Clone + IntoIterator<Item = &Self>) {
        let mut all = Vec::new();
        for c in conductors.into_iter() {
            if c.get_config().has_rendezvous_bootstrap() {
                panic!(
                    "exchange_peer_info cannot reliably be used with rendezvous bootstrap servers"
                );
            }

            if let Some(dpki_dna_hash) = c
                .running_services()
                .dpki
                .as_ref()
                .map(|dpki| dpki.cell_id.dna_hash())
            {
                // Ensure the space is created for DPKI so the agent db exists
                c.spaces.get_or_create_space(dpki_dna_hash).unwrap();
            }
            for env in c.spaces.get_from_spaces(|s| s.p2p_agents_db.clone()) {
                all.push(env.clone());
            }
        }
        crate::conductor::p2p_agent_store::exchange_peer_info(all).await;
    }

    /// Drop the specified agent keys from each conductor's peer table
    pub async fn forget_peer_info(
        conductors: impl IntoIterator<Item = &Self>,
        agents_to_forget: impl IntoIterator<Item = &AgentPubKey>,
    ) {
        let mut all = Vec::new();
        for c in conductors.into_iter() {
            for env in c.spaces.get_from_spaces(|s| s.p2p_agents_db.clone()) {
                all.push(env.clone());
            }
        }

        crate::conductor::p2p_agent_store::forget_peer_info(all, agents_to_forget).await;
    }

    /// Let each conductor know about each others' agents so they can do networking
    pub async fn exchange_peer_info_sampled(
        conductors: impl IntoIterator<Item = &Self>,
        rng: &mut StdRng,
        s: usize,
    ) {
        let mut all = Vec::new();
        for c in conductors.into_iter() {
            for env in c.spaces.get_from_spaces(|s| s.p2p_agents_db.clone()) {
                all.push(env.clone());
            }
        }
        let connectivity = covering(rng, all.len(), s);
        crate::conductor::p2p_agent_store::exchange_peer_info_sparse(all, connectivity).await;
    }

    /// Wait for at least one gossip round to have completed for the given cell
    ///
    /// Note that this is really a crutch. If gossip starts fast enough then this is unnecessary
    /// but that doesn't necessarily happen. Waiting for gossip to have started before, for example,
    /// waiting for something else like consistency is useful to ensure that communication has
    /// actually started.
    pub async fn require_initial_gossip_activity_for_cell(
        &self,
        cell: &SweetCell,
        min_peers: u32,
        timeout: Duration,
    ) -> anyhow::Result<()> {
        let handle = self.raw_handle();

        let installed_app = handle
            .find_app_containing_cell(cell.cell_id())
            .await?
            .ok_or(anyhow::anyhow!("Could not find app containing cell"))?;

        let wait_start = Instant::now();
        loop {
            let (number_of_peers, completed_rounds) = handle
                .network_info(
                    installed_app.id(),
                    &NetworkInfoRequestPayload {
                        agent_pub_key: cell.agent_pubkey().clone(),
                        dnas: vec![cell.cell_id.dna_hash().clone()],
                        last_time_queried: None, // Just care about seeing the first data
                    },
                )
                .await?
                .first()
                .map_or((0, 0), |info| {
                    (
                        info.current_number_of_peers,
                        info.completed_rounds_since_last_time_queried,
                    )
                });

            if number_of_peers >= min_peers && completed_rounds > 0 {
                tracing::info!(
                    "Took {}s for cell {} to complete {} gossip rounds",
                    wait_start.elapsed().as_secs(),
                    cell.cell_id(),
                    completed_rounds
                );
                return Ok(());
            }

            tokio::time::sleep(std::time::Duration::from_millis(500)).await;

            if wait_start.elapsed() > timeout {
                anyhow::bail!(
                    "Timed out waiting for gossip to start for cell {}",
                    cell.cell_id()
                );
            }
        }
    }

    /// Instantiate a source chain object for the given agent and DNA hash.
    pub async fn get_agent_source_chain(
        &self,
        agent_key: &AgentPubKey,
        dna_hash: &DnaHash,
    ) -> SourceChain {
        SourceChain::new(
            self.get_or_create_authored_db(dna_hash, agent_key.clone())
                .unwrap(),
            self.get_dht_db(dna_hash).unwrap(),
            self.get_dht_db_cache(dna_hash).unwrap(),
            self.keystore().clone(),
            agent_key.clone(),
        )
        .await
        .unwrap()
    }

    /// Retries getting a list of peers from the conductor until all the given peers are in the response.
    ///
    /// You can optionally filter by `cell_id`. That is used in the `get_agent_infos` call to the conductor, so you
    /// can see how that works in the conductor docs.
    ///
    /// If the max_wait is reached then this function will return a "Timeout" error.
    pub async fn wait_for_peer_visible<P: IntoIterator<Item = AgentPubKey>>(
        &self,
        peers: P,
        cell_id: Option<CellId>,
        max_wait: Duration,
    ) -> ConductorApiResult<()> {
        let handle = self.raw_handle();

        let peers = peers.into_iter().collect::<HashSet<_>>();

        tokio::time::timeout(max_wait, async move {
            loop {
                let infos = handle
                    .get_agent_infos(cell_id.clone())
                    .await?
                    .into_iter()
                    .map(|p| AgentPubKey::from_kitsune(&p.agent))
                    .collect::<HashSet<_>>();
                if infos.is_superset(&peers) {
                    break;
                }
                tokio::time::sleep(Duration::from_millis(100)).await;
            }

            Ok(())
        })
        .await
        .map_err(|_| ConductorApiError::other("Timeout"))?
    }

    /// Getter
    pub fn rendezvous(&self) -> Option<&DynSweetRendezvous> {
        self.rendezvous.as_ref()
    }
}

/// You do not need to do anything with this type. While it is held it will keep polling a websocket
/// receiver.
pub struct WsPollRecv(tokio::task::JoinHandle<()>);

impl Drop for WsPollRecv {
    fn drop(&mut self) {
        self.0.abort();
    }
}

impl WsPollRecv {
    /// Create a new [WsPollRecv] that will poll the given [WebsocketReceiver] for messages.
    /// The type of the messages being received must be specified. For example
    ///
    /// ```no_run
    /// # #[tokio::main]
    /// # async fn main() -> anyhow::Result<()>
    /// # {
    ///
    /// use holochain::sweettest::{websocket_client_by_port, WsPollRecv};
    /// use holochain_conductor_api::AdminResponse;
    ///
    /// let (tx, rx) = websocket_client_by_port(3000).await?;
    /// let _rx = WsPollRecv::new::<AdminResponse>(rx);
    ///
    /// # Ok(())
    /// # }
    /// ```
    pub fn new<D>(mut rx: WebsocketReceiver) -> Self
    where
        D: std::fmt::Debug,
        SerializedBytes: TryInto<D, Error = SerializedBytesError>,
    {
        Self(tokio::task::spawn(async move {
            while rx.recv::<D>().await.is_ok() {}
        }))
    }
}

/// Connect to a websocket server at the given port.
///
/// Note that the [WebsocketReceiver] returned by this function will need to be polled. This can be
/// done with a [WsPollRecv].
/// If this is an app client, you will need to authenticate the connection before you can send any
/// other requests.
pub async fn websocket_client_by_port(
    port: u16,
) -> WebsocketResult<(WebsocketSender, WebsocketReceiver)> {
    connect(
        Arc::new(WebsocketConfig::CLIENT_DEFAULT),
        ConnectRequest::new(
            format!("localhost:{port}")
                .to_socket_addrs()?
                .next()
                .ok_or_else(|| Error::other("Could not resolve localhost"))?,
        ),
    )
    .await
}

/// Create an authentication token for an app client and authenticate the connection.
pub async fn authenticate_app_ws_client(
    app_sender: WebsocketSender,
    admin_port: u16,
    installed_app_id: InstalledAppId,
) {
    let (admin_tx, admin_rx) = websocket_client_by_port(admin_port).await.unwrap();
    let _admin_rx = WsPollRecv::new::<AdminResponse>(admin_rx);

    let token_response: AdminResponse = admin_tx
        .request(AdminRequest::IssueAppAuthenticationToken(
            installed_app_id.into(),
        ))
        .await
        .unwrap();
    let token = match token_response {
        AdminResponse::AppAuthenticationTokenIssued(issued) => issued.token,
        _ => panic!("unexpected response"),
    };

    app_sender
        .authenticate(AppAuthenticationRequest { token })
        .await
        .unwrap();
}

impl Drop for SweetConductor {
    fn drop(&mut self) {
        if let Some(handle) = self.handle.take() {
            tokio::task::spawn(handle.shutdown());
        }
    }
}

impl AsRef<SweetConductorHandle> for SweetConductor {
    fn as_ref(&self) -> &SweetConductorHandle {
        self.handle
            .as_ref()
            .expect("Tried to use a conductor that is offline")
    }
}

impl std::ops::Deref for SweetConductor {
    type Target = SweetConductorHandle;

    fn deref(&self) -> &Self::Target {
        self.handle
            .as_ref()
            .expect("Tried to use a conductor that is offline")
    }
}

impl std::borrow::Borrow<SweetConductorHandle> for SweetConductor {
    fn borrow(&self) -> &SweetConductorHandle {
        self.handle
            .as_ref()
            .expect("Tried to use a conductor that is offline")
    }
}

impl std::fmt::Debug for SweetConductor {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("SweetConductor")
            .field("db_dir", &self.db_dir)
            .field("config", &self.config)
            .field("dnas", &self.dnas)
            .finish()
    }
}

fn covering(rng: &mut StdRng, n: usize, s: usize) -> Vec<HashSet<usize>> {
    let nodes: Vec<_> = (0..n)
        .map(|i| {
            let peers: HashSet<_> = std::iter::repeat_with(|| rng.gen_range(0..n))
                .filter(|j| i != *j)
                .take(s)
                .collect();
            peers
        })
        .collect();
    let mut visited = HashSet::<usize>::new();
    let mut queue = vec![0];
    while let Some(next) = queue.pop() {
        let unvisited: Vec<_> = nodes[next]
            .iter()
            .filter(|p| !visited.contains(p))
            .copied()
            .collect();
        queue.extend(unvisited.iter());
        visited.extend(unvisited.iter());
        if visited.len() == n {
            break;
        }
    }
    if visited.len() < n {
        panic!("Covering could not be created. Try a higher s value.");
    }
    nodes
}



================================================
File: crates/holochain/src/sweettest/sweet_conductor_batch.rs
================================================
use super::{SweetAppBatch, SweetConductor, SweetConductorConfig};
use crate::conductor::api::error::ConductorApiResult;
use crate::sweettest::*;
use ::fixt::prelude::StdRng;
use futures::future;
use hdk::prelude::*;
use holochain_types::prelude::*;
use std::path::PathBuf;

/// A collection of SweetConductors, with methods for operating on the entire collection
#[derive(derive_more::Into, derive_more::IntoIterator, derive_more::Deref)]
pub struct SweetConductorBatch(Vec<SweetConductor>);

impl SweetConductorBatch {
    /// Constructor with validation
    pub fn new(conductors: Vec<SweetConductor>) -> Self {
        let paths: HashSet<PathBuf> = conductors
            .iter()
            .filter_map(|c| {
                c.config
                    .data_root_path
                    .as_ref()
                    .map(|data_path| data_path.as_ref().clone())
            })
            .collect();
        assert_eq!(
            conductors.len(),
            paths.len(),
            "Some conductors in a SweetConductorBatch share the same data path (or don't have a path)!"
        );
        Self(conductors)
    }

    /// Map the given ConductorConfigs into SweetConductors, each with its own new TestEnvironments
    pub async fn from_configs<C, I>(configs: I) -> SweetConductorBatch
    where
        C: Into<SweetConductorConfig>,
        I: IntoIterator<Item = C>,
    {
        let conductors = Self::new(
            future::join_all(configs.into_iter().map(|c| SweetConductor::from_config(c))).await,
        );

        let dpki_cells = conductors.dpki_cells();
        if !dpki_cells.is_empty() {
            conductors.exchange_peer_info().await;
            await_consistency(10, dpki_cells.as_slice()).await.unwrap();
        }

        conductors
    }

    /// Create SweetConductors from the given ConductorConfigs, each with its own new TestEnvironments,
    /// using a "rendezvous" bootstrap server for peer discovery.
    ///
    /// Also await consistency for DPKI cells, if DPKI is enabled.
    pub async fn from_configs_rendezvous<C, I>(configs: I) -> SweetConductorBatch
    where
        C: Into<SweetConductorConfig>,
        I: IntoIterator<Item = C>,
    {
        let rendezvous = SweetLocalRendezvous::new().await;
        let conductors = Self::new(
            future::join_all(
                configs
                    .into_iter()
                    .map(|c| SweetConductor::from_config_rendezvous(c, rendezvous.clone())),
            )
            .await,
        );

        let not_full_bootstrap = conductors
            .iter()
            .any(|c| !c.get_config().has_rendezvous_bootstrap());

        let dpki_cells = conductors.dpki_cells();
        if !dpki_cells.is_empty() {
            // Typically we expect either all nodes are using a rendezvous bootstrap, or none are.
            // To cover all cases, we say if any are not using bootstrap, we'll exchange peer info
            // for everyone, even though this may be incorrect.
            if not_full_bootstrap {
                conductors.exchange_peer_info().await;
            }
            await_consistency(15, dpki_cells.as_slice()).await.unwrap();
        }

        conductors
    }

    /// Create the given number of new SweetConductors, each with its own new TestEnvironments
    pub async fn from_config<C: Clone + Into<SweetConductorConfig>>(
        num: usize,
        config: C,
    ) -> SweetConductorBatch {
        let config = config.into();
        Self::from_configs(std::iter::repeat(config).take(num)).await
    }

    /// Create a number of SweetConductors from the given ConductorConfig, each with its own new TestEnvironments.
    /// using a "rendezvous" bootstrap server for peer discovery.
    ///
    /// Also await consistency for DPKI cells, if DPKI is enabled.
    pub async fn from_config_rendezvous<C>(num: usize, config: C) -> SweetConductorBatch
    where
        C: Into<SweetConductorConfig> + Clone,
    {
        Self::from_configs_rendezvous(std::iter::repeat(config).take(num)).await
    }

    /// Create the given number of new SweetConductors, each with its own new TestEnvironments
    pub async fn from_standard_config(num: usize) -> SweetConductorBatch {
        Self::from_configs(std::iter::repeat_with(SweetConductorConfig::standard).take(num)).await
    }

    /// Create the given number of new SweetConductors, each with its own new TestEnvironments
    pub async fn from_standard_config_rendezvous(num: usize) -> SweetConductorBatch {
        Self::from_config_rendezvous(num, SweetConductorConfig::rendezvous(true)).await
    }

    /// Iterate over the SweetConductors
    pub fn iter(&self) -> impl Iterator<Item = &SweetConductor> {
        self.0.iter()
    }

    /// Iterate over the SweetConductors, mutably
    pub fn iter_mut(&mut self) -> impl Iterator<Item = &mut SweetConductor> {
        self.0.iter_mut()
    }

    /// Convert to a Vec
    pub fn into_inner(self) -> Vec<SweetConductor> {
        self.0
    }

    /// Get the conductor at an index.
    pub fn get(&self, i: usize) -> Option<&SweetConductor> {
        self.0.get(i)
    }

    /// Add an existing conductor to this batch
    pub fn add_conductor(&mut self, c: SweetConductor) {
        self.0.push(c);
    }

    /// Create and add a new conductor to this batch
    pub async fn add_conductor_from_config<C>(&mut self, c: C)
    where
        C: Into<SweetConductorConfig>,
    {
        let conductor =
            if let Some(rendezvous) = self.0.first().and_then(|c| c.get_rendezvous_config()) {
                SweetConductor::from_config_rendezvous(c, rendezvous).await
            } else {
                SweetConductor::from_config(c).await
            };

        self.0.push(conductor);
    }

    /// Opinionated app setup.
    /// Creates one app on each Conductor in this batch, creating a new AgentPubKey for each.
    /// The created AgentPubKeys can be retrieved via each SweetApp.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn setup_app<'a>(
        &mut self,
        installed_app_id: &str,
        dnas_with_roles: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)> + Clone,
    ) -> ConductorApiResult<SweetAppBatch> {
        let apps = self
            .0
            .iter_mut()
            .map(|conductor| {
                let dnas_with_roles = dnas_with_roles.clone();
                async move { conductor.setup_app(installed_app_id, dnas_with_roles).await }
            })
            .collect::<Vec<_>>();

        Ok(future::join_all(apps)
            .await
            .into_iter()
            .collect::<Result<Vec<_>, _>>()?
            .into())
    }

    /// Opinionated app setup. Creates one app on each Conductor in this batch,
    /// using the given agents and DnaFiles.
    ///
    /// The number of Agents passed in must be the same as the number of Conductors
    /// in this batch. Each Agent will be used to create one app on one Conductor,
    /// hence the "zipped" in the function name
    ///
    /// Returns a batch of SweetApps, sorted in the same order as the Conductors in
    /// this batch.
    pub async fn setup_app_for_zipped_agents<'a>(
        &mut self,
        installed_app_id: &str,
        agents: impl IntoIterator<Item = &AgentPubKey> + Clone,
        dna_files: impl IntoIterator<Item = &'a (impl DnaWithRole + 'a)> + Clone,
    ) -> ConductorApiResult<SweetAppBatch> {
        if agents.clone().into_iter().count() != self.0.len() {
            panic!(
                "setup_app_for_zipped_agents must take as many Agents as there are Conductors in this batch."
            )
        }

        let apps = self
            .0
            .iter_mut()
            .zip(agents.into_iter())
            .map(|(conductor, agent)| {
                conductor.setup_app_for_agent(installed_app_id, agent.clone(), dna_files.clone())
            })
            .collect::<Vec<_>>();

        Ok(future::join_all(apps)
            .await
            .into_iter()
            .collect::<Result<Vec<_>, _>>()?
            .into())
    }

    /// Let each conductor know about each others' agents so they can do networking
    pub async fn exchange_peer_info(&self) {
        SweetConductor::exchange_peer_info(&self.0).await
    }

    /// Let each conductor know about each others' agents so they can do networking
    pub async fn forget_peer_info(&self, agents_to_forget: impl IntoIterator<Item = &AgentPubKey>) {
        SweetConductor::forget_peer_info(&self.0, agents_to_forget).await
    }

    /// Let each conductor know about each others' agents so they can do networking
    pub async fn exchange_peer_info_sampled(&self, rng: &mut StdRng, s: usize) {
        SweetConductor::exchange_peer_info_sampled(&self.0, rng, s).await
    }

    /// Let a conductor know about all agents on some other conductor.
    pub async fn reveal_peer_info(&self, observer: usize, seen: usize) {
        let observer_conductor = &self.0[observer];
        let mut observer_envs = Vec::new();
        for env in observer_conductor
            .spaces
            .get_from_spaces(|s| s.p2p_agents_db.clone())
        {
            observer_envs.push(env.clone());
        }

        let seen_conductor = &self.0[seen];
        let mut seen_envs = Vec::new();
        for env in seen_conductor
            .spaces
            .get_from_spaces(|s| s.p2p_agents_db.clone())
        {
            seen_envs.push(env.clone());
        }

        crate::conductor::p2p_agent_store::reveal_peer_info(observer_envs, seen_envs).await;
    }

    /// Get the DPKI cell for each conductor, if applicable
    pub fn dpki_cells(&self) -> Vec<SweetCell> {
        self.0.iter().filter_map(|c| c.dpki_cell()).collect()
    }

    /// Force trigger all dht ops that haven't received
    /// enough validation receipts yet.
    pub async fn force_all_publish_dht_ops(&self) {
        for c in self.0.iter() {
            c.force_all_publish_dht_ops().await;
        }
    }

    /// Make the temp db dir persistent
    pub fn persist_dbs(&mut self) {
        for c in self.0.iter_mut() {
            let _ = c.persist_dbs();
        }
    }
}

impl std::ops::Index<usize> for SweetConductorBatch {
    type Output = SweetConductor;

    fn index(&self, index: usize) -> &Self::Output {
        &self.0[index]
    }
}

impl std::ops::IndexMut<usize> for SweetConductorBatch {
    fn index_mut(&mut self, index: usize) -> &mut Self::Output {
        &mut self.0[index]
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_conductor_config.rs
================================================
use std::sync::{atomic::AtomicUsize, Arc};

use holochain_conductor_api::{
    conductor::{ConductorConfig, ConductorTuningParams},
    AdminInterfaceConfig, InterfaceDriver,
};
use holochain_types::websocket::AllowedOrigins;
use kitsune_p2p_types::config::KitsuneP2pConfig;

use super::{DynSweetRendezvous, SweetConductor};

pub(crate) static NUM_CREATED: AtomicUsize = AtomicUsize::new(0);

/// Wrapper around ConductorConfig with some helpful builder methods
#[derive(Clone, derive_more::Deref, derive_more::DerefMut, derive_more::Into)]
pub struct SweetConductorConfig {
    #[deref]
    #[deref_mut]
    #[into]
    config: ConductorConfig,

    // Helps to keep owned references alive
    rendezvous: Option<DynSweetRendezvous>,
}

impl From<ConductorConfig> for SweetConductorConfig {
    fn from(config: ConductorConfig) -> Self {
        Self {
            config,
            rendezvous: None,
        }
    }
}

impl From<KitsuneP2pConfig> for SweetConductorConfig {
    fn from(network: KitsuneP2pConfig) -> Self {
        ConductorConfig {
            network,
            admin_interfaces: Some(vec![AdminInterfaceConfig {
                driver: InterfaceDriver::Websocket {
                    port: 0,
                    allowed_origins: AllowedOrigins::Any,
                },
            }]),
            tuning_params: Some(ConductorTuningParams {
                sys_validation_retry_delay: Some(std::time::Duration::from_secs(1)),
                countersigning_resolution_retry_delay: Some(std::time::Duration::from_secs(3)),
                countersigning_resolution_retry_limit: None,
                min_publish_interval: None,
            }),
            ..Default::default()
        }
        .into()
    }
}

impl SweetConductorConfig {
    /// Rewrite the config to point to the given rendezvous server
    pub fn apply_rendezvous(mut self, rendezvous: &DynSweetRendezvous) -> Self {
        self.rendezvous = Some(rendezvous.clone());
        let network = &mut self.network;

        if network.bootstrap_service.is_some()
            && network.bootstrap_service.as_ref().unwrap().to_string() == "rendezvous:"
        {
            network.bootstrap_service = Some(url2::url2!("{}", rendezvous.bootstrap_addr()));
        }

        {
            for t in network.transport_pool.iter_mut() {
                if let kitsune_p2p_types::config::TransportConfig::WebRTC { signal_url, .. } = t {
                    if signal_url == "rendezvous:" {
                        *signal_url = rendezvous.sig_addr().to_string();
                    }
                }
            }
        }

        self
    }

    /// Standard config for SweetConductors
    pub fn standard() -> Self {
        let mut c = SweetConductorConfig::from(KitsuneP2pConfig::mem())
            .tune(|tune| {
                tune.gossip_loop_iteration_delay_ms = 500;
                tune.gossip_peer_on_success_next_gossip_delay_ms = 1000;
                tune.gossip_peer_on_error_next_gossip_delay_ms = 1000;
                tune.gossip_round_timeout_ms = 10_000;
            })
            .tune_conductor(|tune| {
                tune.sys_validation_retry_delay = Some(std::time::Duration::from_secs(1));
            });

        // Allow device seed generation to exercise key derivation in sweettests.
        c.device_seed_lair_tag = Some("sweet-conductor-device-seed".to_string());
        c.danger_generate_throwaway_device_seed = true;
        c
    }

    /// Disable DPKI, which is on by default.
    /// You would want to disable DPKI in situations where you're testing unusual situations
    /// such as tests which disable networking, tests which use pregenerated agent keys,
    /// or any situation where it's known that DPKI is irrelevant.
    pub fn no_dpki(mut self) -> Self {
        self.dpki = holochain_conductor_api::conductor::DpkiConfig::disabled();
        self
    }

    /// Disable DPKI in a situation where we would like to run DPKI in a test, but the test
    /// only passes if it's disabled and we can't figure out why.
    #[cfg(feature = "test_utils")]
    pub fn no_dpki_mustfix(mut self) -> Self {
        tracing::warn!("Disabling DPKI for a test which should pass with DPKI enabled. TODO: fix");
        self.dpki = holochain_conductor_api::conductor::DpkiConfig::disabled();
        self
    }

    /// Rendezvous config for SweetConductors
    pub fn rendezvous(bootstrap: bool) -> Self {
        let mut config = Self::standard();

        if bootstrap {
            config.network.bootstrap_service = Some(url2::url2!("rendezvous:"));
        }

        {
            config.network.transport_pool =
                vec![kitsune_p2p_types::config::TransportConfig::WebRTC {
                    signal_url: "rendezvous:".into(),
                    webrtc_config: None,
                }];
        }

        config
    }

    /// Getter
    pub fn get_rendezvous(&self) -> Option<DynSweetRendezvous> {
        self.rendezvous.clone()
    }

    /// Build a SweetConductor from this config
    pub async fn build_conductor(self) -> SweetConductor {
        SweetConductor::from_config(self).await
    }

    /// Set network tuning params.
    pub fn tune(
        mut self,
        f: impl FnOnce(&mut kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams),
    ) -> Self {
        let r = &mut self.network.tuning_params;
        let mut tuning = (**r).clone();
        f(&mut tuning);
        *r = Arc::new(tuning);
        self
    }

    /// Set network tuning params.
    pub fn set_tuning_params(
        mut self,
        tuning_params: kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams,
    ) -> Self {
        self.network.tuning_params = Arc::new(tuning_params);
        self
    }

    /// Apply a function to the conductor's tuning parameters to customise them.
    pub fn tune_conductor(mut self, f: impl FnOnce(&mut ConductorTuningParams)) -> Self {
        if let Some(ref mut params) = self.tuning_params {
            f(params);
        }
        self
    }

    /// Completely disable networking
    pub fn no_networking(mut self) -> Self {
        self.network = self.network.clone().tune(|mut tp| {
            tp.disable_publish = true;
            tp.disable_recent_gossip = true;
            tp.disable_historical_gossip = true;
            tp
        });
        self
    }

    /// Disable publishing
    pub fn no_publish(mut self) -> Self {
        self.network = self.network.clone().tune(|mut tp| {
            tp.disable_publish = true;
            tp
        });
        self
    }

    /// Disable publishing and recent gossip
    pub fn historical_only(mut self) -> Self {
        self.network = self.network.clone().tune(|mut tp| {
            tp.disable_publish = true;
            tp.disable_recent_gossip = true;
            tp
        });
        self
    }

    /// Disable recent op gossip, but keep agent gossip
    pub fn historical_and_agent_gossip_only(mut self) -> Self {
        self.network = self.network.clone().tune(|mut tp| {
            tp.disable_publish = true;
            // keep recent gossip for agent gossip, but gossip no ops.
            tp.danger_gossip_recent_threshold_secs = 0;
            tp
        });
        self
    }

    /// Disable publishing and historical gossip
    pub fn recent_only(mut self) -> Self {
        self.network = self.network.clone().tune(|mut tp| {
            tp.disable_publish = true;
            tp.disable_historical_gossip = true;
            tp
        });
        self
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_conductor_config_rendezvous.rs
================================================
use std::sync::{Arc, Mutex};

/// How conductors should learn about each other / speak to each other.
/// Signal/TURN + bootstrap in tx5 mode.
pub trait SweetRendezvous: 'static + Send + Sync {
    /// Get the bootstrap address.
    fn bootstrap_addr(&self) -> &str;

    /// Get the signal server address.
    fn sig_addr(&self) -> &str;
}

/// Trait object rendezvous.
pub type DynSweetRendezvous = Arc<dyn SweetRendezvous>;

/// Local rendezvous infrastructure for unit testing.
pub struct SweetLocalRendezvous {
    bs_addr: String,
    bs_shutdown: Option<kitsune_p2p_bootstrap::BootstrapShutdown>,

    turn_srv: Option<tx5_go_pion_turn::Tx5TurnServer>,
    sig_addr: String,
    sig_hnd: Mutex<Option<sbd_server::SbdServer>>,
    sig_ip: std::net::IpAddr,
    sig_port: u16,
}

impl Drop for SweetLocalRendezvous {
    fn drop(&mut self) {
        if let Some(s) = self.bs_shutdown.take() {
            s();
        }
        if let Some(s) = self.turn_srv.take() {
            tokio::task::spawn(async move {
                let _ = s.stop().await;
            });
        }
    }
}

async fn spawn_sig(ip: std::net::IpAddr, port: u16) -> (String, u16, sbd_server::SbdServer) {
    let sig_hnd = sbd_server::SbdServer::new(Arc::new(sbd_server::Config {
        bind: vec![format!("{ip}:{port}")],
        limit_clients: 100,
        disable_rate_limiting: true,
        ..Default::default()
    }))
    .await
    .unwrap();

    let sig_addr = *sig_hnd.bind_addrs().first().unwrap();
    let sig_port = sig_addr.port();
    let sig_addr = format!("ws://{sig_addr}");
    tracing::info!("RUNNING SIG: {sig_addr:?}");

    (sig_addr, sig_port, sig_hnd)
}

impl SweetLocalRendezvous {
    /// Create a new local rendezvous instance.
    #[allow(clippy::new_ret_no_self)]
    pub async fn new() -> DynSweetRendezvous {
        Self::new_raw().await
    }

    /// Create a new local rendezvous instance.
    pub async fn new_raw() -> Arc<Self> {
        let mut addr = None;

        for iface in get_if_addrs::get_if_addrs().expect("failed to get_if_addrs") {
            if iface.ip().is_ipv6() {
                continue;
            }
            addr = Some(iface.ip());
            break;
        }

        let addr = addr.expect("no matching network interfaces found");

        let (bs_driver, bs_addr, bs_shutdown) = kitsune_p2p_bootstrap::run((addr, 0), Vec::new())
            .await
            .unwrap();
        tokio::task::spawn(bs_driver);
        let bs_addr = format!("http://{bs_addr}");
        tracing::info!("RUNNING BOOTSTRAP: {bs_addr:?}");

        {
            let (turn_addr, turn_srv) = tx5_go_pion_turn::test_turn_server().await.unwrap();
            tracing::info!("RUNNING TURN: {turn_addr:?}");

            let (sig_addr, sig_port, sig_hnd) = spawn_sig(addr, 0).await;

            let sig_hnd = Mutex::new(Some(sig_hnd));

            Arc::new(Self {
                bs_addr,
                bs_shutdown: Some(bs_shutdown),
                turn_srv: Some(turn_srv),
                sig_addr,
                sig_hnd,
                sig_ip: addr,
                sig_port,
            })
        }
    }

    /// Drop (shutdown) the signal server.
    pub async fn drop_sig(&self) {
        self.sig_hnd.lock().unwrap().take();

        // wait up to 1 second until the socket is actually closed
        for _ in 0..100 {
            tokio::time::sleep(std::time::Duration::from_millis(10)).await;

            match tokio::net::TcpStream::connect((self.sig_ip, self.sig_port)).await {
                Ok(_) => (),
                Err(_) => break,
            }
        }
    }

    /// Start (or restart) the signal server.
    pub async fn start_sig(&self) {
        self.drop_sig().await;

        let (_, _, sig_hnd) = spawn_sig(self.sig_ip, self.sig_port).await;

        *self.sig_hnd.lock().unwrap() = Some(sig_hnd);
    }
}

impl SweetRendezvous for SweetLocalRendezvous {
    /// Get the bootstrap address.
    fn bootstrap_addr(&self) -> &str {
        self.bs_addr.as_str()
    }

    /// Get the signal server address.
    fn sig_addr(&self) -> &str {
        self.sig_addr.as_str()
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_conductor_handle.rs
================================================
use super::SweetZome;
use crate::conductor::{api::error::ConductorApiResult, ConductorHandle};
use holochain_types::prelude::*;

/// A wrapper around ConductorHandle with more convenient methods for testing
/// and a cleanup drop
#[derive(shrinkwraprs::Shrinkwrap, derive_more::From)]
pub struct SweetConductorHandle(pub(crate) ConductorHandle);

impl SweetConductorHandle {
    /// Make a zome call to a Cell, as if that Cell were the caller. Most common case.
    /// No capability is necessary, since the authorship capability is automatically granted.
    pub async fn call<I, O>(
        &self,
        zome: &SweetZome,
        fn_name: impl Into<FunctionName>,
        payload: I,
    ) -> O
    where
        I: serde::Serialize + std::fmt::Debug,
        O: serde::de::DeserializeOwned + std::fmt::Debug,
    {
        self.call_fallible(zome, fn_name, payload).await.unwrap()
    }

    /// Like `call`, but without the unwrap
    pub async fn call_fallible<I, O>(
        &self,
        zome: &SweetZome,
        fn_name: impl Into<FunctionName>,
        payload: I,
    ) -> ConductorApiResult<O>
    where
        I: serde::Serialize + std::fmt::Debug,
        O: serde::de::DeserializeOwned + std::fmt::Debug,
    {
        self.call_from_fallible(zome.cell_id().agent_pubkey(), None, zome, fn_name, payload)
            .await
    }
    /// Make a zome call to a Cell, as if some other Cell were the caller. More general case.
    /// Can optionally provide a capability.
    pub async fn call_from<I, O>(
        &self,
        provenance: &AgentPubKey,
        cap_secret: Option<CapSecret>,
        zome: &SweetZome,
        fn_name: impl Into<FunctionName>,
        payload: I,
    ) -> O
    where
        I: Serialize + std::fmt::Debug,
        O: serde::de::DeserializeOwned + std::fmt::Debug,
    {
        self.call_from_fallible(provenance, cap_secret, zome, fn_name, payload)
            .await
            .unwrap()
    }

    /// Like `call_from`, but without the unwrap
    pub async fn call_from_fallible<I, O>(
        &self,
        provenance: &AgentPubKey,
        cap_secret: Option<CapSecret>,
        zome: &SweetZome,
        fn_name: impl Into<FunctionName>,
        payload: I,
    ) -> ConductorApiResult<O>
    where
        I: Serialize + std::fmt::Debug,
        O: serde::de::DeserializeOwned + std::fmt::Debug,
    {
        self.0
            .easy_call_zome(
                provenance,
                cap_secret,
                zome.cell_id().clone(),
                zome.name().clone(),
                fn_name,
                payload,
            )
            .await
    }

    /// Intentionally private clone function, only to be used internally
    pub(super) fn clone_privately(&self) -> Self {
        Self(self.0.clone())
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_consistency.rs
================================================
//! Methods for awaiting consistency between cells of the same DNA

use crate::{
    prelude::*,
    test_utils::{wait_for_integration_diff, ConsistencyConditions, ConsistencyResult},
};
use std::time::Duration;

use super::*;

/// A duration expressed properly, or just as seconds
#[derive(derive_more::From, Debug)]
pub enum DurationOrSeconds {
    /// Proper duration
    Duration(Duration),
    /// Just seconds
    Seconds(u64),
}

impl DurationOrSeconds {
    /// Get the proper duration
    pub fn into_duration(self) -> Duration {
        match self {
            Self::Duration(d) => d,
            Self::Seconds(s) => Duration::from_secs(s),
        }
    }
}

/// Wait for all cells to reach consistency
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn await_consistency<'a, I: IntoIterator<Item = &'a SweetCell>>(
    timeout: impl Into<DurationOrSeconds>,
    all_cells: I,
) -> ConsistencyResult {
    await_consistency_advanced(timeout, (), all_cells.into_iter().map(|c| (c, true))).await
}

/// Wait for all cells to reach consistency
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn await_consistency_conditional<'a, I: IntoIterator<Item = &'a SweetCell>>(
    timeout: impl Into<DurationOrSeconds>,
    conditions: impl Into<ConsistencyConditions>,
    all_cells: I,
) -> ConsistencyResult {
    await_consistency_advanced(
        timeout,
        conditions,
        all_cells.into_iter().map(|c| (c, true)),
    )
    .await
}

/// Wait for all cells to reach consistency,
/// with the option to specify that some cells are offline.
///
/// Cells paired with a `false` value will have their authored ops counted towards the total,
/// but not their integrated ops (since they are not online to integrate things).
/// This is useful for tests where nodes go offline.
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub async fn await_consistency_advanced<'a, I: IntoIterator<Item = (&'a SweetCell, bool)>>(
    timeout: impl Into<DurationOrSeconds>,
    conditions: impl Into<ConsistencyConditions>,
    all_cells: I,
) -> ConsistencyResult {
    #[allow(clippy::type_complexity)]
    let all_cell_dbs: Vec<(
        AgentPubKey,
        DbRead<DbKindAuthored>,
        Option<DbRead<DbKindDht>>,
    )> = all_cells
        .into_iter()
        .map(|(c, online)| {
            (
                c.agent_pubkey().clone(),
                c.authored_db().clone().into(),
                online.then(|| c.dht_db().clone().into()),
            )
        })
        .collect();
    let all_cell_dbs: Vec<_> = all_cell_dbs
        .iter()
        .map(|c| (&c.0, &c.1, c.2.as_ref()))
        .collect();
    wait_for_integration_diff(
        &all_cell_dbs[..],
        timeout.into().into_duration(),
        conditions.into(),
    )
    .await
}



================================================
File: crates/holochain/src/sweettest/sweet_dna.rs
================================================
use holochain_p2p::dht::spacetime::STANDARD_QUANTUM_TIME;
use holochain_types::inline_zome::InlineZomeSet;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasmPair;
use std::path::Path;

/// Helpful constructors for DnaFiles used in tests
#[derive(Clone, Debug, derive_more::From, derive_more::Into, shrinkwraprs::Shrinkwrap)]
pub struct SweetDnaFile(DnaFile);

impl SweetDnaFile {
    /// Create a DnaFile from a path to a *.dna bundle
    pub async fn from_bundle(path: &Path) -> DnaResult<DnaFile> {
        Self::from_bundle_with_overrides(path, DnaModifiersOpt::<SerializedBytes>::none()).await
    }

    /// Create a DnaFile from a path to a *.dna bundle, applying the specified
    /// modifier overrides
    pub async fn from_bundle_with_overrides<P, E>(
        path: &Path,
        modifiers: DnaModifiersOpt<P>,
    ) -> DnaResult<DnaFile>
    where
        P: TryInto<SerializedBytes, Error = E>,
        SerializedBytesError: From<E>,
    {
        Ok(DnaBundle::read_from_file(path)
            .await?
            .into_dna_file(modifiers.serialized().map_err(SerializedBytesError::from)?)
            .await?
            .0)
    }

    /// Create a DnaFile from a collection of Zomes
    pub async fn from_zomes<I, C, D>(
        network_seed: String,
        integrity_zomes: Vec<I>,
        coordinator_zomes: Vec<C>,
        wasms: Vec<D>,
        properties: SerializedBytes,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>)
    where
        I: Into<IntegrityZome>,
        C: Into<CoordinatorZome>,
        D: Into<wasm::DnaWasm>,
    {
        let integrity_zomes: Vec<IntegrityZome> =
            integrity_zomes.into_iter().map(Into::into).collect();
        let coordinator_zomes: Vec<CoordinatorZome> =
            coordinator_zomes.into_iter().map(Into::into).collect();
        let iz: IntegrityZomes = integrity_zomes
            .clone()
            .into_iter()
            .map(IntegrityZome::into_inner)
            .collect();
        let cz: CoordinatorZomes = coordinator_zomes
            .clone()
            .into_iter()
            .map(CoordinatorZome::into_inner)
            .collect();
        let dna_def = DnaDefBuilder::default()
            .modifiers(DnaModifiers {
                network_seed,
                properties: properties.clone(),
                origin_time: Timestamp::HOLOCHAIN_EPOCH,
                quantum_time: STANDARD_QUANTUM_TIME,
            })
            .integrity_zomes(iz)
            .coordinator_zomes(cz)
            .build()
            .unwrap();

        let dna_file = DnaFile::new(dna_def, wasms.into_iter().map(Into::into)).await;
        (dna_file, integrity_zomes, coordinator_zomes)
    }

    /// Create a DnaFile from a collection of Zomes,
    /// with a random network seed
    pub async fn unique_from_zomes<I, C, D>(
        integrity_zomes: Vec<I>,
        coordinator_zomes: Vec<C>,
        wasms: Vec<D>,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>)
    where
        I: Into<IntegrityZome>,
        C: Into<CoordinatorZome>,
        D: Into<wasm::DnaWasm>,
    {
        Self::from_zomes(
            random_network_seed(),
            integrity_zomes,
            coordinator_zomes,
            wasms,
            SerializedBytes::default(),
        )
        .await
    }

    /// Create a DnaFile from a collection of TestWasm
    pub async fn from_test_wasms<W>(
        network_seed: String,
        wasms: Vec<W>,
        properties: SerializedBytes,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>)
    where
        W: Into<TestWasmPair<IntegrityZome, CoordinatorZome>>
            + Into<TestWasmPair<wasm::DnaWasm>>
            + Clone,
    {
        let (integrity_zomes, coordinator_zomes) = wasms
            .clone()
            .into_iter()
            .map(|w| {
                let TestWasmPair::<IntegrityZome, CoordinatorZome> {
                    integrity,
                    coordinator,
                } = w.into();
                (integrity.into_inner(), coordinator.into_inner())
            })
            .unzip();

        let wasms = wasms
            .into_iter()
            .flat_map(|w| {
                let TestWasmPair::<DnaWasm> {
                    integrity,
                    coordinator,
                } = w.into();
                [integrity, coordinator]
            })
            .collect();

        Self::from_zomes(
            network_seed,
            integrity_zomes,
            coordinator_zomes,
            wasms,
            properties,
        )
        .await
    }

    /// Create a DnaFile from a collection of TestWasm
    /// with a random network seed
    pub async fn unique_from_test_wasms<W>(
        test_wasms: Vec<W>,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>)
    where
        W: Into<TestWasmPair<IntegrityZome, CoordinatorZome>>
            + Into<TestWasmPair<wasm::DnaWasm>>
            + Clone,
    {
        let (dna, integrity_zomes, coordinator_zomes) = Self::from_test_wasms(
            random_network_seed(),
            test_wasms,
            SerializedBytes::default(),
        )
        .await;
        (dna, integrity_zomes, coordinator_zomes)
    }

    /// Create a DnaFile from a collection of InlineZomes (no Wasm)
    pub async fn from_inline_zomes(
        network_seed: String,
        zomes: impl Into<InlineZomeSet>,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>) {
        let mut zomes = zomes.into();
        let coordinator_zomes: Vec<CoordinatorZome> = zomes
            .coordinator_zomes
            .into_iter()
            .map(|(n, z)| (n.into(), z.into()))
            .map(|t| {
                let dep = zomes.dependencies.remove(&t.0);
                let mut z: CoordinatorZome = t.into();
                if let Some(dep) = dep {
                    z.set_dependency(dep);
                }
                z
            })
            .collect();
        Self::from_zomes(
            network_seed,
            zomes
                .integrity_order
                .into_iter()
                .map(|n| zomes.integrity_zomes.remove_entry(n).unwrap())
                .map(|(n, z)| (n.into(), z.into()))
                .collect(),
            coordinator_zomes,
            Vec::<wasm::DnaWasm>::with_capacity(0),
            SerializedBytes::default(),
        )
        .await
    }

    /// Create a DnaFile from a collection of InlineZomes (no Wasm),
    /// with a random network seed
    pub async fn unique_from_inline_zomes(
        zomes: impl Into<InlineZomeSet>,
    ) -> (DnaFile, Vec<IntegrityZome>, Vec<CoordinatorZome>) {
        Self::from_inline_zomes(random_network_seed(), zomes).await
    }

    /// Create a unique DnaFile with no validation and no zome functions
    pub async fn unique_empty() -> DnaFile {
        Self::unique_from_inline_zomes(InlineZomeSet::new([], []))
            .await
            .0
    }
}

/// Helpful constructors for DnaDefs used in tests
pub struct SweetDnaDef;

impl SweetDnaDef {
    /// Create a DnaDef with a random network seed, useful for testing
    // TODO: move fully into sweettest when possible
    pub fn unique_from_zomes(
        integrity_zomes: Vec<IntegrityZome>,
        coordinator_zomes: Vec<CoordinatorZome>,
    ) -> DnaDef {
        DnaDef::unique_from_zomes(integrity_zomes, coordinator_zomes)
    }
}



================================================
File: crates/holochain/src/sweettest/sweet_zome.rs
================================================
use hdk::prelude::*;
use holochain_types::inline_zome::InlineZomeSet;
use serde::de::DeserializeOwned;

/// A reference to a Zome in a Cell created by a SweetConductor installation function.
/// Think of it as a partially applied SweetCell, with the ZomeName baked in.
#[derive(Clone, Debug, derive_more::Constructor)]
pub struct SweetZome {
    cell_id: CellId,
    name: ZomeName,
}

impl SweetZome {
    /// Accessor
    pub fn cell_id(&self) -> &CellId {
        &self.cell_id
    }

    /// Accessor
    pub fn name(&self) -> &ZomeName {
        &self.name
    }
}

#[derive(Default, Clone)]
/// A helper for creating an [`InlineZomeSet`] consisting of a single
/// integrity zome and a single coordinator zome.
pub struct SweetInlineZomes(pub InlineZomeSet);

impl SweetInlineZomes {
    /// Zome name for the integrity zome.
    pub const INTEGRITY: &'static str = "integrity";
    /// Zome name for the coordinator zome.
    pub const COORDINATOR: &'static str = "coordinator";

    /// Create a single integrity zome with the [`ZomeName`] "integrity"
    /// and coordinator zome with the [`ZomeName`] Coordinator.
    pub fn new(entry_defs: Vec<EntryDef>, num_link_types: u8) -> Self {
        Self(
            InlineZomeSet::new_unique(
                [(Self::INTEGRITY, entry_defs, num_link_types)],
                [Self::COORDINATOR],
            )
            .with_dependency(Self::COORDINATOR, Self::INTEGRITY),
        )
    }

    /// Add a function to the integrity zome.
    pub fn integrity_function<F, I, O>(self, name: &str, f: F) -> Self
    where
        F: Fn(BoxApi, I) -> InlineZomeResult<O> + 'static + Send + Sync,
        I: DeserializeOwned + std::fmt::Debug,
        O: Serialize + std::fmt::Debug,
    {
        Self(self.0.function(Self::INTEGRITY, name, f))
    }

    /// Add a function to the coordinator_zome.
    pub fn function<F, I, O>(self, name: &str, f: F) -> Self
    where
        F: Fn(BoxApi, I) -> InlineZomeResult<O> + 'static + Send + Sync,
        I: DeserializeOwned + std::fmt::Debug,
        O: Serialize + std::fmt::Debug,
    {
        Self(self.0.function(Self::COORDINATOR, name, f))
    }
}

impl From<SweetInlineZomes> for InlineZomeSet {
    fn from(s: SweetInlineZomes) -> Self {
        s.0
    }
}



================================================
File: crates/holochain/src/test_utils/big_stack_test.rs
================================================
/// Wrap a test body in this macro to give it a bigger stack. Expects the body to
/// be async.
#[macro_export]
macro_rules! big_stack_test {
    ($what_do:expr, $size:literal) => {
        tokio::runtime::Builder::new_multi_thread()
            // Need a bigger stack for this test for some reason.
            .thread_stack_size($size)
            .enable_all()
            .build()
            .unwrap()
            .block_on(async move {
                // This spawns a task with a big stack. The outer block on does
                // NOT have the larger stack size.
                tokio::task::spawn(tokio::time::timeout(
                    std::time::Duration::from_secs(60),
                    $what_do,
                ))
                .await
                .unwrap()
            })
            .unwrap()
    };

    ($what_do:expr) => {
        $crate::big_stack_test!($what_do, 11_000_000)
    };
}



================================================
File: crates/holochain/src/test_utils/conductor_setup.rs
================================================
#![allow(missing_docs)]

use super::host_fn_caller::HostFnCaller;
use super::install_app;
use super::setup_app_inner;
use crate::conductor::api::CellConductorApi;
use crate::conductor::api::CellConductorApiT;
use crate::conductor::interface::SignalBroadcaster;
use crate::conductor::ConductorHandle;
use crate::core::queue_consumer::QueueTriggers;
use crate::core::ribosome::real_ribosome::RealRibosome;
use crate::core::ribosome::RibosomeT;
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_chc::ChcImpl;
use holochain_keystore::MetaLairClient;
use holochain_p2p::actor::HolochainP2pRefToDna;
use holochain_p2p::HolochainP2pDna;
use holochain_serialized_bytes::SerializedBytes;
use holochain_state::prelude::test_db_dir;
use holochain_types::db_cache::DhtDbQueryCache;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;
use holochain_wasm_test_utils::TestZomes;
use kitsune_p2p_types::config::KitsuneP2pConfig;
use std::collections::BTreeMap;
use std::collections::HashMap;
use std::convert::TryFrom;
use tempfile::TempDir;

/// A "factory" for HostFnCaller, which will produce them when given a ZomeName
pub struct CellHostFnCaller {
    pub cell_id: CellId,
    pub authored_db: DbWrite<DbKindAuthored>,
    pub dht_db: DbWrite<DbKindDht>,
    pub dht_db_cache: DhtDbQueryCache,
    pub cache: DbWrite<DbKindCache>,
    pub ribosome: RealRibosome,
    pub network: HolochainP2pDna,
    pub keystore: MetaLairClient,
    pub signal_tx: SignalBroadcaster,
    pub triggers: QueueTriggers,
    pub cell_conductor_api: CellConductorApi,
}

impl CellHostFnCaller {
    pub async fn new(
        cell_id: &CellId,
        handle: &ConductorHandle,
        dna_file: &DnaFile,
        chc: Option<ChcImpl>,
    ) -> Self {
        let authored_db = handle.get_authored_db(cell_id.dna_hash()).unwrap();
        let dht_db = handle.get_dht_db(cell_id.dna_hash()).unwrap();
        let dht_db_cache = handle.get_dht_db_cache(cell_id.dna_hash()).unwrap();
        let cache = handle.get_cache_db(cell_id).await.unwrap();
        let keystore = handle.keystore().clone();
        let network = handle
            .holochain_p2p()
            .to_dna(cell_id.dna_hash().clone(), chc);
        let triggers = handle.get_cell_triggers(cell_id).await.unwrap();
        let cell_conductor_api = CellConductorApi::new(handle.clone(), cell_id.clone());

        let ribosome = handle.get_ribosome(dna_file.dna_hash()).unwrap();
        let signal_tx = handle.signal_broadcaster();
        CellHostFnCaller {
            cell_id: cell_id.clone(),
            authored_db,
            dht_db,
            dht_db_cache,
            cache,
            ribosome,
            network,
            keystore,
            signal_tx,
            triggers,
            cell_conductor_api,
        }
    }

    /// Create a HostFnCaller for a specific zome and call
    pub fn get_api<I: Into<ZomeName>>(&self, zome_name: I) -> HostFnCaller {
        let zome_name: ZomeName = zome_name.into();
        let zome_path = (self.cell_id.clone(), zome_name).into();
        let call_zome_handle = self.cell_conductor_api.clone().into_call_zome_handle();
        HostFnCaller {
            authored_db: self.authored_db.clone(),
            dht_db: self.dht_db.clone(),
            dht_db_cache: self.dht_db_cache.clone(),
            cache: self.cache.clone(),
            ribosome: self.ribosome.clone(),
            zome_path,
            network: self.network.clone(),
            keystore: self.keystore.clone(),
            signal_tx: self.signal_tx.clone(),
            call_zome_handle,
        }
    }
}

/// Everything you need to run a test that uses the conductor
// MAYBE: rewrite as sweettests if possible
pub struct ConductorTestData {
    _db_dir: TempDir,
    handle: ConductorHandle,
    cell_apis: BTreeMap<CellId, CellHostFnCaller>,
}

impl ConductorTestData {
    pub async fn new(
        envs: TempDir,
        dna_files: Vec<DnaFile>,
        agents: Vec<AgentPubKey>,
        network_config: KitsuneP2pConfig,
    ) -> (Self, HashMap<DnaHash, Vec<CellId>>) {
        let num_agents = agents.len();
        let num_dnas = dna_files.len();
        let mut cells = Vec::with_capacity(num_dnas * num_agents);
        let mut cell_id_by_dna_file = Vec::with_capacity(num_dnas);
        for (i, dna_file) in dna_files.iter().enumerate() {
            let mut cell_ids = Vec::with_capacity(num_agents);
            for (j, agent_id) in agents.iter().enumerate() {
                let cell_id = CellId::new(dna_file.dna_hash().to_owned(), agent_id.clone());
                cells.push((
                    InstalledCell::new(cell_id.clone(), format!("agent-{}-{}", i, j)),
                    None,
                ));
                cell_ids.push(cell_id);
            }
            cell_id_by_dna_file.push((dna_file, cell_ids));
        }

        let (_app_api, handle) = setup_app_inner(
            envs.path().to_path_buf().into(),
            vec![("test_app", cells)],
            dna_files.clone(),
            Some(network_config),
        )
        .await;

        let mut cell_apis = BTreeMap::new();

        for (dna_file, cell_ids) in cell_id_by_dna_file.iter() {
            for cell_id in cell_ids {
                cell_apis.insert(
                    cell_id.clone(),
                    CellHostFnCaller::new(cell_id, &handle, dna_file, None).await,
                );
            }
        }

        let this = Self {
            _db_dir: envs,
            // app_api,
            handle,
            cell_apis,
        };
        let installed = cell_id_by_dna_file
            .into_iter()
            .map(|(dna_file, cell_ids)| (dna_file.dna_hash().clone(), cell_ids))
            .collect();
        (this, installed)
    }

    /// Create a new conductor and test data
    pub async fn two_agents(zomes: Vec<TestWasm>, with_bob: bool) -> Self {
        Self::two_agents_inner(zomes, with_bob, None).await
    }

    /// New test data that creates a conductor using a custom network config
    pub async fn with_network_config(
        zomes: Vec<TestWasm>,
        with_bob: bool,
        network: KitsuneP2pConfig,
    ) -> Self {
        Self::two_agents_inner(zomes, with_bob, Some(network)).await
    }

    async fn two_agents_inner(
        zomes: Vec<TestWasm>,
        with_bob: bool,
        network: Option<KitsuneP2pConfig>,
    ) -> Self {
        let dna_file = DnaFile::new(
            DnaDef {
                name: "conductor_test".to_string(),
                modifiers: DnaModifiers {
                    network_seed: "ba1d046d-ce29-4778-914b-47e6010d2faf".to_string(),
                    properties: SerializedBytes::try_from(()).unwrap(),
                    origin_time: Timestamp::HOLOCHAIN_EPOCH,
                    quantum_time: holochain_p2p::dht::spacetime::STANDARD_QUANTUM_TIME,
                },
                integrity_zomes: zomes
                    .clone()
                    .into_iter()
                    .map(TestZomes::from)
                    .map(|z| z.integrity.into_inner())
                    .collect(),
                coordinator_zomes: zomes
                    .clone()
                    .into_iter()
                    .map(TestZomes::from)
                    .map(|z| z.coordinator.into_inner())
                    .collect(),
            },
            zomes.into_iter().flat_map(Vec::<DnaWasm>::from),
        )
        .await;

        let mut agents = vec![fake_agent_pubkey_1()];
        if with_bob {
            agents.push(fake_agent_pubkey_2());
        }

        let (this, _) = Self::new(
            test_db_dir(),
            vec![dna_file],
            agents,
            network.unwrap_or_default(),
        )
        .await;

        this
    }

    /// Shutdown the conductor
    pub async fn shutdown_conductor(&mut self) {
        self.handle.shutdown().await.unwrap().unwrap();
    }

    /// Bring bob online if he isn't already
    pub async fn bring_bob_online(&mut self) {
        let dna_file = self.alice_call_data().ribosome.dna_file().clone();
        if self.bob_call_data().is_none() {
            let bob_agent_id = fake_agent_pubkey_2();
            let bob_cell_id = CellId::new(dna_file.dna_hash().clone(), bob_agent_id.clone());
            let bob_installed_cell = InstalledCell::new(bob_cell_id.clone(), "bob_handle".into());
            let cell_data = vec![(bob_installed_cell, None)];
            install_app("bob_app", cell_data, vec![dna_file.clone()], self.handle()).await;
            self.cell_apis.insert(
                bob_cell_id.clone(),
                CellHostFnCaller::new(&bob_cell_id, &self.handle(), &dna_file, None).await,
            );
        }
    }

    pub fn handle(&self) -> ConductorHandle {
        self.handle.clone()
    }

    #[allow(clippy::iter_nth_zero)]
    pub fn alice_call_data(&self) -> &CellHostFnCaller {
        match self.cell_apis.values().len() {
            0 => unreachable!(),
            1 => self.cell_apis.values().next().unwrap(),
            2 => self.cell_apis.values().nth(1).unwrap(),
            _ => unimplemented!(),
        }
    }

    pub fn bob_call_data(&self) -> Option<&CellHostFnCaller> {
        match self.cell_apis.values().len() {
            0 => unreachable!(),
            1 => None,
            2 => self.cell_apis.values().next(),
            _ => unimplemented!(),
        }
    }

    #[allow(clippy::iter_nth_zero)]
    pub fn alice_call_data_mut(&mut self) -> &mut CellHostFnCaller {
        let key = self.cell_apis.keys().nth(0).unwrap().clone();
        self.cell_apis.get_mut(&key).unwrap()
    }

    pub fn get_cell(&mut self, cell_id: &CellId) -> Option<&mut CellHostFnCaller> {
        self.cell_apis.get_mut(cell_id)
    }
}



================================================
File: crates/holochain/src/test_utils/consistency.rs
================================================
//! Utilities for testing the consistency of the dht.

use std::{
    collections::{HashMap, HashSet},
    sync::Arc,
    time::Duration,
};

use futures::stream::StreamExt;
use holo_hash::{AgentPubKey, DhtOpHash, DnaHash};
use holochain_p2p::{
    dht_arc::{DhtArc, DhtLocation},
    AgentPubKeyExt, DhtOpHashExt,
};
use holochain_sqlite::prelude::{
    DatabaseResult, DbKindAuthored, DbKindDht, DbKindP2pAgents, ReadAccess,
};
use holochain_state::prelude::*;
use kitsune_p2p::{KitsuneAgent, KitsuneOpHash};
use kitsune_p2p_types::consistency::*;
use rusqlite::named_params;

use crate::conductor::ConductorHandle;

struct Stores {
    agent: Arc<KitsuneAgent>,
    authored_db: DbRead<DbKindAuthored>,
    p2p_agents_db: DbRead<DbKindP2pAgents>,
}

#[derive(Clone)]
struct Reporter(tokio::sync::mpsc::Sender<SessionMessage>, Arc<KitsuneAgent>);

const CONCURRENCY: usize = 100;

/// A helper for checking consistency of all published ops for all cells in all conductors
/// has reached consistency in a sharded context.
pub async fn local_machine_session(conductors: &[ConductorHandle], timeout: Duration) {
    // For each space get all the cells, their db and the p2p envs.
    let mut spaces = HashMap::new();
    for (i, c) in conductors.iter().enumerate() {
        for cell_id in c.running_cell_ids() {
            let space = spaces
                .entry(cell_id.dna_hash().clone())
                .or_insert_with(|| vec![None; conductors.len()]);
            if space[i].is_none() {
                let p2p_agents_db: DbRead<DbKindP2pAgents> =
                    c.get_p2p_db(cell_id.dna_hash()).into();
                space[i] = Some((p2p_agents_db, Vec::new()));
            }
            space[i].as_mut().unwrap().1.push((
                c.get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())
                    .unwrap()
                    .into(),
                c.get_dht_db(cell_id.dna_hash()).unwrap().into(),
                cell_id.agent_pubkey().to_kitsune(),
            ));
        }
    }

    // Run a consistency session for each space.
    for (_, conductors) in spaces {
        // The agents we need to wait for.
        let mut wait_for_agents = HashSet::new();

        // Maps to environments.
        let mut agent_dht_map = HashMap::new();
        let mut agent_p2p_map = HashMap::new();

        // All the agents that should be held.
        let mut all_agents = Vec::new();
        // All the op hashes that should be held.
        let mut all_hashes = Vec::new();
        let (tx, rx) = tokio::sync::mpsc::channel(1000);

        // Gather the expected agents and op hashes from each conductor.
        for (p2p_agents_db, agents) in conductors.into_iter().flatten() {
            wait_for_agents.extend(agents.iter().map(|(_, _, agent)| agent.clone()));
            agent_dht_map.extend(agents.iter().cloned().map(|(_, dht, agent)| (agent, dht)));
            agent_p2p_map.extend(
                agents
                    .iter()
                    .cloned()
                    .map(|(_, _, a)| (a, p2p_agents_db.clone())),
            );
            let (a, h) = gather_conductor_data(p2p_agents_db, agents).await;
            all_agents.extend(a);
            all_hashes.extend(h);
        }

        // Spawn a background task that will run each
        // cells self consistency check against the data that
        // they are expected to hold.
        tokio::spawn(expect_all(
            tx,
            timeout,
            all_agents,
            all_hashes,
            agent_dht_map,
            agent_p2p_map,
        ));

        // Wait up to the timeout for all the agents to report success.
        wait_for_consistency(rx, wait_for_agents, timeout).await;
    }
}

/// Get consistency for a particular hash.
pub async fn local_machine_session_with_hashes(
    handles: Vec<&ConductorHandle>,
    hashes: impl Iterator<Item = (DhtLocation, DhtOpHash)>,
    space: &DnaHash,
    timeout: Duration,
) {
    // Grab the environments and cells for each conductor in this space.
    let mut conductors = vec![None; handles.len()];
    for (i, c) in handles.iter().enumerate() {
        for cell_id in c.running_cell_ids() {
            if cell_id.dna_hash() != space {
                continue;
            }
            if conductors[i].is_none() {
                let p2p_agents_db: DbRead<DbKindP2pAgents> =
                    c.get_p2p_db(cell_id.dna_hash()).into();
                conductors[i] = Some((p2p_agents_db, Vec::new()));
            }
            conductors[i].as_mut().unwrap().1.push((
                c.get_dht_db(cell_id.dna_hash()).unwrap().into(),
                cell_id.agent_pubkey().to_kitsune(),
            ));
        }
    }

    // Convert the hashes to kitsune.
    let all_hashes = hashes
        .into_iter()
        .map(|(l, h)| (l, h.into_kitsune_raw()))
        .collect::<Vec<_>>();
    // The agents we need to wait for.
    let mut wait_for_agents = HashSet::new();

    // Maps to environments.
    let mut agent_dht_map = HashMap::new();
    let mut agent_p2p_map = HashMap::new();

    // All the agents that should be held.
    let mut all_agents = Vec::new();
    let (tx, rx) = tokio::sync::mpsc::channel(1000);

    // Gather the expected agents from each conductor.
    for (p2p_agents_db, agents) in conductors.into_iter().flatten() {
        wait_for_agents.extend(agents.iter().map(|(_, agent)| agent.clone()));
        agent_dht_map.extend(agents.iter().cloned().map(|(e, a)| (a, e)));
        agent_p2p_map.extend(
            agents
                .iter()
                .cloned()
                .map(|(_, a)| (a, p2p_agents_db.clone())),
        );
        for (_, agent) in &agents {
            if let Some(storage_arc) = request_arc(&p2p_agents_db, (**agent).clone())
                .await
                .unwrap()
            {
                all_agents.push((agent.clone(), storage_arc));
            }
        }
    }

    // Spawn a background task that will run each
    // cells self consistency check against the data that
    // they are expected to hold.
    tokio::spawn(expect_all(
        tx,
        timeout,
        all_agents,
        all_hashes,
        agent_dht_map,
        agent_p2p_map,
    ));

    // Wait up to the timeout for all the agents to report success.
    wait_for_consistency(rx, wait_for_agents, timeout).await;
}

impl Reporter {
    /// Send a report back.
    async fn send_report(&self, report: SessionReport) {
        if self
            .0
            .send(SessionMessage {
                from: self.1.clone(),
                report,
            })
            .await
            .is_err()
        {
            tracing::error!("Failed to message for consistency session");
        }
    }
}

/// Wait for all agents to report success, timeout or failure.
/// Additionally print out debug tracing with some statistics.
#[cfg_attr(feature = "instrument", tracing::instrument(skip(rx, agents)))]
async fn wait_for_consistency(
    mut rx: tokio::sync::mpsc::Receiver<SessionMessage>,
    mut agents: HashSet<Arc<KitsuneAgent>>,
    timeout: Duration,
) {
    // When the session began.
    let start = tokio::time::Instant::now();
    // When the session is expected to end with a buffer to allow agents to timeout first.
    let deadline = tokio::time::Instant::now() + timeout + Duration::from_secs(1);

    // Stats.
    let total_agents = agents.len();
    let mut timeouts = 0;
    let mut errors = 0;
    let mut success = 0;
    let mut average_time = Duration::default();
    let mut amount_held = HashMap::new();
    let avg_held = |amount_held: &HashMap<_, _>| {
        let (p_agent_held, p_hash_held) = amount_held.values().fold(
            (0.0, 0.0),
            |(mut p_agent_held, mut p_hash_held), (ma, ea, mh, eh)| {
                p_agent_held += (*ea - *ma) as f32 / *ea as f32;
                p_hash_held += (*eh - *mh) as f32 / *eh as f32;
                (p_agent_held, p_hash_held)
            },
        );
        let avg_agent_held = p_agent_held / amount_held.len() as f32 * 100.0;
        let avg_hash_held = p_hash_held / amount_held.len() as f32 * 100.0;
        (avg_agent_held.round(), avg_hash_held.round())
    };

    // While we haven't timed out collect messages from all agents, print traces and update stats.
    while let Ok(Some(SessionMessage { from, report })) =
        tokio::time::timeout_at(deadline, rx.recv()).await
    {
        // Incase the future is always ready we need to check for timeout here as well.
        if tokio::time::Instant::now() > deadline {
            break;
        }
        match report {
            SessionReport::KeepAlive {
                missing_agents,
                missing_hashes,
                expected_agents,
                expected_hashes,
            } => {
                let e = amount_held
                    .entry(from.clone())
                    .or_insert_with(|| (0, 0, 0, 0));
                e.0 = missing_agents;
                e.1 = expected_agents;
                e.2 = missing_hashes;
                e.3 = expected_hashes;
                tracing::debug!(
                    "{:?} is still missing {} of {} agents and {} of {} hashes",
                    from,
                    missing_agents,
                    expected_agents,
                    missing_hashes,
                    expected_hashes,
                );
            }
            SessionReport::Complete { elapsed_ms } => {
                agents.remove(&from);
                tracing::debug!("{:?} has reached consistency in {}ms", from, elapsed_ms);
                average_time += Duration::from_millis(elapsed_ms as u64);
                success += 1;
                if agents.is_empty() {
                    break;
                }
            }
            SessionReport::Timeout {
                missing_agents,
                missing_hashes,
            } => {
                agents.remove(&from);
                tracing::debug!(
                    "{:?} has timed out before reaching consistency. \nMissing Agents: {:?}\nMissing Hashes: {:?}",
                    from,
                    missing_agents,
                    missing_hashes
                );
                timeouts += 1;
                if agents.is_empty() {
                    break;
                }
            }
            SessionReport::Error { error } => {
                agents.remove(&from);
                tracing::debug!(
                    "{:?} has failed the consistency session with error {}",
                    from,
                    error
                );
                errors += 1;
                if agents.is_empty() {
                    break;
                }
            }
        }
        let (avg_agent_held, avg_hash_held) = avg_held(&amount_held);
        tracing::debug!(
            "{} of {} agents have still not reached consistency in {:?}. The average consistency is currently reached in {:?}. {}% agents held, {}% hashes held.",
            agents.len(),
            total_agents,
            start.elapsed(),
            average_time.checked_div(success as u32).unwrap_or_default(),
            avg_agent_held,
            avg_hash_held,
        );
    }
    if tokio::time::Instant::now() > deadline {
        timeouts += agents.len();
        agents.clear();
        tracing::debug!(
            "Timed out with {} of {} agents have still not reaching consistency",
            agents.len(),
            total_agents
        );
    }
    let (avg_agent_held, avg_hash_held) = avg_held(&amount_held);
    tracing::debug!(
        "
REPORT:
Total elapsed: {:?}
Successful agents: {} in an average of {:?}
Timed out agents: {}
Failed out agents: {}
Total agents: {}
Average agents held: {}%.
Average hashes held: {}%.
        ",
        start.elapsed(),
        success,
        average_time.checked_div(success as u32).unwrap_or_default(),
        timeouts,
        errors,
        total_agents,
        avg_agent_held,
        avg_hash_held,
    );
}

/// Gather all the published op hashes and agents from a conductor.
async fn gather_conductor_data(
    p2p_agents_db: DbRead<DbKindP2pAgents>,
    agents: Vec<(DbRead<DbKindAuthored>, DbRead<DbKindDht>, Arc<KitsuneAgent>)>,
) -> (
    Vec<(Arc<KitsuneAgent>, DhtArc)>,
    Vec<(DhtLocation, KitsuneOpHash)>,
) {
    // Create the stores iterator with the environments to search.
    let stores = agents
        .iter()
        .cloned()
        .map(|(authored_db, _, agent)| Stores {
            agent,
            authored_db,
            p2p_agents_db: p2p_agents_db.clone(),
        });
    let all_published_data = gather_published_data(stores, CONCURRENCY)
        .await
        .expect("Failed to gather published data from conductor");
    let mut all_hashes = Vec::new();
    let mut all_agents = Vec::with_capacity(all_published_data.len());

    // Collect all the published hashes and agents.
    for PublishedData {
        agent,
        storage_arc,
        published_hashes,
    } in all_published_data
    {
        all_hashes.extend(published_hashes);
        all_agents.push((agent, storage_arc));
    }
    (all_agents, all_hashes)
}

/// Generate the consistency session and then check all agents concurrently.
async fn expect_all(
    tx: tokio::sync::mpsc::Sender<SessionMessage>,
    timeout: Duration,
    all_agents: Vec<(Arc<KitsuneAgent>, DhtArc)>,
    all_hashes: Vec<(DhtLocation, KitsuneOpHash)>,
    agent_dht_map: HashMap<Arc<KitsuneAgent>, DbRead<DbKindDht>>,
    agent_p2p_map: HashMap<Arc<KitsuneAgent>, DbRead<DbKindP2pAgents>>,
) {
    let iter = generate_session(&all_agents, &all_hashes, timeout, agent_dht_map);
    check_all(iter, tx, agent_p2p_map).await;
}

/// Generate the consistency sessions for each agent along with their environments.
/// This is where we check which agents should be holding which hashes and agents.
fn generate_session<'iter>(
    all_agents: &'iter Vec<(Arc<KitsuneAgent>, DhtArc)>,
    all_hashes: &'iter Vec<(DhtLocation, KitsuneOpHash)>,
    timeout: Duration,
    agent_dht_map: HashMap<Arc<KitsuneAgent>, DbRead<DbKindDht>>,
) -> impl Iterator<Item = (Arc<KitsuneAgent>, ConsistencySession, DbRead<DbKindDht>)> + 'iter {
    all_agents
        .iter()
        .map(move |(agent, arc)| {
            let mut published_hashes = Vec::new();
            let mut expected_agents = Vec::new();
            for (basis_loc, hash) in all_hashes.iter() {
                if arc.contains(*basis_loc) {
                    published_hashes.push(hash.clone());
                }
            }
            for (agent, _) in all_agents.iter() {
                if arc.contains(kitsune_p2p::KitsuneBinType::get_loc(&(**agent))) {
                    expected_agents.push(agent.clone());
                }
            }
            (
                agent.clone(),
                ExpectedData {
                    expected_agents,
                    expected_hashes: published_hashes,
                },
            )
        })
        .map(move |(agent, expected_data)| {
            (
                agent,
                ConsistencySession {
                    keep_alive: Some(Duration::from_secs(1)),
                    frequency: Duration::from_millis(100),
                    timeout,
                    expected_data,
                },
            )
        })
        .filter_map(move |(agent, expected_session)| {
            agent_dht_map
                .get(&agent)
                .cloned()
                .map(|db| (agent, expected_session, db))
        })
}

/// Concurrently check all agents for consistency.
/// Report back the results on the channel.
/// Checks will report timeouts and failures.
async fn check_all(
    iter: impl Iterator<Item = (Arc<KitsuneAgent>, ConsistencySession, DbRead<DbKindDht>)>,
    tx: tokio::sync::mpsc::Sender<SessionMessage>,
    agent_p2p_map: HashMap<Arc<KitsuneAgent>, DbRead<DbKindP2pAgents>>,
) {
    futures::stream::iter(iter)
        .for_each_concurrent(CONCURRENCY, |(agent, expected_session, dht_db)| {
            let tx = tx.clone();
            let p2p_agents_db = agent_p2p_map
                .get(&agent)
                .cloned()
                .expect("Must contain all p2p envs, this is a bug.");
            let reporter = Reporter(tx, agent);
            check_expected_data(reporter, expected_session, dht_db, p2p_agents_db)
        })
        .await;
}

/// Check the expected data against for a single agent.
async fn check_expected_data(
    reporter: Reporter,
    session: ConsistencySession,
    dht_db: DbRead<DbKindDht>,
    p2p_agents_db: DbRead<DbKindP2pAgents>,
) {
    if let Err(e) =
        check_expected_data_inner(reporter.clone(), session, dht_db, p2p_agents_db).await
    {
        reporter
            .send_report(SessionReport::Error {
                error: e.to_string(),
            })
            .await;
    }
}

/// The check expected data inner loop.
/// This runs for each agent until success, failure or timeout.
/// All outcomes are reported back on the channel.
async fn check_expected_data_inner(
    reporter: Reporter,
    session: ConsistencySession,
    dht_db: DbRead<DbKindDht>,
    p2p_agents_db: DbRead<DbKindP2pAgents>,
) -> DatabaseResult<()> {
    // Unpack the session.
    let ConsistencySession {
        keep_alive,
        frequency,
        timeout,
        expected_data:
            ExpectedData {
                expected_agents,
                mut expected_hashes,
            },
    } = session;

    // When we started.
    let start = tokio::time::Instant::now();
    // When we should finish.
    let deadline = tokio::time::Instant::now() + timeout;
    // The last time we sent a keep alive.
    let mut last_keep_alive = start;
    // How frequently we should poll the database.
    let mut frequency = tokio::time::interval(frequency);
    frequency.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Delay);

    // The agents and hashes we are still missing.
    let mut missing_agents = Vec::with_capacity(expected_agents.len());
    let mut missing_hashes = Vec::with_capacity(expected_hashes.len());

    // If we have not timed out then check at the set frequency.
    while tokio::time::timeout_at(deadline, frequency.tick())
        .await
        .is_ok()
    {
        // If the frequency interval is always ready the timeout
        // won't check so we also need to check for timeout here.
        if tokio::time::Instant::now() > deadline {
            break;
        }

        // Check the agents.
        missing_agents = check_agents(&p2p_agents_db, &expected_agents)
            .await?
            .collect();

        // Check the hashes.
        check_hashes(&dht_db, &mut expected_hashes, &mut missing_hashes).await?;

        // If both are now empty we report success.
        if missing_agents.is_empty() && missing_hashes.is_empty() {
            reporter
                .send_report(SessionReport::Complete {
                    elapsed_ms: start.elapsed().as_millis() as u32,
                })
                .await;
            return Ok(());
        }

        // If it's time to send a keep alive then do so now.
        if keep_alive.map_or(false, |k| last_keep_alive.elapsed() > k) {
            reporter
                .send_report(SessionReport::KeepAlive {
                    missing_agents: missing_agents.len() as u32,
                    expected_agents: expected_agents.len() as u32,
                    missing_hashes: missing_hashes.len() as u32,
                    expected_hashes: expected_hashes.len() as u32,
                })
                .await;

            // Update the last keep alive time.
            last_keep_alive = tokio::time::Instant::now();
        }
    }

    // We have not succeeded by now so we have timed out.
    reporter
        .send_report(SessionReport::Timeout {
            missing_agents: missing_agents.into_iter().cloned().collect(),
            missing_hashes,
        })
        .await;
    Ok(())
}

/// Check the agent is holding the expected agents in their peer store.
// Seems these lifetimes are actually needed.
#[allow(clippy::needless_lifetimes)]
async fn check_agents<'iter>(
    p2p_agents_db: &DbRead<DbKindP2pAgents>,
    expected_agents: &'iter [Arc<KitsuneAgent>],
) -> DatabaseResult<impl Iterator<Item = &'iter Arc<KitsuneAgent>> + 'iter> {
    // Poll the peer database for the currently held agents.
    let agents_held: HashSet<_> = p2p_agents_db
        .p2p_list_agents()
        .await?
        .into_iter()
        .map(|a| a.agent.clone())
        .collect();

    // Filter out the currently held agents from the expected agents to return any missing.
    Ok(expected_agents
        .iter()
        .filter(move |a| !agents_held.contains(&(**a))))
}

/// Check the op hashes we are meant to be holding.
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
async fn check_hashes(
    dht_db: &DbRead<DbKindDht>,
    expected_hashes: &mut Vec<KitsuneOpHash>,
    missing_hashes: &mut Vec<KitsuneOpHash>,
) -> DatabaseResult<()> {
    // Clear the missing hashes from the last check. This doesn't affect allocation.
    missing_hashes.clear();

    // We need to swap these hashes so we can move them into the async_reader
    // without reallocating.
    let expected = std::mem::replace(expected_hashes, Vec::with_capacity(0));
    let mut missing = std::mem::replace(missing_hashes, Vec::with_capacity(0));

    // Poll the vault database for each expected hashes existence.
    let mut r = dht_db
                .read_async(move |txn| {
                    for hash in &expected {
                        // TODO: This might be too slow, could instead save the holochain hash versions.
                        let h_hash: DhtOpHash = DhtOpHashExt::from_kitsune_raw(hash.clone());
                        let integrated: bool = txn.query_row(
                            "
                            SELECT EXISTS(
                                SELECT 1 FROM DhtOp WHERE hash = :hash AND when_integrated IS NOT NULL
                            )
                            ",
                            named_params! {
                                ":hash": h_hash,
                            },
                            |row| row.get(0),
                        )?;
                        if !integrated {
                            missing.push(hash.clone());
                        }
                    }
                    DatabaseResult::Ok((expected, missing))
                })
                .await?;
    // Put the data back.
    std::mem::swap(&mut r.0, expected_hashes);
    std::mem::swap(&mut r.1, missing_hashes);
    Ok(())
}

/// Concurrently Gather all published op hashes and agent's storage arcs.
async fn gather_published_data(
    iter: impl Iterator<Item = Stores>,
    concurrency: usize,
) -> StateQueryResult<Vec<PublishedData>> {
    use futures::stream::TryStreamExt;
    let iter = iter.map(|stores| async move {
        let published_hashes = request_published_ops(&stores.authored_db, None)
            .await?
            .into_iter()
            .map(|(l, h, _)| (l, h))
            .collect();
        let storage_arc = request_arc(&stores.p2p_agents_db, (*stores.agent).clone()).await?;
        Ok(storage_arc.map(|storage_arc| {
            // The line below was added when migrating to rust edition 2021, per
            // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
            let _ = &stores;
            PublishedData {
                agent: stores.agent,
                storage_arc,
                published_hashes,
            }
        }))
    });
    futures::stream::iter(iter)
        .buffer_unordered(concurrency)
        .try_filter_map(futures::future::ok)
        .try_collect()
        .await
}

/// Request the published hashes for the given agent.
pub async fn request_published_ops<AuthorDb>(
    db: &AuthorDb,
    author: Option<AgentPubKey>,
) -> StateQueryResult<Vec<(DhtLocation, KitsuneOpHash, DhtOp)>>
where
    AuthorDb: ReadAccess<DbKindAuthored>,
{
    db.read_async(|txn| {
        // Collect all ops except StoreEntry's that are private.
        let sql_common = "
        SELECT
        DhtOp.hash as dht_op_hash,
        DhtOp.storage_center_loc as loc,
        DhtOp.type as dht_type,
        Action.blob as action_blob,
        Action.author as author,
        Entry.blob as entry_blob
        FROM DhtOp
        JOIN
        Action ON DhtOp.action_hash = Action.hash
        LEFT JOIN
        Entry ON Action.entry_hash = Entry.hash
        WHERE
        (DhtOp.type != :store_entry OR Action.private_entry = 0)
        ";

        let r = if let Some(author) = author {
            txn.prepare(&format!(
                "
                        {}
                        AND
                        Action.author = :author
                    ",
                sql_common
            ))?
            .query_and_then(
                named_params! {
                    ":store_entry": ChainOpType::StoreEntry,
                    ":author": author,
                },
                |row| {
                    let h: DhtOpHash = row.get("dht_op_hash")?;
                    let loc: u32 = row.get("loc")?;
                    let op = holochain_state::query::map_sql_dht_op(false, "dht_type", row)?;

                    Ok((loc.into(), h.into_kitsune_raw(), op))
                },
            )?
            .collect::<StateQueryResult<_>>()?
        } else {
            txn.prepare(sql_common)?
                .query_and_then(
                    named_params! {
                        ":store_entry": ChainOpType::StoreEntry,
                    },
                    |row| {
                        let h: DhtOpHash = row.get("dht_op_hash")?;
                        let loc: u32 = row.get("loc")?;
                        let op = holochain_state::query::map_sql_dht_op(false, "dht_type", row)?;
                        StateQueryResult::Ok((loc.into(), h.into_kitsune_raw(), op))
                    },
                )?
                .collect::<StateQueryResult<_>>()?
        };
        StateQueryResult::Ok(r)
    })
    .await
}

/// Request the storage arc for the given agent.
async fn request_arc(
    db: &DbRead<DbKindP2pAgents>,
    agent: KitsuneAgent,
) -> StateQueryResult<Option<DhtArc>> {
    Ok(db
        .p2p_get_agent(&agent)
        .await?
        .map(|info| info.storage_arc()))
}



================================================
File: crates/holochain/src/test_utils/hc_stress_test.rs
================================================
//! automated behavioral testing of hc-stress-test zomes

use crate::sweettest::*;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::*;
use std::sync::{atomic, Arc, Mutex};

const MAX: std::time::Duration = std::time::Duration::from_secs(60 * 60 * 24 * 365 * 10);

/// Define the lifetime behavior of a test node.
#[derive(Debug, Clone, Copy)]
pub enum BehaviorLifetime {
    /// The node will continue to exist.
    Forever,

    /// The node will shutdown and be dropped after the given duration.
    Shutdown {
        /// The minimum time to wait before shutdown.
        wait_min: std::time::Duration,

        /// The maximum time to wait before shutdown.
        wait_max: std::time::Duration,
    },
}

const SHUTDOWN_30_S: BehaviorLifetime = BehaviorLifetime::Shutdown {
    wait_min: std::time::Duration::from_secs(25),
    wait_max: std::time::Duration::from_secs(35),
};

const SHUTDOWN_3_M: BehaviorLifetime = BehaviorLifetime::Shutdown {
    wait_min: std::time::Duration::from_secs(160),
    wait_max: std::time::Duration::from_secs(200),
};

/// Define the publish behavior of a test node.
#[derive(Debug, Clone, Copy)]
pub enum BehaviorPublish {
    /// The node will never publish authored content.
    None,

    /// The node will publish authored content.
    Publish {
        /// The minimum size in bytes of content to be published.
        byte_count_min: usize,

        /// The maximum size in bytes of content to be published.
        byte_count_max: usize,

        /// The number of times this node will publish (None for infinite).
        publish_count: Option<usize>,

        /// The minimum time to wait between publishes.
        wait_min: std::time::Duration,

        /// The maximum time to wait between publishes.
        wait_max: std::time::Duration,
    },
}

const PUBLISH_LARGE_5_M: BehaviorPublish = BehaviorPublish::Publish {
    byte_count_min: 1024 * 1024,
    byte_count_max: 1024 * 1024 * 3,
    publish_count: None,
    wait_min: std::time::Duration::from_secs(30 * 9),
    wait_max: std::time::Duration::from_secs(30 * 11),
};

const PUBLISH_LARGE_SINGLE: BehaviorPublish = BehaviorPublish::Publish {
    byte_count_min: 1024 * 1024,
    byte_count_max: 1024 * 1024 * 3,
    publish_count: Some(1),
    wait_min: MAX,
    wait_max: MAX,
};

const PUBLISH_SMALL_1_M: BehaviorPublish = BehaviorPublish::Publish {
    byte_count_min: 32,
    byte_count_max: 1024,
    publish_count: None,
    wait_min: std::time::Duration::from_secs(50),
    wait_max: std::time::Duration::from_secs(70),
};

const PUBLISH_SMALL_SINGLE: BehaviorPublish = BehaviorPublish::Publish {
    byte_count_min: 32,
    byte_count_max: 1024,
    publish_count: Some(1),
    wait_min: MAX,
    wait_max: MAX,
};

/// Define the query behavior of a test node.
#[derive(Debug, Clone, Copy)]
pub enum BehaviorQuery {
    /// The node will never query data from other nodes.
    None,

    /// The node will fetch hashes only, but will never request content.
    Shallow {
        /// The minimum time to wait between queries.
        wait_min: std::time::Duration,

        /// The maximum time to wait between queries.
        wait_max: std::time::Duration,
    },

    /// The node will fetch hashes, then entry data.
    Full {
        /// The minimum time to wait between queries.
        wait_min: std::time::Duration,

        /// The maximum time to wait between queries.
        wait_max: std::time::Duration,
    },
}

const QUERY_SHALLOW_15_S: BehaviorQuery = BehaviorQuery::Shallow {
    wait_min: std::time::Duration::from_secs(13),
    wait_max: std::time::Duration::from_secs(17),
};

const QUERY_FULL_15_S: BehaviorQuery = BehaviorQuery::Full {
    wait_min: std::time::Duration::from_secs(13),
    wait_max: std::time::Duration::from_secs(17),
};

/// Report what happened with a behavior.
pub trait Report: 'static + Send {
    /// A node has been spawned into the runner.
    fn spawn(&mut self, node_id: usize);

    /// A node has shutdown and was removed from the runner.
    fn shutdown(&mut self, node_id: usize, runtime: std::time::Duration);

    /// Result of a publish attempt.
    fn publish(
        &mut self,
        node_id: usize,
        runtime: std::time::Duration,
        byte_count: usize,
        hash: ActionHash,
    );

    /// Result of a shallow fetch attempt.
    fn fetch_shallow(
        &mut self,
        node_id: usize,
        runtime: std::time::Duration,
        hash_list: Vec<ActionHash>,
    );

    /// Result of a full fetch attempt.
    fn fetch_full(&mut self, node_id: usize, runtime: std::time::Duration, hash: ActionHash);
}

/// Run an hc-stress-test behavior test.
pub struct HcStressTestRunner<R: Report>(Arc<Mutex<R>>);

impl<R: Report> HcStressTestRunner<R> {
    /// Construct a new runner instance.
    pub fn new(r: Arc<Mutex<R>>) -> Self {
        Self(r)
    }

    /// Add a node to the runner with given behavior.
    /// Returns the node_id that was added.
    pub fn add_node(
        &self,
        mut node: HcStressTest,
        lifetime: BehaviorLifetime,
        publish: Vec<(u8, BehaviorPublish)>,
        query: Vec<(u8, BehaviorQuery)>,
    ) -> usize {
        use rand::Rng;

        let report = self.0.clone();
        let init_time = std::time::Instant::now();

        static NODE_ID: atomic::AtomicUsize = atomic::AtomicUsize::new(1);
        let node_id = NODE_ID.fetch_add(1, atomic::Ordering::Relaxed);

        tokio::task::spawn(async move {
            struct OnDrop<R: Report>(Arc<Mutex<R>>, usize, std::time::Instant);
            impl<R: Report> Drop for OnDrop<R> {
                fn drop(&mut self) {
                    self.0.lock().unwrap().shutdown(self.1, self.2.elapsed());
                }
            }

            let _on_drop = OnDrop(report.clone(), node_id, init_time);

            report.lock().unwrap().spawn(node_id);

            let mut now = std::time::Instant::now();

            let shutdown_at = match lifetime {
                BehaviorLifetime::Forever => now.checked_add(MAX).unwrap(),
                BehaviorLifetime::Shutdown { wait_min, wait_max } => now
                    .checked_add(rand::thread_rng().gen_range(wait_min..=wait_max))
                    .unwrap(),
            };

            struct PubData {
                pub next_at: std::time::Instant,
                pub cell: u8,
                pub count: usize,
                pub bc_min: usize,
                pub bc_max: usize,
                pub w_min: std::time::Duration,
                pub w_max: std::time::Duration,
            }

            let mut next_publish_at = Vec::new();

            for p in &publish {
                next_publish_at.push(match p {
                    (cell, BehaviorPublish::None) => PubData {
                        next_at: now.checked_add(MAX).unwrap(),
                        cell: *cell,
                        count: 0,
                        bc_min: 0,
                        bc_max: 0,
                        w_min: std::time::Duration::MAX,
                        w_max: std::time::Duration::MAX,
                    },
                    (
                        cell,
                        BehaviorPublish::Publish {
                            byte_count_min,
                            byte_count_max,
                            publish_count,
                            wait_min,
                            wait_max,
                        },
                    ) => {
                        let count = match publish_count {
                            None => usize::MAX,
                            Some(c) => *c,
                        };
                        PubData {
                            next_at: now,
                            cell: *cell,
                            count,
                            bc_min: *byte_count_min,
                            bc_max: *byte_count_max,
                            w_min: *wait_min,
                            w_max: *wait_max,
                        }
                    }
                });
            }

            struct QueryData {
                pub next_at: std::time::Instant,
                pub cell: u8,
                pub is_full: bool,
                pub w_min: std::time::Duration,
                pub w_max: std::time::Duration,
            }

            let mut next_query_at = Vec::new();

            for q in &query {
                next_query_at.push(match q {
                    (cell, BehaviorQuery::None) => QueryData {
                        next_at: now.checked_add(MAX).unwrap(),
                        cell: *cell,
                        is_full: false,
                        w_min: std::time::Duration::MAX,
                        w_max: std::time::Duration::MAX,
                    },
                    (cell, BehaviorQuery::Shallow { wait_min, wait_max }) => QueryData {
                        next_at: now,
                        cell: *cell,
                        is_full: false,
                        w_min: *wait_min,
                        w_max: *wait_max,
                    },
                    (cell, BehaviorQuery::Full { wait_min, wait_max }) => QueryData {
                        next_at: now,
                        cell: *cell,
                        is_full: true,
                        w_min: *wait_min,
                        w_max: *wait_max,
                    },
                });
            }

            loop {
                now = std::time::Instant::now();

                if now >= shutdown_at {
                    break;
                }

                let mut next_check_at = shutdown_at;

                for p in &mut next_publish_at {
                    now = std::time::Instant::now();

                    let should_publish = if now >= p.next_at {
                        p.next_at = now
                            .checked_add(rand::thread_rng().gen_range(p.w_min..=p.w_max))
                            .unwrap();
                        if p.count > 0 {
                            p.count -= 1;
                            true
                        } else {
                            p.next_at = now.checked_add(MAX).unwrap();
                            false
                        }
                    } else {
                        false
                    };

                    if p.next_at < next_check_at {
                        next_check_at = p.next_at;
                    }

                    if should_publish {
                        let bytes = {
                            let mut rng = rand::thread_rng();
                            let count = rng.gen_range(p.bc_min..=p.bc_max);
                            rand_utf8::rand_utf8(&mut rng, count)
                        };

                        let rec = node.create_file(p.cell, &bytes).await;
                        let hash = HcStressTest::record_to_action_hash(&rec);

                        report.lock().unwrap().publish(
                            node_id,
                            init_time.elapsed(),
                            bytes.len(),
                            hash,
                        );
                    }
                }

                for q in &mut next_query_at {
                    now = std::time::Instant::now();

                    if now >= q.next_at {
                        q.next_at = now
                            .checked_add(rand::thread_rng().gen_range(q.w_min..=q.w_max))
                            .unwrap();

                        let shallow_list = node.get_all_images(q.cell).await;

                        report.lock().unwrap().fetch_shallow(
                            node_id,
                            init_time.elapsed(),
                            shallow_list.clone(),
                        );

                        if q.is_full {
                            for hash in shallow_list {
                                if let Some(rec) = node.get_file(q.cell, hash).await {
                                    let hash = HcStressTest::record_to_action_hash(&rec);
                                    report.lock().unwrap().fetch_full(
                                        node_id,
                                        init_time.elapsed(),
                                        hash,
                                    );
                                }
                            }
                        }
                    }

                    if q.next_at < next_check_at {
                        next_check_at = q.next_at;
                    }
                }

                now = std::time::Instant::now();

                let wait_dur = next_check_at.saturating_duration_since(now);

                tokio::time::sleep(wait_dur).await;
            }
        });

        node_id
    }
}

fn uid() -> i64 {
    use rand::Rng;
    rand::thread_rng().gen()
}

/// A conductor running the hc_stress_test app.
pub struct HcStressTest {
    conductor: Option<SweetConductor>,
    cells: Vec<SweetCell>,
}

impl Drop for HcStressTest {
    fn drop(&mut self) {
        if let Some(mut conductor) = self.conductor.take() {
            tokio::task::spawn(async move {
                // MAYBE: someday it'd be nice to know this conductor isn't
                //        phantom running in the background, but as it is
                //        we are ignoring the shutdown errors (which it
                //        totally generates). We mostly just care that it
                //        hasn't panicked any tokio task threads.
                let _ = conductor.try_shutdown().await;
            });
        }
    }
}

impl HcStressTest {
    /// Helper to provide the SweetDnaFile from compiled test wasms.
    pub async fn test_dna(network_seed: String) -> DnaFile {
        let (dna, _, _) = SweetDnaFile::from_zomes(
            network_seed,
            vec![TestIntegrityWasm::HcStressTestIntegrity],
            vec![TestCoordinatorWasm::HcStressTestCoordinator],
            vec![
                DnaWasm::from(TestIntegrityWasm::HcStressTestIntegrity),
                DnaWasm::from(TestCoordinatorWasm::HcStressTestCoordinator),
            ],
            SerializedBytes::default(),
        )
        .await;
        dna
    }

    /// Given a new/blank sweet conductor and the hc_stress_test dna
    /// (see [HcStressTest::test_dna]), install the dna, returning
    /// a conductor running the hc_stress_test app.
    pub async fn new(mut conductor: SweetConductor, dna_files: &[DnaFile]) -> Self {
        let app = conductor.setup_app("app", dna_files).await.unwrap();
        let cells = app.into_cells();

        Self {
            conductor: Some(conductor),
            cells,
        }
    }

    /// Extract the ActionHash from a Record.
    pub fn record_to_action_hash(record: &Record) -> ActionHash {
        record.signed_action.hashed.hash.clone()
    }

    /// Extract the file data from a Record.
    pub fn record_to_file_data(record: &Record) -> String {
        match record {
            Record {
                entry: RecordEntry::Present(Entry::App(AppEntryBytes(bytes))),
                ..
            } => {
                #[derive(Debug, serde::Deserialize)]
                struct F<'a> {
                    #[serde(with = "serde_bytes")]
                    data: &'a [u8],
                    #[allow(dead_code)]
                    uid: i64,
                }
                let f: F<'_> = decode(bytes.bytes()).unwrap();
                String::from_utf8_lossy(f.data).to_string()
            }
            _ => panic!("record does not contain file data"),
        }
    }

    /// Call the `create_file` zome function.
    pub async fn create_file(&mut self, cell: u8, data: &str) -> Record {
        #[derive(Debug, serde::Serialize)]
        struct F<'a> {
            #[serde(with = "serde_bytes")]
            data: &'a [u8],
            uid: i64,
        }
        self.conductor
            .as_ref()
            .unwrap()
            .call(
                &self.cells[cell as usize].zome(TestCoordinatorWasm::HcStressTestCoordinator),
                "create_file",
                F {
                    data: data.as_bytes(),
                    uid: uid(),
                },
            )
            .await
    }

    /// Call the `get_all_images` zome function.
    pub async fn get_all_images(&mut self, cell: u8) -> Vec<ActionHash> {
        self.conductor
            .as_ref()
            .unwrap()
            .call(
                &self.cells[cell as usize].zome(TestCoordinatorWasm::HcStressTestCoordinator),
                "get_all_images",
                (),
            )
            .await
    }

    /// Call the `get_file` zome function.
    pub async fn get_file(&mut self, cell: u8, hash: ActionHash) -> Option<Record> {
        self.conductor
            .as_ref()
            .unwrap()
            .call_fallible(
                &self.cells[cell as usize].zome(TestCoordinatorWasm::HcStressTestCoordinator),
                "get_file",
                hash,
            )
            .await
            .ok()
    }
}

mod local_behavior_1;
pub use local_behavior_1::*;

mod local_behavior_2;
pub use local_behavior_2::*;



================================================
File: crates/holochain/src/test_utils/host_fn_caller.rs
================================================
#![allow(missing_docs)]

use crate::conductor::api::CellConductorApi;
use crate::conductor::api::CellConductorApiT;
use crate::conductor::api::CellConductorReadHandle;
use crate::conductor::ConductorHandle;
use crate::core::ribosome::host_fn;
use crate::core::ribosome::real_ribosome::RealRibosome;
use crate::core::ribosome::CallContext;
use crate::core::ribosome::HostContext;
use crate::core::ribosome::InvocationAuth;
use crate::core::ribosome::RibosomeT;
use crate::core::ribosome::ZomeCallHostAccess;
use crate::core::ribosome::ZomeCallInvocation;
use crate::core::workflow::call_zome_function_authorized;
use hdk::prelude::*;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holo_hash::AnyDhtHash;
use holochain_conductor_services::DpkiImpl;
use holochain_keystore::MetaLairClient;
use holochain_p2p::actor::GetLinksOptions;
use holochain_p2p::actor::HolochainP2pRefToDna;
use holochain_p2p::{HolochainP2pDna, HolochainP2pDnaT};
use holochain_state::host_fn_workspace::SourceChainWorkspace;
use holochain_types::db_cache::DhtDbQueryCache;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasmPair;
use std::sync::Arc;
use tokio::sync::broadcast;
use unwrap_to::unwrap_to;

// Commit entry types //
// Useful for when you want to commit something
// that will match entry defs
pub const POST_ID: &str = "post";
pub const POST_INDEX: EntryDefIndex = EntryDefIndex(0);
pub const MSG_ID: &str = "msg";
pub const MSG_INDEX: EntryDefIndex = EntryDefIndex(1);
pub const VALID_ID: &str = "always_validates";
