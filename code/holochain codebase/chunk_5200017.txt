    fn from(w: EntryRateWeight) -> Self {
        Self {
            bucket_id: w.bucket_id,
            units: w.units,
        }
    }
}



================================================
File: crates/holochain_integrity_types/src/record.rs
================================================
//! Defines a Record, the basic unit of Holochain data.

use std::borrow::Borrow;

use crate::action::conversions::WrongActionError;
use crate::action::ActionHashed;
use crate::action::CreateLink;
use crate::action::DeleteLink;
use crate::entry_def::EntryVisibility;
use crate::signature::Signature;
use crate::Entry;
use crate::{Action, ActionHashedContainer, ActionSequenceAndHash};
use holo_hash::ActionHash;
use holo_hash::HasHash;
use holo_hash::HashableContent;
use holo_hash::HoloHashOf;
use holo_hash::HoloHashed;
use holo_hash::PrimitiveHashType;
use holochain_serialized_bytes::prelude::*;

/// a chain record containing the signed action along with the
/// entry if the action type has one.
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, SerializedBytes)]
pub struct Record<A = SignedActionHashed> {
    /// The signed action for this record
    pub signed_action: A,
    /// If there is an entry associated with this action it will be here.
    /// If not, there will be an enum variant explaining the reason.
    pub entry: RecordEntry<Entry>,
}

impl<A> AsRef<A> for Record<A> {
    fn as_ref(&self) -> &A {
        &self.signed_action
    }
}

impl ActionHashedContainer for Record {
    fn action(&self) -> &Action {
        self.action()
    }

    fn action_hash(&self) -> &ActionHash {
        self.action_address()
    }
}

impl ActionSequenceAndHash for Record {
    fn action_seq(&self) -> u32 {
        self.action().action_seq()
    }

    fn address(&self) -> &ActionHash {
        self.action_address()
    }
}

/// Represents the different ways the entry_address reference within an action
/// can be interpreted
#[derive(Clone, Debug, PartialEq, Eq, Hash, Serialize, Deserialize, SerializedBytes)]
pub enum RecordEntry<E: Borrow<Entry> = Entry> {
    /// The Action has an entry_address reference, and the Entry is accessible.
    Present(E),
    /// The Action has an entry_address reference, but we are in a public
    /// context and the entry is private.
    Hidden,
    /// The Action does not contain an entry_address reference, so there will
    /// never be an associated Entry.
    NA,
    /// The Action has an entry but was stored without it.
    /// This can happen when you receive gossip of just an action
    /// when the action type is a [`crate::EntryCreationAction`],
    /// in particular for certain DhtOps
    NotStored,
}

impl<E: Borrow<Entry>> From<E> for RecordEntry<E> {
    fn from(entry: E) -> Self {
        RecordEntry::Present(entry)
    }
}

impl<E: Borrow<Entry>> RecordEntry<E> {
    /// Constructor based on Action data
    pub fn new(vis: Option<&EntryVisibility>, maybe_entry: Option<E>) -> Self {
        match (maybe_entry, vis) {
            (Some(entry), Some(_)) => RecordEntry::Present(entry),
            (None, Some(EntryVisibility::Private)) => RecordEntry::Hidden,
            (None, None) => RecordEntry::NA,
            (Some(_), None) => {
                // TODO this is a problem case but it is reachable
                unreachable!("Entry is present for an action type which has no entry reference")
            }
            (None, Some(EntryVisibility::Public)) => RecordEntry::NotStored,
        }
    }

    /// Provides entry data by reference if it exists
    ///
    /// Collapses the enum down to the two possibilities of
    /// extant or nonextant Entry data
    pub fn as_option(&self) -> Option<&E> {
        if let RecordEntry::Present(ref entry) = self {
            Some(entry)
        } else {
            None
        }
    }

    /// Provides entry data as owned value if it exists.
    ///
    /// Collapses the enum down to the two possibilities of
    /// extant or nonextant Entry data
    pub fn into_option(self) -> Option<E> {
        if let RecordEntry::Present(entry) = self {
            Some(entry)
        } else {
            None
        }
    }

    /// Provides deserialized app entry if it exists
    ///
    /// same as as_option but handles deserialization
    /// anything other than RecordEntry::Present returns None
    /// a present entry that fails to deserialize cleanly is an error
    /// a present entry that deserializes cleanly is returned as the provided type A
    pub fn to_app_option<A: TryFrom<SerializedBytes, Error = SerializedBytesError>>(
        &self,
    ) -> Result<Option<A>, SerializedBytesError> {
        match self.as_option().map(|e| e.borrow()) {
            Some(Entry::App(eb)) => Ok(Some(A::try_from(SerializedBytes::from(eb.to_owned()))?)),
            _ => Ok(None),
        }
    }

    /// Use a reference to the Entry, if present
    pub fn as_ref<'a>(&'a self) -> RecordEntry<&'a E>
    where
        &'a E: Borrow<Entry>,
    {
        match self {
            RecordEntry::Present(ref e) => RecordEntry::Present(e),
            RecordEntry::Hidden => RecordEntry::Hidden,
            RecordEntry::NA => RecordEntry::NA,
            RecordEntry::NotStored => RecordEntry::NotStored,
        }
    }

    /// Provides CapGrantEntry if it exists
    ///
    /// same as as_option but handles cap grants
    /// anything other tha RecordEntry::Present for a Entry::CapGrant returns None
    pub fn to_grant_option(&self) -> Option<crate::entry::CapGrantEntry> {
        match self.as_option().map(|e| e.borrow()) {
            Some(Entry::CapGrant(cap_grant_entry)) => Some(cap_grant_entry.to_owned()),
            _ => None,
        }
    }

    /// If no entry is available, return Hidden, else return Present
    pub fn or_hidden(entry: Option<E>) -> Self {
        entry.map(Self::Present).unwrap_or(Self::Hidden)
    }

    /// If no entry is available, return NotApplicable, else return Present
    pub fn or_not_applicable(entry: Option<E>) -> Self {
        entry.map(Self::Present).unwrap_or(Self::NA)
    }

    /// If no entry is available, return NotStored, else return Present
    pub fn or_not_stored(entry: Option<E>) -> Self {
        entry.map(Self::Present).unwrap_or(Self::NotStored)
    }
}

/// Alias for record with ref entry
pub type RecordEntryRef<'a> = RecordEntry<&'a Entry>;

/// The hashed action and the signature that signed it
pub type SignedActionHashed = SignedHashed<Action>;

impl AsRef<SignedActionHashed> for SignedActionHashed {
    fn as_ref(&self) -> &SignedActionHashed {
        self
    }
}

#[derive(Clone, Debug, Eq, Serialize, Deserialize)]
/// Any content that has been hashed and signed.
pub struct SignedHashed<T>
where
    T: HashableContent,
{
    /// The hashed content.
    pub hashed: HoloHashed<T>,
    /// The signature of the content.
    pub signature: Signature,
}

impl Record {
    /// Raw record constructor.  Used only when we know that the values are valid.
    /// NOTE: this will NOT hide private entry data if present!
    pub fn new(signed_action: SignedActionHashed, maybe_entry: Option<Entry>) -> Self {
        let maybe_visibility = signed_action.action().entry_visibility();
        let entry = RecordEntry::new(maybe_visibility, maybe_entry);
        Self {
            signed_action,
            entry,
        }
    }

    /// Access the signature from this record's signed action
    pub fn signature(&self) -> &Signature {
        self.signed_action.signature()
    }

    /// Mutable reference to the Action content.
    /// This is useless and dangerous in production usage.
    /// Guaranteed to make hashes and signatures mismatch whatever the Action is mutated to (at least).
    /// This may be useful for tests that rely heavily on mocked and fixturated data.
    #[cfg(feature = "test_utils")]
    pub fn as_action_mut(&mut self) -> &mut Action {
        &mut self.signed_action.hashed.content
    }

    /// If the Record contains private entry data, set the RecordEntry
    /// to Hidden so that it cannot be leaked. If the entry was hidden,
    /// return it separately.
    pub fn privatized(self) -> (Self, Option<Entry>) {
        let (entry, hidden) = if let Some(EntryVisibility::Private) = self
            .signed_action
            .action()
            .entry_data()
            .map(|(_, entry_type)| entry_type.visibility())
        {
            match self.entry {
                RecordEntry::Present(entry) => (RecordEntry::Hidden, Some(entry)),
                other => (other, None),
            }
        } else {
            (self.entry, None)
        };
        let privatized = Self {
            signed_action: self.signed_action,
            entry,
        };
        (privatized, hidden)
    }

    /// Access the action address from this record's signed action
    pub fn action_address(&self) -> &ActionHash {
        self.signed_action.action_address()
    }

    /// Access the Action from this record's signed action
    pub fn action(&self) -> &Action {
        self.signed_action.action()
    }

    /// Access the ActionHashed from this record's signed action portion
    pub fn action_hashed(&self) -> &ActionHashed {
        &self.signed_action.hashed
    }

    /// Access the Entry portion of this record as a RecordEntry,
    /// which includes the context around the presence or absence of the entry.
    pub fn entry(&self) -> &RecordEntry {
        &self.entry
    }
}

impl<A> Record<A> {
    /// Mutable reference to the RecordEntry.
    /// This is useless and dangerous in production usage.
    /// Guaranteed to make hashes and signatures mismatch whatever the RecordEntry is mutated to (at least).
    /// This may be useful for tests that rely heavily on mocked and fixturated data.
    #[cfg(feature = "test_utils")]
    pub fn as_entry_mut(&mut self) -> &mut RecordEntry {
        &mut self.entry
    }

    /// Break this record into its components
    pub fn into_inner(self) -> (A, RecordEntry) {
        (self.signed_action, self.entry)
    }

    /// The inner signed-action
    pub fn signed_action(&self) -> &A {
        &self.signed_action
    }
}

#[cfg(feature = "hashing")]
impl<T> SignedHashed<T>
where
    T: HashableContent,
    <T as holo_hash::HashableContent>::HashType: holo_hash::hash_type::HashTypeSync,
{
    /// Create a new signed and hashed content by hashing the content, but without checking
    /// the signature.
    pub fn new_unchecked(content: T, signature: Signature) -> Self {
        let hashed = HoloHashed::from_content_sync(content);
        Self { hashed, signature }
    }
}

impl<T> std::hash::Hash for SignedHashed<T>
where
    T: HashableContent,
{
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.signature.hash(state);
        self.as_hash().hash(state);
    }
}

impl<T> std::cmp::PartialEq for SignedHashed<T>
where
    T: HashableContent,
{
    fn eq(&self, other: &Self) -> bool {
        self.hashed == other.hashed && self.signature == other.signature
    }
}

impl<T> SignedHashed<T>
where
    T: HashableContent,
{
    /// Destructure into a [`HoloHashed`] and [`Signature`].
    pub fn into_inner(self) -> (HoloHashed<T>, Signature) {
        (self.hashed, self.signature)
    }

    /// Access the already-calculated hash stored in this wrapper type.
    pub fn as_hash(&self) -> &HoloHashOf<T> {
        &self.hashed.hash
    }

    /// Create with an existing signature.
    pub fn with_presigned(hashed: HoloHashed<T>, signature: Signature) -> Self {
        Self { hashed, signature }
    }

    /// Access the signature portion.
    pub fn signature(&self) -> &Signature {
        &self.signature
    }
}

impl SignedActionHashed {
    /// Access the Action Hash.
    pub fn action_address(&self) -> &ActionHash {
        &self.hashed.hash
    }

    /// Access the Action portion.
    pub fn action(&self) -> &Action {
        &self.hashed.content
    }

    /// Create a new SignedActionHashed from a type that implements into `Action` and
    /// has the same hash bytes.
    /// The caller must make sure the hash does not change.
    pub fn raw_from_same_hash<T>(other: SignedHashed<T>) -> Self
    where
        T: Into<Action>,
        T: HashableContent<HashType = holo_hash::hash_type::Action>,
    {
        let SignedHashed {
            hashed: HoloHashed { content, hash },
            signature,
        } = other;
        let action = content.into();
        let hashed = ActionHashed::with_pre_hashed(action, hash);
        Self { hashed, signature }
    }
}

impl<C: HashableContent<HashType = T>, T: PrimitiveHashType> HashableContent for SignedHashed<C> {
    type HashType = C::HashType;

    fn hash_type(&self) -> Self::HashType {
        T::new()
    }

    fn hashable_content(&self) -> holo_hash::HashableContentBytes {
        holo_hash::HashableContentBytes::Prehashed39(self.hashed.as_hash().get_raw_39().to_vec())
    }
}

impl<C: HashableContent> HasHash for SignedHashed<C> {
    type HashType = C::HashType;

    fn as_hash(&self) -> &HoloHashOf<C> {
        self.hashed.as_hash()
    }

    fn into_hash(self) -> HoloHashOf<C> {
        self.hashed.into_hash()
    }
}

impl<T> From<SignedHashed<T>> for HoloHashed<T>
where
    T: HashableContent,
{
    fn from(sh: SignedHashed<T>) -> HoloHashed<T> {
        sh.hashed
    }
}

impl From<ActionHashed> for Action {
    fn from(action_hashed: ActionHashed) -> Action {
        action_hashed.into_content()
    }
}

impl From<SignedActionHashed> for Action {
    fn from(signed_action_hashed: SignedActionHashed) -> Action {
        ActionHashed::from(signed_action_hashed).into()
    }
}

impl From<Record> for Option<Entry> {
    fn from(e: Record) -> Self {
        e.entry.into_option()
    }
}

impl TryFrom<Record> for CreateLink {
    type Error = WrongActionError;
    fn try_from(value: Record) -> Result<Self, Self::Error> {
        value
            .into_inner()
            .0
            .into_inner()
            .0
            .into_content()
            .try_into()
    }
}

impl TryFrom<Record> for DeleteLink {
    type Error = WrongActionError;
    fn try_from(value: Record) -> Result<Self, Self::Error> {
        value
            .into_inner()
            .0
            .into_inner()
            .0
            .into_content()
            .try_into()
    }
}



================================================
File: crates/holochain_integrity_types/src/signature.rs
================================================
//! Signature for authenticity of data
use holo_hash::AgentPubKey;
use holochain_secure_primitive::secure_primitive;
use holochain_serialized_bytes::prelude::*;

/// Ed25519 signatures are always the same length, 64 bytes.
pub const SIGNATURE_BYTES: usize = 64;

/// The raw bytes of a signature.
#[derive(Clone, PartialOrd, Hash, Ord)]
// The equality is not different, it's just constant time, so we can derive a hash.
// For an actually secure thing we wouldn't want to just assume a safe default hashing
// But that is not what clippy is complaining about here.
#[allow(clippy::derived_hash_with_manual_eq)]
pub struct Signature(pub [u8; SIGNATURE_BYTES]);

// This is more for convenience/convention that being worried
// about things like constant time equality.
// Signature verification should always defer to the host.
// What's nice about this is that we can easily handle fixed size signatures.
secure_primitive!(Signature, SIGNATURE_BYTES);

/// The output of ephemeral signing.
/// The private key for this public key has been discarded by this point.
/// The signatures match the public key provided but cannot be reproduced
/// or forged because the private key no longer exists.
/// The signatures match the input items positionally in the vector,
/// it is up to the caller to reconstruct/align/zip them back together.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct EphemeralSignatures {
    /// The public key associated with the now-discarded private key used to sign.
    pub key: holo_hash::AgentPubKey,
    /// The signatures for the input data to be matched in order, pairwise.
    pub signatures: Vec<Signature>,
}

/// Mirror struct for Sign that includes a signature to verify against a key and data.
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, SerializedBytes)]
pub struct VerifySignature {
    /// The public key associated with the private key that should be used to
    /// verify the signature.
    pub key: holo_hash::AgentPubKey,

    /// The signature being verified.
    pub signature: Signature,

    /// The signed data
    #[serde(with = "serde_bytes")]
    pub data: Vec<u8>,
}

impl AsRef<Signature> for VerifySignature {
    fn as_ref(&self) -> &Signature {
        &self.signature
    }
}

impl AsRef<holo_hash::AgentPubKey> for VerifySignature {
    fn as_ref(&self) -> &AgentPubKey {
        &self.key
    }
}

impl VerifySignature {
    /// Alias for as_ref for data.
    pub fn as_data_ref(&self) -> &[u8] {
        self.data.as_ref()
    }

    /// Alias for as_ref for signature.
    pub fn as_signature_ref(&self) -> &Signature {
        self.as_ref()
    }

    /// Alias for as_ref for agent key.
    pub fn as_key_ref(&self) -> &holo_hash::AgentPubKey {
        self.as_ref()
    }

    /// construct a new VerifySignature struct.
    pub fn new<D>(
        key: holo_hash::AgentPubKey,
        signature: Signature,
        data: D,
    ) -> Result<Self, SerializedBytesError>
    where
        D: serde::Serialize + std::fmt::Debug,
    {
        Ok(Self {
            key,
            signature,
            data: holochain_serialized_bytes::encode(&data)?,
        })
    }

    /// construct a new Sign struct from raw bytes.
    pub fn new_raw(key: holo_hash::AgentPubKey, signature: Signature, data: Vec<u8>) -> Self {
        Self {
            key,
            signature,
            data,
        }
    }
}

#[test]
fn signature_roundtrip() {
    let bytes = Signature::from([1u8; 64]);
    let json = serde_json::to_string(&bytes).unwrap();
    let _: Signature = serde_json::from_str(&json).unwrap();
}



================================================
File: crates/holochain_integrity_types/src/trace.rs
================================================
//! Types related to the `debug` host function

use holochain_serialized_bytes::prelude::*;

/// Maps directly to the tracing Levels but here to define the interface.
/// See <https://docs.rs/tracing-core/0.1.17/tracing_core/struct.Level.html>
#[derive(PartialEq, Eq, serde::Serialize, serde::Deserialize, Debug, Clone)]
#[allow(clippy::upper_case_acronyms)]
pub enum Level {
    /// Error.
    ERROR,
    /// Warning.
    WARN,
    /// Info.
    INFO,
    /// Debug.
    DEBUG,
    /// Trace.
    TRACE,
}

#[cfg(feature = "tracing")]
impl From<&tracing::Level> for Level {
    fn from(level: &tracing::Level) -> Self {
        match *level {
            tracing::Level::ERROR => Self::ERROR,
            tracing::Level::WARN => Self::WARN,
            tracing::Level::INFO => Self::INFO,
            tracing::Level::DEBUG => Self::DEBUG,
            tracing::Level::TRACE => Self::TRACE,
        }
    }
}

/// Representation of message to be logged via the `debug` host function
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub struct TraceMsg {
    /// A formatted string to be forwarded to `tracing` on the host side.
    ///
    /// The host will provide:
    /// - Timestamps
    /// - ANSI coloured levels
    ///
    /// The guest should provide:
    /// - Useful message
    /// - Line numbers etc.
    pub msg: String,
    /// Severity level for the message.
    pub level: Level,
}



================================================
File: crates/holochain_integrity_types/src/validate.rs
================================================
use holo_hash::AgentPubKey;
use holo_hash::AnyDhtHash;
use holochain_serialized_bytes::prelude::*;

use crate::chain::ChainFilter;

#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, SerializedBytes)]
pub enum ValidateCallbackResult {
    Valid,
    Invalid(String),
    /// Subconscious needs to map this to either pending or abandoned based on context that the
    /// wasm can't possibly have.
    UnresolvedDependencies(UnresolvedDependencies),
}

#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
/// Unresolved dependencies that are either a set of hashes
/// or an agent activity query.
pub enum UnresolvedDependencies {
    Hashes(Vec<AnyDhtHash>),
    AgentActivity(AgentPubKey, ChainFilter),
}

/// The level of validation package required by
/// an entry.
#[derive(
    Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, serde::Serialize, serde::Deserialize,
)]
pub enum RequiredValidationType {
    /// Just the record (default)
    Record,
    /// All chain items of the same entry type
    SubChain,
    /// The entire chain
    Full,
}

impl Default for RequiredValidationType {
    fn default() -> Self {
        Self::Record
    }
}



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305.rs
================================================
use crate::prelude::*;
pub mod data;
pub mod encrypted_data;
pub mod key_ref;
pub mod nonce;
pub mod x25519;
use holochain_serialized_bytes::prelude::*;

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct XSalsa20Poly1305Decrypt {
    pub key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
    pub encrypted_data: crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData,
}

impl XSalsa20Poly1305Decrypt {
    pub fn new(
        key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
        encrypted_data: crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData,
    ) -> Self {
        Self {
            key_ref,
            encrypted_data,
        }
    }

    pub fn as_key_ref_ref(&self) -> &crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef {
        &self.key_ref
    }

    pub fn as_encrypted_data_ref(
        &self,
    ) -> &crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData {
        &self.encrypted_data
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct X25519XSalsa20Poly1305Decrypt {
    pub recipient: X25519PubKey,
    pub sender: X25519PubKey,
    pub encrypted_data: XSalsa20Poly1305EncryptedData,
}

impl X25519XSalsa20Poly1305Decrypt {
    pub fn new(
        recipient: X25519PubKey,
        sender: X25519PubKey,
        encrypted_data: XSalsa20Poly1305EncryptedData,
    ) -> Self {
        Self {
            recipient,
            sender,
            encrypted_data,
        }
    }

    pub fn as_sender_ref(&self) -> &X25519PubKey {
        &self.sender
    }

    pub fn as_recipient_ref(&self) -> &X25519PubKey {
        &self.recipient
    }

    pub fn as_encrypted_data_ref(&self) -> &XSalsa20Poly1305EncryptedData {
        &self.encrypted_data
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct Ed25519XSalsa20Poly1305Decrypt {
    pub recipient: AgentPubKey,
    pub sender: AgentPubKey,
    pub encrypted_data: XSalsa20Poly1305EncryptedData,
}

impl Ed25519XSalsa20Poly1305Decrypt {
    pub fn new(
        recipient: AgentPubKey,
        sender: AgentPubKey,
        encrypted_data: XSalsa20Poly1305EncryptedData,
    ) -> Self {
        Self {
            recipient,
            sender,
            encrypted_data,
        }
    }

    pub fn as_sender_ref(&self) -> &AgentPubKey {
        &self.sender
    }

    pub fn as_recipient_ref(&self) -> &AgentPubKey {
        &self.recipient
    }

    pub fn as_encrypted_data_ref(&self) -> &XSalsa20Poly1305EncryptedData {
        &self.encrypted_data
    }
}



================================================
File: crates/holochain_integrity_types/src/zome.rs
================================================
//! A `Zome` is a module of app-defined code which can be run by Holochain.
//! A group of Zomes are composed to form a `DnaDef`.
//!
//! Real-world Holochain Zomes are written in Wasm.
//! This module also provides for an "inline" zome definition, which is written
//! using Rust closures, and is useful for quickly defining zomes on-the-fly
//! for tests.

use std::borrow::Cow;

use holochain_serialized_bytes::prelude::*;

/// ZomeName as a String.
#[derive(Clone, Debug, Serialize, Hash, Deserialize, Ord, Eq, PartialEq, PartialOrd)]
#[repr(transparent)]
pub struct ZomeName(pub Cow<'static, str>);

impl ZomeName {
    /// Create an unknown zome name.
    pub fn unknown() -> Self {
        "UnknownZomeName".into()
    }

    /// Create a zome name from a string.
    pub fn new<S: ToString>(s: S) -> Self {
        ZomeName(s.to_string().into())
    }
}

impl std::fmt::Display for ZomeName {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl From<&str> for ZomeName {
    fn from(s: &str) -> Self {
        Self(s.to_string().into())
    }
}

impl From<String> for ZomeName {
    fn from(s: String) -> Self {
        Self(s.into())
    }
}

/// A single function name.
#[repr(transparent)]
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, PartialOrd, Ord, Eq, Hash)]
pub struct FunctionName(pub String);

impl FunctionName {
    /// Create a new function name.
    pub fn new<S: ToString>(s: S) -> Self {
        FunctionName(s.to_string())
    }
}

impl std::fmt::Display for FunctionName {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl From<FunctionName> for String {
    fn from(function_name: FunctionName) -> Self {
        function_name.0
    }
}

impl From<String> for FunctionName {
    fn from(s: String) -> Self {
        Self(s)
    }
}

impl From<&str> for FunctionName {
    fn from(s: &str) -> Self {
        Self::from(s.to_string())
    }
}

impl AsRef<str> for FunctionName {
    fn as_ref(&self) -> &str {
        self.0.as_ref()
    }
}



================================================
File: crates/holochain_integrity_types/src/zome_io.rs
================================================
use holochain_serialized_bytes::prelude::*;

// Every externed function that the zome developer exposes to holochain returns `ExternIO`.
// The zome developer can expose callbacks in a "sparse" way based on names and the functions
// can take different input (e.g. validation vs. hooks like init, etc.).
// All we can say is that some SerializedBytes are being received and returned.
// In the case of ZomeExtern functions exposed to a client, the data input/output is entirely
// arbitrary so we can't say anything at all. In this case the happ developer must BYO
// deserialization context to match the client, either directly or via. the HDK.
// Note though, that _unlike_ zome externs, the host _does_ know exactly the guest should be
// returning for callbacks, it's just that the unpacking of the return happens in two steps:
// - first the sparse callback is triggered with SB input/output
// - then the guest inflates the expected input or the host the expected output based on the
//   callback flavour

#[derive(serde::Serialize, serde::Deserialize, Clone, Debug, PartialEq, Eq)]
#[serde(transparent)]
#[repr(transparent)]
pub struct ExternIO(#[serde(with = "serde_bytes")] pub Vec<u8>);

impl ExternIO {
    pub fn encode<I>(input: I) -> Result<Self, SerializedBytesError>
    where
        I: serde::Serialize + std::fmt::Debug,
    {
        Ok(Self(holochain_serialized_bytes::encode(&input)?))
    }
    pub fn decode<O>(&self) -> Result<O, SerializedBytesError>
    where
        O: serde::de::DeserializeOwned + std::fmt::Debug,
    {
        holochain_serialized_bytes::decode(&self.0)
    }

    pub fn into_vec(self) -> Vec<u8> {
        self.into()
    }
    pub fn as_bytes(&self) -> &[u8] {
        self.as_ref()
    }
}

impl AsRef<[u8]> for ExternIO {
    fn as_ref(&self) -> &[u8] {
        &self.0
    }
}

impl From<Vec<u8>> for ExternIO {
    fn from(v: Vec<u8>) -> Self {
        Self(v)
    }
}

impl From<ExternIO> for Vec<u8> {
    fn from(extern_io: ExternIO) -> Self {
        extern_io.0
    }
}



================================================
File: crates/holochain_integrity_types/src/action/builder.rs
================================================
use super::EntryType;
use super::MigrationTarget;
use super::Timestamp;
use crate::action;
use crate::link::LinkTag;
use crate::link::LinkType;
use crate::ActionUnweighed;
use crate::ActionWeighed;
use crate::EntryRateWeight;
use crate::MembraneProof;
use crate::RateWeight;
use crate::ZomeIndex;
use action::Dna;
use holo_hash::ActionHash;
use holo_hash::AgentPubKey;
use holo_hash::AnyLinkableHash;
use holo_hash::EntryHash;

#[derive(Clone, Debug)]
pub struct ActionBuilderCommon {
    pub author: AgentPubKey,
    pub timestamp: Timestamp,
    pub action_seq: u32,
    pub prev_action: ActionHash,
}

impl ActionBuilderCommon {
    pub fn new(
        author: AgentPubKey,
        timestamp: Timestamp,
        action_seq: u32,
        prev_action: ActionHash,
    ) -> Self {
        Self {
            author,
            timestamp,
            action_seq,
            prev_action,
        }
    }
}

/// Builder for non-genesis Actions
///
/// SourceChain::put takes one of these rather than a raw Action, so that it
/// can inject the proper values via `ActionBuilderCommon`, rather than requiring
/// surrounding code to construct a proper Action outside of the context of
/// the SourceChain.
///
/// Genesis actions cannot be built with this method, because the way
/// SourceChain is written, the agent key is not known until after genesis is
/// performed, and the agent key is one of the values injected rather than
/// provided by this builder. (There is also the problem that the Dna action
/// is a special case as it has no `prev_action`, and generalizing
/// `ActionBuilderCommon` just to make that optional is a bit inconvenient.)
/// `SourceChain::genesis` already handles genesis in one fell swoop.
pub trait ActionBuilder<U: ActionUnweighed>: Sized {
    fn build(self, common: ActionBuilderCommon) -> U;
}

macro_rules! builder_variant {
    ( $name: ident <$weight : ty> { $($field: ident : $t: ty),* $(,)? } ) => {

        #[derive(Clone, Debug, PartialEq, Eq)]
        pub struct $name {
            $(pub $field : $t,)*
        }


        #[allow(clippy::new_without_default)]
        impl $name {
            pub fn new($($field : $t),* ) -> Self {
                Self {
                    $($field,)*
                }
            }
        }

        impl ActionBuilder<action::$name<()>> for $name {
            fn build(self, common: ActionBuilderCommon) -> action::$name<()> {
                let ActionBuilderCommon {
                    author,
                    timestamp,
                    action_seq,
                    prev_action,
                } = common;

                action::$name {
                    weight: (),
                    author,
                    timestamp,
                    action_seq,
                    prev_action,
                    $($field : self.$field,)*
                }
            }
        }


        impl ActionWeighed for action::$name {
            type Unweighed = action::$name<()>;
            type Weight = $weight;

            fn into_action(self) -> action::Action {
                action::Action::$name(self)
            }

            fn unweighed(self) -> action::$name<()> {
                action::$name::<()> {
                    weight: (),
                    author: self.author,
                    timestamp: self.timestamp,
                    action_seq: self.action_seq,
                    prev_action: self.prev_action,
                    $($field: self.$field),*
                }
            }
        }

        impl ActionUnweighed for action::$name<()> {
            type Weighed = action::$name;
            type Weight = $weight;

            fn weighed(self, weight: $weight) -> action::$name {
                action::$name {
                    weight,
                    author: self.author,
                    timestamp: self.timestamp,
                    action_seq: self.action_seq,
                    prev_action: self.prev_action,
                    $($field: self.$field),*
                }
            }
        }

        #[cfg(feature = "test_utils")]
        impl action::$name {
            pub fn from_builder(common: ActionBuilderCommon, $($field : $t),*) -> Self {
                let builder = $name {
                    $($field,)*
                };

                builder.build(common).weighed(Default::default())
            }
        }
    };

    ( $name: ident { $($field: ident : $t: ty),* $( $(,)? | $($dfield: ident : $dt: ty),* )? $(,)? } ) => {

        #[derive(Clone, Debug, PartialEq, Eq)]
        pub struct $name {
            $(pub $field : $t,)*
            $( $(pub $dfield : $dt),* )?
        }

        #[allow(clippy::new_without_default)]
        impl $name {
            pub fn new($($field : $t),* ) -> Self {
                Self {
                    $($field,)*
                    $( $($dfield : Default::default()),* )?
                }
            }

            pub fn new_full($($field : $t,)* $( $($dfield : $dt),* )? ) -> Self {
                Self {
                    $($field,)*
                    $( $($dfield),* )?
                }
            }
        }

        impl ActionWeighed for action::$name {
            type Unweighed = action::$name;
            type Weight = ();

            fn into_action(self) -> action::Action {
                action::Action::$name(self)
            }

            fn unweighed(self) -> Self::Unweighed {
                self
            }

        }

        impl ActionUnweighed for action::$name {
            type Weighed = action::$name;
            type Weight = ();

            fn weighed(self, _weight: ()) -> action::$name {
                self
            }
        }

        impl ActionBuilder<action::$name> for $name {
            fn build(self, common: ActionBuilderCommon) -> action::$name {
                let ActionBuilderCommon {
                    author,
                    timestamp,
                    action_seq,
                    prev_action,
                } = common;

                action::$name {
                    author,
                    timestamp,
                    action_seq,
                    prev_action,
                    $($field : self.$field,)*
                    $( $($dfield : self.$dfield),* )?
                }
            }
        }

        impl From<($name, ActionBuilderCommon)> for action::$name {
            fn from((n, h): ($name, ActionBuilderCommon)) -> action::$name {
                n.build(h)
            }
        }

        #[cfg(feature = "test_utils")]
        impl action::$name {
            pub fn from_builder(common: ActionBuilderCommon, $($field : $t),*) -> Self {
                let builder = $name {
                    $($field,)*
                    $( $($dfield : Default::default()),* )?
                };

                builder.build(common)
            }
        }
    }
}

builder_variant!(InitZomesComplete {});

builder_variant!(CreateLink<RateWeight> {
    base_address: AnyLinkableHash,
    target_address: AnyLinkableHash,
    zome_index: ZomeIndex,
    link_type: LinkType,
    tag: LinkTag,
});

builder_variant!(DeleteLink {
    link_add_address: ActionHash,
    base_address: AnyLinkableHash,
});

builder_variant!(OpenChain {
    prev_target: MigrationTarget,
    close_hash: ActionHash,
});

builder_variant!(CloseChain {
    new_target: Option<MigrationTarget>,
});

builder_variant!(Create<EntryRateWeight> {
    entry_type: EntryType,
    entry_hash: EntryHash,
});

builder_variant!(Update<EntryRateWeight> {
    original_entry_address: EntryHash,
    original_action_address: ActionHash,

    entry_type: EntryType,
    entry_hash: EntryHash,
});

builder_variant!(Delete<RateWeight> {
    deletes_address: ActionHash,
    deletes_entry_address: EntryHash,
});

builder_variant!(AgentValidationPkg {
    membrane_proof: Option<MembraneProof>,
});

/// The Dna action can't implement ActionBuilder because it lacks a
/// `prev_action` field, so this helper is provided as a special case
#[cfg(feature = "test_utils")]
impl Dna {
    pub fn from_builder(hash: holo_hash::DnaHash, builder: ActionBuilderCommon) -> Self {
        Self {
            author: builder.author,
            timestamp: builder.timestamp,
            hash,
        }
    }
}

// some more manual implementations for Dna

impl ActionWeighed for Dna {
    type Unweighed = Dna;
    type Weight = ();

    fn into_action(self) -> action::Action {
        action::Action::Dna(self)
    }

    fn unweighed(self) -> Self::Unweighed {
        self
    }
}

impl ActionUnweighed for Dna {
    type Weighed = Dna;
    type Weight = ();

    fn weighed(self, _weight: ()) -> Dna {
        self
    }
}



================================================
File: crates/holochain_integrity_types/src/action/conversions.rs
================================================
use super::*;

impl From<u8> for ZomeIndex {
    fn from(a: u8) -> Self {
        Self(a)
    }
}

impl From<ZomeIndex> for u8 {
    fn from(a: ZomeIndex) -> Self {
        a.0
    }
}

impl std::fmt::Display for ZomeIndex {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl From<u8> for EntryDefIndex {
    fn from(a: u8) -> Self {
        Self(a)
    }
}

#[derive(PartialEq, Eq, Debug, Clone)]
pub struct WrongActionError(pub String);

impl std::fmt::Display for WrongActionError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "Tried to unwrap an action to the wrong variant")
    }
}

impl std::error::Error for WrongActionError {}

impl TryFrom<Action> for Update {
    type Error = WrongActionError;
    fn try_from(value: Action) -> Result<Self, Self::Error> {
        match value {
            Action::Update(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl<'a> TryFrom<&'a Action> for &'a Update {
    type Error = WrongActionError;
    fn try_from(value: &'a Action) -> Result<Self, Self::Error> {
        match value {
            Action::Update(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl TryFrom<Action> for Delete {
    type Error = WrongActionError;
    fn try_from(value: Action) -> Result<Self, Self::Error> {
        match value {
            Action::Delete(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl<'a> TryFrom<&'a Action> for &'a Delete {
    type Error = WrongActionError;
    fn try_from(value: &'a Action) -> Result<Self, Self::Error> {
        match value {
            Action::Delete(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl TryFrom<Action> for CreateLink {
    type Error = WrongActionError;
    fn try_from(value: Action) -> Result<Self, Self::Error> {
        match value {
            Action::CreateLink(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl<'a> TryFrom<&'a Action> for &'a CreateLink {
    type Error = WrongActionError;
    fn try_from(value: &'a Action) -> Result<Self, Self::Error> {
        match value {
            Action::CreateLink(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl TryFrom<Action> for DeleteLink {
    type Error = WrongActionError;
    fn try_from(value: Action) -> Result<Self, Self::Error> {
        match value {
            Action::DeleteLink(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}

impl<'a> TryFrom<&'a Action> for &'a DeleteLink {
    type Error = WrongActionError;
    fn try_from(value: &'a Action) -> Result<Self, Self::Error> {
        match value {
            Action::DeleteLink(h) => Ok(h),
            _ => Err(WrongActionError(format!("{:?}", value))),
        }
    }
}



================================================
File: crates/holochain_integrity_types/src/capability/claim.rs
================================================
use super::CapSecret;
use holo_hash::*;
use holochain_serialized_bytes::prelude::*;

/// System entry to hold a capability token claim for use as a caller.
/// Stored by a claimant so they can remember what's necessary to exercise
/// this capability by sending the secret to the grantor.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash, SerializedBytes)]
pub struct CapClaim {
    /// A string by which to later query for saved claims.
    /// This does not need to be unique within a source chain.
    pub tag: String,
    /// AgentPubKey of agent who authored the corresponding CapGrant.
    pub grantor: AgentPubKey,
    /// The secret needed to exercise this capability.
    /// This is the only bit sent over the wire to attempt a remote call.
    /// Note that the grantor may have revoked the corresponding grant since we received the claim
    /// so claims are only ever a 'best effort' basis.
    pub secret: CapSecret,
}

impl CapClaim {
    /// Constructor.
    pub fn new(tag: String, grantor: AgentPubKey, secret: CapSecret) -> Self {
        CapClaim {
            tag,
            grantor,
            secret,
        }
    }

    /// Access the secret.
    pub fn secret(&self) -> &CapSecret {
        &self.secret
    }

    /// Access the tag
    pub fn tag(&self) -> &str {
        &self.tag
    }

    /// Access the grantor
    pub fn grantor(&self) -> &AgentPubKey {
        &self.grantor
    }
}



================================================
File: crates/holochain_integrity_types/src/capability/grant.rs
================================================
use super::CapSecret;
use crate::zome::FunctionName;
use crate::zome::ZomeName;
use holo_hash::*;
use serde::Deserialize;
use serde::Serialize;
use std::collections::BTreeSet;

/// Represents a _potentially_ valid access grant to a zome call.
/// Zome call response will be Unauthorized without a valid grant.
///
/// The CapGrant is not always a dedicated entry in the chain.
/// Notably AgentPubKey entries in the current chain act like root access to local zome calls.
///
/// A `CapGrant` is valid if it matches the function, agent and secret for a given zome call.
///
/// See `.is_valid()`
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[allow(clippy::large_enum_variant)]
pub enum CapGrant {
    /// Grants the capability of calling every extern to the calling agent, provided the calling
    /// agent is the local chain author.
    /// This grant is compared to the current `Entry::Agent` entry on the source chain.
    ChainAuthor(AgentPubKey),

    /// Any agent other than the chain author is attempting to call an extern.
    /// The pubkey of the calling agent is secured by the cryptographic handshake at the network
    /// layer and the caller must provide a secret that we check for in a private entry in the
    /// local chain.
    RemoteAgent(ZomeCallCapGrant),
}

impl From<holo_hash::AgentPubKey> for CapGrant {
    fn from(agent_hash: holo_hash::AgentPubKey) -> Self {
        CapGrant::ChainAuthor(agent_hash)
    }
}

/// The entry for the ZomeCall capability grant.
/// This data is committed to the callee's source chain as a private entry.
/// The remote calling agent must provide a secret and we source their pubkey from the active
/// network connection. This must match the strictness of the CapAccess.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash)]
pub struct ZomeCallCapGrant {
    /// A string by which to later query for saved grants.
    /// This does not need to be unique within a source chain.
    pub tag: String,
    /// Specifies who may claim this capability, and by what means
    pub access: CapAccess,
    /// Set of functions to which this capability grants ZomeCall access
    pub functions: GrantedFunctions,
    // @todo the payloads to curry to the functions
    // pub curry_payloads: CurryPayloads,
}

/// The outbound DTO of a ZomeCall capability grant info request.
/// CapAccess secrets are omitted, Access types and assignees are provided under CapAccessInfo.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash)]
pub struct DesensitizedZomeCallCapGrant {
    /// A string by which to later query for saved grants.
    /// This does not need to be unique within a source chain.
    pub tag: String,
    /// Specifies who may claim this capability, and by what means omitting secrets
    pub access: CapAccessInfo,
    /// Set of functions to which this capability grants ZomeCall access
    pub functions: GrantedFunctions,
}

impl From<ZomeCallCapGrant> for DesensitizedZomeCallCapGrant {
    /// Create a new Desensitized ZomeCall capability grant
    fn from(zccg: ZomeCallCapGrant) -> Self {
        DesensitizedZomeCallCapGrant {
            tag: zccg.tag,
            access: CapAccessInfo {
                access_type: zccg.access.as_variant_string().to_string(),
                assignees: match &zccg.access {
                    CapAccess::Assigned { assignees, .. } => Some(assignees.clone()),
                    _ => None,
                },
            },
            functions: zccg.functions,
        }
    }
}

impl ZomeCallCapGrant {
    /// Constructor
    pub fn new(
        tag: String,
        access: CapAccess,
        functions: GrantedFunctions,
        // @todo curry_payloads: CurryPayloads,
    ) -> Self {
        Self {
            tag,
            access,
            functions,
            // @todo curry_payloads,
        }
    }
}

impl From<ZomeCallCapGrant> for CapGrant {
    /// Create a new ZomeCall capability grant
    fn from(zccg: ZomeCallCapGrant) -> Self {
        CapGrant::RemoteAgent(zccg)
    }
}

impl CapGrant {
    /// Given a grant, is it valid in isolation?
    /// In a world of CRUD, some new entry might update or delete an existing one, but we can check
    /// if a grant is valid in a standalone way.
    pub fn is_valid(
        &self,
        given_function: &GrantedFunction,
        given_agent: &AgentPubKey,
        given_secret: Option<&CapSecret>,
    ) -> bool {
        match self {
            // Grant is always valid if the author matches the check agent.
            CapGrant::ChainAuthor(author) => author == given_agent,
            // Otherwise we need to do more work…
            CapGrant::RemoteAgent(ZomeCallCapGrant {
                access, functions, ..
            }) => {
                // The checked function needs to be in the grant…
                let granted = match functions {
                    GrantedFunctions::All => true,
                    GrantedFunctions::Listed(fns) => fns.contains(given_function),
                };
                granted
                // The agent needs to be valid…
                && match access {
                    // The grant is assigned so the agent needs to match…
                    CapAccess::Assigned { assignees, .. } => assignees.contains(given_agent),
                    // The grant has no assignees so is always valid…
                    _ => true,
                }
                // The secret needs to match…
                && match access {
                    // Unless the extern is unrestricted.
                    CapAccess::Unrestricted => true,
                    // note the PartialEq implementation is constant time for secrets
                    CapAccess::Transferable { secret, .. } => given_secret.map(|given| secret == given).unwrap_or(false),
                    CapAccess::Assigned { secret, .. } => given_secret.map(|given| secret == given).unwrap_or(false),
                }
            }
        }
    }
}

/// Represents access requirements for capability grants.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum CapAccess {
    /// No restriction: callable by anyone.
    Unrestricted,
    /// Callable by anyone who can provide the secret.
    Transferable {
        /// The secret.
        secret: CapSecret,
    },
    /// Callable by anyone in the list of assignees who possesses the secret.
    Assigned {
        /// The secret.
        secret: CapSecret,
        /// Agents who can use this grant.
        assignees: BTreeSet<AgentPubKey>,
    },
}

/// Implements ().into() shorthand for CapAccess::Unrestricted
impl From<()> for CapAccess {
    fn from(_: ()) -> Self {
        Self::Unrestricted
    }
}

/// Implements secret.into() shorthand for CapAccess::Transferable(secret)
impl From<CapSecret> for CapAccess {
    fn from(secret: CapSecret) -> Self {
        Self::Transferable { secret }
    }
}

/// Implements (secret, assignees).into() shorthand for CapAccess::Assigned { secret, assignees }
impl From<(CapSecret, BTreeSet<AgentPubKey>)> for CapAccess {
    fn from((secret, assignees): (CapSecret, BTreeSet<AgentPubKey>)) -> Self {
        Self::Assigned { secret, assignees }
    }
}

/// Implements (secret, agent_pub_key).into() shorthand for
/// CapAccess::Assigned { secret, assignees: hashset!{ agent } }
impl From<(CapSecret, AgentPubKey)> for CapAccess {
    fn from((secret, assignee): (CapSecret, AgentPubKey)) -> Self {
        let mut assignees = BTreeSet::new();
        assignees.insert(assignee);
        Self::from((secret, assignees))
    }
}

impl CapAccess {
    /// Return variant denominator as string slice
    pub fn as_variant_string(&self) -> &str {
        match self {
            CapAccess::Unrestricted => "unrestricted",
            CapAccess::Transferable { .. } => "transferable",
            CapAccess::Assigned { .. } => "assigned",
        }
    }
}

/// Represents access info for capability grants .
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash)]
pub struct CapAccessInfo {
    /// The access type.
    access_type: String,
    /// Agents who can use this grant.
    assignees: Option<BTreeSet<AgentPubKey>>,
}

/// a single zome/function pair
pub type GrantedFunction = (ZomeName, FunctionName);
/// A collection of zome/function pairs

#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, Hash)]
#[serde(tag = "type", content = "value", rename_all = "snake_case")]
pub enum GrantedFunctions {
    /// grant all zomes all functions
    All,
    /// grant to specified zomes and functions
    Listed(BTreeSet<GrantedFunction>),
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn cap_grant_is_valid() {
        let agent1 = AgentPubKey::from_raw_36(vec![1; 36]);
        let agent2 = AgentPubKey::from_raw_36(vec![2; 36]);
        let assignees: BTreeSet<_> = [agent1.clone()].into_iter().collect();
        let secret: CapSecret = [1; 64].into();
        let secret_wrong: CapSecret = [2; 64].into();
        let tag = "tag".to_string();

        let g1: CapGrant = ZomeCallCapGrant {
            tag: tag.clone(),
            access: CapAccess::Transferable { secret },
            functions: GrantedFunctions::All,
        }
        .into();

        let g2: CapGrant = ZomeCallCapGrant {
            tag: tag.clone(),
            access: CapAccess::Assigned {
                secret,
                assignees: assignees.clone(),
            },
            functions: GrantedFunctions::All,
        }
        .into();

        assert!(g1.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            Some(&secret),
        ));

        assert!(g1.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent2,
            Some(&secret),
        ));

        assert!(!g1.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            Some(&secret_wrong),
        ));

        assert!(!g1.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            None,
        ));

        assert!(g2.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            Some(&secret),
        ));

        assert!(!g2.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent2,
            Some(&secret),
        ));

        assert!(!g2.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            None,
        ));

        assert!(!g2.is_valid(
            &(ZomeName("zome".into()), FunctionName("fn".into())),
            &agent1,
            Some(&secret_wrong),
        ));
    }
}



================================================
File: crates/holochain_integrity_types/src/capability/secret.rs
================================================
use holochain_secure_primitive::secure_primitive;
use holochain_serialized_bytes::prelude::*;

/// The number of bits we want for a comfy secret.
pub const CAP_SECRET_BITS: usize = 512;
/// The number of bytes we want for a comfy secret.
pub const CAP_SECRET_BYTES: usize = CAP_SECRET_BITS / 8;
/// A fixed size array of bytes that a secret must be.
pub type CapSecretBytes = [u8; CAP_SECRET_BYTES];

/// A CapSecret is used by a caller to prove to a callee access to a committed CapGrant.
///
/// It is a random, unique identifier for the capability, which is shared by
/// the grantor to allow access to others. The grantor can optionally further restrict usage of the
/// secret to specific agents.
///
// @todo enforce that secrets are unique across all grants in a chain.
// The PartialEq impl by subtle *should* be compatible with default Hash impl
#[allow(clippy::derived_hash_with_manual_eq)]
#[derive(Clone, Copy, Hash, SerializedBytes)]
pub struct CapSecret(CapSecretBytes);

// Capability secrets are not cryptographic secrets.
// They aren't used in any cryptographic algorithm.
// They are closer to API keys in that they may provide access to specific functions on a specific
// device if it is accepting incoming connections. Still secret but there are mitigating factors
// such as the ability to revoke a secret, and to assign it to specific recipients ahead of time
// if they are a known closed set.
secure_primitive!(CapSecret, CAP_SECRET_BYTES);



================================================
File: crates/holochain_integrity_types/src/chain/test.rs
================================================
use super::*;
use holo_hash::ActionHash;

fn hash(i: u8) -> ActionHash {
    ActionHash::from_raw_36(vec![i; 36])
}

#[test]
fn can_serialize() {
    let filter = ChainFilter::new(hash(0));
    let sb = SerializedBytes::try_from(&filter).unwrap();
    let result = ChainFilter::try_from(sb).unwrap();
    assert_eq!(filter, result);
}

#[test]
fn take_n_is_min() {
    assert_eq!(
        ChainFilter::new(hash(0)).take(0),
        ChainFilter::new(hash(0)).take(0).take(5)
    );
    assert_eq!(
        ChainFilter::new(hash(0)).take(0),
        ChainFilter::new(hash(0)).take(5).take(0)
    );
}

#[test]
fn until_hash_is_a_set() {
    assert_eq!(
        ChainFilter::new(hash(0)).until(hash(0)).until(hash(1)),
        ChainFilter::new(hash(0))
            .until(hash(0))
            .until(hash(0))
            .until(hash(1))
            .until(hash(1)),
    );
}



================================================
File: crates/holochain_integrity_types/src/countersigning/error.rs
================================================
use crate::Role;

/// Errors related to the secure primitive macro.
#[derive(Debug, PartialEq, Eq)]
pub enum CounterSigningError {
    /// Agent index is out of bounds for the signing session.
    AgentIndexOutOfBounds,
    /// An empty vector was used to build session data.
    MissingResponse,
    /// Session responses needs to be same length as the signing agents.
    CounterSigningSessionResponsesLength(usize, usize),
    /// Session response agents all need to be in the correct positions.
    CounterSigningSessionResponsesOrder(u8, usize),
    /// Enzyme must match for required and optional signers if set.
    EnzymeMismatch(
        Option<(holo_hash::AgentPubKey, Vec<Role>)>,
        Option<(holo_hash::AgentPubKey, Vec<Role>)>,
    ),
    /// If there are optional signers the session MUST be enzymatic.
    NonEnzymaticOptionalSigners,
    /// Agents length cannot be longer than max or less than min.
    AgentsLength(usize),
    /// Optional agents length cannot be shorter then minimum.
    OptionalAgentsLength(u8, usize),
    /// Optional agents length must be majority of the signers list.
    MinOptionalAgents(u8, usize),
    /// There cannot be duplicates in the agents list.
    AgentsDupes(Vec<holo_hash::AgentPubKey>),
    /// The session times must validate.
    CounterSigningSessionTimes(crate::CounterSigningSessionTimes),
}

impl std::error::Error for CounterSigningError {}

impl core::fmt::Display for CounterSigningError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            CounterSigningError::AgentIndexOutOfBounds => {
                write!(f, "Agent index is out of bounds for the signing session.")
            }
            CounterSigningError::MissingResponse => write!(
                f,
                "Attempted to build CounterSigningSessionData with an empty response vector."
            ),
            CounterSigningError::CounterSigningSessionResponsesLength(resp, num_agents) => {
                write!(f,
                    "The countersigning session responses ({}) did not match the number of signing agents ({})",
                    resp,
                    num_agents
                )
            }
            CounterSigningError::CounterSigningSessionResponsesOrder(index, pos) => write!(f,
                    "The countersigning session response with agent index {} was found in index position {}",
                    index, pos
            ),
            CounterSigningError::EnzymeMismatch(required_signer, optional_signer) => write!(f,
                "The enzyme is mismatche for required signer {:?} and optional signer {:?}",
                required_signer, optional_signer

            ),
            CounterSigningError::NonEnzymaticOptionalSigners => write!(f, "There are optional signers without an enzyme."),
            CounterSigningError::AgentsLength(len) => {
                write!(f, "The signing agents list is too long or short {}", len)
            },
            CounterSigningError::OptionalAgentsLength(min, len) => {
                write!(f, "The optional signing agents list length is {} which is less than the minimum {} required to sign", len, min)
            },
            CounterSigningError::MinOptionalAgents(min, len) => {
                write!(f, "The minimum optional agents {} is not a majority of {}", min, len)
            },
            CounterSigningError::AgentsDupes(agents) => write!(
                f,
                "The signing agents list contains duplicates {:?}",
                agents
            ),
            CounterSigningError::CounterSigningSessionTimes(times) => write!(
                f,
                "The countersigning session times were not valid {:?}",
                times
            ),
        }
    }
}



================================================
File: crates/holochain_integrity_types/src/entry/app_entry_bytes.rs
================================================
use super::EntryError;
use super::ENTRY_SIZE_LIMIT;
use holochain_serialized_bytes::prelude::*;

/// Newtype for the bytes comprising an App entry
#[derive(Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct AppEntryBytes(pub SerializedBytes);

impl std::fmt::Debug for AppEntryBytes {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!(
            "AppEntryBytes({})",
            holochain_util::hex::many_bytes_string(self.0.bytes())
        ))
    }
}

impl AppEntryBytes {
    /// Get the inner SerializedBytes
    pub fn into_sb(self) -> SerializedBytes {
        self.0
    }
}

impl AsRef<SerializedBytes> for AppEntryBytes {
    fn as_ref(&self) -> &SerializedBytes {
        &self.0
    }
}

impl std::borrow::Borrow<SerializedBytes> for AppEntryBytes {
    fn borrow(&self) -> &SerializedBytes {
        &self.0
    }
}

impl std::ops::Deref for AppEntryBytes {
    type Target = SerializedBytes;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl TryFrom<SerializedBytes> for AppEntryBytes {
    type Error = EntryError;

    fn try_from(sb: SerializedBytes) -> Result<Self, EntryError> {
        let size = sb.bytes().len();
        if size > ENTRY_SIZE_LIMIT {
            Err(EntryError::EntryTooLarge(size))
        } else {
            Ok(Self(sb))
        }
    }
}

impl From<AppEntryBytes> for SerializedBytes {
    fn from(aeb: AppEntryBytes) -> Self {
        UnsafeBytes::from(aeb.0).into()
    }
}



================================================
File: crates/holochain_integrity_types/src/entry/error.rs
================================================
use super::*;

/// Errors involving app entry creation
#[derive(Debug, Clone, PartialEq)]
pub enum EntryError {
    /// The entry is too large to be created
    EntryTooLarge(usize),

    /// SerializedBytes passthrough
    SerializedBytes(SerializedBytesError),
}

impl std::error::Error for EntryError {
    fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {
        match self {
            EntryError::EntryTooLarge(_) => None,
            EntryError::SerializedBytes(e) => e.source(),
        }
    }
}

impl From<SerializedBytesError> for EntryError {
    fn from(e: SerializedBytesError) -> Self {
        Self::SerializedBytes(e)
    }
}

impl core::fmt::Display for EntryError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            EntryError::EntryTooLarge(bytes)=> write!(
                f,
                "Attempted to create an Entry whose size exceeds the limit.\nEntry size: {}\nLimit: {}",
                bytes,
                ENTRY_SIZE_LIMIT
            ),
            EntryError::SerializedBytes(s) => s.fmt(f),
        }
    }
}



================================================
File: crates/holochain_integrity_types/src/info/test.rs
================================================
use crate::UnitEnum;

use super::*;

#[derive(Debug, PartialEq, Eq, Copy, Clone)]
enum LinkTypes {
    A,
    B,
}
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
enum LinkZomes {
    A(LinkTypes),
    B(LinkTypes),
}

fn make_map(ids: &[(u8, u8)]) -> ScopedZomeTypes<LinkType> {
    ScopedZomeTypes(
        ids.iter()
            .map(|(zome_index, len)| ((*zome_index).into(), (0..*len).map(|t| t.into()).collect()))
            .collect(),
    )
}

fn make_entry_map(ids: &[(u8, u8)]) -> ScopedZomeTypes<EntryDefIndex> {
    ScopedZomeTypes(
        ids.iter()
            .map(|(zome_index, len)| ((*zome_index).into(), (0..*len).map(|t| t.into()).collect()))
            .collect(),
    )
}

impl From<LinkTypes> for ZomeLinkTypesKey {
    fn from(lt: LinkTypes) -> Self {
        match lt {
            LinkTypes::A => ZomeTypesKey {
                zome_index: 0.into(),
                type_index: 0.into(),
            },
            LinkTypes::B => ZomeTypesKey {
                zome_index: 0.into(),
                type_index: 1.into(),
            },
        }
    }
}

impl From<LinkZomes> for ZomeLinkTypesKey {
    fn from(lt: LinkZomes) -> Self {
        match lt {
            LinkZomes::A(lt) => ZomeTypesKey {
                zome_index: 0.into(),
                type_index: ZomeLinkTypesKey::from(lt).type_index,
            },
            LinkZomes::B(lt) => ZomeTypesKey {
                zome_index: 1.into(),
                type_index: ZomeLinkTypesKey::from(lt).type_index,
            },
        }
    }
}

impl LinkTypes {
    fn iter() -> impl Iterator<Item = Self> {
        use LinkTypes::*;
        [A, B].into_iter()
    }
}

impl LinkZomes {
    fn iter() -> impl Iterator<Item = Self> {
        use LinkZomes::*;
        LinkTypes::iter().map(A).chain(LinkTypes::iter().map(B))
    }
}

#[test]
fn can_map_to_key() {
    let map = make_map(&[(12, 2)]);
    assert_eq!(
        map.get(LinkTypes::A).unwrap(),
        ScopedLinkType {
            zome_index: 12.into(),
            zome_type: 0.into()
        }
    );
    assert_eq!(
        map.get(LinkTypes::B).unwrap(),
        ScopedLinkType {
            zome_index: 12.into(),
            zome_type: 1.into()
        }
    );

    let map = make_map(&[(12, 2), (3, 2)]);
    assert_eq!(
        map.get(LinkZomes::A(LinkTypes::A)).unwrap(),
        ScopedLinkType {
            zome_index: 12.into(),
            zome_type: 0.into()
        }
    );
    assert_eq!(
        map.get(LinkZomes::A(LinkTypes::B)).unwrap(),
        ScopedLinkType {
            zome_index: 12.into(),
            zome_type: 1.into()
        }
    );
    assert_eq!(
        map.get(LinkZomes::B(LinkTypes::A)).unwrap(),
        ScopedLinkType {
            zome_index: 3.into(),
            zome_type: 0.into()
        }
    );
    assert_eq!(
        map.get(LinkZomes::B(LinkTypes::B)).unwrap(),
        ScopedLinkType {
            zome_index: 3.into(),
            zome_type: 1.into()
        }
    );
}

#[test]
fn can_map_from_scoped_type() {
    let map = make_map(&[(12, 2)]);
    assert_eq!(
        map.find(
            LinkTypes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 0.into()
            }
        )
        .unwrap(),
        LinkTypes::A
    );
    assert_eq!(
        map.find(
            LinkTypes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 1.into()
            }
        )
        .unwrap(),
        LinkTypes::B
    );
    assert_eq!(
        map.find(
            LinkTypes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 3.into()
            }
        ),
        None
    );
    assert_eq!(
        map.find(
            LinkTypes::iter(),
            ScopedLinkType {
                zome_index: 13.into(),
                zome_type: 1.into()
            }
        ),
        None
    );

    let map = make_map(&[(12, 2), (3, 2)]);
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 0.into()
            }
        )
        .unwrap(),
        LinkZomes::A(LinkTypes::A),
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 1.into()
            }
        )
        .unwrap(),
        LinkZomes::A(LinkTypes::B),
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 3.into(),
                zome_type: 0.into()
            }
        )
        .unwrap(),
        LinkZomes::B(LinkTypes::A),
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 3.into(),
                zome_type: 1.into()
            }
        )
        .unwrap(),
        LinkZomes::B(LinkTypes::B),
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 3.into(),
                zome_type: 2.into()
            }
        ),
        None
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 12.into(),
                zome_type: 2.into()
            }
        ),
        None
    );
    assert_eq!(
        map.find(
            LinkZomes::iter(),
            ScopedLinkType {
                zome_index: 14.into(),
                zome_type: 0.into()
            }
        ),
        None
    );
}

type Entry = ();
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
struct A;
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
struct B;
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
enum EntryTypes {
    A(A),
    B(B),
}
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
enum EntryZomes {
    A(EntryTypes),
    B(EntryTypes),
}

impl UnitEnum for EntryTypes {
    type Unit = UnitEntry;

    fn to_unit(&self) -> Self::Unit {
        match self {
            EntryTypes::A(_) => Self::Unit::A,
            EntryTypes::B(_) => Self::Unit::B,
        }
    }

    fn unit_iter() -> Box<dyn Iterator<Item = Self::Unit>> {
        todo!()
    }
}
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
enum UnitEntry {
    A,
    B,
}

impl From<Entry> for A {
    fn from(_: Entry) -> Self {
        A {}
    }
}

impl From<Entry> for B {
    fn from(_: Entry) -> Self {
        B {}
    }
}

impl From<(ZomeEntryTypesKey, Entry)> for EntryTypes {
    fn from((k, entry): (ZomeEntryTypesKey, Entry)) -> Self {
        match k {
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(0),
                type_index: EntryDefIndex(0),
            } => EntryTypes::A(entry.into()),
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(0),
                type_index: EntryDefIndex(1),
            } => EntryTypes::B(entry.into()),
            _ => unreachable!(),
        }
    }
}

impl From<ZomeEntryTypesKey> for UnitEntry {
    fn from(k: ZomeEntryTypesKey) -> Self {
        match k {
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(0),
                type_index: EntryDefIndex(0),
            } => Self::A,
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(0),
                type_index: EntryDefIndex(1),
            } => Self::B,
            _ => unreachable!(),
        }
    }
}
impl From<(ZomeEntryTypesKey, Entry)> for EntryZomes {
    fn from((k, entry): (ZomeEntryTypesKey, Entry)) -> Self {
        match k {
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(0),
                type_index,
            } => {
                let k = ZomeTypesKey {
                    zome_index: 0.into(),
                    type_index,
                };
                let unit: <EntryTypes as UnitEnum>::Unit = k.into();
                let r = match unit {
                    UnitEntry::A => EntryTypes::A(entry.into()),
                    UnitEntry::B => EntryTypes::B(entry.into()),
                };
                Self::A(r)
            }
            ZomeTypesKey {
                zome_index: ZomeDependencyIndex(1),
                type_index,
            } => {
                let k = ZomeTypesKey {
                    zome_index: 0.into(),
                    type_index,
                };
                let unit: <EntryTypes as UnitEnum>::Unit = k.into();
                let r = match unit {
                    UnitEntry::A => EntryTypes::A(entry.into()),
                    UnitEntry::B => EntryTypes::B(entry.into()),
                };
                Self::B(r)
            }
            _ => unreachable!(),
        }
    }
}

#[test]
fn can_map_entry_from_scoped_type() {
    let map = make_entry_map(&[(12, 2), (34, 2)]);
    let find_key = |zome_index: u8, zome_type: u8| {
        let input = ScopedEntryDefIndex {
            zome_index: zome_index.into(),
            zome_type: zome_type.into(),
        };

        map.find_key(input).unwrap()
    };
    assert_eq!(
        find_key(12, 0),
        ZomeEntryTypesKey {
            zome_index: 0.into(),
            type_index: 0.into()
        }
    );
    assert_eq!(EntryTypes::from((find_key(12, 0), ())), EntryTypes::A(A {}));
    assert_eq!(EntryTypes::from((find_key(12, 1), ())), EntryTypes::B(B {}));

    assert_eq!(
        EntryZomes::from((find_key(12, 0), ())),
        EntryZomes::A(EntryTypes::A(A {}))
    );
    assert_eq!(
        EntryZomes::from((find_key(12, 1), ())),
        EntryZomes::A(EntryTypes::B(B {}))
    );
    assert_eq!(
        EntryZomes::from((find_key(34, 0), ())),
        EntryZomes::B(EntryTypes::A(A {}))
    );
    assert_eq!(
        EntryZomes::from((find_key(34, 1), ())),
        EntryZomes::B(EntryTypes::B(B {}))
    );
}



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305/data.rs
================================================
/// Data that can be encrypted with secretbox.
#[derive(PartialEq, Eq, serde::Serialize, serde::Deserialize, Debug, Clone)]
pub struct XSalsa20Poly1305Data(#[serde(with = "serde_bytes")] Vec<u8>);
pub type SecretBoxData = XSalsa20Poly1305Data;
pub type BoxData = XSalsa20Poly1305Data;

impl From<Vec<u8>> for XSalsa20Poly1305Data {
    fn from(v: Vec<u8>) -> Self {
        Self(v)
    }
}

impl AsRef<[u8]> for XSalsa20Poly1305Data {
    fn as_ref(&self) -> &[u8] {
        &self.0
    }
}



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305/encrypted_data.rs
================================================
use crate::x_salsa20_poly1305::nonce::XSalsa20Poly1305Nonce;

#[derive(PartialEq, Eq, serde::Serialize, serde::Deserialize, Debug, Clone)]
pub struct XSalsa20Poly1305EncryptedData {
    nonce: XSalsa20Poly1305Nonce,
    #[serde(with = "serde_bytes")]
    encrypted_data: Vec<u8>,
}

impl XSalsa20Poly1305EncryptedData {
    pub fn new(nonce: XSalsa20Poly1305Nonce, encrypted_data: Vec<u8>) -> Self {
        Self {
            nonce,
            encrypted_data,
        }
    }

    pub fn as_nonce_ref(&self) -> &XSalsa20Poly1305Nonce {
        &self.nonce
    }

    pub fn as_encrypted_data_ref(&self) -> &[u8] {
        &self.encrypted_data
    }
}



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305/key_ref.rs
================================================
use holochain_serialized_bytes::prelude::*;
use std::sync::Arc;

/// Key refs represent shared secrets stored in the keystore.
/// They can either be user-specified, or auto-generated at time of
/// secret creation, or ingestion.
#[derive(Debug, Clone, SerializedBytes)]
pub struct XSalsa20Poly1305KeyRef(Arc<[u8]>);

impl serde::Serialize for XSalsa20Poly1305KeyRef {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        serializer.serialize_bytes(&self.0)
    }
}

impl<'de> serde::Deserialize<'de> for XSalsa20Poly1305KeyRef {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        #[derive(serde::Deserialize)]
        #[serde(transparent)]
        struct I(#[serde(with = "serde_bytes")] Vec<u8>);
        let inner: I = serde::Deserialize::deserialize(deserializer)?;
        Ok(Self(inner.0.into_boxed_slice().into()))
    }
}

impl std::ops::Deref for XSalsa20Poly1305KeyRef {
    type Target = [u8];

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl AsRef<[u8]> for XSalsa20Poly1305KeyRef {
    fn as_ref(&self) -> &[u8] {
        &self.0
    }
}

impl std::borrow::Borrow<[u8]> for XSalsa20Poly1305KeyRef {
    fn borrow(&self) -> &[u8] {
        &self.0
    }
}

impl PartialEq for XSalsa20Poly1305KeyRef {
    fn eq(&self, other: &Self) -> bool {
        use subtle::ConstantTimeEq;
        self.0.ct_eq(&other.0).into()
    }
}

impl Eq for XSalsa20Poly1305KeyRef {}

impl From<&[u8]> for XSalsa20Poly1305KeyRef {
    #[inline(always)]
    fn from(b: &[u8]) -> Self {
        b.to_vec().into()
    }
}

impl<const N: usize> From<[u8; N]> for XSalsa20Poly1305KeyRef {
    #[inline(always)]
    fn from(b: [u8; N]) -> Self {
        b.to_vec().into()
    }
}

impl From<&Vec<u8>> for XSalsa20Poly1305KeyRef {
    #[inline(always)]
    fn from(b: &Vec<u8>) -> Self {
        b.clone().into()
    }
}

impl From<Vec<u8>> for XSalsa20Poly1305KeyRef {
    #[inline(always)]
    fn from(b: Vec<u8>) -> Self {
        b.into_boxed_slice().into()
    }
}

impl From<Box<[u8]>> for XSalsa20Poly1305KeyRef {
    #[inline(always)]
    fn from(b: Box<[u8]>) -> Self {
        Self(b.into())
    }
}



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305/nonce.rs
================================================
use holochain_secure_primitive::secure_primitive;
use holochain_serialized_bytes::prelude::*;

pub const NONCE_BYTES: usize = 24;

#[derive(Clone, Copy, SerializedBytes)]
pub struct XSalsa20Poly1305Nonce([u8; NONCE_BYTES]);
pub type SecretBoxNonce = XSalsa20Poly1305Nonce;

// A nonce is public but it does need to inherit all the fixed array serialization and in the
// future it will be useful to have generation from random bytes as it MUST be UNIQUE.
// Currently lair does the nonce generation for us.
secure_primitive!(XSalsa20Poly1305Nonce, NONCE_BYTES);



================================================
File: crates/holochain_integrity_types/src/x_salsa20_poly1305/x25519.rs
================================================
use holochain_secure_primitive::secure_primitive;
use holochain_serialized_bytes::prelude::*;

pub const X25519_PUB_KEY_BYTES: usize = 32;

#[derive(Clone, Copy, SerializedBytes)]
pub struct X25519PubKey([u8; X25519_PUB_KEY_BYTES]);

secure_primitive!(X25519PubKey, X25519_PUB_KEY_BYTES);



================================================
File: crates/holochain_keystore/README.md
================================================
# holochain_keystore

<!-- cargo-rdme start -->

A Keystore is a secure repository of private keys. MetaLairClient is a
reference to a Keystore. MetaLairClient allows async generation of keypairs,
and usage of those keypairs, reference by the public AgentPubKey.

## Examples

```rust
use holo_hash::AgentPubKey;
use holochain_keystore::*;
use holochain_serialized_bytes::prelude::*;

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    tokio::task::spawn(async move {
        let keystore = holochain_keystore::spawn_test_keystore().await.unwrap();
        let agent_pubkey = AgentPubKey::new_random(&keystore).await.unwrap();

        #[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
        struct MyData(Vec<u8>);

        let my_data_1 = MyData(b"signature test data 1".to_vec());

        let signature = agent_pubkey.sign(&keystore, &my_data_1).await.unwrap();

        assert!(agent_pubkey.verify_signature(&signature, &my_data_1).await.unwrap());
    }).await.unwrap();
}
```

<!-- cargo-rdme end -->

License: CAL-1.0



================================================
File: crates/holochain_keystore/Cargo.toml
================================================
[package]
name = "holochain_keystore"
version = "0.5.0-dev.20"
description = "keystore for libsodium keypairs"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_keystore"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "nacl", "libsodium", "cryptography"]
categories = ["cryptography"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
base64 = "0.22"
futures = "0.3"
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash", features = [
  "full",
] }
holochain_serialized_bytes = "=0.0.55"
holochain_zome_types = { path = "../holochain_zome_types", version = "^0.5.0-dev.17" }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../kitsune_p2p/types" }
holochain_secure_primitive = { version = "^0.5.0-dev.1", path = "../holochain_secure_primitive" }
holochain_util = { version = "^0.5.0-dev.1", path = "../holochain_util" }
lair_keystore = { version = "0.5.3", default-features = false }
must_future = "0.1.2"
nanoid = "0.4"
one_err = "0.0.8"
parking_lot = "0.12"
serde = { version = "1.0", features = ["derive"] }
serde_bytes = "0.11"
sodoken = "=0.0.11"
thiserror = "1.0.22"
tokio = { version = "1.27", features = ["full"] }
tracing = "0.1"
shrinkwraprs = "0.3"
derive_more = "0.99"
schemars = "0.8.21"

[dev-dependencies]
assert_cmd = "2.0.4"
serde_yaml = "0.9.10"
tempdir = "0.3.7"

[lints]
workspace = true

[features]
default = ["sqlite"]

test_utils = []

instrument = []

sqlite-encrypted = [
  "holo_hash/sqlite-encrypted",
  "lair_keystore/rusqlite-bundled-sqlcipher-vendored-openssl",
]

sqlite = ["holo_hash/sqlite", "lair_keystore/rusqlite-bundled"]



================================================
File: crates/holochain_keystore/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.20

## 0.5.0-dev.19

## 0.5.0-dev.18

## 0.5.0-dev.17

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.24

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

- Update Lair version to 0.4.1, see the Lair changelog [here](https://github.com/holochain/lair/blob/main/crates/lair_keystore/CHANGELOG.md#041). \#3249

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

- Change licensing from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.67

## 0.0.66

## 0.0.65

## 0.0.64

## 0.0.63

## 0.0.62

## 0.0.61

## 0.0.60

## 0.0.59

## 0.0.58

## 0.0.57

## 0.0.56

## 0.0.55

## 0.0.54

## 0.0.53

- Add lair disconnect detection / reconnect loop with backoff for keystore resiliency. [\#1529](https://github.com/holochain/holochain/pull/1529)

## 0.0.52

## 0.0.51

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

## 0.0.43

## 0.0.42

## 0.0.41

- Docs: Crate README generated from crate level doc comments [\#1392](https://github.com/holochain/holochain/pull/1392).

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

## 0.0.21

## 0.0.20

## 0.0.19

## 0.0.18

## 0.0.17

## 0.0.16

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

- Update to lair 0.0.7 which updates to rusqlite 0.26.0 [\#1023](https://github.com/holochain/holochain/pull/1023)
  - provides `bundled-sqlcipher-vendored-openssl` to ease build process on non-windows systems (windows is still using `bundled` which doesn’t provide at-rest encryption).

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

- Updated to lair 0.0.3
  - switch to sqlite/sqlcipher for keystore backing database
  - enable encryption via passphrase (not on windows)

## 0.0.2

## 0.0.1



================================================
File: crates/holochain_keystore/src/agent_pubkey_ext.rs
================================================
use crate::*;
use holochain_zome_types::prelude::*;
use kitsune_p2p_types::dependencies::lair_keystore_api;
use lair_keystore_api::LairResult;
use must_future::MustBoxFuture;
use std::sync::Arc;

/// Extend holo_hash::AgentPubKey with additional signature functionality
/// from Keystore.
pub trait AgentPubKeyExt {
    /// create a new agent keypair in given keystore, returning the AgentPubKey
    fn new_random(
        keystore: &MetaLairClient,
    ) -> MustBoxFuture<'static, LairResult<holo_hash::AgentPubKey>>
    where
        Self: Sized;

    /// sign some arbitrary raw bytes
    fn sign_raw(
        &self,
        keystore: &MetaLairClient,
        data: Arc<[u8]>,
    ) -> MustBoxFuture<'static, LairResult<Signature>>;

    /// verify a signature for given raw bytes with this agent public_key is valid
    fn verify_signature_raw(
        &self,
        signature: &Signature,
        data: Arc<[u8]>,
    ) -> MustBoxFuture<'static, KeystoreResult<bool>>;

    // -- provided -- //

    /// sign some arbitrary data
    fn sign<S>(
        &self,
        keystore: &MetaLairClient,
        input: S,
    ) -> MustBoxFuture<'static, LairResult<Signature>>
    where
        S: Serialize + std::fmt::Debug,
    {
        use futures::future::FutureExt;

        let data = match holochain_serialized_bytes::encode(&input) {
            Err(e) => {
                return async move { Err(one_err::OneErr::new(e)) }.boxed().into();
            }
            Ok(data) => data,
        };

        self.sign_raw(keystore, data.into())
    }

    /// verify a signature for given data with this agent public_key is valid
    fn verify_signature<D>(
        &self,
        signature: &Signature,
        data: D,
    ) -> MustBoxFuture<'static, KeystoreResult<bool>>
    where
        D: TryInto<SerializedBytes, Error = SerializedBytesError>,
    {
        use futures::future::FutureExt;

        let data = match data.try_into() {
            Err(e) => {
                tracing::error!("Serialization Error: {:?}", e);
                return async move { Err(e.into()) }.boxed().into();
            }
            Ok(data) => data,
        };

        let data: Vec<u8> = UnsafeBytes::from(data).into();

        self.verify_signature_raw(signature, data.into())
    }
}

impl AgentPubKeyExt for holo_hash::AgentPubKey {
    fn new_random(
        keystore: &MetaLairClient,
    ) -> MustBoxFuture<'static, LairResult<holo_hash::AgentPubKey>>
    where
        Self: Sized,
    {
        let f = keystore.new_sign_keypair_random();
        MustBoxFuture::new(f)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(keystore, data)))]
    fn sign_raw(
        &self,
        keystore: &MetaLairClient,
        data: Arc<[u8]>,
    ) -> MustBoxFuture<'static, LairResult<Signature>> {
        let f = keystore.sign(self.clone(), data);
        MustBoxFuture::new(f)
    }

    fn verify_signature_raw(
        &self,
        signature: &Signature,
        data: Arc<[u8]>,
    ) -> MustBoxFuture<'static, KeystoreResult<bool>> {
        let mut pub_key = [0; 32];
        pub_key.copy_from_slice(self.get_raw_32());
        let pub_key = <lair_keystore_api::prelude::BinDataSized<32>>::from(pub_key);
        let sig = signature.0;

        MustBoxFuture::new(async move {
            pub_key
                .verify_detached(sig.into(), data)
                .await
                .map_err(KeystoreError::LairError)
        })
    }
}



================================================
File: crates/holochain_keystore/src/crude_mock_keystore.rs
================================================
//! Defines a crude mock Keystore which always returns the same Error for every
//! call. This is about as close as we can get to a true mock which would allow
//! tweaking individual handlers, hence why this is a "crude" mock.

use std::sync::atomic::AtomicBool;
use std::sync::Arc;

use futures::FutureExt;
use kitsune_p2p_types::dependencies::lair_keystore_api::lair_client::client_traits::AsLairClient;
use kitsune_p2p_types::dependencies::lair_keystore_api::prelude::{LairApiEnum, LairClient};
use kitsune_p2p_types::dependencies::lair_keystore_api::LairResult;

use crate::spawn_test_keystore;
use crate::MetaLairClient;

/// Spawn a test keystore which always returns the same LairError for every call.
pub async fn spawn_crude_mock_keystore<F>(err_fn: F) -> MetaLairClient
where
    F: Fn() -> one_err::OneErr + Send + Sync + 'static,
{
    let (s, _) = tokio::sync::mpsc::unbounded_channel();
    MetaLairClient(
        Arc::new(parking_lot::Mutex::new(LairClient(Arc::new(
            CrudeMockKeystore(Arc::new(err_fn)),
        )))),
        s,
    )
}

/// Spawn a test keystore that can switch between mocked and real.
/// It starts off as real and can be switched to the given callback mock
/// using the [`MockLairControl`].
pub async fn spawn_real_or_mock_keystore<F>(
    func: F,
) -> LairResult<(MetaLairClient, MockLairControl)>
where
    F: Fn(LairApiEnum) -> LairResult<LairApiEnum> + Send + Sync + 'static,
{
    let real = spawn_test_keystore().await?;
    let use_mock = Arc::new(AtomicBool::new(false));
    let mock = RealOrMockKeystore {
        mock: Box::new(func),
        real,
        use_mock: use_mock.clone(),
    };

    let control = MockLairControl(use_mock);

    let (s, _) = tokio::sync::mpsc::unbounded_channel();
    Ok((
        MetaLairClient(
            Arc::new(parking_lot::Mutex::new(LairClient(Arc::new(mock)))),
            s,
        ),
        control,
    ))
}
/// A keystore which always returns the same LairError for every call.
struct RealOrMockKeystore {
    mock: Box<dyn Fn(LairApiEnum) -> LairResult<LairApiEnum> + Send + Sync + 'static>,
    real: MetaLairClient,
    use_mock: Arc<AtomicBool>,
}

/// Control if a mocked lair keystore is using
/// the real keystore or the mock callback.
pub struct MockLairControl(Arc<AtomicBool>);

impl MockLairControl {
    /// Use the mock callback.
    pub fn use_mock(&self) {
        self.0.store(true, std::sync::atomic::Ordering::SeqCst);
    }

    /// Use the real test keystore.
    pub fn use_real(&self) {
        self.0.store(false, std::sync::atomic::Ordering::SeqCst);
    }

    /// Is the keystore using the mock?
    pub fn using_mock(&self) -> bool {
        self.0.load(std::sync::atomic::Ordering::SeqCst)
    }
}
/// A keystore which always returns the same LairError for every call.
struct CrudeMockKeystore(Arc<dyn Fn() -> one_err::OneErr + Send + Sync + 'static>);

impl AsLairClient for CrudeMockKeystore {
    fn get_enc_ctx_key(&self) -> sodoken::BufReadSized<32> {
        unimplemented!()
    }

    fn get_dec_ctx_key(&self) -> sodoken::BufReadSized<32> {
        unimplemented!()
    }

    fn shutdown(&self) -> futures::future::BoxFuture<'static, LairResult<()>> {
        unimplemented!()
    }

    fn request(
        &self,
        _request: LairApiEnum,
    ) -> futures::future::BoxFuture<'static, LairResult<LairApiEnum>> {
        let err = (self.0)();
        async move { Err(err) }.boxed()
    }
}

impl AsLairClient for RealOrMockKeystore {
    fn get_enc_ctx_key(&self) -> sodoken::BufReadSized<32> {
        self.real.cli().0.get_enc_ctx_key()
    }

    fn get_dec_ctx_key(&self) -> sodoken::BufReadSized<32> {
        self.real.cli().0.get_dec_ctx_key()
    }

    fn shutdown(&self) -> futures::future::BoxFuture<'static, LairResult<()>> {
        self.real.cli().0.shutdown().boxed()
    }

    fn request(
        &self,
        request: LairApiEnum,
    ) -> futures::future::BoxFuture<'static, LairResult<LairApiEnum>> {
        if self.use_mock.load(std::sync::atomic::Ordering::SeqCst) {
            let r = (self.mock)(request);
            async move { r }.boxed()
        } else {
            AsLairClient::request(&*self.real.cli().0 .0, request)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::agent_pubkey_ext::AgentPubKeyExt;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_crude_mock_keystore() {
        tokio::task::spawn(async move {
            let keystore = spawn_crude_mock_keystore(|| "err".into()).await;

            assert_eq!(
                holo_hash::AgentPubKey::new_random(&keystore).await,
                Err(one_err::OneErr::new("err"))
            );
            // let agent = holo_hash::AgentPubKey::new_from_pure_entropy(&keystore)
            //     .await
            //     .unwrap();

            // #[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
            // struct MyData(Vec<u8>);

            // let data = MyData(b"signature test data 1".to_vec());

            // assert_eq!(
            //     agent.sign(&keystore, &data).await,
            //     Err(KeystoreError::LairError(LairError::other("err")))
            // );
        })
        .await
        .unwrap();
    }
}



================================================
File: crates/holochain_keystore/src/error.rs
================================================
use crate::*;
use holochain_secure_primitive::SecurePrimitiveError;
use holochain_zome_types::signature::Signature;

/// Keystore Error Type.
#[derive(Debug, thiserror::Error)]
pub enum KeystoreError {
    /// Error serializing data.
    #[error("SerializedBytesError: {0}")]
    SerializedBytesError(#[from] SerializedBytesError),

    /// Used by dependants to specify an invalid signature of some data
    #[error("Invalid signature {0:?}, for {1}")]
    InvalidSignature(Signature, String),

    /// Error from Lair
    #[error(transparent)]
    LairError(one_err::OneErr),

    /// Used in TryFrom implementations for some zome types.
    #[error("Secure primitive error: {0}")]
    SecurePrimitiveError(#[from] SecurePrimitiveError),

    /// Unexpected Internal Error.
    #[error("Other: {0}")]
    Other(String),
}

/// alias
pub type KeystoreResult<T> = Result<T, KeystoreError>;

impl std::cmp::PartialEq for KeystoreError {
    fn eq(&self, o: &Self) -> bool {
        format!("{:?}", self) == format!("{:?}", o)
    }
}

impl From<String> for KeystoreError {
    fn from(e: String) -> Self {
        KeystoreError::Other(e)
    }
}

impl From<&String> for KeystoreError {
    fn from(e: &String) -> Self {
        e.to_string().into()
    }
}

impl From<&str> for KeystoreError {
    fn from(e: &str) -> Self {
        e.to_string().into()
    }
}



================================================
File: crates/holochain_keystore/src/lair_keystore.rs
================================================
//! Keystore backed by lair_keystore_api.

use crate::*;
use ::lair_keystore::server::StandaloneServer;
use kitsune_p2p_types::dependencies::{lair_keystore_api, url2};
use lair_keystore_api::prelude::*;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::io::AsyncWriteExt;

/// Spawn a new keystore backed by lair_keystore_api.
pub async fn spawn_lair_keystore(
    connection_url: url2::Url2,
    passphrase: sodoken::BufRead,
) -> LairResult<MetaLairClient> {
    MetaLairClient::new(connection_url, passphrase).await
}

/// Spawn an in-process keystore backed by lair_keystore.
/// @param config_path - path to the lair config yaml file
pub async fn spawn_lair_keystore_in_proc(
    config_path: &PathBuf,
    passphrase: sodoken::BufRead,
) -> LairResult<MetaLairClient> {
    let config = get_config(config_path, passphrase.clone()).await?;
    let connection_url = config.connection_url.clone();

    // rather than using the in-proc server directly,
    // use the actual standalone server so we get the pid-checks, etc
    let mut server = StandaloneServer::new(config).await?;

    server.run(passphrase.clone()).await?;

    // just incase a Drop gets impld at some point...
    std::mem::forget(server);

    // now, just connect to it : )
    spawn_lair_keystore(connection_url.into(), passphrase).await
}

async fn get_config(
    config_path: &PathBuf,
    passphrase: sodoken::BufRead,
) -> LairResult<LairServerConfig> {
    match read_config(config_path).await {
        Ok(config) => Ok(config),
        Err(_) => write_config(config_path, passphrase).await,
    }
}

async fn read_config(config_path: &PathBuf) -> LairResult<LairServerConfig> {
    let bytes = tokio::fs::read(config_path).await?;

    let config = LairServerConfigInner::from_bytes(&bytes)?;

    Ok(Arc::new(config))
}

async fn write_config(
    config_path: &std::path::Path,
    passphrase: sodoken::BufRead,
) -> LairResult<LairServerConfig> {
    let lair_root = config_path
        .parent()
        .ok_or_else(|| one_err::OneErr::from("InvalidLairConfigDir"))?;

    tokio::fs::DirBuilder::new()
        .recursive(true)
        .create(&lair_root)
        .await?;

    let config = LairServerConfigInner::new(lair_root, passphrase).await?;

    let mut config_f = tokio::fs::OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(config_path)
        .await?;

    config_f.write_all(config.to_string().as_bytes()).await?;
    config_f.shutdown().await?;
    drop(config_f);

    Ok(Arc::new(config))
}



================================================
File: crates/holochain_keystore/src/lib.rs
================================================
#![deny(missing_docs)]
#![allow(clippy::needless_doctest_main)]
//! A Keystore is a secure repository of private keys. MetaLairClient is a
//! reference to a Keystore. MetaLairClient allows async generation of keypairs,
//! and usage of those keypairs, reference by the public AgentPubKey.
//!
//! # Examples
//!
//! ```rust,no_run
//! use holo_hash::AgentPubKey;
//! use std::path::Path;
//! use std::path::PathBuf;
//! use holochain_keystore::*;
//! use holochain_keystore::lair_keystore::*;
//! use holochain_serialized_bytes::prelude::*;
//!
//! #[tokio::main(flavor = "multi_thread")]
//! async fn main() {
//!     tokio::task::spawn(async move {
//!         let mut passphrase = sodoken::BufWrite::new_mem_locked(32).unwrap();
//!         passphrase.write_lock().copy_from_slice(b"passphrase");
//!
//!         let keystore = spawn_lair_keystore_in_proc(&PathBuf::from("/"), passphrase.to_read()).await.unwrap();
//!         let agent_pubkey = AgentPubKey::new_random(&keystore).await.unwrap();
//!
//!         #[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
//!         struct MyData(Vec<u8>);
//!
//!         let my_data_1 = MyData(b"signature test data 1".to_vec());
//!
//!         let signature = agent_pubkey.sign(&keystore, &my_data_1).await.unwrap();
//!
//!         assert!(agent_pubkey.verify_signature(&signature, &my_data_1).await.unwrap());
//!     }).await.unwrap();
//! }
//! ```

use holochain_serialized_bytes::prelude::*;

mod error;
pub use error::*;

mod meta_lair_client;
pub use meta_lair_client::*;

mod agent_pubkey_ext;
pub use agent_pubkey_ext::*;
use kitsune_p2p_types::dependencies::lair_keystore_api::prelude::PwHashLimits;

pub mod lair_keystore;

pub mod paths;

mod test_keystore;
pub use test_keystore::*;

#[cfg(feature = "test_utils")]
pub mod crude_mock_keystore;

/// Construct a simple in-memory in-process keystore.
///
/// # Examples
///
/// ```
/// use holo_hash::AgentPubKey;
/// use holochain_keystore::*;
/// use holochain_serialized_bytes::prelude::*;
///
/// #[tokio::main(flavor = "multi_thread")]
/// async fn main() {
///     tokio::task::spawn(async move {
///         let keystore = spawn_mem_keystore().await.unwrap();
///         let agent_pubkey = AgentPubKey::new_random(&keystore).await.unwrap();
///
///         #[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
///         struct MyData(Vec<u8>);
///
///         let my_data_1 = MyData(b"signature test data 1".to_vec());
///
///         let signature = agent_pubkey.sign(&keystore, &my_data_1).await.unwrap();
///
///         assert!(agent_pubkey.verify_signature(&signature, &my_data_1).await.unwrap());
///     }).await.unwrap();
/// }
/// ```
#[cfg(feature = "test_utils")]
pub async fn spawn_mem_keystore() -> LairResult<MetaLairClient> {
    use ::lair_keystore::dependencies::lair_keystore_api;
    use std::sync::Arc;

    // in-memory secure random passphrase
    let passphrase = sodoken::BufWrite::new_mem_locked(32)?;
    sodoken::random::bytes_buf(passphrase.clone()).await?;
    let passphrase = passphrase.to_read();

    // in-mem / in-proc config
    let config = Arc::new(
        PwHashLimits::Minimum
            .with_exec(|| {
                lair_keystore_api::config::LairServerConfigInner::new("/", passphrase.clone())
            })
            .await?,
    );

    // the keystore
    let keystore = lair_keystore_api::in_proc_keystore::InProcKeystore::new(
        config,
        lair_keystore_api::mem_store::create_mem_store_factory(),
        passphrase,
    )
    .await?;

    // return the client
    let client = keystore.new_client().await?;
    let (s, _) = tokio::sync::mpsc::unbounded_channel();
    Ok(MetaLairClient(Arc::new(parking_lot::Mutex::new(client)), s))
}



================================================
File: crates/holochain_keystore/src/meta_lair_client.rs
================================================
use holo_hash::AgentPubKey;
use holochain_zome_types::prelude::*;
use kitsune_p2p_types::dependencies::{lair_keystore_api, url2};
use lair_keystore_api::prelude::{X25519PubKey, *};
use parking_lot::Mutex;
use std::future::Future;
use std::sync::Arc;

pub use kitsune_p2p_types::dependencies::lair_keystore_api::LairResult;

const TIME_CHECK_FREQ: std::time::Duration = std::time::Duration::from_secs(5);
const CON_CHECK_STUB_TAG: &str = "HC_CON_CHK_STUB";
const RECON_INIT_MS: u64 = 100;
const RECON_MAX_MS: u64 = 5000;

type Esnd = tokio::sync::mpsc::UnboundedSender<()>;

/// Abstraction around runtime switching/upgrade of lair keystore / client.
#[derive(Clone)]
pub struct MetaLairClient(pub(crate) Arc<Mutex<LairClient>>, pub(crate) Esnd);

/// A lair error could indicate a connection problem or user error.
/// If we get any error state, we send a signal to our connection validation
/// task indicating it should check our connection health.
macro_rules! echk {
    ($esnd:ident, $code:expr) => {{
        match $code {
            Err(err) => {
                let _ = $esnd.send(());
                return Err(err);
            }
            Ok(r) => r,
        }
    }};
}

impl MetaLairClient {
    pub(crate) async fn new(
        connection_url: url2::Url2,
        passphrase: sodoken::BufRead,
    ) -> LairResult<Self> {
        use lair_keystore_api::ipc_keystore::*;
        let opts = IpcKeystoreClientOptions {
            connection_url: connection_url.clone().into(),
            passphrase: passphrase.clone(),
            exact_client_server_version_match: true,
        };

        let client = ipc_keystore_connect_options(opts).await?;
        let inner = Arc::new(Mutex::new(client));

        let (c_check_send, mut c_check_recv) = tokio::sync::mpsc::unbounded_channel();
        // initial check
        let _ = c_check_send.send(());

        // setup timeout for connection check
        {
            let c_check_send = c_check_send.clone();
            tokio::task::spawn(async move {
                loop {
                    tokio::time::sleep(TIME_CHECK_FREQ).await;
                    if c_check_send.send(()).is_err() {
                        break;
                    }
                }
            });
        }

        // setup the connection check logic
        {
            let inner = inner.clone();
            let stub_tag: Arc<str> = CON_CHECK_STUB_TAG.to_string().into();
            tokio::task::spawn(async move {
                use tokio::sync::mpsc::error::TryRecvError;
                'top_loop: while c_check_recv.recv().await.is_some() {
                    'drain_queue: loop {
                        match c_check_recv.try_recv() {
                            Ok(_) => (),
                            Err(TryRecvError::Empty) => break 'drain_queue,
                            Err(TryRecvError::Disconnected) => break 'top_loop,
                        }
                    }

                    let client = inner.lock().clone();

                    // optimistic check - most often the stub will be there
                    if client.get_entry(stub_tag.clone()).await.is_ok() {
                        continue;
                    }

                    // on the first run of a new install we need to create
                    let _ = client.new_seed(stub_tag.clone(), None, false).await;

                    // then we can exit early again
                    if client.get_entry(stub_tag.clone()).await.is_ok() {
                        continue;
                    }

                    // we couldn't fetch the stub, enter our reconnect loop
                    let mut backoff_ms = RECON_INIT_MS;
                    'reconnect: loop {
                        'drain_queue2: loop {
                            match c_check_recv.try_recv() {
                                Ok(_) => (),
                                Err(TryRecvError::Empty) => break 'drain_queue2,
                                Err(TryRecvError::Disconnected) => break 'top_loop,
                            }
                        }

                        backoff_ms *= 2;
                        if backoff_ms >= RECON_MAX_MS {
                            backoff_ms = RECON_MAX_MS;
                        }
                        tokio::time::sleep(std::time::Duration::from_millis(backoff_ms)).await;
                        let opts = IpcKeystoreClientOptions {
                            connection_url: connection_url.clone().into(),
                            passphrase: passphrase.clone(),
                            exact_client_server_version_match: true,
                        };

                        tracing::warn!("lair connection lost, attempting reconnect");

                        let client = match ipc_keystore_connect_options(opts).await {
                            Err(err) => {
                                tracing::error!(?err, "lair connect error");
                                continue 'reconnect;
                            }
                            Ok(client) => client,
                        };

                        *inner.lock() = client;

                        tracing::info!("lair reconnect success");

                        break 'reconnect;
                    }
                }
            });
        }

        Ok(MetaLairClient(inner, c_check_send))
    }

    /// Create a MetaLairClient from a LairClient
    pub async fn from_client(client: LairClient) -> LairResult<Self> {
        let inner = Arc::new(Mutex::new(client));
        let (c_check_send, _) = tokio::sync::mpsc::unbounded_channel();
        Ok(MetaLairClient(inner, c_check_send))
    }

    /// Get the raw underlying lair client instance.
    pub fn lair_client(&self) -> LairClient {
        self.0.lock().clone()
    }

    pub(crate) fn cli(&self) -> (LairClient, Esnd) {
        (self.0.lock().clone(), self.1.clone())
    }

    /// Shutdown this keystore client
    pub fn shutdown(&self) -> impl Future<Output = LairResult<()>> + 'static + Send {
        let (client, _esnd) = self.cli();
        async move { client.shutdown().await }
    }

    /// Construct a new randomized signature keypair
    pub fn new_sign_keypair_random(
        &self,
    ) -> impl Future<Output = LairResult<holo_hash::AgentPubKey>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            let tag = nanoid::nanoid!();
            let info = echk!(esnd, client.new_seed(tag.into(), None, false).await);
            let pub_key = holo_hash::AgentPubKey::from_raw_32(info.ed25519_pub_key.0.to_vec());
            Ok(pub_key)
        }
    }

    /// Generate a new signature for given keypair / data
    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, data)))]
    pub fn sign(
        &self,
        pub_key: holo_hash::AgentPubKey,
        data: Arc<[u8]>,
    ) -> impl Future<Output = LairResult<Signature>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            tokio::time::timeout(std::time::Duration::from_secs(30), async move {
                let mut pub_key_2 = [0; 32];
                pub_key_2.copy_from_slice(pub_key.get_raw_32());
                let sig = echk!(
                    esnd,
                    client.sign_by_pub_key(pub_key_2.into(), None, data).await
                );
                Ok(Signature(*sig.0))
            })
            .await
            .map_err(one_err::OneErr::new)?
        }
    }

    /// Construct a new randomized shared secret, associated with given tag
    pub fn new_shared_secret(
        &self,
        tag: Arc<str>,
    ) -> impl Future<Output = LairResult<()>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            // shared secrets are exportable
            // (it's hard to make them useful otherwise : )
            let exportable = true;
            let _info = echk!(esnd, client.new_seed(tag, None, exportable).await);
            Ok(())
        }
    }

    /// Export a shared secret identified by `tag` using box encryption.
    pub fn shared_secret_export(
        &self,
        tag: Arc<str>,
        sender_pub_key: X25519PubKey,
        recipient_pub_key: X25519PubKey,
    ) -> impl Future<Output = LairResult<([u8; 24], Arc<[u8]>)>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            Ok(echk!(
                esnd,
                client
                    .export_seed_by_tag(tag, sender_pub_key, recipient_pub_key, None)
                    .await
            ))
        }
    }

    /// Retrieve a list of the AgentPubKey values which are stored
    /// in the keystore available for use.
    pub fn list_public_keys(
        &self,
    ) -> impl Future<Output = LairResult<Vec<AgentPubKey>>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            let seed_infos = echk!(esnd, client.list_entries().await);
            Ok(seed_infos
                .into_iter()
                .filter_map(|lair_entry_info| {
                    if let LairEntryInfo::Seed { tag: _, seed_info } = lair_entry_info {
                        Some(AgentPubKey::from_raw_32(seed_info.ed25519_pub_key.to_vec()))
                    } else {
                        None
                    }
                })
                .collect())
        }
    }

    /// Import a shared secret to be indentified by `tag` using box decryption.
    pub fn shared_secret_import(
        &self,
        sender_pub_key: X25519PubKey,
        recipient_pub_key: X25519PubKey,
        nonce: [u8; 24],
        cipher: Arc<[u8]>,
        tag: Arc<str>,
    ) -> impl Future<Output = LairResult<()>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            // shared secrets are exportable
            // (it's hard to make them useful otherwise : )
            let exportable = true;
            let _info = echk!(
                esnd,
                client
                    .import_seed(
                        sender_pub_key,
                        recipient_pub_key,
                        None,
                        nonce,
                        cipher,
                        tag,
                        exportable,
                    )
                    .await
            );
            Ok(())
        }
    }

    /// Encrypt using a shared secret / xsalsa20poly1305 secretbox.
    pub fn shared_secret_encrypt(
        &self,
        tag: Arc<str>,
        data: Arc<[u8]>,
    ) -> impl Future<Output = LairResult<([u8; 24], Arc<[u8]>)>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            Ok(echk!(
                esnd,
                client.secretbox_xsalsa_by_tag(tag, None, data).await
            ))
        }
    }

    /// Decrypt using a shared secret / xsalsa20poly1305 secretbox.
    pub fn shared_secret_decrypt(
        &self,
        tag: Arc<str>,
        nonce: [u8; 24],
        cipher: Arc<[u8]>,
    ) -> impl Future<Output = LairResult<Arc<[u8]>>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            Ok(echk!(
                esnd,
                client
                    .secretbox_xsalsa_open_by_tag(tag, None, nonce, cipher)
                    .await
            ))
        }
    }

    /// Construct a new randomized encryption keypair
    pub fn new_x25519_keypair_random(
        &self,
    ) -> impl Future<Output = LairResult<X25519PubKey>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            let tag = nanoid::nanoid!();
            let info = echk!(esnd, client.new_seed(tag.into(), None, false).await);
            let pub_key = info.x25519_pub_key;
            Ok(pub_key)
        }
    }

    /// Encrypt an authenticated "box"ed message to a specific recipient.
    pub fn crypto_box_xsalsa(
        &self,
        sender_pub_key: X25519PubKey,
        recipient_pub_key: X25519PubKey,
        data: Arc<[u8]>,
    ) -> impl Future<Output = LairResult<([u8; 24], Arc<[u8]>)>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            Ok(echk!(
                esnd,
                client
                    .crypto_box_xsalsa_by_pub_key(sender_pub_key, recipient_pub_key, None, data)
                    .await
            ))
        }
    }

    /// Decrypt an authenticated "box"ed message from a specific sender.
    pub fn crypto_box_xsalsa_open(
        &self,
        sender_pub_key: X25519PubKey,
        recipient_pub_key: X25519PubKey,
        nonce: [u8; 24],
        data: Arc<[u8]>,
    ) -> impl Future<Output = LairResult<Arc<[u8]>>> + 'static + Send {
        let (client, esnd) = self.cli();
        async move {
            Ok(echk!(
                esnd,
                client
                    .crypto_box_xsalsa_open_by_pub_key(
                        sender_pub_key,
                        recipient_pub_key,
                        None,
                        nonce,
                        data,
                    )
                    .await
            ))
        }
    }

    /// Get a tls cert from lair for use in conductor
    pub fn get_or_create_tls_cert_by_tag(
        &self,
        tag: Arc<str>,
    ) -> impl Future<Output = LairResult<(CertDigest, Arc<[u8]>, sodoken::BufRead)>> + 'static + Send
    {
        let (client, esnd) = self.cli();
        async move {
            // don't echk! this top one, it may be a valid error
            let info = match client.get_entry(tag.clone()).await {
                Ok(info) => match info {
                    LairEntryInfo::WkaTlsCert { cert_info, .. } => cert_info,
                    oth => {
                        return Err(format!(
                            "invalid entry type, expecting wka tls cert: {:?}",
                            oth
                        )
                        .into())
                    }
                },
                Err(_) => {
                    let esnd = esnd.clone();
                    echk!(esnd, client.new_wka_tls_cert(tag.clone()).await)
                }
            };
            let pk = echk!(esnd, client.get_wka_tls_cert_priv_key(tag).await);

            Ok((info.digest, info.cert.to_vec().into(), pk))
        }
    }
}



================================================
File: crates/holochain_keystore/src/paths.rs
================================================
//! Paths for the keystore.

use std::path::PathBuf;

use schemars::JsonSchema;

/// Subdirectory of the data directory where the conductor stores its
/// keystore. Keep the path short so that when it's used in CI the path doesn't
/// get too long to be used as a domain socket
pub const KEYSTORE_DIRECTORY: &str = "ks";

/// Newtype to make sure we never accidentaly use or not use the keystore path.
/// Intentionally has no default value.
#[derive(
    shrinkwraprs::Shrinkwrap,
    derive_more::From,
    Debug,
    PartialEq,
    serde::Serialize,
    serde::Deserialize,
    Clone,
    JsonSchema,
)]
pub struct KeystorePath(PathBuf);



================================================
File: crates/holochain_keystore/src/test_keystore.rs
================================================
//! DANGER! This is an in-memory keystore for testing, DO NOT USE THIS IN PRODUCTION!

use crate::*;
use base64::engine::general_purpose::URL_SAFE_NO_PAD;
use base64::Engine;
use kitsune_p2p_types::dependencies::lair_keystore_api;
use std::sync::Arc;

/// First Test Agent Pub Key
pub const TEST_AGENT_PK_1: &str = "uhCAkJCuynkgVdMn_bzZ2ZYaVfygkn0WCuzfFspczxFnZM1QAyXoo";
const SEED_1: &str = "m-U7gdxW1A647O-4wkuCWOvtGGVfHEsxNScFKiL8-k8";

/// Second Test Agent Pub Key
pub const TEST_AGENT_PK_2: &str = "uhCAk39SDf7rynCg5bYgzroGaOJKGKrloI1o57Xao6S-U5KNZ0dUH";
const SEED_2: &str = "v9I5GT3xVKPcaa4uyd2pcuJromf5zv1-OaahYOLBAWY";

/// Third Test Agent Pub Key
pub const TEST_AGENT_PK_3: &str = "uhCAkwfTgZ5eDJwI6ZV5vGt-kg8cVgXvcf35XKj6HnMv4PBH8noYB";
const SEED_3: &str = "NE_0oUEATrsTR0o7JM1H8I6X6dtXg51iZvtCHAw6Fgg";

/// Fourth Test Agent Pub Key
pub const TEST_AGENT_PK_4: &str = "uhCAkQHMlYam1PRiYJCzAwQ0AUxIMwOoOvxgXS67N_YPOMj-fGx6X";
const SEED_4: &str = "2o79pTXHaK1FTPZeBiJo2lCgXW_P0ULjX_5Div_2qxU";

fn r(s: &str) -> Vec<u8> {
    URL_SAFE_NO_PAD.decode(s).unwrap()
}

fn s(s: &str) -> [u8; 32] {
    let r_ = r(s);
    let mut o = [0; 32];
    o.copy_from_slice(&r_);
    o
}

/// Construct a new test keystore with the new lair api.
pub async fn spawn_test_keystore() -> LairResult<MetaLairClient> {
    // in-memory secure random passphrase
    let passphrase = sodoken::BufWrite::new_mem_locked(32)?;
    sodoken::random::bytes_buf(passphrase.clone()).await?;

    // in-mem / in-proc config
    let config = Arc::new(
        PwHashLimits::Minimum
            .with_exec(|| {
                lair_keystore_api::config::LairServerConfigInner::new("/", passphrase.to_read())
            })
            .await?,
    );

    // the keystore
    let keystore = lair_keystore_api::in_proc_keystore::InProcKeystore::new(
        config,
        lair_keystore_api::mem_store::create_mem_store_factory(),
        passphrase.to_read(),
    )
    .await?;

    // get the store and inject test seeds
    let store = keystore.store().await?;
    store
        .insert_seed(s(SEED_1).into(), TEST_AGENT_PK_1.into(), false)
        .await?;
    store
        .insert_seed(s(SEED_2).into(), TEST_AGENT_PK_2.into(), false)
        .await?;
    store
        .insert_seed(s(SEED_3).into(), TEST_AGENT_PK_3.into(), false)
        .await?;
    store
        .insert_seed(s(SEED_4).into(), TEST_AGENT_PK_4.into(), false)
        .await?;

    // return the client
    let client = keystore.new_client().await?;
    let (s, _) = tokio::sync::mpsc::unbounded_channel();
    Ok(MetaLairClient(Arc::new(parking_lot::Mutex::new(client)), s))
}

/// Generate a test keystore pre-populated with a couple of test key-pairs.
pub fn test_keystore() -> MetaLairClient {
    holochain_util::tokio_helper::block_on(
        async move { spawn_test_keystore().await.unwrap() },
        std::time::Duration::from_secs(5),
    )
    .expect("timeout elapsed")
}



================================================
File: crates/holochain_keystore/src/bin/test-keystore-srv.rs
================================================
use kitsune_p2p_types::dependencies::lair_keystore_api;
use lair_keystore_api::dependencies::hc_seed_bundle;
use lair_keystore_api::ipc_keystore::*;
use lair_keystore_api::prelude::*;
use std::sync::Arc;

fn load_conf(conf_path: &std::path::Path) -> LairResult<LairServerConfig> {
    let bytes = std::fs::read(conf_path)?;
    let inner = LairServerConfigInner::from_bytes(&bytes)?;
    Ok(Arc::new(inner))
}

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    let mut arg_iter = std::env::args_os();
    arg_iter.next().unwrap();
    let path = std::path::PathBuf::from(arg_iter.next().expect("require lair path"));

    let mut conf_path = path.clone();
    conf_path.push("lair-config.yaml");

    // set up a passphrase
    let passphrase = sodoken::BufRead::from(&b"passphrase"[..]);

    // create the config for the test server
    let config = match load_conf(&conf_path) {
        Ok(config) => config,
        Err(_) => {
            let conf = Arc::new(
                hc_seed_bundle::PwHashLimits::Minimum
                    .with_exec(|| LairServerConfigInner::new(&path, passphrase.clone()))
                    .await
                    .unwrap(),
            );
            std::fs::write(conf_path, conf.to_string().as_bytes()).unwrap();
            conf
        }
    };

    // create an in-process keystore with an in-memory store
    let keystore = IpcKeystoreServer::new(
        config,
        lair_keystore_api::mem_store::create_mem_store_factory(),
        passphrase.clone(),
    )
    .await
    .unwrap();

    let config = keystore.get_config();
    println!("{}", config);

    println!("OK");

    futures::future::pending::<()>().await;
}



================================================
File: crates/holochain_keystore/tests/test_reconnect.rs
================================================
use assert_cmd::cargo::CommandCargoExt;
use holochain_keystore::lair_keystore::*;
use holochain_keystore::MetaLairClient;
use kitsune_p2p_types::dependencies::url2;
use std::io::BufRead;
use std::sync::Arc;

struct Proc(std::process::Child);

impl Drop for Proc {
    fn drop(&mut self) {
        self.0.kill().unwrap();
        self.0.wait().unwrap();
    }
}

struct Cli(MetaLairClient);

impl Drop for Cli {
    fn drop(&mut self) {
        let fut = self.0.shutdown();
        tokio::task::spawn(fut);
    }
}

impl std::ops::Deref for Cli {
    type Target = MetaLairClient;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

fn run_test_keystore(dir: &std::path::Path) -> (Proc, url2::Url2) {
    let mut cmd = std::process::Command::cargo_bin("test-keystore-srv").unwrap();
    cmd.arg(dir).stdout(std::process::Stdio::piped());

    println!("{:?}", cmd);

    let mut cmd = cmd.spawn().unwrap();

    let mut yaml = String::new();
    let mut lines = std::io::BufReader::new(cmd.stdout.take().unwrap()).lines();
    for line in lines.by_ref() {
        let line = line.unwrap();
        if line == "OK" {
            break;
        }
        yaml.push_str(&line);
        yaml.push('\n');
    }

    tokio::task::spawn(async move { for _line in lines {} });

    #[derive(Debug, serde::Deserialize)]
    #[serde(rename_all = "camelCase")]
    struct Conf {
        connection_url: url2::Url2,
    }

    let conf: Conf = serde_yaml::from_str(&yaml).unwrap();

    (Proc(cmd), conf.connection_url)
}

async fn connect_cli(connection_url: url2::Url2) -> Cli {
    let passphrase = sodoken::BufRead::from(&b"passphrase"[..]);
    let cli = spawn_lair_keystore(connection_url, passphrase)
        .await
        .unwrap();

    Cli(cli)
}

#[tokio::test(flavor = "multi_thread")]
#[cfg_attr(
    target_os = "macos",
    ignore = "path too long error, broken on macos inside a Nix shell"
)]
async fn test_reconnect() {
    let tmpdir = tempdir::TempDir::new("lair keystore test").unwrap();
    let tag: Arc<str> = "test-tag".into();

    let start = std::time::Instant::now();

    let (proc, url) = run_test_keystore(tmpdir.path());
    let cli = connect_cli(url).await;
    cli.get_or_create_tls_cert_by_tag(tag.clone())
        .await
        .unwrap();

    println!("launch to first test call in {:?}", start.elapsed());

    drop(proc);

    tokio::time::sleep(std::time::Duration::from_millis(100)).await;

    assert!(cli
        .get_or_create_tls_cert_by_tag(tag.clone())
        .await
        .is_err());

    let (proc, _url) = run_test_keystore(tmpdir.path());

    let mut all_good = false;

    for _ in 0..10 {
        tokio::time::sleep(std::time::Duration::from_millis(100)).await;
        if cli.get_or_create_tls_cert_by_tag(tag.clone()).await.is_ok() {
            all_good = true;
            break;
        }
    }

    drop(cli);
    drop(proc);

    if !all_good {
        panic!("Reconnect was never successful");
    }
}



================================================
File: crates/holochain_metrics/README.md
================================================
# holochain_metrics

metrics helpers



================================================
File: crates/holochain_metrics/Cargo.toml
================================================
[package]
name = "holochain_metrics"
version = "0.5.0-dev.1"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"
description = "metrics helpers"
license = "Apache-2.0"
documentation = "https://docs.rs/holochain_metrics"
repository = "https://github.com/holochain/holochain"

# reminder - do not use workspace deps
[dependencies]
influxive = { version = "=0.0.4-alpha.1", optional = true }
opentelemetry_api = { version = "=0.20.0", features = ["metrics"] }
tracing = "0.1.37"

[lints]
workspace = true

[features]
default = ["influxive"]



================================================
File: crates/holochain_metrics/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0-dev.0

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

- Change the license from MIT to Apache-2.0.

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2



================================================
File: crates/holochain_metrics/src/lib.rs
================================================
#![deny(missing_docs)]
#![deny(unsafe_code)]
//! Initialize holochain metrics.
//! This crate should only be used in binaries to initialize the actual
//! metrics collection. Libraries should just use the opentelemetry_api
//! to report metrics if any collector has been initialized.
//!
//! ## Environment Variables
//!
//! When calling `HolochainMetricsConfig::new(&path).init()`, the actual
//! metrics instance that will be created is largely controlled by
//! the existence of environment variables.
//!
//! Curently, by default, the Null metrics collector will be used, meaning
//! metrics will not be collected, and all metrics operations will be no-ops.
//!
//! If you wish to enable metrics, the current options are:
//!
//! - InfluxDB as a zero-config child-process.
//!   - Enable via environment variable: `HOLOCHAIN_INFLUXIVE_CHILD_SVC=1`
//!   - The binaries `influxd` and `influx` will be downloaded and verified
//!     before automatically being run as a child process, and set up
//!     to be reported to. The InfluxDB UI will be available on a randomly
//!     assigned port (currently only reported in the trace logging).
//! - InfluxDB as a pre-existing system process.
//!   - Enable via environment variable: `HOLOCHAIN_INFLUXIVE_EXTERNAL=1`
//!   - Configure via environment variables:
//!     - `HOLOCHAIN_INFLUXIVE_EXTERNAL_HOST=[my influxdb url]` where a default InfluxDB install will need `http://localhost:8086` and otherwise can be found by running `influx config` in a terminal.
//!     - `HOLOCHAIN_INFLUXIVE_EXTERNAL_BUCKET=[my influxdb bucket name]` but it's simplest to use `influxive` if you plan to import the provided dashboards.
//!     - `HOLOCHAIN_INFLUXIVE_EXTERNAL_TOKEN=[my influxdb auth token]`
//!   - The influxdb auth token must have permission to write to all buckets
//!   - Metrics will be set up to report to this already running InfluxDB.
//!
//! ## Metric Naming Conventions
//!
//! We will largely attempt to follow the guidelines for metric naming
//! enumerated at
//! [https://opentelemetry.io/docs/specs/otel/metrics/semantic_conventions/](https://opentelemetry.io/docs/specs/otel/metrics/semantic_conventions/),
//! with additional rules made to fit with our particular project.
//! We will also attempt to keep this documentation up-to-date on a best-effort
//! basis to act as an example and registry of metrics avaliable in Holochain,
//! and related support dependency crates managed by the organization.
//!
//! Generic naming convention rules:
//!
//! - Dot notation logical module hierarchy. This need not, and perhaps should
//!   not, match the rust crate/module hierarchy. As we may rearange crates
//!   and modules, but the metric names themselves should remain more
//!   consistant.
//!   - Examples:
//!     - `hc.db`
//!     - `hc.workflow.integration`
//!     - `kitsune.gossip`
//!     - `tx5.signal`
//! - A dot notation metric name or context should follow the logical module
//!   name. The thing that can be charted should be the actual metric. Related
//!   context that may want to be filtered for the chart should be attributes.
//!   For example, a "request" may have two separate metrics, "duration", and
//!   "byte.count", which both may have the filtering attribute "remote_id".
//!   - Examples
//!     - ```
//!         use opentelemetry_api::{Context, KeyValue, metrics::Unit};
//!         let req_dur = opentelemetry_api::global::meter("tx5")
//!             .f64_histogram("tx5.signal.request.duration")
//!             .with_description("tx5 signal server request duration")
//!             .with_unit(Unit::new("s"))
//!             .init();
//!         req_dur.record(&Context::new(), 0.42, &[
//!             KeyValue::new("remote_id", "abcd"),
//!         ]);
//!       ```
//!     - ```
//!         use opentelemetry_api::{Context, KeyValue, metrics::Unit};
//!         let req_size = opentelemetry_api::global::meter("tx5")
//!             .u64_histogram("tx5.signal.request.byte.count")
//!             .with_description("tx5 signal server request byte count")
//!             .with_unit(Unit::new("By"))
//!             .init();
//!         req_size.record(&Context::new(), 42, &[
//!             KeyValue::new("remote_id", "abcd"),
//!         ]);
//!       ```
//!
//! ## Metric Name Registry
//!
//! | Full Metric Name | Type | Unit (optional) | Description | Attributes |
//! | ---------------- | ---- | --------------- | ----------- | ---------- |
//! | `kitsune.peer.send.duration` | `f64_histogram` | `s` | When kitsune sends data to a remote peer. |- `remote_id`: the base64 remote peer id.<br />- `is_error`: if the send failed. |
//! | `kitsune.peer.send.byte.count` | `u64_histogram` | `By` | When kitsune sends data to a remote peer. |- `remote_id`: the base64 remote peer id.<br />- `is_error`: if the send failed. |
//! | `kitsune.gossip.generate_op_blooms.duration` | `f64_histogram` | `s` | The time taken to generate op blooms for gossip. | - `space`: The space (dna_hash representation) that gossip is being performed for.<br />- `batch_size`: The number of ops that were included in the bloom batch for this observation. |
//! | `kitsune.gossip.generate_op_region_set.duration` | `f64_histogram` | `s` | The time taken to generate op region sets for gossip. | - `space`: The space (dna_hash representation) that gossip is being performed for. |
//! | `tx5.conn.ice.send` | `u64_observable_counter` | `By` | Bytes sent on ice channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `tx5.conn.ice.recv` | `u64_observable_counter` | `By` | Bytes received on ice channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `tx5.conn.data.send` | `u64_observable_counter` | `By` | Bytes sent on data channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `tx5.conn.data.recv` | `u64_observable_counter` | `By` | Bytes received on data channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `tx5.conn.data.send.message.count` | `u64_observable_counter` | | Message count sent on data channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `tx5.conn.data.recv.message.count` | `u64_observable_counter` | | Message count received on data channel. |- `remote_id`: the base64 remote peer id.<br />- `state_uniq`: endpoint identifier.<br />- `conn_uniq`: connection identifier. |
//! | `hc.conductor.p2p_event.duration`  | `f64_histogram` | `s` | The time spent processing a p2p event. |- `dna_hash`: The DNA hash that this event is being sent on behalf of. |
//! | `hc.conductor.post_commit.duration` | `f64_histogram` | `s` | The time spent executing a post commit. |- `dna_hash`: The DNA hash that this post commit is running for.<br />- `agent`: The agent running the post commit. |
//! | `hc.conductor.workflow.duration` | `f64_histogram` | `s` | The time spent running a workflow. |- `workflow`: The name of the workflow.<br />- `dna_hash`: The DNA hash that this workflow is running for.<br />- `agent`: (optional) The agent that this workflow is running for if the workflow is cell bound. |
//! | `hc.cascade.duration` | `f64_histogram` | `s` | The time taken to execute a cascade query. | |
//! | `hc.db.pool.utilization` | `f64_gauge` | | The utilisation of connections in the pool. |- `kind`: The kind of database such as Conductor, Wasm or Dht etc.<br />- `id`: The unique identifier for this database if multiple instances can exist, such as a Dht database. |
//! | `hc.db.connections.use_time` | `f64_histogram` | `s` | The time between borrowing a connection and returning it to the pool. |- `kind`: The kind of database such as Conductor, Wasm or Dht etc.<br />- `id`: The unique identifier for this database if multiple instances can exist, such as a Dht database. |
//! | `hc.ribosome.wasm.usage` | `u64_counter` | | The metered usage of a wasm ribosome. | - `dna`: The DNA hash that this wasm is metered for.<br />- `zome`: The zome that this wasm is metered for.<br />- `fn`: The function that this wasm is metered for.<br />- `agent`: The agent that this wasm is metered for (if there is one). |

#[cfg(feature = "influxive")]
const DASH_NETWORK_STATS: &[u8] = include_bytes!("dashboards/networkstats.json");
#[cfg(feature = "influxive")]
const DASH_TX5: &[u8] = include_bytes!("dashboards/tx5.json");
#[cfg(feature = "influxive")]
const DASH_DATABASE: &[u8] = include_bytes!("dashboards/database.json");
#[cfg(feature = "influxive")]
const DASH_CONDUCTOR: &[u8] = include_bytes!("dashboards/conductor.json");

/// Configuration for holochain metrics.
pub enum HolochainMetricsConfig {
    /// Metrics are disabled.
    Disabled,

    #[cfg(feature = "influxive")]
    /// Use influxive to connect to an already running InfluxDB instance.
    /// NOTE: this means we cannot initialize any dashboards.
    InfluxiveExternal {
        /// The writer config for connecting to the external influxdb instance.
        writer_config: influxive::InfluxiveWriterConfig,

        /// The meter provider config for setting up opentelemetry.
        otel_config: influxive::InfluxiveMeterProviderConfig,

        /// The url for the external influxdb instance.
        host: String,

        /// The bucket to write to in this external influxdb instance.
        bucket: String,

        /// The authentication token to use for writing to this external
        /// influxdb instance.
        token: String,
    },

    #[cfg(feature = "influxive")]
    /// Use influxive as a child service to write metrics.
    InfluxiveChildSvc {
        /// The child service config for running the influxd server.
        child_svc_config: Box<influxive::InfluxiveChildSvcConfig>,

        /// The meter provider config for setting up opentelemetry.
        otel_config: influxive::InfluxiveMeterProviderConfig,
    },
}

impl HolochainMetricsConfig {
    /// Initialize a new default metrics config.
    ///
    /// The output of this function is largely controlled by environment
    /// variables, please see the [crate-level documentation](crate) for usage.
    pub fn new(root_path: &std::path::Path) -> Self {
        #[cfg(feature = "influxive")]
        {
            const E_CHILD_SVC: &str = "HOLOCHAIN_INFLUXIVE_CHILD_SVC";

            const E_EXTERNAL: &str = "HOLOCHAIN_INFLUXIVE_EXTERNAL";
            const E_EXTERNAL_HOST: &str = "HOLOCHAIN_INFLUXIVE_EXTERNAL_HOST";
            const E_EXTERNAL_BUCKET: &str = "HOLOCHAIN_INFLUXIVE_EXTERNAL_BUCKET";
            const E_EXTERNAL_TOKEN: &str = "HOLOCHAIN_INFLUXIVE_EXTERNAL_TOKEN";

            if std::env::var_os(E_CHILD_SVC).is_some() {
                let mut database_path = std::path::PathBuf::from(root_path);
                database_path.push("influxive");
                return Self::InfluxiveChildSvc {
                    child_svc_config: Box::new(
                        influxive::InfluxiveChildSvcConfig::default()
                            .with_database_path(Some(database_path)),
                    ),
                    otel_config: influxive::InfluxiveMeterProviderConfig::default(),
                };
            }

            if std::env::var_os(E_EXTERNAL).is_some() {
                let host = match std::env::var(E_EXTERNAL_HOST) {
                    Ok(host) => host,
                    Err(err) => {
                        tracing::error!(env = %E_EXTERNAL_HOST, ?err, "invalid");
                        return Self::Disabled;
                    }
                };
                let bucket = match std::env::var(E_EXTERNAL_BUCKET) {
                    Ok(bucket) => bucket,
                    Err(err) => {
                        tracing::error!(env = %E_EXTERNAL_BUCKET, ?err, "invalid");
                        return Self::Disabled;
                    }
                };
                let token = match std::env::var(E_EXTERNAL_TOKEN) {
                    Ok(token) => token,
                    Err(err) => {
                        tracing::error!(env = %E_EXTERNAL_TOKEN, ?err, "invalid");
                        return Self::Disabled;
                    }
                };
                return Self::InfluxiveExternal {
                    writer_config: influxive::InfluxiveWriterConfig::default(),
                    otel_config: influxive::InfluxiveMeterProviderConfig::default(),
                    host,
                    bucket,
                    token,
                };
            }
        }

        #[cfg(not(feature = "influxive"))]
        {
            let _root_path = root_path;
        }

        Self::Disabled
    }

    /// Initialize holochain metrics based on this configuration.
    pub async fn init(self) {
        match self {
            Self::Disabled => {
                tracing::info!("Running without metrics");
            }
            #[cfg(feature = "influxive")]
            Self::InfluxiveExternal {
                writer_config,
                otel_config,
                host,
                bucket,
                token,
            } => {
                Self::init_influxive_external(writer_config, otel_config, host, bucket, token);
            }
            #[cfg(feature = "influxive")]
            Self::InfluxiveChildSvc {
                child_svc_config,
                otel_config,
            } => {
                Self::init_influxive_child_svc(*child_svc_config, otel_config).await;
            }
        }
    }

    #[cfg(feature = "influxive")]
    fn init_influxive_external(
        writer_config: influxive::InfluxiveWriterConfig,
        otel_config: influxive::InfluxiveMeterProviderConfig,
        host: String,
        bucket: String,
        token: String,
    ) {
        tracing::info!(?writer_config, %host, %bucket, "initializing holochain_metrics");

        let meter_provider = influxive::influxive_external_meter_provider_token_auth(
            writer_config,
            otel_config,
            host,
            bucket,
            token,
        );

        // setup opentelemetry to use our metrics collector
        opentelemetry_api::global::set_meter_provider(meter_provider);
    }

    #[cfg(feature = "influxive")]
    async fn init_influxive_child_svc(
        child_svc_config: influxive::InfluxiveChildSvcConfig,
        otel_config: influxive::InfluxiveMeterProviderConfig,
    ) {
        tracing::info!(?child_svc_config, "initializing holochain_metrics");

        match influxive::influxive_child_process_meter_provider(child_svc_config, otel_config).await
        {
            Ok((influxive, meter_provider)) => {
                // apply templates
                if let Ok(cur) = influxive.list_dashboards().await {
                    // only initialize dashboards if the db is new
                    if cur.contains("\"dashboards\": []") {
                        if let Err(err) = influxive.apply(DASH_NETWORK_STATS).await {
                            tracing::warn!(?err, "failed to initialize network stats dashboard");
                        }
                        if let Err(err) = influxive.apply(DASH_TX5).await {
                            tracing::warn!(?err, "failed to initialize tx5 dashboard");
                        }
                        if let Err(err) = influxive.apply(DASH_DATABASE).await {
                            tracing::warn!(?err, "failed to initialize database dashboard");
                        }
                        if let Err(err) = influxive.apply(DASH_CONDUCTOR).await {
                            tracing::warn!(?err, "failed to initialize conductor dashboard");
                        }
                    }
                }

                // setup opentelemetry to use our metrics collector
                opentelemetry_api::global::set_meter_provider(meter_provider);

                tracing::info!(host = %influxive.get_host(), "influxive metrics running");
            }
            Err(err) => {
                tracing::warn!(?err, "unable to initialize local metrics");
            }
        }
    }
}



================================================
File: crates/holochain_metrics/src/dashboards/conductor.json
================================================
[{"apiVersion":"influxdata.com/v2alpha1","kind":"Dashboard","metadata":{"name":"ecstatic-benz-6ba001"},"spec":{"charts":[{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"P2P Event duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.p2p_event.duration.s\")"}],"staticLegend":{},"width":4,"xCol":"_time","yCol":"_value"},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Publish DHT ops workflow duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.workflow.duration.s\")\n  |> filter(fn: (r) => r[\"workflow\"] == \"publish_dht_ops_consumer\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":4,"xCol":"_time","yCol":"_value","yPos":4},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"App validation workflow duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.workflow.duration.s\")\n  |> filter(fn: (r) => r[\"workflow\"] == \"app_validation_consumer\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":4,"xCol":"_time","xPos":4,"yCol":"_value"},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Sys validation workflow duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.workflow.duration.s\")\n  |> filter(fn: (r) => r[\"workflow\"] == \"sys_validation_consumer\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":4,"xCol":"_time","xPos":4,"yCol":"_value","yPos":4},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Integrate DHT ops workflow duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.workflow.duration.s\")\n  |> filter(fn: (r) => r[\"workflow\"] == \"integrate_dht_ops_consumer\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":4,"xCol":"_time","xPos":8,"yCol":"_value"},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Validation receipt workflow duration","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.conductor.workflow.duration.s\")\n  |> filter(fn: (r) => r[\"workflow\"] == \"validation_receipt_consumer\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":4,"xCol":"_time","xPos":8,"yCol":"_value","yPos":4}],"name":"Holochain Conductor"}}]


================================================
File: crates/holochain_metrics/src/dashboards/database.json
================================================
[{"apiVersion":"influxdata.com/v2alpha1","kind":"Dashboard","metadata":{"name":"pedantic-kapitsa-2a5001"},"spec":{"charts":[{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Conductor db connection use time","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.connections.use_time.s\")\n  |> filter(fn: (r) => r[\"kind\"] == \"conductor\")\n  |> filter(fn: (r) => r[\"id\"] == \"Conductor\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":6,"xCol":"_time","yCol":"_value"},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"All DHT databases connection use time","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.connections.use_time.s\")\n  |> filter(fn: (r) => r[\"kind\"] == \"dht\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":6,"xCol":"_time","yCol":"_value","yPos":4},{"colors":[{"id":"0","name":"laser","type":"min","hex":"#00C9FF"},{"id":"1","name":"comet","type":"max","hex":"#9394FF","value":100}],"decimalPlaces":0,"height":4,"kind":"Gauge","name":"Conductor database load","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.pool.utilization\")\n  |> filter(fn: (r) => r[\"kind\"] == \"conductor\")\n  |> filter(fn: (r) => r[\"id\"] == \"Conductor\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"suffix":"%","width":3,"yPos":8},{"colors":[{"id":"0","name":"laser","type":"min","hex":"#00C9FF"},{"id":"1","name":"comet","type":"max","hex":"#9394FF","value":100}],"decimalPlaces":0,"height":4,"kind":"Gauge","name":"WASM database load","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.pool.utilization\")\n  |> filter(fn: (r) => r[\"kind\"] == \"wasm\")\n  |> filter(fn: (r) => r[\"id\"] == \"Wasm\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"suffix":"%","width":3,"xPos":3,"yPos":8},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Wasm db connection use time","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.connections.use_time.s\")\n  |> filter(fn: (r) => r[\"kind\"] == \"wasm\")\n  |> filter(fn: (r) => r[\"id\"] == \"Wasm\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":6,"xCol":"_time","xPos":6,"yCol":"_value"},{"axes":[{"name":"x"},{"label":"seconds","name":"y"}],"binSize":10,"colors":[{"hex":"#000000"},{"hex":"#E69F00"},{"hex":"#56B4E9"},{"hex":"#009E73"},{"hex":"#F0E442"},{"hex":"#0072B2"},{"hex":"#D55E00"},{"hex":"#CC79A7"}],"height":4,"kind":"Heatmap","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"All authored databases connection use time","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.connections.use_time.s\")\n  |> filter(fn: (r) => r[\"kind\"] == \"authored\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"width":6,"xCol":"_time","xPos":6,"yCol":"_value","yPos":4},{"colors":[{"id":"0","name":"laser","type":"min","hex":"#00C9FF"},{"id":"1","name":"comet","type":"max","hex":"#9394FF","value":100}],"decimalPlaces":0,"height":4,"kind":"Gauge","name":"DHT databases average load","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.pool.utilization\")\n  |> filter(fn: (r) => r[\"kind\"] == \"dht\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"suffix":"%","width":3,"xPos":6,"yPos":8},{"colors":[{"id":"0","name":"laser","type":"min","hex":"#00C9FF"},{"id":"1","name":"comet","type":"max","hex":"#9394FF","value":100}],"decimalPlaces":0,"height":4,"kind":"Gauge","name":"Authored databases average load","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"hc.db.pool.utilization\")\n  |> filter(fn: (r) => r[\"kind\"] == \"authored\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")"}],"staticLegend":{},"suffix":"%","width":3,"xPos":9,"yPos":8}],"name":"Holochain Database"}}]


================================================
File: crates/holochain_metrics/src/dashboards/networkstats.json
================================================
[{"apiVersion":"influxdata.com/v2alpha1","kind":"Dashboard","metadata":{"name":"righteous-mayer-011001"},"spec":{"charts":[{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"B0X5KMQ9gPqMQvl06Yl3t","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"Qfh0jIF7eWvHvJtknUcGi","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"I2J-beJjByUE0fh2g-5oU","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"SucessCallTime","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"kitsune.peer.send.duration.s\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> filter(fn: (r) => r[\"is_error\"] == \"false\")"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","yCol":"_value"},{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"B0X5KMQ9gPqMQvl06Yl3t","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"Qfh0jIF7eWvHvJtknUcGi","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"I2J-beJjByUE0fh2g-5oU","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"ErrorCallTime","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"kitsune.peer.send.duration.s\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")\n  |> filter(fn: (r) => r[\"is_error\"] == \"true\")"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","xPos":4,"yCol":"_value"},{"axes":[{"name":"x"}],"binCount":30,"colors":[{"id":"B0X5KMQ9gPqMQvl06Yl3t","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"Qfh0jIF7eWvHvJtknUcGi","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"I2J-beJjByUE0fh2g-5oU","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"fillColumns":["_start","_stop","_field","_measurement","is_error","remote_id"],"height":4,"kind":"Histogram","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"CallSizeHistogram","position":"stacked","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"kitsune.peer.send.byte.count.By\")\n  |> filter(fn: (r) => r[\"_field\"] == \"value\")"}],"staticLegend":{},"width":4,"xCol":"_value","xPos":8}],"name":"NetworkStats"}}]



================================================
File: crates/holochain_metrics/src/dashboards/tx5.json
================================================
[{"apiVersion":"influxdata.com/v2alpha1","kind":"Dashboard","metadata":{"name":"friendly-newton-bcb001"},"spec":{"charts":[{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"19KQVrepsKKiJharUsQRL","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"FhLy7t4V0RZX2QmEFw08Y","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"v8IB6UJ2wuRwD8QxSsVxS","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Message Count Received","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"tx5.conn.data.recv.message.count\")\n  |> drop(columns: [\"conn_uniq\"])  \n  |> sort(columns: [\"_time\"], desc: false)"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","yCol":"_value"},{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"19KQVrepsKKiJharUsQRL","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"FhLy7t4V0RZX2QmEFw08Y","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"v8IB6UJ2wuRwD8QxSsVxS","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Bytes Received","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"tx5.conn.data.recv.By\")\n  |> drop(columns: [\"conn_uniq\"])  \n  |> sort(columns: [\"_time\"], desc: false)"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","yCol":"_value","yPos":4},{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"19KQVrepsKKiJharUsQRL","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"FhLy7t4V0RZX2QmEFw08Y","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"v8IB6UJ2wuRwD8QxSsVxS","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Message Count Sent","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"tx5.conn.data.send.message.count\")\n  |> drop(columns: [\"conn_uniq\"])  \n  |> sort(columns: [\"_time\"], desc: false)"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","xPos":4,"yCol":"_value"},{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"19KQVrepsKKiJharUsQRL","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"FhLy7t4V0RZX2QmEFw08Y","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"v8IB6UJ2wuRwD8QxSsVxS","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Bytes Sent","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"tx5.conn.data.send.By\")\n  |> drop(columns: [\"conn_uniq\"])  \n  |> sort(columns: [\"_time\"], desc: false)"}],"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","xPos":4,"yCol":"_value","yPos":4},{"axes":[{"base":"10","name":"x","scale":"linear"},{"base":"10","name":"y","scale":"linear"}],"colorizeRows":true,"colors":[{"id":"O3aLWnkE56AGRCu6zCxUy","name":"Nineteen Eighty Four","type":"scale","hex":"#31C0F6"},{"id":"kzi9LOJHiS97efftC_3Kw","name":"Nineteen Eighty Four","type":"scale","hex":"#A500A5"},{"id":"dCwK-lfuiQaxty6sIoicN","name":"Nineteen Eighty Four","type":"scale","hex":"#FF7E27"}],"geom":"line","height":4,"hoverDimension":"auto","kind":"Xy","legendColorizeRows":true,"legendOpacity":1,"legendOrientationThreshold":100000000,"name":"Connection Count","opacity":1,"orientationThreshold":100000000,"position":"overlaid","queries":[{"query":"from(bucket: \"influxive\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"tx5.endpoint.conn.count\")"}],"shade":true,"staticLegend":{"colorizeRows":true,"opacity":1,"orientationThreshold":100000000,"widthRatio":1},"width":4,"widthRatio":1,"xCol":"_time","xPos":8,"yCol":"_value"}],"description":"Tx5 Metrics Dashboard","name":"Tx5"}}]


================================================
File: crates/holochain_nonce/README.md
================================================
# holochain_nonce

License: CAL-1.0



================================================
File: crates/holochain_nonce/Cargo.toml
================================================
[package]
name = "holochain_nonce"
version = "0.5.0-dev.2"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"
description = "This crate is for generating nonces."
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_nonces"

# reminder - do not use workspace deps
[dependencies]
getrandom = { version = "0.2.7", default-features = false, features = ["std"] }
holochain_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
holochain_secure_primitive = { version = "^0.5.0-dev.1", path = "../holochain_secure_primitive", default-features = false }

[lints]
workspace = true



================================================
File: crates/holochain_nonce/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

- Change the license from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

- New package to extract `fresh_nonce()` util out of `holochain_state`



================================================
File: crates/holochain_nonce/src/lib.rs
================================================
use holochain_secure_primitive::secure_primitive;
use holochain_timestamp::Timestamp;
use std::{error::Error, time::Duration};

/// 256 Bit generic nonce.
#[derive(Clone, Copy)]
pub struct Nonce256Bits([u8; 32]);
secure_primitive!(Nonce256Bits, 32);

impl Nonce256Bits {
    pub fn into_inner(self) -> [u8; 32] {
        self.0
    }
}

/// Rather arbitrary but we expire nonces after 5 mins.
pub const FRESH_NONCE_EXPIRES_AFTER: Duration = Duration::from_secs(60 * 5);

/// Generate a fresh nonce.
///
/// The nonce will be valid from the given `now` timestamp until `now` + [FRESH_NONCE_EXPIRES_AFTER].
/// A new nonce and the expiry are returned as a tuple.
///
/// Note: the expiry isn't managed by this function. It's up to the caller to enforce the expiry
/// time of the nonce.
pub fn fresh_nonce(
    now: Timestamp,
) -> Result<(Nonce256Bits, Timestamp), Box<dyn Error + std::marker::Send + Sync>> {
    let mut bytes = [0; 32];
    getrandom::getrandom(&mut bytes)?;
    let nonce = Nonce256Bits::from(bytes);
    let expires: Timestamp = (now + FRESH_NONCE_EXPIRES_AFTER)?;
    Ok((nonce, expires))
}

#[cfg(test)]
pub mod test {
    use holochain_timestamp::Timestamp;

    use crate::{fresh_nonce, FRESH_NONCE_EXPIRES_AFTER};

    #[test]
    fn test_fresh_nonce() {
        let now = Timestamp::now();
        let (nonce, expires) = fresh_nonce(now).unwrap();
        let (nonce_2, expires_2) = fresh_nonce(now).unwrap();
        assert!(nonce != nonce_2);
        assert_eq!(expires, expires_2);
        assert_eq!(expires, (now + FRESH_NONCE_EXPIRES_AFTER).unwrap());
    }
}



================================================
File: crates/holochain_p2p/README.md
================================================
# holochain_p2p

[![Project](https://img.shields.io/badge/project-holochain-blue.svg?style=flat-square)](http://holochain.org/)
[![Forum](https://img.shields.io/badge/chat-forum%2eholochain%2enet-blue.svg?style=flat-square)](https://forum.holochain.org)
[![Chat](https://img.shields.io/badge/chat-chat%2eholochain%2enet-blue.svg?style=flat-square)](https://chat.holochain.org)

[![Twitter Follow](https://img.shields.io/twitter/follow/holochain.svg?style=social&label=Follow)](https://twitter.com/holochain)
License: [![License: CAL 1.0](https://img.shields.io/badge/License-CAL%201.0-blue.svg)](https://github.com/holochain/cryptographic-autonomy-license)

Current version: 0.0.1

holochain specific wrapper around more generic p2p module

## Contribute
Holochain is an open source project.  We welcome all sorts of participation and are actively working on increasing surface area to accept it.  Please see our [contributing guidelines](/CONTRIBUTING.md) for our general practices and protocols on participating in the community, as well as specific expectations around things like code formatting, testing practices, continuous integration, etc.

* Connect with us on our [forum](https://forum.holochain.org)

## License
 [![License: CAL 1.0](https://img.shields.io/badge/License-CAL-1.0-blue.svg)](https://github.com/holochain/cryptographic-autonomy-license)

Copyright (C) 2019 - 2024, Holochain Foundation

This program is free software: you can redistribute it and/or modify it under the terms of the license
provided in the LICENSE file (CAL-1.0).  This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE.



================================================
File: crates/holochain_p2p/Cargo.toml
================================================
[package]
name = "holochain_p2p"
version = "0.5.0-dev.21"
description = "holochain specific wrapper around more generic p2p module"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_p2p"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
async-trait = "0.1"
derive_more = "0.99"
fixt = { path = "../fixt", version = "^0.5.0-dev.1" }
futures = "0.3"
ghost_actor = "0.3.0-alpha.6"
holo_hash = { version = "^0.5.0-dev.7", path = "../holo_hash" }
holochain_chc = { version = "^0.2.0-dev.21", path = "../holochain_chc" }
holochain_keystore = { version = "^0.5.0-dev.20", path = "../holochain_keystore" }
holochain_serialized_bytes = "=0.0.55"
holochain_types = { version = "^0.5.0-dev.21", path = "../holochain_types" }
holochain_zome_types = { version = "^0.5.0-dev.17", path = "../holochain_zome_types" }
kitsune_p2p = { version = "^0.5.0-dev.13", path = "../kitsune_p2p/kitsune_p2p" }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../kitsune_p2p/types" }
holochain_nonce = { version = "^0.5.0-dev.2", path = "../holochain_nonce" }
mockall = "0.11.3"
holochain_trace = { version = "^0.5.0-dev.1", path = "../holochain_trace" }
serde = { version = "1.0", features = ["derive"] }
serde_json = { version = "1.0.51", features = ["preserve_order"] }
thiserror = "1.0.22"
tokio = { version = "1.27", features = ["full"] }
tokio-stream = "0.1"

kitsune2_api = "0.0.1-alpha.1"
bytes = "1.10"
holochain_sqlite = { version = "^0.5.0-dev.19", path = "../holochain_sqlite" }

[dev-dependencies]
holochain_p2p = { path = ".", features = ["test_utils"] }
holochain_sqlite = { version = "^0.5.0-dev.19", path = "../holochain_sqlite", features = [
  "test_utils",
] }

[lints]
workspace = true

[features]

test_utils = [
  "holochain_zome_types/test_utils",
  "holochain_zome_types/fixturators",
  "holochain_types/test_utils",
  "kitsune_p2p/test_utils",
  "ghost_actor/test_utils",
]

mock_network = ["test_utils", "kitsune_p2p/mock_network"]

sqlite-encrypted = [
  "holo_hash/sqlite-encrypted",
  "holochain_keystore/sqlite-encrypted",
  "kitsune_p2p/sqlite-encrypted",
  "kitsune_p2p_types/sqlite-encrypted",
]
sqlite = [
  "holo_hash/sqlite",
  "holochain_keystore/sqlite",
  "kitsune_p2p/sqlite",
  "kitsune_p2p_types/sqlite",
]

# Enables tracing instrumentation 
# (we experience segfaults in some tests if there is too much instrumentation)
instrument = []



================================================
File: crates/holochain_p2p/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.21

## 0.5.0-dev.20

## 0.5.0-dev.19

- Add implementation of Kitsune2 PeerMetaStore.

## 0.5.0-dev.18

## 0.5.0-dev.17

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.28

## 0.4.0-dev.27

## 0.4.0-dev.26

## 0.4.0-dev.25

## 0.4.0-dev.24

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.46

## 0.3.0-beta-dev.45

## 0.3.0-beta-dev.44

## 0.3.0-beta-dev.43

- Add `GenericNetwork` type that allows for mocking a network using \[`MockHolochainP2pDnaT`\].

## 0.3.0-beta-dev.42

## 0.3.0-beta-dev.41

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

- Change the license from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.7

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.69

## 0.0.68

## 0.0.67

## 0.0.66

## 0.0.65

## 0.0.64

## 0.0.63

## 0.0.62

## 0.0.61

## 0.0.60

## 0.0.59

## 0.0.58

## 0.0.57

## 0.0.56

## 0.0.55

## 0.0.54

## 0.0.53

## 0.0.52

## 0.0.51

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

## 0.0.43

## 0.0.42

## 0.0.41

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

## 0.0.21

## 0.0.20

## 0.0.19

## 0.0.18

## 0.0.17

- BREAKING: Wire message `CallRemote` Takes `from_agent`. [\#1091](https://github.com/holochain/holochain/pull/1091)

## 0.0.16

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/holochain_p2p/src/lib.rs
================================================
#![deny(missing_docs)]
//! holochain specific wrapper around more generic p2p module

use holo_hash::*;
use holochain_chc::ChcImpl;
use holochain_serialized_bytes::prelude::*;
use holochain_types::prelude::*;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::OpHashSized;
use mockall::automock;
use std::sync::Arc;

mod types;
pub use types::actor::FetchContextExt;
pub use types::actor::HolochainP2pRef;
pub use types::actor::HolochainP2pSender;
pub use types::AgentPubKeyExt; // why is this not included by * above???
pub use types::*;

mod spawn;
use ghost_actor::dependencies::tracing;
use ghost_actor::dependencies::tracing_futures::Instrument;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
pub use spawn::*;
#[cfg(feature = "test_utils")]
pub use test::stub_network;
#[cfg(feature = "test_utils")]
pub use test::HolochainP2pDnaFixturator;

pub use kitsune_p2p;

mod peer_meta_store;
pub use peer_meta_store::*;

#[automock]
#[allow(clippy::too_many_arguments)]
#[async_trait::async_trait]
/// A wrapper around HolochainP2pSender that partially applies the dna_hash / agent_pub_key.
/// I.e. a sender that is tied to a specific cell.
pub trait HolochainP2pDnaT: Send + Sync + 'static {
    /// owned getter
    fn dna_hash(&self) -> DnaHash;

    /// The p2p module must be informed at runtime which dna/agent pairs it should be tracking.
    async fn join(
        &self,
        agent: AgentPubKey,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<crate::dht::Arq>,
    ) -> actor::HolochainP2pResult<()>;

    /// If a cell is disabled, we'll need to \"leave\" the network module as well.
    async fn leave(&self, agent: AgentPubKey) -> actor::HolochainP2pResult<()>;

    /// Invoke a zome function on a remote node (if you have been granted the capability).
    #[allow(clippy::too_many_arguments)]
    async fn call_remote(
        &self,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> actor::HolochainP2pResult<SerializedBytes>;

    /// Invoke a zome function on a remote node (if you have been granted the capability).
    /// This is a fire-and-forget operation, a best effort will be made
    /// to forward the signal, but if the conductor network is overworked
    /// it may decide not to deliver some of the signals.
    async fn send_remote_signal(
        &self,
        to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
    ) -> actor::HolochainP2pResult<()>;

    /// Publish data to the correct neighborhood.
    #[allow(clippy::ptr_arg)]
    async fn publish(
        &self,
        request_validation_receipt: bool,
        countersigning_session: bool,
        basis_hash: holo_hash::OpBasis,
        source: AgentPubKey,
        op_hash_list: Vec<OpHashSized>,
        timeout_ms: Option<u64>,
        reflect_ops: Option<Vec<DhtOp>>,
    ) -> actor::HolochainP2pResult<()>;

    /// Publish a countersigning op.
    async fn publish_countersign(
        &self,
        flag: bool,
        basis_hash: holo_hash::OpBasis,
        op: DhtOp,
    ) -> actor::HolochainP2pResult<()>;

    /// Get an entry from the DHT.
    async fn get(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetOptions,
    ) -> actor::HolochainP2pResult<Vec<WireOps>>;

    /// Get metadata from the DHT.
    async fn get_meta(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetMetaOptions,
    ) -> actor::HolochainP2pResult<Vec<MetadataSet>>;

    /// Get links from the DHT.
    async fn get_links(
        &self,
        link_key: WireLinkKey,
        options: actor::GetLinksOptions,
    ) -> actor::HolochainP2pResult<Vec<WireLinkOps>>;

    /// Get a count of links from the DHT.
    async fn count_links(
        &self,
        query: WireLinkQuery,
    ) -> actor::HolochainP2pResult<CountLinksResponse>;

    /// Get agent activity from the DHT.
    async fn get_agent_activity(
        &self,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: actor::GetActivityOptions,
    ) -> actor::HolochainP2pResult<Vec<AgentActivityResponse>>;

    /// Get agent activity deterministically from the DHT.
    async fn must_get_agent_activity(
        &self,
        author: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> actor::HolochainP2pResult<Vec<MustGetAgentActivityResponse>>;

    /// Send a validation receipt to a remote node.
    async fn send_validation_receipts(
        &self,
        to_agent: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> actor::HolochainP2pResult<()>;

    /// Check if an agent is an authority for a hash.
    async fn authority_for_hash(
        &self,
        basis: holo_hash::OpBasis,
    ) -> actor::HolochainP2pResult<bool>;

    /// Messages between agents driving a countersigning session.
    async fn countersigning_session_negotiation(
        &self,
        agents: Vec<AgentPubKey>,
        message: event::CountersigningSessionNegotiationMessage,
    ) -> actor::HolochainP2pResult<()>;

    /// New data has been integrated and is ready for gossiping.
    async fn new_integrated_data(&self) -> actor::HolochainP2pResult<()>;

    /// Get the storage arcs of the agents currently in this space.
    async fn storage_arcs(&self) -> actor::HolochainP2pResult<Vec<kitsune2_api::DhtArc>>;

    /// Access to the specified CHC
    fn chc(&self) -> Option<ChcImpl>;
}

/// A wrapper around HolochainP2pSender that partially applies the dna_hash / agent_pub_key.
/// I.e. a sender that is tied to a specific cell.
#[derive(Clone)]
pub struct HolochainP2pDna {
    sender: ghost_actor::GhostSender<actor::HolochainP2p>,
    dna_hash: Arc<DnaHash>,
    chc: Option<ChcImpl>,
}

impl From<HolochainP2pDna> for GenericNetwork {
    fn from(value: HolochainP2pDna) -> Self {
        Arc::new(value)
    }
}

#[async_trait::async_trait]
impl HolochainP2pDnaT for HolochainP2pDna {
    /// owned getter
    fn dna_hash(&self) -> DnaHash {
        (*self.dna_hash).clone()
    }

    /// The p2p module must be informed at runtime which dna/agent pairs it should be tracking.
    async fn join(
        &self,
        agent: AgentPubKey,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<crate::dht::Arq>,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .join(
                (*self.dna_hash).clone(),
                agent,
                maybe_agent_info,
                initial_arq,
            )
            .await
    }

    /// If a cell is disabled, we'll need to \"leave\" the network module as well.
    async fn leave(&self, agent: AgentPubKey) -> actor::HolochainP2pResult<()> {
        self.sender.leave((*self.dna_hash).clone(), agent).await
    }

    /// Invoke a zome function on a remote node (if you have been granted the capability).
    async fn call_remote(
        &self,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> actor::HolochainP2pResult<SerializedBytes> {
        self.sender
            .call_remote(
                (*self.dna_hash).clone(),
                to_agent,
                zome_call_params_serialized,
                signature,
            )
            .await
    }

    /// Invoke a zome function on a remote node (if you have been granted the capability).
    /// This is a fire-and-forget operation, a best effort will be made
    /// to forward the signal, but if the conductor network is overworked
    /// it may decide not to deliver some of the signals.
    async fn send_remote_signal(
        &self,
        to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .send_remote_signal((*self.dna_hash).clone(), to_agent_list)
            .await
    }

    /// Publish data to the correct neighborhood.
    async fn publish(
        &self,
        request_validation_receipt: bool,
        countersigning_session: bool,
        basis_hash: holo_hash::OpBasis,
        source: AgentPubKey,
        op_hash_list: Vec<OpHashSized>,
        timeout_ms: Option<u64>,
        reflect_ops: Option<Vec<DhtOp>>,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .publish(
                (*self.dna_hash).clone(),
                request_validation_receipt,
                countersigning_session,
                basis_hash,
                source,
                op_hash_list,
                timeout_ms,
                reflect_ops,
            )
            .await
    }

    /// Publish a countersigning op.
    async fn publish_countersign(
        &self,
        flag: bool,
        basis_hash: holo_hash::OpBasis,
        op: DhtOp,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .publish_countersign((*self.dna_hash).clone(), flag, basis_hash, op)
            .await
    }

    /// Get [`ChainOp::StoreRecord`] or [`ChainOp::StoreEntry`] from the DHT.
    async fn get(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetOptions,
    ) -> actor::HolochainP2pResult<Vec<WireOps>> {
        self.sender
            .get((*self.dna_hash).clone(), dht_hash, options)
            .instrument(tracing::debug_span!("HolochainP2p::get"))
            .await
    }

    /// Get metadata from the DHT.
    async fn get_meta(
        &self,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetMetaOptions,
    ) -> actor::HolochainP2pResult<Vec<MetadataSet>> {
        self.sender
            .get_meta((*self.dna_hash).clone(), dht_hash, options)
            .await
    }

    /// Get links from the DHT.
    async fn get_links(
        &self,
        link_key: WireLinkKey,
        options: actor::GetLinksOptions,
    ) -> actor::HolochainP2pResult<Vec<WireLinkOps>> {
        self.sender
            .get_links((*self.dna_hash).clone(), link_key, options)
            .await
    }

    /// Get a count of links from the DHT.
    async fn count_links(
        &self,
        query: WireLinkQuery,
    ) -> actor::HolochainP2pResult<CountLinksResponse> {
        self.sender
            .count_links((*self.dna_hash).clone(), query)
            .await
    }

    /// Get agent activity from the DHT.
    async fn get_agent_activity(
        &self,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: actor::GetActivityOptions,
    ) -> actor::HolochainP2pResult<Vec<AgentActivityResponse>> {
        self.sender
            .get_agent_activity((*self.dna_hash).clone(), agent, query, options)
            .await
    }

    async fn must_get_agent_activity(
        &self,
        author: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> actor::HolochainP2pResult<Vec<MustGetAgentActivityResponse>> {
        self.sender
            .must_get_agent_activity((*self.dna_hash).clone(), author, filter)
            .await
    }

    /// Send a validation receipt to a remote node.
    async fn send_validation_receipts(
        &self,
        to_agent: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .send_validation_receipts((*self.dna_hash).clone(), to_agent, receipts)
            .await
    }

    /// Check if an agent is an authority for a hash.
    async fn authority_for_hash(
        &self,
        dht_hash: holo_hash::OpBasis,
    ) -> actor::HolochainP2pResult<bool> {
        self.sender
            .authority_for_hash((*self.dna_hash).clone(), dht_hash)
            .await
    }

    async fn countersigning_session_negotiation(
        &self,
        agents: Vec<AgentPubKey>,
        message: event::CountersigningSessionNegotiationMessage,
    ) -> actor::HolochainP2pResult<()> {
        self.sender
            .countersigning_session_negotiation((*self.dna_hash).clone(), agents, message)
            .await
    }

    async fn new_integrated_data(&self) -> actor::HolochainP2pResult<()> {
        self.sender
            .new_integrated_data((*self.dna_hash).clone())
            .await
    }

    async fn storage_arcs(&self) -> HolochainP2pResult<Vec<kitsune2_api::DhtArc>> {
        self.sender.storage_arcs((*self.dna_hash).clone()).await
    }

    fn chc(&self) -> Option<ChcImpl> {
        self.chc.clone()
    }
}

use crate::actor::HolochainP2pResult;
pub use kitsune_p2p::dht;
pub use kitsune_p2p::dht_arc;

#[allow(unused)]
#[cfg(any(test, feature = "test_utils"))]
mod test;



================================================
File: crates/holochain_p2p/src/peer_meta_store.rs
================================================
use bytes::Bytes;
use futures::future::BoxFuture;
use holochain_sqlite::db::DbWrite;
use holochain_sqlite::error::DatabaseResult;
use holochain_sqlite::prelude::DbKindPeerMetaStore;
use holochain_sqlite::rusqlite::types::{FromSql, FromSqlResult, ToSqlOutput, ValueRef};
use holochain_sqlite::rusqlite::{named_params, ToSql};
use holochain_sqlite::sql::sql_peer_meta_store;
use kitsune2_api::{K2Error, K2Result, PeerMetaStore, Timestamp, Url};

/// Holochain implementation of a Kitsune2 [PeerMetaStore].
#[derive(Debug)]
pub struct HolochainPeerMetaStore {
    db: DbWrite<DbKindPeerMetaStore>,
}

struct BytesSql(Bytes);

impl ToSql for BytesSql {
    #[inline]
    fn to_sql(&self) -> holochain_sqlite::rusqlite::Result<ToSqlOutput<'_>> {
        Ok(ToSqlOutput::from(&self.0[..]))
    }
}

impl FromSql for BytesSql {
    #[inline]
    fn column_result(value: ValueRef<'_>) -> FromSqlResult<Self> {
        Ok(BytesSql(Bytes::copy_from_slice(value.as_blob()?)))
    }
}

impl HolochainPeerMetaStore {
    /// Create a new [HolochainPeerMetaStore] from a database handle.
    pub async fn create(db: DbWrite<DbKindPeerMetaStore>) -> DatabaseResult<Self> {
        // Prune any expired entries on startup
        db.write_async(|txn| -> DatabaseResult<()> {
            txn.execute(sql_peer_meta_store::PRUNE, [])?;

            Ok(())
        })
        .await?;

        Ok(Self { db })
    }
}

impl PeerMetaStore for HolochainPeerMetaStore {
    fn put(
        &self,
        peer: Url,
        key: String,
        value: Bytes,
        expiry: Option<Timestamp>,
    ) -> BoxFuture<'_, K2Result<()>> {
        let db = self.db.clone();

        Box::pin(async move {
            db.write_async(move |txn| -> DatabaseResult<()> {
                txn.execute(
                    sql_peer_meta_store::INSERT,
                    named_params! {
                        ":peer_url": peer.as_str(),
                        ":meta_key": key,
                        ":meta_value": BytesSql(value),
                        ":expires_at": expiry.map(|e| e.as_micros()),
                    },
                )?;

                Ok(())
            })
            .await
            .map_err(|e| K2Error::other_src("Failed to put peer meta", e))
        })
    }

    fn get(&self, peer: Url, key: String) -> BoxFuture<'_, K2Result<Option<Bytes>>> {
        let db = self.db.clone();

        Box::pin(async move {
            db.write_async(move |txn| -> DatabaseResult<Option<Bytes>> {
                let value = match txn.query_row(
                    sql_peer_meta_store::GET,
                    named_params! {
                        ":peer_url": peer.as_str(),
                        ":meta_key": key,
                    },
                    |row| row.get::<_, BytesSql>(0),
                ) {
                    Ok(value) => Some(value.0),
                    Err(holochain_sqlite::rusqlite::Error::QueryReturnedNoRows) => None,
                    Err(e) => return Err(e.into()),
                };

                Ok(value)
            })
            .await
            .map_err(|e| K2Error::other_src("Failed to get peer meta", e))
        })
    }

    fn delete(&self, peer: Url, key: String) -> BoxFuture<'_, K2Result<()>> {
        let db = self.db.clone();

        Box::pin(async move {
            db.write_async(move |txn| -> DatabaseResult<()> {
                txn.execute(
                    sql_peer_meta_store::DELETE,
                    named_params! {
                        ":peer_url": peer.as_str(),
                        ":meta_key": key,
                    },
                )?;

                Ok(())
            })
            .await
            .map_err(|e| K2Error::other_src("Failed to delete peer meta", e))
        })
    }
}



================================================
File: crates/holochain_p2p/src/spawn.rs
================================================
use crate::actor::*;
use crate::event::*;

mod actor;
use actor::*;

/// Spawn a new HolochainP2p actor.
/// Conductor will call this on initialization.
pub async fn spawn_holochain_p2p(
    config: kitsune_p2p::dependencies::kitsune_p2p_types::config::KitsuneP2pConfig,
    tls_config: kitsune_p2p::dependencies::kitsune_p2p_types::tls::TlsConfig,
    host: kitsune_p2p::HostApi,
    compat: NetworkCompatParams,
) -> HolochainP2pResult<(
    ghost_actor::GhostSender<HolochainP2p>,
    HolochainP2pEventReceiver,
)> {
    let (evt_send, evt_recv) = futures::channel::mpsc::channel(10);

    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let channel_factory = builder.channel_factory().clone();

    let sender = channel_factory.create_channel::<HolochainP2p>().await?;

    tokio::task::spawn(builder.spawn(
        HolochainP2pActor::new(config, tls_config, channel_factory, evt_send, host, compat).await?,
    ));

    Ok((sender, evt_recv))
}

/// Some parameters used as part of a protocol compability check during tx5 preflight
#[derive(Debug, Default, serde::Serialize, serde::Deserialize)]
pub struct NetworkCompatParams {
    /// The UUID of the installed DPKI service.
    /// If the service is backed by a Dna, this is the core 32 bytes of the DnaHash.
    pub dpki_uuid: Option<[u8; 32]>,
}



================================================
File: crates/holochain_p2p/src/test.rs
================================================
use crate::actor::*;
use crate::HolochainP2pDna;
use crate::*;
use ::fixt::prelude::*;
use holo_hash::fixt::DnaHashFixturator;
use holo_hash::AgentPubKey;
use holo_hash::DnaHash;
use holochain_nonce::Nonce256Bits;
use holochain_zome_types::fixt::ActionFixturator;
use kitsune2_api::DhtArc;
use kitsune_p2p::dht::Arq;
struct StubNetwork;

impl ghost_actor::GhostHandler<HolochainP2p> for StubNetwork {}
impl ghost_actor::GhostControlHandler for StubNetwork {}

#[allow(unused_variables)]
impl HolochainP2pHandler for StubNetwork {
    fn handle_join(
        &mut self,
        dna_hash: DnaHash,
        agent_pub_key: AgentPubKey,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<Arq>,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_leave(
        &mut self,
        dna_hash: DnaHash,
        agent_pub_key: AgentPubKey,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_call_remote(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> HolochainP2pHandlerResult<SerializedBytes> {
        Err("stub".into())
    }

    fn handle_send_remote_signal(
        &mut self,
        dna_hash: DnaHash,
        to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_publish(
        &mut self,
        dna_hash: DnaHash,
        request_validation_receipt: bool,
        countersigning_session: bool,
        basis_hash: holo_hash::OpBasis,
        source: AgentPubKey,
        op_hash_list: Vec<OpHashSized>,
        timeout_ms: Option<u64>,
        reflect_ops: Option<Vec<DhtOp>>,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_publish_countersign(
        &mut self,
        dna_hash: DnaHash,
        flag: bool,
        basis_hash: holo_hash::OpBasis,
        op: DhtOp,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_get(
        &mut self,
        dna_hash: DnaHash,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetOptions,
    ) -> HolochainP2pHandlerResult<Vec<WireOps>> {
        Err("stub".into())
    }

    fn handle_get_meta(
        &mut self,
        dna_hash: DnaHash,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetMetaOptions,
    ) -> HolochainP2pHandlerResult<Vec<MetadataSet>> {
        Err("stub".into())
    }

    fn handle_get_links(
        &mut self,
        dna_hash: DnaHash,
        link_key: WireLinkKey,
        options: actor::GetLinksOptions,
    ) -> HolochainP2pHandlerResult<Vec<WireLinkOps>> {
        Err("stub".into())
    }

    fn handle_count_links(
        &mut self,
        dna_hash: DnaHash,
        query: WireLinkQuery,
    ) -> HolochainP2pHandlerResult<CountLinksResponse> {
        Err("stub".into())
    }

    fn handle_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: actor::GetActivityOptions,
    ) -> HolochainP2pHandlerResult<Vec<AgentActivityResponse>> {
        Err("stub".into())
    }

    fn handle_must_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> HolochainP2pHandlerResult<Vec<MustGetAgentActivityResponse>> {
        Err("stub".into())
    }

    fn handle_send_validation_receipts(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_new_integrated_data(&mut self, dna_hash: DnaHash) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_authority_for_hash(
        &mut self,
        dna_hash: DnaHash,
        basis_hash: OpBasis,
    ) -> HolochainP2pHandlerResult<bool> {
        Err("stub".into())
    }
    fn handle_countersigning_session_negotiation(
        &mut self,
        dna_hash: DnaHash,
        agents: Vec<AgentPubKey>,
        message: event::CountersigningSessionNegotiationMessage,
    ) -> HolochainP2pHandlerResult<()> {
        Err("stub".into())
    }

    fn handle_dump_network_metrics(
        &mut self,
        dna_hash: Option<DnaHash>,
    ) -> HolochainP2pHandlerResult<String> {
        Err("stub".into())
    }

    fn handle_dump_network_stats(&mut self) -> HolochainP2pHandlerResult<String> {
        Err("stub".into())
    }

    fn handle_get_diagnostics(
        &mut self,
        dna_hash: DnaHash,
    ) -> HolochainP2pHandlerResult<kitsune_p2p::gossip::sharded_gossip::KitsuneDiagnostics> {
        Err("stub".into())
    }

    fn handle_storage_arcs(&mut self, dna_hash: DnaHash) -> HolochainP2pHandlerResult<Vec<DhtArc>> {
        Err("stub".into())
    }
}

/// Spawn a stub network that doesn't respond to any messages.
/// Use `test_network()` if you want a real test network.
pub async fn stub_network() -> ghost_actor::GhostSender<HolochainP2p> {
    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let channel_factory = builder.channel_factory().clone();

