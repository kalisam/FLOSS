    }
}

/// Value in the peer database that tracks an Agent's representation as signed by that agent.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct AgentInfoSigned(pub Arc<AgentInfoInner>);

impl std::ops::Deref for AgentInfoSigned {
    type Target = AgentInfoInner;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl serde::Serialize for AgentInfoSigned {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        let encode = AgentInfoSignedEncodeRef {
            agent: &self.agent,
            signature: &self.signature,
            agent_info: &self.encoded_bytes,
        };
        encode.serialize(serializer)
    }
}

impl<'de> serde::Deserialize<'de> for AgentInfoSigned {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        let AgentInfoSignedEncode {
            agent,
            signature,
            agent_info,
        } = AgentInfoSignedEncode::deserialize(deserializer)?;

        let mut bytes: &[u8] = &agent_info;
        let info: AgentInfoEncode =
            crate::codec::rmp_decode(&mut bytes).map_err(serde::de::Error::custom)?;
        let mut bytes: &[u8] = &info.meta_info;
        let meta: AgentMetaInfoEncode =
            crate::codec::rmp_decode(&mut bytes).map_err(serde::de::Error::custom)?;

        if agent != info.agent {
            return Err(serde::de::Error::custom("agent mismatch"));
        }

        let storage_arq = meta.arq_size.to_arq(agent.get_loc());

        let AgentInfoEncode {
            space,
            agent,
            urls,
            signed_at_ms,
            expires_after_ms,
            ..
        } = info;

        let inner = AgentInfoInner {
            space,
            agent,
            storage_arq,
            url_list: urls,
            signed_at_ms,
            expires_at_ms: signed_at_ms + expires_after_ms,
            signature,
            encoded_bytes: agent_info,
        };

        Ok(AgentInfoSigned(Arc::new(inner)))
    }
}

impl AgentInfoSigned {
    /// Construct and sign a new AgentInfoSigned instance.
    pub async fn sign<'a, R, F>(
        space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
        meta: impl Into<AgentMetaInfoEncode>,
        url_list: UrlList,
        signed_at_ms: u64,
        expires_at_ms: u64,
        f: F,
    ) -> KitsuneResult<Self>
    where
        R: std::future::Future<Output = KitsuneResult<Arc<KitsuneSignature>>>,
        F: FnOnce(&[u8]) -> R,
    {
        let meta = meta.into();
        let storage_arq = meta.arq_size.to_arq(agent.get_loc());

        let mut buf = Vec::new();
        crate::codec::rmp_encode(&mut buf, meta).map_err(KitsuneError::other)?;
        let meta = buf.into_boxed_slice();

        let info = AgentInfoEncode {
            space: space.clone(),
            agent: agent.clone(),
            urls: url_list.clone(),
            signed_at_ms,
            expires_after_ms: expires_at_ms - signed_at_ms,
            meta_info: meta,
        };
        let mut buf = Vec::new();
        crate::codec::rmp_encode(&mut buf, info).map_err(KitsuneError::other)?;
        let encoded_bytes = buf.into_boxed_slice();

        let signature = f(&encoded_bytes).await?;

        let inner = AgentInfoInner {
            space,
            agent,
            storage_arq,
            url_list,
            signed_at_ms,
            expires_at_ms,
            signature,
            encoded_bytes,
        };

        Ok(Self(Arc::new(inner)))
    }

    /// decode from msgpack
    pub fn decode(b: &[u8]) -> KitsuneResult<Self> {
        let mut bytes: &[u8] = b;
        crate::codec::rmp_decode(&mut bytes).map_err(KitsuneError::other)
    }

    /// encode as msgpack
    pub fn encode(&self) -> KitsuneResult<Box<[u8]>> {
        let mut buf = Vec::new();
        crate::codec::rmp_encode(&mut buf, self).map_err(KitsuneError::other)?;
        Ok(buf.into_boxed_slice())
    }

    /// Accessor
    pub fn agent(&self) -> Arc<KitsuneAgent> {
        self.agent.clone()
    }

    /// Convert arq to arc
    pub fn storage_arc(&self) -> DhtArc {
        self.storage_arq.to_dht_arc_std()
    }
}

#[cfg(test)]
mod tests {
    use dht::arq::ArqSize;

    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn agent_info() {
        let space = Arc::new(KitsuneSpace(vec![0x01; 36]));
        let agent = Arc::new(KitsuneAgent(vec![0x02; 36]));

        let info = AgentInfoSigned::sign(
            space.clone(),
            agent.clone(),
            ArqSize::empty(),
            vec![],
            42,
            69,
            |_| async move { Ok(Arc::new(vec![0x03; 64].into())) },
        )
        .await
        .unwrap();

        assert_eq!(info.space, space);
        assert_eq!(info.agent, agent);

        let mut enc = Vec::new();
        crate::codec::rmp_encode(&mut enc, &info).unwrap();
        let mut bytes: &[u8] = &enc;
        let info2: AgentInfoSigned = crate::codec::rmp_decode(&mut bytes).unwrap();
        assert_eq!(info, info2);
    }
}



================================================
File: crates/kitsune_p2p/types/src/async_lazy.rs
================================================
//! utility for lazy init-ing things

use futures::future::{BoxFuture, FutureExt, Shared};

/// utility for lazy init-ing things
/// note how new is not async so we can do it in an actor handler
pub struct AsyncLazy<O: 'static + Clone + Send + Sync> {
    fut: Shared<BoxFuture<'static, O>>,
}

impl<O: 'static + Clone + Send + Sync> AsyncLazy<O> {
    /// sync create a new lazy-init value
    /// works best with `Arc<>` types, but anything
    /// `'static + Clone + Send + Sync` will do.
    pub fn new<F>(f: F) -> Self
    where
        F: 'static + std::future::Future<Output = O> + Send,
    {
        Self {
            fut: f.boxed().shared(),
        }
    }

    /// async get the value of this lazy type
    /// will return once the initialization future completes
    pub fn get(&self) -> impl std::future::Future<Output = O> + 'static {
        self.fut.clone()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;

    #[tokio::test(flavor = "multi_thread")]
    async fn async_lazy() {
        let s = AsyncLazy::new(async move {
            tokio::time::sleep(std::time::Duration::from_millis(20)).await;
            Arc::new(42)
        });
        assert_eq!(
            vec![Arc::new(42), Arc::new(42)],
            futures::future::join_all(vec![s.get(), s.get()]).await
        );
        assert_eq!(42, *s.get().await);
        assert_eq!(42, *s.get().await);
    }
}



================================================
File: crates/kitsune_p2p/types/src/bootstrap.rs
================================================
//! Types for the bootstrap server
use crate::bin_types::{KitsuneBinType, KitsuneSpace};
use crate::tx_utils::TxUrl;
use std::collections::HashSet;
use std::sync::Arc;

/// The number of random agent infos we want to collect from the bootstrap service when we want to
/// populate an empty local space.
// @todo expose this to network config.
const RANDOM_LIMIT_DEFAULT: u32 = 16;

/// Struct to be encoded for the `random` op.
#[derive(serde::Deserialize, serde::Serialize, Clone)]
pub struct RandomQuery {
    /// The space to get random agents from.
    pub space: Arc<KitsuneSpace>,
    /// The maximum number of random agents to retrieve for this query.
    pub limit: RandomLimit,
}

impl Default for RandomQuery {
    fn default() -> Self {
        Self {
            // This is useless, it's here as a placeholder so that ..Default::default() syntax
            // works for limits, not because you'd actually ever want a "default" space.
            space: Arc::new(KitsuneSpace::new(vec![0; 36])),
            limit: RandomLimit::default(),
        }
    }
}

#[derive(serde::Deserialize, serde::Serialize, derive_more::From, derive_more::Into, Clone)]
/// Limit of random peers to return.
pub struct RandomLimit(pub u32);

impl Default for RandomLimit {
    fn default() -> Self {
        Self(RANDOM_LIMIT_DEFAULT)
    }
}

/// The result of storing a new agent info with Kitsune's host.
#[derive(Default, Debug)]
pub struct AgentInfoPut {
    /// URLs that were in the previous agent info for the agent but are no longer present.
    pub removed_urls: HashSet<TxUrl>,
}



================================================
File: crates/kitsune_p2p/types/src/codec.rs
================================================
//! Encoding / Decoding utilities.

/// Encode a serde::Serialize item as message-pack data to given writer.
/// You may wish to first wrap your writer in a BufWriter.
pub fn rmp_encode<W, S>(write: &mut W, item: S) -> Result<(), std::io::Error>
where
    W: std::io::Write,
    S: serde::Serialize,
{
    let mut se = rmp_serde::encode::Serializer::new(write).with_struct_map();
    item.serialize(&mut se)
        .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;
    Ok(())
}

/// Decode message-pack data from given reader into an owned item.
/// You may wish to first wrap your reader in a BufReader.
pub fn rmp_decode<R, D>(r: &mut R) -> Result<D, std::io::Error>
where
    R: std::io::Read,
    for<'de> D: Sized + serde::Deserialize<'de>,
{
    let mut de = rmp_serde::decode::Deserializer::new(r);
    D::deserialize(&mut de).map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))
}

/// Apply to a data item to indicate it can be encoded / decoded.
pub trait Codec: Clone + Sized {
    /// Variant identifier (for debugging or as a cheap discriminant).
    fn variant_type(&self) -> &'static str;

    /// Encode this item to given writer.
    /// You may wish to first wrap your writer in a BufWriter.
    fn encode<W>(&self, w: &mut W) -> Result<(), std::io::Error>
    where
        W: std::io::Write;

    /// Encode this item to an owned vector of bytes.
    /// Uses `encode()` internally.
    fn encode_vec(&self) -> Result<Vec<u8>, std::io::Error> {
        let mut data = Vec::new();
        self.encode(&mut data)?;
        Ok(data)
    }

    /// Decode a reader into this item.
    /// You may wish to first wrap your reader in a BufReader.
    fn decode<R>(r: &mut R) -> Result<Self, std::io::Error>
    where
        R: std::io::Read;

    /// Decode a range of bytes into this item.
    /// Will also return the byte count read.
    /// Uses `decode()` internally.
    fn decode_ref(r: &[u8]) -> Result<(u64, Self), std::io::Error> {
        let mut r = std::io::Cursor::new(r);
        let item = Self::decode(&mut r)?;
        Ok((r.position(), item))
    }
}

/// DSL-style macro for generating a serialization protocol message enum.
///
/// DSL:
///
/// ```ignore
/// /// [codec doc here]
/// codec $codec_name {
///     /// [var doc here]
///     $var_name($var_id) {
///         /// [type doc here]
///         $type_name.$type_idx: $type_ty,
///     },
/// }
/// ```
///
/// - $codec_name - camel-case codec enum name
/// - $var_name   - camel-case variant/struct name
/// - $var_id     - protocol variant identifier byte (u8) literal
/// - $type_name  - snake-case type name
/// - $type_idx   - zero-index type index in message array (usize)
/// - $type_ty    - type rust type
///
/// E.G.:
///
/// ```ignore
/// /// My codec is awesome.
/// codec MyCodec {
///     /// My codec has only one variant.
///     MyVariant(0x00) {
///         /// My variant has only one type
///         my_type.0: String,
///     },
/// }
/// ```
#[macro_export]
macro_rules! write_codec_enum {
    ($(#[doc = $codec_doc:expr])* codec $codec_name:ident {$(
        $(#[doc = $var_doc:expr])* $var_name:ident($var_id:literal) {$(
            $(#[doc = $type_doc:expr])* $type_name:ident.$type_idx:literal: $type_ty:ty,
        )*},
    )*}) => {
        $crate::dependencies::paste::item! {
            $(
                $(#[doc = $var_doc])*
                #[derive(Clone, Debug, serde::Serialize, serde::Deserialize, PartialEq, Eq)]
                pub struct [< $var_name:camel >] {
                    $(
                        $(#[doc = $type_doc])* pub [< $type_name:snake >]: $type_ty,
                    )*
                }

                impl $crate::codec::Codec for [< $var_name:camel >] {
                    fn variant_type(&self) -> &'static str {
                        concat!(
                            stringify!([< $codec_name:camel >]),
                            "::",
                            stringify!([< $var_name:camel >]),
                        )
                    }

                    fn encode<W>(&self, w: &mut W) -> ::std::io::Result<()>
                    where
                        W: ::std::io::Write
                    {
                        #[cfg(debug_assertions)]
                        #[allow(dead_code)]
                        {
                            const MSG: &str = "type index must begin at 0 and increment by exactly 1 per type - switching type order will break parsing compatibility";
                            let mut _idx = -1;
                            $(
                                _idx += 1;
                                assert_eq!(_idx, $type_idx, "{}", MSG);
                            )*
                        }
                        let t: (
                            $(&$type_ty,)*
                        ) = (
                            $(&self.[< $type_name:snake >],)*
                        );
                        $crate::codec::rmp_encode(w, &t)
                    }

                    fn decode<R>(r: &mut R) -> ::std::io::Result<Self>
                    where
                        R: ::std::io::Read
                    {
                        let (
                            $([< $type_name:snake >],)*
                        ): (
                            $($type_ty,)*
                        ) = $crate::codec::rmp_decode(r)?;
                        Ok([< $var_name:camel >] {
                            $(
                                [< $type_name:snake >],
                            )*
                        })
                    }
                }
            )*

            $(#[doc = $codec_doc])*
            #[derive(Clone, Debug, PartialEq, Eq)]
            pub enum [< $codec_name:camel >] {
                $(
                    $(#[doc = $var_doc])*
                    [< $var_name:camel >]([< $var_name:camel >]),
                )*
            }

            impl [< $codec_name:camel >] {
                $(
                    /// Variant constructor helper function.
                    pub fn [< $var_name:snake >]($(
                        [< $type_name:snake >]: $type_ty,
                    )*) -> Self {
                        Self::[< $var_name:camel >]([< $var_name:camel >] {
                            $(
                                [< $type_name:snake >],
                            )*
                        })
                    }
                )*
            }

            impl $crate::codec::Codec for [< $codec_name:camel >] {
                fn variant_type(&self) -> &'static str {
                    match self {
                        $(
                            Self::[< $var_name:camel >](data) =>
                                $crate::codec::Codec::variant_type(data),
                        )*
                    }
                }

                fn encode<W>(&self, w: &mut W) -> ::std::io::Result<()>
                where
                    W: ::std::io::Write
                {
                    match self {
                        $(
                            Self::[< $var_name:camel >](data) => {
                                ::std::io::Write::write_all(w, &[$var_id])?;
                                $crate::codec::Codec::encode(data, w)
                            }
                        )*
                    }
                }

                fn decode<R>(r: &mut R) -> Result<Self, ::std::io::Error>
                where
                    R: ::std::io::Read
                {
                    let mut c = [0_u8; 1];
                    ::std::io::Read::read_exact(r, &mut c)?;
                    match c[0] {
                        $(
                            $var_id => {
                                Ok(Self::[< $var_name:camel >]($crate::codec::Codec::decode(r)?))
                            },
                        )*
                        _ => Err(::std::io::Error::new(::std::io::ErrorKind::Other, "invalid protocol byte")),
                    }
                }
            }
        }
    };
}

#[cfg(test)]
mod tests {
    #![allow(dead_code)]

    use super::*;
    use std::sync::Arc;

    #[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
    pub struct Sub(pub Vec<u8>);

    write_codec_enum! {
        /// Codec
        codec Bob {
            /// variant
            BobOne(0x42) {
                /// type 1
                yay.0: bool,

                /// type 2
                age.1: u32,

                /// type 3
                sub.2: Arc<Sub>,
            },
            /// nother variant
            BobTwo(0x43) {
            },
        }
    }

    #[test]
    fn test_encode_decode() {
        let bob = Bob::bob_one(true, 42, Arc::new(Sub(b"test".to_vec())));
        let data = bob.encode_vec().unwrap();
        let res = Bob::decode_ref(&data).unwrap().1;
        assert_eq!(bob, res);
    }
}



================================================
File: crates/kitsune_p2p/types/src/combinators.rs
================================================
//! Combinator functions, for more easeful functional programming

/// Return the first element of a 2-tuple
pub fn first<A, B>(tup: (A, B)) -> A {
    tup.0
}

/// Return the second element of a 2-tuple
pub fn second<A, B>(tup: (A, B)) -> B {
    tup.1
}



================================================
File: crates/kitsune_p2p/types/src/config.rs
================================================
//! Kitsune Config Tuning Params
#![allow(missing_docs)]

use crate::tx_utils::TxUrl;
use schemars::JsonSchema;
use url2::Url2;

/// Fifteen minutes
pub const RECENT_THRESHOLD_DEFAULT: std::time::Duration = std::time::Duration::from_secs(60 * 15);

/// Wrapper for the actual KitsuneP2pTuningParams struct
/// so the widely used type def can be an Arc<>
pub mod tuning_params_struct {
    use ghost_actor::dependencies::tracing;
    use kitsune_p2p_dht::{
        prelude::{ArqClamping, LocalStorageConfig},
        ArqStrat,
    };
    use kitsune_p2p_dht_arc::DEFAULT_MIN_PEERS;
    use std::collections::HashMap;

    macro_rules! mk_tune {
        ($($(#[doc = $doc:expr])* $(#[cfg($cfg:meta)])? $i:ident: $t:ty = $d:expr, $(#[cfg_not($cfg_alt:meta)] $i_alt:ident: $t_alt:ty = $d_alt:expr,)?)*) => {
            /// Network tuning parameters.
            /// This is serialized carefully so all the values can be represented
            /// as strings in YAML - and we will be able to proceed with a printed
            /// warning for tuning params that are removed, but still specified in
            /// configs.
            #[non_exhaustive]
            #[derive(Clone, Debug, PartialEq, schemars::JsonSchema)]
            pub struct KitsuneP2pTuningParams {
                $(
                    $(#[doc = $doc])*
                    $(#[cfg($cfg)])?
                    pub $i: $t,
                    $(#[cfg(not($cfg_alt))]
                        pub $i: $t,
                    )?
                )*
            }

            impl Default for KitsuneP2pTuningParams {
                fn default() -> Self {
                    Self {
                        $(
                            $(#[cfg($cfg)])?
                            $i: $d,
                            $(#[cfg(not($cfg_alt))]
                                $i: $d_alt,
                            )?
                        )*
                    }
                }
            }

            impl serde::Serialize for KitsuneP2pTuningParams {
                fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
                where
                    S: serde::Serializer,
                {
                    use serde::ser::SerializeMap;
                    let mut m = serializer.serialize_map(None)?;
                    $(
                        $(#[cfg($cfg)])?
                        m.serialize_entry(
                            stringify!($i),
                            &format!("{}", &self.$i),
                        )?;
                        $(#[cfg(not($cfg_alt))]
                            m.serialize_entry(
                                stringify!($i),
                                &format!("{}", &self.$i),
                            )?;
                        )?
                    )*
                    m.end()
                }
            }

            impl<'de> serde::Deserialize<'de> for KitsuneP2pTuningParams {
                fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
                where
                    D: serde::Deserializer<'de>,
                {
                    let result = <HashMap<String, String>>::deserialize(deserializer)?;
                    let mut out = KitsuneP2pTuningParams::default();
                    for (k, v) in result.into_iter() {
                        match k.as_str() {
                            $(
                                $(#[cfg($cfg)])?
                                stringify!($i) => match v.parse::<$t>() {
                                    Ok(v) => out.$i = v,
                                    Err(e) => tracing::warn!("failed to parse {}: {}", k, e),
                                },
                                $(#[cfg(not($cfg_alt))]
                                    stringify!($i) => match v.parse::<$t>() {
                                        Ok(v) => out.$i = v,
                                        Err(e) => tracing::warn!("failed to parse {}: {}", k, e),
                                    },
                                )?
                            )*
                            _ => tracing::warn!("INVALID TUNING PARAM: '{}'", k),
                        }
                    }
                    Ok(out)
                }
            }
        };
    }

    mk_tune! {
        /// Gossip strategy to use. [Default: "sharded-gossip"]
        gossip_strategy: String = "sharded-gossip".to_string(),

        /// Delay between gossip loop iteration. [Default: 1s]
        gossip_loop_iteration_delay_ms: u32 = 1000,

        /// The gossip loop will attempt to rate-limit output
        /// to this count megabits per second. [Default: 100.0]
        gossip_outbound_target_mbps: f64 = 100.0,

        /// The gossip loop will attempt to rate-limit input
        /// to this count megabits per second. [Default: 100.0]
        gossip_inbound_target_mbps: f64 = 100.0,

        /// The gossip loop will attempt to rate-limit outbound
        /// traffic for the historic loop (if there is one)
        /// to this count megabits per second. [Default: 100.0]
        gossip_historic_outbound_target_mbps: f64 = 100.0,

        /// The gossip loop will attempt to rate-limit inbound
        /// traffic for the historic loop (if there is one)
        /// to this count megabits per second. [Default: 100.0]
        gossip_historic_inbound_target_mbps: f64 = 100.0,

        /// The gossip loop accomodates this amount of excess capacity
        /// before enacting the target rate limit, expressed as a ratio
        /// of the target rate limit. For instance, if the historic
        /// outbound target is 10mbps, a burst ratio of 50 will allow
        /// an extra 500mb of outbound traffic before the target rate
        /// limiting kicks in (and this extra capacity will take 50
        /// seconds to "refill"). [Default: 100.0]
        gossip_burst_ratio: f64 = 100.0,

        /// How long should we hold off talking to a peer
        /// we've previously spoken successfully to.
        /// [Default: 1 minute]
        gossip_peer_on_success_next_gossip_delay_ms: u32 = 1000 * 60,

        /// How long should we hold off talking to a peer
        /// we've previously gotten errors speaking to.
        /// [Default: 5 minute]
        gossip_peer_on_error_next_gossip_delay_ms: u32 = 1000 * 60 * 5,

        /// How often should we update and publish our agent info?
        /// [Default: 5 minutes]
        gossip_agent_info_update_interval_ms: u32 = 1000 * 60 * 5,

        /// The timeout for a gossip round if there is no contact.
        /// [Default: 1 minute]
        gossip_round_timeout_ms: u64 = 1000 * 60,

        /// The target redundancy is the number of peers we expect to hold any
        /// given Op.
        gossip_redundancy_target: f64 = DEFAULT_MIN_PEERS as f64,

        /// The max number of bytes of data to send in a single message.
        ///
        /// This setting was more relevant when entire Ops were being gossiped,
        /// but now that only hashes are gossiped, it would take a lot of hashes
        /// to reach this limit (1MB = approx 277k hashes).
        ///
        /// Payloads larger than this are split into multiple batches
        /// when possible.
        gossip_max_batch_size: u32 = 1_000_000,

        /// Should gossip dynamically resize storage arcs?
        ///
        /// This is an unstable feature must be enabled with the "unstable-sharding" Cargo feature.
        /// This setting is not available without the feature.
        ///
        /// [Default: true]
        #[cfg(feature = "unstable-sharding")]
        gossip_dynamic_arcs: bool = true,

        /// Override the configured arc management strategy?
        ///
        /// This is part of an unstable sharding feature and can only be set to "full" or "empty".
        /// The default value is "full", where all nodes will be expected to hold all data.
        /// If the "unstable-sharding" feature is enabled then this can also be set to "none" to
        /// allow sharding to be managed by Kitsune.
        ///
        /// If [KitsuneP2pTuningParams::gossip_dynamic_arcs] is `true`, Kitsune adjusts the
        /// gossip_arc to match the current network conditions for the given DNA.
        ///
        /// If unsure, please keep this setting at the default "none",
        /// meaning no arc clamping. Setting options are:
        /// - "none" - Keep the default auto-adjust behavior.
        /// - "empty" - Makes you a freeloader, contributing nothing
        ///   to the network. Please don't choose this option without
        ///   a good reason, such as being on a bandwidth constrained
        ///   mobile device!
        /// - "full" - Indicates that you commit to serve and hold all
        ///   data from all agents, and be a potential target for all
        ///   get requests. This could be a significant investment of
        ///   bandwidth. Don't take this responsibility lightly.
        #[cfg(feature = "unstable-sharding")]
        gossip_arc_clamping: String = "none".to_string(),
        #[cfg_not(feature = "unstable-sharding")]
        gossip_arc_clamping: String = "full".to_string(),

        /// Default timeout for rpc single. [Default: 60s]
        default_rpc_single_timeout_ms: u32 = 1000 * 60,

        /// Default agent count for rpc multi. [Default: 3]
        default_rpc_multi_remote_agent_count: u8 = 3,

        /// Default remote request grace ms. [Default: 3s]
        /// If we already have results from other sources,
        /// but made any additional outgoing remote requests,
        /// we'll wait at least this long for additional responses.
        default_rpc_multi_remote_request_grace_ms: u64 = 1000 * 3,

        /// Default agent expires after milliseconds. [Default: 20 minutes]
        agent_info_expires_after_ms: u32 = 1000 * 60 * 20,

        /// Tls in-memory session storage capacity. [Default: 512]
        tls_in_mem_session_storage: u32 = 512,

        /// How often should NAT nodes refresh their proxy contract?
        /// [Default: 2 minutes]
        proxy_keepalive_ms: u32 = 1000 * 60 * 2,

        /// How often should proxy nodes prune their ProxyTo list?
        /// Note - to function this should be > proxy_keepalive_ms.
        /// [Default: 5 minutes]
        proxy_to_expire_ms: u32 = 1000 * 60 * 5,

        /// Mainly used as the for_each_concurrent limit,
        /// this restricts the number of active polled futures
        /// on a single thread.
        /// [Default: 4096]
        concurrent_limit_per_thread: usize = 4096,

        /// tx5 timeout used for passive background operations
        /// like reads / responds.
        /// [Default: 60 seconds]
        tx5_implicit_timeout_ms: u32 = 1000 * 60,

        /// Maximum count of open connections.
        /// [Default: 4096]
        tx5_connection_count_max: u32 = 4096,

        /// Max backend send buffer bytes (per connection).
        /// [Default: 64 KiB]
        tx5_send_buffer_bytes_max: u32 = 64 * 1024,

        /// Max backend recv buffer bytes (per connection).
        /// [Default: 64 KiB]
        tx5_recv_buffer_bytes_max: u32 = 64 * 1024,

        /// Maximum receive message reconstruction bytes in memory
        /// (accross entire endpoint).
        /// [Default: 512 MiB]
        tx5_incoming_message_bytes_max: u32 = 512 * 1024 * 1024,

        /// Maximum size of an individual message.
        /// [Default: 16 MiB]
        tx5_message_size_max: u32 = 16 * 1024 * 1024,

        /// Internal event channel size.
        /// [Default: 1024]
        tx5_internal_event_channel_size: u32 = 1024,

        /// Default timeout for network operations.
        /// [Default: 60]
        tx5_timeout_s: u32 = 60,

        /// Starting backoff duration for retries.
        /// [Default: 60]
        tx5_backoff_start_s: u32 = 5,

        /// Max backoff duration for retries.
        /// [Default: 60]
        tx5_backoff_max_s: u32 = 60,

        /// Tx5 ban time in seconds.
        tx5_ban_time_s: u32 = 10,

        /// Tx5 min ephemeral port
        tx5_min_ephemeral_udp_port: u16 = 1,

        /// Tx5 max ephemeral port
        tx5_max_ephemeral_udp_port: u16 = 65535,

        /// Set this to `true` to enable verbose webrtc backend tracing.
        tx5_backend_tracing_enabled: bool = false,

        /// The backend tx5 module to be used. This defaults to "go_pion"
        /// which is also the default if unspecified or invalid.
        /// Options currently inclued:
        /// - "go_pion" - based off the golang pion webrtc library
        /// - "mem" - a stub memory backend for performance/validation testing
        tx5_backend_module: String = "go_pion".to_string(),

        /// The additional tx5 backend module config. This should be
        /// a json object with backend module specific configuration
        /// as specified by tx5. Defaults to an empty object ("{}").
        tx5_backend_module_config: String = "{}".to_string(),

        /// if you would like to be able to use an external tool
        /// to debug the QUIC messages sent and received by kitsune
        /// you'll need the decryption keys.
        /// The default of `"no_keylog"` is secure and will not write any keys
        /// Setting this to `"env_keylog"` will write to a keylog specified
        /// by the `SSLKEYLOGFILE` environment variable, or do nothing if
        /// it is not set, or is not writable.
        danger_tls_keylog: String = "no_keylog".to_string(),

        /// Set the cutoff time when gossip switches over from recent
        /// to historical gossip.
        ///
        /// This is dangerous to change, because gossip may not be
        /// possible with nodes using a different setting for this threshold.
        /// Do not change this except in testing environments.
        /// [Default: 15 minutes]
        danger_gossip_recent_threshold_secs: u64 = super::RECENT_THRESHOLD_DEFAULT.as_secs(),

        /// Don't publish ops, only rely on gossip. Useful for testing the efficacy of gossip.
        disable_publish: bool = false,

        /// Disable recent gossip. Useful for testing Historical gossip in isolation.
        /// Note that this also disables agent gossip!
        disable_recent_gossip: bool = false,

        /// Disable historical gossip. Useful for testing Recent gossip in isolation.
        disable_historical_gossip: bool = false,

        /// Control the backoff multiplier for the time delay between checking in with the bootstrap server.
        /// The default value of `2` causes the delay to grow quickly up to the max time of 1 hour.
        /// For testing consider using `1` to prevent the delay from growing.
        bootstrap_check_delay_backoff_multiplier: u32 = 2,

        /// Set the bootstrap fetch maximum backoff time.
        /// The default value is 60 * 5 s = five minutes.
        /// The minimum value is 60 s = one minute.
        bootstrap_max_delay_s: u32 = 60 * 5,
    }

    impl KitsuneP2pTuningParams {
        /// Generate a KitsuneTimeout instance
        /// based on the tuning parameter tx5_implicit_timeout_ms
        pub fn implicit_timeout(&self) -> crate::KitsuneTimeout {
            crate::KitsuneTimeout::from_millis(self.tx5_implicit_timeout_ms as u64)
        }

        /// Get the gossip recent threshold param as a proper Duration
        pub fn danger_gossip_recent_threshold(&self) -> std::time::Duration {
            std::time::Duration::from_secs(self.danger_gossip_recent_threshold_secs)
        }

        /// get the tx5_ban_time_s param as a Duration.
        pub fn tx5_ban_time(&self) -> std::time::Duration {
            std::time::Duration::from_secs(self.tx5_ban_time_s as u64)
        }

        /// returns true if we should initialize a tls keylog
        /// based on the `SSLKEYLOGFILE` environment variable
        pub fn use_env_tls_keylog(&self) -> bool {
            self.danger_tls_keylog == "env_keylog"
        }

        /// The timeout for a gossip round if there is no contact.
        pub fn gossip_round_timeout(&self) -> std::time::Duration {
            std::time::Duration::from_millis(self.gossip_round_timeout_ms)
        }

        /// Parse the gossip_arc_clamping string as a proper type
        pub fn arc_clamping(&self) -> Option<ArqClamping> {
            match self.gossip_arc_clamping.to_lowercase().as_str() {
                "none" => None,
                "empty" => Some(ArqClamping::Empty),
                "full" => Some(ArqClamping::Full),
                other => panic!("Invalid kitsune tuning param: arc_clamping = '{}'", other),
            }
        }

        /// Create a standard ArqStrat from the tuning params
        pub fn to_arq_strat(&self) -> ArqStrat {
            let local_storage = LocalStorageConfig {
                arc_clamping: self.arc_clamping(),
            };
            ArqStrat::standard(local_storage, self.gossip_redundancy_target)
        }
    }
}

/// We don't want to clone these tuning params over-and-over.
/// They should normally be passed around as an Arc.
pub type KitsuneP2pTuningParams = std::sync::Arc<tuning_params_struct::KitsuneP2pTuningParams>;

/// Configure the kitsune actor.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, PartialEq, JsonSchema)]
pub struct KitsuneP2pConfig {
    /// List of sub-transports to be included in this pool
    pub transport_pool: Vec<TransportConfig>,

    /// The service used for peers to discover each before they are peers.
    #[schemars(schema_with = "holochain_util::jsonschema::url2_schema")]
    pub bootstrap_service: Option<Url2>,

    /// Network tuning parameters. These are managed loosely,
    /// as they are subject to change. If you specify a tuning parameter
    /// that no longer exists, or a value that does not parse,
    /// a warning will be printed in the tracing log.
    #[serde(default)]
    pub tuning_params: KitsuneP2pTuningParams,

    /// All tracing logs from kitsune tasks will be instrumented to contain this string,
    /// so that logs from multiple instances in the same process can be disambiguated.
    #[serde(default)]
    pub tracing_scope: Option<String>,
}

impl Default for KitsuneP2pConfig {
    fn default() -> Self {
        Self::mem()
    }
}

impl KitsuneP2pConfig {
    /// Testing-only memory backend.
    pub fn mem() -> Self {
        Self {
            transport_pool: vec![TransportConfig::Mem {}],
            bootstrap_service: None,
            tuning_params: KitsuneP2pTuningParams::default(),
            tracing_scope: None,
        }
    }

    #[cfg(feature = "test_utils")]
    pub fn from_signal_addr(socket_addr: std::net::SocketAddr) -> Self {
        let signal_url = format!("ws://{:?}", socket_addr);
        Self {
            transport_pool: vec![TransportConfig::WebRTC {
                signal_url,
                webrtc_config: None,
            }],
            bootstrap_service: None,
            tuning_params: KitsuneP2pTuningParams::default(),
            tracing_scope: None,
        }
    }
}

#[allow(dead_code)]
fn cnv_bind_to(bind_to: &Option<url2::Url2>) -> TxUrl {
    match bind_to {
        Some(bind_to) => bind_to.clone().into(),
        None => TxUrl::from_str_panicking("kitsune-quic://0.0.0.0:0"),
    }
}

impl KitsuneP2pConfig {
    /// This config is making use of tx5 transport
    #[allow(dead_code)] // because of feature flipping
    pub fn is_tx5(&self) -> bool {
        {
            if let Some(t) = self.transport_pool.first() {
                return matches!(t, TransportConfig::Mem {} | TransportConfig::WebRTC { .. });
            }
        }
        false
    }

    /// Return a copy with the tuning params altered
    pub fn tune(
        mut self,
        f: impl Fn(
            tuning_params_struct::KitsuneP2pTuningParams,
        ) -> tuning_params_struct::KitsuneP2pTuningParams,
    ) -> Self {
        let tp = (*self.tuning_params).clone();
        self.tuning_params = std::sync::Arc::new(f(tp));
        self
    }
}

/// Configure the network bindings for underlying kitsune transports.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, PartialEq, JsonSchema)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum TransportConfig {
    /// Configure to use Tx5 WebRTC for kitsune networking.
    #[serde(rename = "webrtc", alias = "web_r_t_c", alias = "web_rtc")]
    WebRTC {
        /// The url of the signal server to connect to for addressability.
        signal_url: String,

        /// Webrtc peer connection config.
        webrtc_config: Option<serde_json::Value>,
    },

    /// A transport that uses the local memory transport protocol
    /// (this is mainly for testing)
    Mem {},
}



================================================
File: crates/kitsune_p2p/types/src/consistency.rs
================================================
//! Types used for consistency checking during tests or dht health checks.
//! These types describe a protocol that can be implemented to gather statistics
//! on data consistency.
//! This is a first prototype so expect this to change.
//!
//! The idea is that a central node can request all the published hashes from a set
//! of nodes on the DHT and then create a consistency session for each node.
//! The sessions can then be sent to each node so they can self-check what they
//! should be holding and then send back small session reports at a set frequency.
//!
//! This allows consistency and health checks to be run at scale with minimal network traffic.
//! It does require honest network nodes. (Although this could be strengthened).
use std::time::Duration;

use crate::bin_types::*;
use dht_arc::{DhtArc, DhtLocation};

use super::*;

/// Data published by an agent.
pub struct PublishedData {
    /// The agent that published the data.
    pub agent: Arc<KitsuneAgent>,
    /// The storage arc of the agent.
    pub storage_arc: DhtArc,
    /// The op hashes published by the agent.
    pub published_hashes: Vec<(DhtLocation, KitsuneOpHash)>,
}

/// A consistency session for an individual agent
/// to self check and report back.
pub struct ConsistencySession {
    /// How often the agent should send keep alives if they
    /// do not have all the expected data yet.
    pub keep_alive: Option<Duration>,
    /// How often the agent should check if they have all the
    /// expected data.
    pub frequency: Duration,
    /// When the agent should timeout the session.
    pub timeout: Duration,
    /// The data the agent should check for.
    pub expected_data: ExpectedData,
}

/// The data the agent is expected to have.
pub struct ExpectedData {
    /// The agents this agent is expected to have in their peer store.
    pub expected_agents: Vec<Arc<KitsuneAgent>>,
    /// The ops this agent is expected to have integrated.
    pub expected_hashes: Vec<KitsuneOpHash>,
}

/// A message from an agent with a report on their status for this session.
pub struct SessionMessage {
    /// The agent that sent the message.
    pub from: Arc<KitsuneAgent>,
    /// The status report.
    pub report: SessionReport,
}

/// The status of this agents session.
pub enum SessionReport {
    /// The session is still running and the agent is missing data.
    KeepAlive {
        /// The number of missing agents.
        missing_agents: u32,
        /// The expected number of agents.
        expected_agents: u32,
        /// The number of missing ops.
        missing_hashes: u32,
        /// The expected number of hashes.
        expected_hashes: u32,
    },
    /// The session is complete and the agent has all the data.
    Complete {
        /// The time it took to complete the session.
        elapsed_ms: u32,
    },
    /// The session timed out and the agent is still missing data.
    Timeout {
        /// The agents that are missing.
        missing_agents: Vec<Arc<KitsuneAgent>>,
        /// The ops that ars missing.
        missing_hashes: Vec<KitsuneOpHash>,
    },
    /// An error has occurred and the session has failed for this agent.
    Error {
        /// The error that occurred.
        error: String,
    },
}



================================================
File: crates/kitsune_p2p/types/src/fetch_pool.rs
================================================
//! Fetch queue types

/// Info about the fetch queue
#[derive(Clone, Debug, Default, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct FetchPoolInfo {
    /// Total number of bytes expected to be received through fetches
    pub op_bytes_to_fetch: usize,

    /// Total number of ops expected to be received through fetches
    pub num_ops_to_fetch: usize,
}

/// Gossip has two distinct variants which share a lot of similarities but
/// are fundamentally different and serve different purposes
#[derive(
    Clone,
    Copy,
    Debug,
    PartialEq,
    Eq,
    Hash,
    derive_more::Display,
    serde::Serialize,
    serde::Deserialize,
)]
pub enum GossipType {
    /// The Recent gossip type is aimed at rapidly syncing the most recent
    /// data. It runs frequently and expects frequent diffs at each round.
    Recent,
    /// The Historical gossip type is aimed at comprehensively syncing the
    /// entire common history of two nodes, filling in gaps in the historical
    /// data. It runs less frequently, and expects diffs to be infrequent
    /// at each round.
    Historical,
}



================================================
File: crates/kitsune_p2p/types/src/fixt.rs
================================================
//! Fixturators for Kitsune P2p types

use crate::agent_info::AgentInfoSigned;
use crate::agent_info::UrlList;
use ::fixt::prelude::*;
use kitsune_p2p_bin_data::fixt::KitsuneAgentFixturator;
use kitsune_p2p_bin_data::fixt::KitsuneSignatureFixturator;
use kitsune_p2p_bin_data::fixt::KitsuneSpaceFixturator;
use kitsune_p2p_dht::arq::ArqSize;
use std::sync::Arc;
use url2::url2;

fixturator!(
    UrlList;
    curve Empty vec![];
    curve Unpredictable {
        let mut rng = ::fixt::rng();
        let vec_len = rng.gen_range(1..3);
        let mut ret = vec![];

        for _ in 0..vec_len {
            let s = fixt!(String).chars().take(10).collect::<String>();
            ret.push(url2!("https://example.com/{}", s).into());
        }
        ret
    };
    curve Predictable {
        let mut rng = ::fixt::rng();
        let vec_len = rng.gen_range(1..3);
        let mut ret = vec![];

        for _ in 0..vec_len {
            let s = fixt!(String, Predictable).chars().take(10).collect::<String>();
            ret.push(url2!("https://example.com/{}", s).into());
        }
        ret
    };
);

fixturator!(
    AgentInfoSigned;
    curve Empty {
        block_on(async move {
            AgentInfoSigned::sign(
                Arc::new(fixt!(KitsuneSpace, Empty)),
                Arc::new(fixt!(KitsuneAgent, Empty)),
                ArqSize {
                    count: 8.into(),
                    power: 17
                },
                fixt!(UrlList, Empty),
                0,
                0,
                |_| async move {
                    Ok(Arc::new(fixt!(KitsuneSignature, Empty)))
                },
            ).await.unwrap()
        })
    };
    curve Unpredictable {
        block_on(async move {
            AgentInfoSigned::sign(
                Arc::new(fixt!(KitsuneSpace, Unpredictable)),
                Arc::new(fixt!(KitsuneAgent, Unpredictable)),
                ArqSize {
                    count: 8.into(),
                    power: 17,
                },
                fixt!(UrlList, Unpredictable),
                0,
                0,
                |_| async move {
                    Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable)))
                },
            ).await.unwrap()
        })
    };
    curve Predictable {
        block_on(async move {
            AgentInfoSigned::sign(
                Arc::new(fixt!(KitsuneSpace, Predictable)),
                Arc::new(fixt!(KitsuneAgent, Predictable)),
                ArqSize {
                    count: 8.into(),
                    power: 17,
                },
                fixt!(UrlList, Empty),
                0,
                0,
                |_| async move {
                    Ok(Arc::new(fixt!(KitsuneSignature, Predictable)))
                },
            ).await.unwrap()
        })
    };
);

/// make fixturators sync for now
fn block_on<F>(f: F) -> F::Output
where
    F: 'static + std::future::Future + Send,
    F::Output: 'static + Send,
{
    tokio::task::block_in_place(move || tokio::runtime::Handle::current().block_on(f))
}



================================================
File: crates/kitsune_p2p/types/src/lib.rs
================================================
#![deny(missing_docs)]
//! Types subcrate for kitsune-p2p.

/// Re-exported dependencies.
pub mod dependencies {
    pub use ::futures;
    pub use ::ghost_actor;
    pub use ::holochain_trace;
    pub use ::lair_keystore_api;
    pub use ::paste;
    pub use ::rustls;
    pub use ::serde;
    pub use ::serde_json;
    pub use ::thiserror;
    pub use ::tokio;
    pub use ::url2;

    #[cfg(feature = "fuzzing")]
    pub use ::proptest;
    #[cfg(feature = "fuzzing")]
    pub use ::proptest_derive;
}

use std::sync::Arc;

/// Typedef for result of `proc_count_now()`.
/// This value is on the scale of microseconds.
pub type ProcCountMicros = i64;

/// Monotonically nondecreasing process tick count, backed by tokio::time::Instant
/// as an i64 to facilitate reference times that may be less than the first
/// call to this function.
/// The returned value is on the scale of microseconds.
pub fn proc_count_now_us() -> ProcCountMicros {
    use once_cell::sync::Lazy;
    use tokio::time::Instant;
    static PROC_COUNT: Lazy<Instant> = Lazy::new(Instant::now);
    let r = *PROC_COUNT;
    Instant::now().saturating_duration_since(r).as_micros() as i64
}

/// Get the elapsed process count duration from a captured `ProcCount` to now.
/// If the duration would be negative, this fn returns a zero Duration.
pub fn proc_count_us_elapsed(pc: ProcCountMicros) -> std::time::Duration {
    let dur = proc_count_now_us() - pc;
    let dur = if dur < 0 { 0 } else { dur as u64 };
    std::time::Duration::from_micros(dur)
}

/// Helper function for the common case of returning this nested Unit type.
pub fn unit_ok_fut<E1, E2>() -> Result<MustBoxFuture<'static, Result<(), E2>>, E1> {
    use futures::FutureExt;
    Ok(async move { Ok(()) }.boxed().into())
}

/// Helper function for the common case of returning this boxed future type.
pub fn ok_fut<E1, R: Send + 'static>(result: R) -> Result<MustBoxFuture<'static, R>, E1> {
    use futures::FutureExt;
    Ok(async move { result }.boxed().into())
}

/// Helper function for the common case of returning this boxed future type.
pub fn box_fut_plain<'a, R: Send + 'a>(result: R) -> BoxFuture<'a, R> {
    use futures::FutureExt;
    async move { result }.boxed()
}

/// Helper function for the common case of returning this boxed future type.
pub fn box_fut<'a, R: Send + 'a>(result: R) -> MustBoxFuture<'a, R> {
    box_fut_plain(result).into()
}

use ::ghost_actor::dependencies::tracing;
use futures::future::BoxFuture;
use ghost_actor::dependencies::must_future::MustBoxFuture;

/// 32 byte binary TLS certificate digest.
pub type CertDigest = lair_keystore_api::encoding_types::BinDataSized<32>;

/// Extension trait for working with CertDigests.
pub trait CertDigestExt {
    /// Construct from a slice. Panicks if `slice.len() != 32`.
    fn from_slice(slice: &[u8]) -> Self;
}

impl CertDigestExt for CertDigest {
    fn from_slice(slice: &[u8]) -> Self {
        let mut out = [0; 32];
        out.copy_from_slice(slice);
        out.into()
    }
}

/// Error related to remote communication.
#[derive(Debug, thiserror::Error)]
#[non_exhaustive]
pub enum KitsuneErrorKind {
    /// Temp error type for internal logic.
    #[error("Unit")]
    Unit,

    /// The operation timed out.
    #[error("Operation timed out")]
    TimedOut(String),

    /// This object is closed, calls on it are invalid.
    #[error("This object is closed, calls on it are invalid.")]
    Closed,

    /// The operation is unauthorized by the host.
    #[error("Unauthorized")]
    Unauthorized,

    /// Bad external input.
    /// Can't proceed, but we don't have to shut everything down, either.
    #[error("Bad external input. Error: {0}  Input: {1}")]
    BadInput(Box<dyn std::error::Error + Send + Sync>, String),

    /// Unspecified error.
    #[error(transparent)]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

impl PartialEq for KitsuneErrorKind {
    fn eq(&self, oth: &Self) -> bool {
        #[allow(clippy::match_like_matches_macro)]
        match (self, oth) {
            (Self::TimedOut(a), Self::TimedOut(b)) => a == b,
            (Self::Closed, Self::Closed) => true,
            _ => false,
        }
    }
}

/// Error related to remote communication.
#[derive(Clone, Debug)]
pub struct KitsuneError(pub Arc<KitsuneErrorKind>);

impl std::fmt::Display for KitsuneError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl std::error::Error for KitsuneError {}

impl KitsuneError {
    /// the "kind" of this KitsuneError
    pub fn kind(&self) -> &KitsuneErrorKind {
        &self.0
    }

    /// Create a bad_input error
    pub fn bad_input(e: impl Into<Box<dyn std::error::Error + Send + Sync>>, i: String) -> Self {
        Self(Arc::new(KitsuneErrorKind::BadInput(e.into(), i)))
    }

    /// promote a custom error type to a KitsuneError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self(Arc::new(KitsuneErrorKind::Other(e.into())))
    }
}

impl From<KitsuneErrorKind> for KitsuneError {
    fn from(k: KitsuneErrorKind) -> Self {
        Self(Arc::new(k))
    }
}

impl From<String> for KitsuneError {
    fn from(s: String) -> Self {
        #[derive(Debug, thiserror::Error)]
        struct OtherError(String);
        impl std::fmt::Display for OtherError {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                write!(f, "{}", self.0)
            }
        }

        KitsuneError::other(OtherError(s))
    }
}

impl From<&str> for KitsuneError {
    fn from(s: &str) -> Self {
        s.to_string().into()
    }
}

impl From<KitsuneError> for () {
    fn from(_: KitsuneError) {}
}

impl From<()> for KitsuneError {
    fn from(_: ()) -> Self {
        KitsuneErrorKind::Unit.into()
    }
}

/// Result type for remote communication.
pub type KitsuneResult<T> = Result<T, KitsuneError>;

mod timeout;
pub use timeout::*;

pub mod agent_info;
pub mod async_lazy;
pub mod bootstrap;
pub mod codec;
pub mod combinators;
pub mod config;
pub mod consistency;
pub mod fetch_pool;
pub mod metrics;
pub mod task_agg;
pub mod tls;
pub use kitsune_p2p_bin_data as bin_types;
pub mod tx_utils;

pub use kitsune_p2p_timestamp::Timestamp;

#[cfg(feature = "fixt")]
pub mod fixt;

pub use fetch_pool::GossipType;

pub use kitsune_p2p_dht as dht;
pub use kitsune_p2p_dht_arc as dht_arc;

/// KitsuneAgent in an Arc
pub type KAgent = Arc<bin_types::KitsuneAgent>;
/// KitsuneBasis in an Arc
pub type KBasis = Arc<bin_types::KitsuneBasis>;
/// KitsuneOpHash in an Arc
pub type KOpHash = Arc<bin_types::KitsuneOpHash>;
/// KitsuneSpace in an Arc
pub type KSpace = Arc<bin_types::KitsuneSpace>;
/// KitsuneOpData in an Arc
pub type KOpData = Arc<bin_types::KitsuneOpData>;



================================================
File: crates/kitsune_p2p/types/src/metrics.rs
================================================
//! Utilities for helping with metric tracking.

use crate::tracing;
use holochain_trace::tracing::Instrument;
use kitsune_p2p_bin_data::KitsuneAgent;
use kitsune_p2p_timestamp::Timestamp;
use std::sync::{
    atomic::{AtomicU64, AtomicUsize, Ordering},
    Arc, Once,
};
use sysinfo::ProcessesToUpdate;

static SYS_INFO: Once = Once::new();

static TASK_COUNT: AtomicUsize = AtomicUsize::new(0);
static USED_MEM_KB: AtomicU64 = AtomicU64::new(0);
static PROC_CPU_USAGE_PCT_1000: AtomicUsize = AtomicUsize::new(0);
static TX_BYTES_PER_SEC: AtomicU64 = AtomicU64::new(0);
static RX_BYTES_PER_SEC: AtomicU64 = AtomicU64::new(0);
static SENDS_PER_SEC: AtomicU64 = AtomicU64::new(0);
static RECVS_PER_SEC: AtomicU64 = AtomicU64::new(0);

macro_rules! _make_cntr {
    (
        $doc:literal,
        $push:ident,
        $pull:ident,
        $m:ident,
        $stat:ident,
    ) => {
        mod $m {
            use super::*;

            pub(crate) static $stat: AtomicU64 = AtomicU64::new(0);
        }

        #[doc = $doc]
        #[allow(dead_code)]
        pub(crate) fn $push(v: u64) {
            $m::$stat.fetch_add(v, Ordering::SeqCst);
        }

        pub(crate) fn $pull() -> u64 {
            $m::$stat
                .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |_| Some(0))
                .unwrap()
        }
    };
}

macro_rules! make_cntr {
    ($doc:literal, $push:ident, $pull:ident,) => {
        paste::paste! {
            _make_cntr!(
                $doc,
                $push,
                $pull,
                [<__ $push:snake>],
                [<$push:snake:upper>],
            );
        }
    };
}

make_cntr!(
    "count the raw number of messages sent out by this process",
    metric_push_raw_send_count,
    metric_pull_raw_send_count,
);

make_cntr!(
    "count the raw number of messages received by this process",
    metric_push_raw_recv_count,
    metric_pull_raw_recv_count,
);

macro_rules! _make_avg {
    (
        $doc:literal,
        $push:ident,
        $pull:ident,
        $m:ident,
        $stat:ident,
    ) => {
        mod $m {
            use super::*;

            pub(crate) static $stat: AtomicU64 = AtomicU64::new(0);
        }

        #[doc = $doc]
        #[allow(dead_code)]
        pub(crate) fn $push(v: u64) {
            // implement this as a bizzarre "drifting" average
            // old entries account for 4/5 of the weight while
            // new entries only effect the difference by one fifth,
            // so they won't start to dominate until > 10 events.
            $m::$stat
                .fetch_update(Ordering::SeqCst, Ordering::SeqCst, move |p| {
                    if p == 0 {
                        Some(v)
                    } else {
                        Some((p * 4 + v) / 5)
                    }
                })
                .unwrap();
        }

        pub(crate) fn $pull() -> u64 {
            $m::$stat.load(Ordering::SeqCst)
        }
    };
}

macro_rules! make_avg {
    ($doc:literal, $push:ident, $pull:ident,) => {
        paste::paste! {
            _make_avg!(
                $doc,
                $push,
                $pull,
                [<__ $push:snake>],
                [<$push:snake:upper>],
            );
        }
    };
}

make_avg!(
    "mark the size of a pool buf when released, so we can tune the shrink size",
    metric_push_pool_buf_release_size,
    metric_pull_pool_buf_release_size,
);

make_avg!(
    "kitsune api request / success response elapsed time",
    metric_push_api_req_res_elapsed_ms,
    metric_pull_api_req_res_elasped_ms,
);

/// Spawns a tokio task with given future/async block.
/// Captures a new TaskCounter instance to track task count.
pub fn metric_task<T, E, F>(f: F) -> tokio::task::JoinHandle<Result<T, E>>
where
    T: 'static + Send,
    E: 'static + Send + std::fmt::Debug,
    F: 'static + Send + std::future::Future<Output = Result<T, E>>,
{
    metric_task_instrumented(None, f)
}

/// Spawns a tokio task with given future/async block.
/// Captures a new TaskCounter instance to track task count.
pub fn metric_task_instrumented<T, E, F>(
    scope: Option<String>,
    f: F,
) -> tokio::task::JoinHandle<Result<T, E>>
where
    T: 'static + Send,
    E: 'static + Send + std::fmt::Debug,
    F: 'static + Send + std::future::Future<Output = Result<T, E>>,
{
    let counter = MetricTaskCounter::new();
    let task = async move {
        let _counter = counter;
        let res = f
            .instrument(tracing::info_span!("kitsune-metric-task", scope = scope))
            .await;
        if let Err(e) = &res {
            ghost_actor::dependencies::tracing::error!(?e, "METRIC TASK ERROR");
        }
        res
    };

    tokio::task::spawn(task)
}

/// System Info.
#[derive(Debug, Clone, serde::Serialize)]
pub struct MetricSysInfo {
    /// Used system memory KB.
    pub used_mem_kb: u64,
    /// Process CPU Usage % x1000.
    pub proc_cpu_usage_pct_1000: usize,
    /// network bytes transmitted (5 sec avg).
    pub tx_bytes_per_sec: u64,
    /// network bytes received (5 sec avg).
    pub rx_bytes_per_sec: u64,
    /// raw message send count (5 sec avg).
    pub sends_per_sec: u64,
    /// raw message recv count (5 sec avg).
    pub recvs_per_sec: u64,
    /// number of active tokio tasks
    pub tokio_task_count: usize,
    /// avg size of released pool bufs (bytes)
    pub avg_pool_buf_release_size: u64,
    /// avg api req/res elapsed time (ms)
    pub avg_api_req_res_elapsed_ms: u64,
}

/// Initialize polling of system usage info
pub fn init_sys_info_poll() {
    struct FiveAvg {
        idx: usize,
        data: [u64; 5],
    }

    impl FiveAvg {
        pub fn new() -> Self {
            Self {
                idx: 0,
                data: [0; 5],
            }
        }

        pub fn push(&mut self, val: u64) {
            self.data[self.idx] = val;
            self.idx += 1;
            if self.idx >= self.data.len() {
                self.idx = 0;
            }
        }

        pub fn avg(&self) -> u64 {
            let mut tot = 0;
            for f in self.data.iter() {
                tot += f;
            }
            tot / self.data.len() as u64
        }
    }

    SYS_INFO.call_once(|| {
        metric_task(async move {
            let mut system = sysinfo::System::new_with_specifics(
                sysinfo::RefreshKind::new()
                    .with_processes(sysinfo::ProcessRefreshKind::new().with_memory().with_cpu()),
            );
            let mut networks = sysinfo::Networks::new();

            let pid = sysinfo::get_current_pid().unwrap();
            let mut tx_avg = FiveAvg::new();
            let mut rx_avg = FiveAvg::new();
            let mut send_avg = FiveAvg::new();
            let mut recv_avg = FiveAvg::new();

            let mut p_count: usize = 0;

            loop {
                system.refresh_processes(ProcessesToUpdate::Some(&[pid]), true);
                networks.refresh_list();

                let proc = system.process(pid).unwrap();

                let mem = proc.memory();
                USED_MEM_KB.store(mem, Ordering::Relaxed);

                let cpu = (proc.cpu_usage() * 1000.0) as usize;
                PROC_CPU_USAGE_PCT_1000.store(cpu, Ordering::Relaxed);

                let mut tx = 0;
                let mut rx = 0;
                for (_n, network) in networks.iter() {
                    tx += network.transmitted();
                    rx += network.received();
                }
                tx_avg.push(tx);
                rx_avg.push(rx);
                TX_BYTES_PER_SEC.store(tx_avg.avg(), Ordering::Relaxed);
                RX_BYTES_PER_SEC.store(rx_avg.avg(), Ordering::Relaxed);

                send_avg.push(metric_pull_raw_send_count());
                SENDS_PER_SEC.store(send_avg.avg(), Ordering::Relaxed);
                recv_avg.push(metric_pull_raw_recv_count());
                RECVS_PER_SEC.store(recv_avg.avg(), Ordering::Relaxed);

                p_count += 1;
                if p_count == 15 {
                    p_count = 0;
                    let sys_info = get_sys_info();
                    tracing::info!(?sys_info);
                }

                tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            }

            // this is needed for type-ing the block, but will never return
            #[allow(unreachable_code)]
            <Result<(), ()>>::Ok(())
        });
    });
}

/// Capture current sys_info metrics. Be sure you invoked `init_sys_info_poll`.
pub fn get_sys_info() -> MetricSysInfo {
    MetricSysInfo {
        used_mem_kb: USED_MEM_KB.load(Ordering::Relaxed),
        proc_cpu_usage_pct_1000: PROC_CPU_USAGE_PCT_1000.load(Ordering::Relaxed),
        tx_bytes_per_sec: TX_BYTES_PER_SEC.load(Ordering::Relaxed),
        rx_bytes_per_sec: RX_BYTES_PER_SEC.load(Ordering::Relaxed),
        sends_per_sec: SENDS_PER_SEC.load(Ordering::Relaxed),
        recvs_per_sec: RECVS_PER_SEC.load(Ordering::Relaxed),
        tokio_task_count: TASK_COUNT.load(Ordering::Relaxed),
        avg_pool_buf_release_size: metric_pull_pool_buf_release_size(),
        avg_api_req_res_elapsed_ms: metric_pull_api_req_res_elasped_ms(),
    }
}

/// Increases task count on `TaskCounter::new()`, decreases on drop.
pub struct MetricTaskCounter(());

impl Drop for MetricTaskCounter {
    fn drop(&mut self) {
        TASK_COUNT.fetch_sub(1, Ordering::Relaxed);
    }
}

impl Default for MetricTaskCounter {
    fn default() -> Self {
        Self::new()
    }
}

impl MetricTaskCounter {
    /// Increase task count while intance exists, decrease when dropped.
    pub fn new() -> Self {
        TASK_COUNT.fetch_add(1, Ordering::Relaxed);
        Self(())
    }
}

const METRIC_KIND_UNKNOWN: &str = "Unknown";
const METRIC_KIND_REACHABILITY_QUOTIENT: &str = "ReachabilityQuotient";
const METRIC_KIND_LATENCY_MICROS: &str = "LatencyMicros";
const METRIC_KIND_AGG_EXTRAP_COV: &str = "AggExtrapCov";

/// An individual metric record
#[derive(Debug)]
pub struct MetricRecord {
    /// kind of this record
    pub kind: MetricRecordKind,

    /// agent associated with this metric (if applicable)
    pub agent: Option<Arc<KitsuneAgent>>,

    /// timestamp this metric was recorded at
    pub recorded_at_utc: Timestamp,

    /// timestamp this metric will expire and be available for pruning
    pub expires_at_utc: Timestamp,

    /// additional data associated with this metric
    pub data: serde_json::Value,
}

/// The type of metric recorded
#[derive(Debug)]
pub enum MetricRecordKind {
    /// Failure to parse metric kind
    Unknown,

    /// ReachabilityQuotient metric kind
    ReachabilityQuotient,

    /// LatencyMicros metric kind
    LatencyMicros,

    /// AggExtrapCov metric kind
    AggExtrapCov,
}

impl MetricRecordKind {
    /// database format of this kind variant
    pub fn to_db(&self) -> &'static str {
        use MetricRecordKind::*;
        match self {
            Unknown => METRIC_KIND_UNKNOWN,
            ReachabilityQuotient => METRIC_KIND_REACHABILITY_QUOTIENT,
            LatencyMicros => METRIC_KIND_LATENCY_MICROS,
            AggExtrapCov => METRIC_KIND_AGG_EXTRAP_COV,
        }
    }

    /// parse a database kind into a rust enum variant
    pub fn from_db(input: &str) -> Self {
        use MetricRecordKind::*;
        if input == METRIC_KIND_REACHABILITY_QUOTIENT {
            ReachabilityQuotient
        } else if input == METRIC_KIND_LATENCY_MICROS {
            LatencyMicros
        } else if input == METRIC_KIND_AGG_EXTRAP_COV {
            AggExtrapCov
        } else {
            Unknown
        }
    }
}

type WriteLenCb = Box<dyn Fn(&'static str, usize) + 'static + Send + Sync>;

/// Metrics callback manager to be injected into the endpoint
pub struct Tx2ApiMetrics {
    write_len: Option<WriteLenCb>,
}

impl Default for Tx2ApiMetrics {
    fn default() -> Self {
        Self::new()
    }
}

impl Tx2ApiMetrics {
    /// Construct a new default Tx2ApiMetrics with no set callbacks
    pub fn new() -> Self {
        Self { write_len: None }
    }

    /// This callback will be invoked when we successfully write data
    /// to a transport connection.
    pub fn set_write_len<F>(mut self, f: F) -> Self
    where
        F: Fn(&'static str, usize) + 'static + Send + Sync,
    {
        let f: WriteLenCb = Box::new(f);
        self.write_len = Some(f);
        self
    }

    #[allow(dead_code)]
    fn write_len(&self, d: &'static str, l: usize) {
        if let Some(cb) = &self.write_len {
            cb(d, l)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::{get_sys_info, init_sys_info_poll, metric_task, TASK_COUNT};
    use std::sync::atomic::Ordering;

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "flakey. https://app.circleci.com/jobs/github/holochain/holochain/12604"]
    async fn test_metric_task() {
        for _ in 0..20 {
            metric_task(async move {
                tokio::time::sleep(std::time::Duration::from_millis(3)).await;
                <Result<(), ()>>::Ok(())
            });
        }
        let gt_task_count = TASK_COUNT.load(Ordering::Relaxed);
        tokio::time::sleep(std::time::Duration::from_millis(100)).await;
        let lt_task_count = TASK_COUNT.load(Ordering::Relaxed);
        assert!(lt_task_count < gt_task_count);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_sys_info() {
        holochain_trace::test_run();
        init_sys_info_poll();
        tokio::time::sleep(std::time::Duration::from_millis(200)).await;
        let sys_info = get_sys_info();
        ghost_actor::dependencies::tracing::info!(?sys_info);
    }
}



================================================
File: crates/kitsune_p2p/types/src/task_agg.rs
================================================
//! Task Aggregation helper utility

use futures::future::{BoxFuture, FutureExt};
use futures::stream::futures_unordered::FuturesUnordered;
use futures::stream::StreamExt;
use parking_lot::Mutex;
use std::sync::Arc;
use tokio::sync::Notify;

/// A "Driver" is a generic static future with no output.
pub type Driver = BoxFuture<'static, ()>;

struct TaskAggInner {
    notify: Arc<Notify>,
    driver_list: Vec<Driver>,
    n_driver: bool,
    d_count: usize,
}

/// Task Aggregation helper struct handle.
#[derive(Clone)]
pub struct TaskAgg(Arc<Mutex<TaskAggInner>>);

impl TaskAgg {
    /// Construct a new Task Aggregation driver and handle.
    pub fn new() -> (Driver, Self) {
        let inner = Arc::new(Mutex::new(TaskAggInner {
            notify: Arc::new(Notify::new()),
            driver_list: Vec::new(),
            n_driver: false,
            d_count: 0,
        }));

        let driver = {
            let inner = inner.clone();
            async move {
                let mut fu = FuturesUnordered::new();
                let mut driver_list = Vec::new();

                loop {
                    let cont = {
                        let mut lock = inner.lock();
                        if lock.d_count == 0 {
                            false
                        } else {
                            driver_list.append(&mut lock.driver_list);
                            if !lock.n_driver {
                                lock.n_driver = true;
                                let n = lock.notify.clone();
                                let inner = inner.clone();
                                driver_list.push(
                                    async move {
                                        n.notified().await;
                                        let mut lock = inner.lock();
                                        lock.n_driver = false;
                                    }
                                    .boxed(),
                                );
                            }
                            true
                        }
                    };

                    if cont {
                        for driver in driver_list.drain(..) {
                            fu.push(driver);
                        }
                    } else {
                        // d_count must be zero, we can break
                        break;
                    }

                    let _ = fu.next().await;
                }
            }
            .boxed()
        };

        (driver, Self(inner))
    }

    /// Push a new driver task into this aggregation construct.
    pub fn push(&self, f: Driver) {
        let inner = self.0.clone();
        let mut lock = self.0.lock();
        lock.d_count += 1;
        lock.driver_list.push(
            async move {
                f.await;
                inner.lock().d_count -= 1;
            }
            .boxed(),
        );
        lock.notify.notify_waiters();
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_task_agg() {
        let (s, r) = tokio::sync::oneshot::channel();

        let (driver, agg) = TaskAgg::new();

        let agg2 = agg.clone();
        agg.push(
            async move {
                agg2.push(
                    async move {
                        println!("test");
                        s.send(()).unwrap();
                    }
                    .boxed(),
                );
            }
            .boxed(),
        );

        driver.await;
        r.await.unwrap();
    }
}



================================================
File: crates/kitsune_p2p/types/src/timeout.rs
================================================
use crate::*;

use std::sync::atomic::{AtomicU64, Ordering};

/// Kitsune Backoff
#[derive(Debug, Clone)]
pub struct KitsuneBackoff {
    timeout: KitsuneTimeout,
    cur_ms: Arc<AtomicU64>,
    max_ms: u64,
}

impl KitsuneBackoff {
    /// backoff constructor
    pub fn new(timeout: KitsuneTimeout, initial_ms: u64, max_ms: u64) -> Self {
        let cur_ms = Arc::new(AtomicU64::new(initial_ms));
        Self {
            timeout,
            cur_ms,
            max_ms,
        }
    }

    /// Wait for the current backoff time (but not longer than timeout expiry)
    /// then increment the current wait duration
    pub async fn wait(&self) {
        // get the current val
        let cur = self.cur_ms.load(Ordering::Relaxed);

        // it's ok if our exponential isn't *exactly* exponential
        // due to multiple threads increasing this simultaneously
        self.cur_ms.fetch_add(cur, Ordering::Relaxed);

        // cap this at max and/or time remaining
        let cur = std::cmp::min(
            cur,
            std::cmp::min(
                self.max_ms,
                // add 1ms to our time remaining so we don't have a weird
                // race condition where we don't wait at all, but our
                // timer hasn't quite expired yet...
                self.timeout.time_remaining().as_millis() as u64 + 1,
            ),
        );

        // wait that time
        if cur > 0 {
            tokio::time::sleep(std::time::Duration::from_millis(cur)).await;
        }
    }
}

/// Kitsune Timeout
#[derive(Debug, Clone, Copy)]
pub struct KitsuneTimeout(tokio::time::Instant);

impl KitsuneTimeout {
    /// Create a new timeout for duration in the future.
    pub fn new(duration: std::time::Duration) -> Self {
        Self(tokio::time::Instant::now().checked_add(duration).unwrap())
    }

    /// Convenience fn to create a new timeout for an amount of milliseconds.
    pub fn from_millis(millis: u64) -> Self {
        Self::new(std::time::Duration::from_millis(millis))
    }

    /// Generate a backoff instance bound to this timeout
    pub fn backoff(&self, initial_ms: u64, max_ms: u64) -> KitsuneBackoff {
        KitsuneBackoff::new(*self, initial_ms, max_ms)
    }

    /// Get Duration until timeout expires.
    pub fn time_remaining(&self) -> std::time::Duration {
        self.0
            .saturating_duration_since(tokio::time::Instant::now())
    }

    /// Has this timeout expired?
    pub fn is_expired(&self) -> bool {
        self.0 <= tokio::time::Instant::now()
    }

    /// `Ok(())` if not expired, `Err(KitsuneError::TimedOut)` if expired.
    pub fn ok(&self, ctx: &str) -> KitsuneResult<()> {
        if self.is_expired() {
            Err(KitsuneErrorKind::TimedOut(ctx.into()).into())
        } else {
            Ok(())
        }
    }

    /// Wrap a future with one that will timeout when this timeout expires.
    pub fn mix<'a, 'b, R, F>(
        &'a self,
        ctx: &str,
        f: F,
    ) -> impl std::future::Future<Output = KitsuneResult<R>> + 'b + Send
    where
        R: 'b,
        F: std::future::Future<Output = KitsuneResult<R>> + 'b + Send,
    {
        let time_remaining = self.time_remaining();
        let ctx = ctx.to_string();
        async move {
            match tokio::time::timeout(time_remaining, f).await {
                Ok(r) => r,
                Err(_) => Err(KitsuneErrorKind::TimedOut(ctx).into()),
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn basic_kitsune_timeout() {
        let t = KitsuneTimeout::new(std::time::Duration::from_millis(40));
        assert!(t.time_remaining().as_millis() > 0);
        assert!(!t.is_expired());
    }

    #[tokio::test]
    async fn expired_kitsune_timeout() {
        let t = KitsuneTimeout::new(std::time::Duration::from_millis(1));
        tokio::time::sleep(std::time::Duration::from_millis(2)).await;
        assert!(t.time_remaining().as_micros() == 0);
        assert!(t.is_expired());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn kitsune_backoff() {
        let t = KitsuneTimeout::from_millis(100);
        let mut times = Vec::new();
        let start = tokio::time::Instant::now();
        let bo = t.backoff(2, 15);
        while !t.is_expired() {
            times.push(start.elapsed().as_millis() as u64);
            bo.wait().await;
        }
        println!("{:?}", times);
        assert!(times.len() > 4);
        assert!(times.len() < 20);
    }
}



================================================
File: crates/kitsune_p2p/types/src/tls.rs
================================================
//! TLS utils for kitsune

use crate::config::*;
use crate::*;
use once_cell::sync::Lazy;

/// Tls Configuration.
#[derive(Clone)]
pub struct TlsConfig {
    /// Cert
    pub cert: Arc<[u8]>,

    /// Cert Priv Key
    pub cert_priv_key: lair_keystore_api::dependencies::sodoken::BufRead,

    /// Cert Digest
    pub cert_digest: CertDigest,
}

impl TlsConfig {
    /// Create a new ephemeral tls certificate that will not be persisted.
    pub async fn new_ephemeral() -> KitsuneResult<Self> {
        let cert = lair_keystore_api::internal::tls::tls_cert_self_signed_new()
            .await
            .map_err(KitsuneError::other)?;

        Ok(Self {
            cert: cert.cert,
            cert_priv_key: cert.priv_key,
            cert_digest: cert.digest.into(),
        })
    }
}

/// Allow only these cipher suites for kitsune Tls.
static CIPHER_SUITES: &[rustls::SupportedCipherSuite] = &[
    rustls::cipher_suite::TLS13_CHACHA20_POLY1305_SHA256,
    rustls::cipher_suite::TLS13_AES_256_GCM_SHA384,
];

/// Single shared keylog file all sessions can report to
static KEY_LOG: Lazy<Arc<dyn rustls::KeyLog>> = Lazy::new(|| Arc::new(rustls::KeyLogFile::new()));

/// Helper to generate rustls configs given a TlsConfig reference.
#[allow(dead_code)]
pub fn gen_tls_configs(
    alpn: &[u8],
    tls: &TlsConfig,
    tuning_params: KitsuneP2pTuningParams,
) -> KitsuneResult<(Arc<rustls::ServerConfig>, Arc<rustls::ClientConfig>)> {
    let cert = rustls::Certificate(tls.cert.to_vec());
    let cert_priv_key = rustls::PrivateKey(tls.cert_priv_key.read_lock().to_vec());

    let root_cert = rustls::Certificate(lair_keystore_api::internal::tls::WK_CA_CERT_DER.to_vec());
    let mut root_store = rustls::RootCertStore::empty();
    root_store.add(&root_cert).unwrap();

    let mut tls_server_config = rustls::ServerConfig::builder()
        .with_cipher_suites(CIPHER_SUITES)
        .with_safe_default_kx_groups()
        .with_protocol_versions(&[&rustls::version::TLS13])
        .map_err(KitsuneError::other)?
        .with_client_cert_verifier(
            rustls::server::AllowAnyAuthenticatedClient::new(root_store).boxed(),
        )
        .with_single_cert(vec![cert.clone()], cert_priv_key.clone())
        .map_err(KitsuneError::other)?;

    if tuning_params.use_env_tls_keylog() {
        tls_server_config.key_log = KEY_LOG.clone();
    }
    tls_server_config.ticketer = rustls::Ticketer::new().map_err(KitsuneError::other)?;
    tls_server_config.session_storage = rustls::server::ServerSessionMemoryCache::new(
        tuning_params.tls_in_mem_session_storage as usize,
    );
    tls_server_config.alpn_protocols.push(alpn.to_vec());

    let tls_server_config = Arc::new(tls_server_config);

    let mut tls_client_config = rustls::ClientConfig::builder()
        .with_cipher_suites(CIPHER_SUITES)
        .with_safe_default_kx_groups()
        .with_protocol_versions(&[&rustls::version::TLS13])
        .map_err(KitsuneError::other)?
        .with_custom_certificate_verifier(TlsServerVerifier::new())
        .with_client_auth_cert(vec![cert], cert_priv_key)
        .map_err(KitsuneError::other)?;

    if tuning_params.use_env_tls_keylog() {
        tls_client_config.key_log = KEY_LOG.clone();
    }
    tls_client_config.resumption = rustls::client::Resumption::in_memory_sessions(
        tuning_params.tls_in_mem_session_storage as usize,
    );
    tls_client_config.alpn_protocols.push(alpn.to_vec());

    let tls_client_config = Arc::new(tls_client_config);

    Ok((tls_server_config, tls_client_config))
}

struct TlsServerVerifier;

impl TlsServerVerifier {
    fn new() -> Arc<Self> {
        Arc::new(Self)
    }
}

impl rustls::client::ServerCertVerifier for TlsServerVerifier {
    fn verify_server_cert(
        &self,
        _end_entity: &rustls::Certificate,
        _intermediates: &[rustls::Certificate],
        _server_name: &rustls::ServerName,
        _scts: &mut dyn Iterator<Item = &[u8]>,
        _ocsp_response: &[u8],
        _now: std::time::SystemTime,
    ) -> Result<rustls::client::ServerCertVerified, rustls::Error> {
        // TODO - check acceptable cert digest

        Ok(rustls::client::ServerCertVerified::assertion())
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils.rs
================================================
#![allow(clippy::never_loop)] // using for block breaking
//! Utilities to help with developing / testing transport layer.

mod active;
pub use active::*;

mod latency;
pub use latency::*;

mod logic_chan;
pub use logic_chan::*;

mod framed;
pub use framed::*;

mod mem_chan;
pub use mem_chan::*;

mod notify_all;
pub use notify_all::*;

mod pool_buf;
pub use pool_buf::*;

mod proxy_url;
pub use proxy_url::*;

mod resource_bucket;
pub use resource_bucket::*;

mod share;
pub use share::*;

mod t_chan;
pub use t_chan::*;

mod tx_url;
pub use tx_url::*;

pub use super::metrics::metric_task;



================================================
File: crates/kitsune_p2p/types/src/tx_utils/active.rs
================================================
use crate::tx_utils::*;
use crate::*;
use futures::future::FutureExt;

#[derive(Clone)]
struct ActiveInner(NotifyAll);

impl ActiveInner {
    pub fn new() -> Self {
        Self(NotifyAll::new())
    }

    pub fn register_kill_cb<F>(&self, cb: F)
    where
        F: FnOnce() + 'static + Send,
    {
        self.0.wait_cb(cb);
    }

    pub fn kill(&self) {
        self.0.notify();
    }

    pub fn is_active(&self) -> bool {
        !self.0.did_notify()
    }

    pub fn fut<'a, 'b, R, F>(
        &'a self,
        f: F,
    ) -> impl std::future::Future<Output = KitsuneResult<R>> + 'b + Send
    where
        R: 'static + Send,
        F: std::future::Future<Output = KitsuneResult<R>> + 'b + Send,
    {
        let f = f.boxed();
        let not = self.0.wait();
        async move {
            match futures::future::select(f, not).await {
                futures::future::Either::Left((v, _)) => v,
                futures::future::Either::Right(_) => Err(KitsuneErrorKind::Closed.into()),
            }
        }
    }
}

/// Active tracking helper for related items.
/// This facilitates e.g. an endpoint with sub connections.
/// The endpoint can close, closing all connections.
/// Or, individual connections can close, without closing the endpoint.
#[derive(Clone)]
pub struct Active(Box<[ActiveInner]>);

impl Default for Active {
    fn default() -> Self {
        Self::new()
    }
}

impl Active {
    /// Create a new active tracker set to "active".
    pub fn new() -> Self {
        Self(Box::new([ActiveInner::new()]))
    }

    /// Mix two active trackers to gether.
    /// The result will be inactive if either parent is inactive.
    pub fn mix(&self, oth: &Self) -> Self {
        let mut out = self.0.to_vec();
        out.extend_from_slice(&oth.0);
        Self(out.into_boxed_slice())
    }

    /// Register a callback to be invoked on kill.
    /// Beware the cb may be invoked multiple times if this Active is mixed.
    pub fn register_kill_cb<F>(&self, cb: F)
    where
        F: Fn() + 'static + Send + Sync,
    {
        type CB = Arc<dyn Fn() + 'static + Send + Sync>;
        let cb: CB = Arc::new(cb);
        for i in self.0.iter() {
            let cb = cb.clone();
            // This needs to be in a closure because it's an Arc<dyn Fn() + 'static + Send + Sync>
            // but clippy doesn't get it.
            #[allow(clippy::redundant_closure)]
            i.register_kill_cb(move || cb());
        }
    }

    /// Kill this active tracker (all trackers if mixed).
    pub fn kill(&self) {
        for i in self.0.iter() {
            i.kill();
        }
    }

    /// If any of the mixed trackers in this instance are not active,
    /// this fn will return false.
    pub fn is_active(&self) -> bool {
        for i in self.0.iter() {
            if !i.is_active() {
                return false;
            }
        }
        true
    }

    /// Wrap a future such that if any of the sub-trackers
    /// within this active tracker instance become inactive
    /// before the future resolve, resolve with a Err::Closed result.
    pub fn fut<'a, 'b, R, F>(
        &'a self,
        f: F,
    ) -> impl std::future::Future<Output = KitsuneResult<R>> + 'b + Send
    where
        R: 'static + Send,
        F: std::future::Future<Output = KitsuneResult<R>> + 'b + Send,
    {
        let mut f = f.boxed();
        for i in self.0.iter() {
            f = i.fut(f).boxed();
        }
        f
    }
}

#[cfg(test)]
mod tests {
    use crate::metrics::metric_task;

    use super::*;
    use std::sync::atomic;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_active_cb() {
        let count = Arc::new(atomic::AtomicUsize::new(0));

        let a1 = Active::new();
        let a2 = Active::new();
        let mix = a1.mix(&a2);

        let c2 = count.clone();
        mix.register_kill_cb(move || {
            c2.fetch_add(1, atomic::Ordering::Relaxed);
        });

        a1.kill();
        a2.kill();
        assert_eq!(2, count.load(atomic::Ordering::Relaxed));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_active() {
        let a1 = Active::new();
        let a2 = Active::new();
        let a3 = Active::new();
        let a4 = Active::new();

        let mix = a1.mix(&a2).mix(&a3).mix(&a4);

        assert!(mix.is_active());

        let f1 = mix.fut(async move {
            tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            Ok(())
        });
        let t1 = metric_task(async move {
            assert!(f1.await.is_ok());
            KitsuneResult::Ok(())
        });

        let f2 = mix.fut(async move {
            tokio::time::sleep(std::time::Duration::from_millis(200)).await;
            Ok(())
        });
        let t2 = metric_task(async move {
            assert!(f2.await.is_err());
            KitsuneResult::Ok(())
        });

        tokio::time::sleep(std::time::Duration::from_millis(120)).await;
        a3.kill();

        t1.await.unwrap().unwrap();
        t2.await.unwrap().unwrap();
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/framed.rs
================================================
use crate::tx_utils::*;
use crate::*;
use futures::future::{BoxFuture, FutureExt};
use futures::io::AsyncReadExt;
use futures::io::AsyncWriteExt;

/// Request/Response bit mask.
const R_MASK: u64 = 1 << 63;

/// Request/Response bit filter.
const R_FILT: u64 = !R_MASK;

/// MsgSize Bytes
const MSG_SIZE_BYTES: usize = 4;

/// MsgId Bytes
const MSG_ID_BYTES: usize = 8;

/// MsgId type
#[derive(Debug)]
pub enum MsgIdType {
    /// Notify-type MsgId
    Notify,

    /// Req-type MsgId
    Req,

    /// Res-type MsgId
    Res,
}

/// 64 bit MsgId - the top bit identifies if Request or Response.
#[derive(Clone, Copy)]
pub struct MsgId(u64);

impl From<u64> for MsgId {
    fn from(v: u64) -> Self {
        Self(v)
    }
}

impl From<MsgId> for u64 {
    fn from(m: MsgId) -> Self {
        m.0
    }
}

impl MsgId {
    /// Create a new MsgId from a raw u64.
    pub fn new(v: u64) -> Self {
        Self(v)
    }

    /// Create a new notify-type MsgId.
    pub fn new_notify() -> Self {
        Self(0)
    }

    /// Get the inner raw value.
    pub fn inner(&self) -> u64 {
        self.0
    }

    /// Get the ID-portion ignoring the req/res bit.
    pub fn as_id(&self) -> u64 {
        self.0 & R_FILT
    }

    /// Get this Id as a request-type MsgId.
    /// (will panic if `as_id() == 0`).
    pub fn as_req(&self) -> Self {
        if self.as_id() == 0 {
            panic!("MsgId::as_id() == 0 cannot be a request-type");
        }
        Self(self.0 & R_FILT)
    }

    /// Get this Id as a response-type MsgId.
    /// (will panic if `as_id() == 0`).
    pub fn as_res(&self) -> Self {
        if self.as_id() == 0 {
            panic!("MsgId::as_id() == 0 cannot be a response-type");
        }
        Self(self.0 | R_MASK)
    }

    /// Get the MsgIdType of this MsgId.
    pub fn get_type(&self) -> MsgIdType {
        if self.is_notify() {
            MsgIdType::Notify
        } else if self.is_req() {
            MsgIdType::Req
        } else if self.is_res() {
            MsgIdType::Res
        } else {
            unreachable!()
        }
    }

    /// Is this MsgId a notify-type?
    pub fn is_notify(&self) -> bool {
        self.0 == 0
    }

    /// Is this MsgId a request-type?
    pub fn is_req(&self) -> bool {
        self.0 != 0 && self.0 & R_MASK == 0
    }

    /// Is this MsgId a response-type?
    pub fn is_res(&self) -> bool {
        self.0 != 0 && self.0 & R_MASK > 0
    }
}

impl std::fmt::Debug for MsgId {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let msg_id_type = self.get_type();
        let id = self.as_id();
        f.debug_struct("MsgId")
            .field("type", &msg_id_type)
            .field("id", &id)
            .finish()
    }
}

type RR = (MsgId, PoolBuf);

/// Efficiently read framed data.
#[cfg_attr(feature = "test_utils", mockall::automock)]
pub trait AsFramedReader: 'static + Send + Unpin {
    /// Read a frame of data from this AsFramedReader instance.
    /// This returns a Vec in case the first read contains multiple small items.
    fn read(&mut self, timeout: KitsuneTimeout) -> BoxFuture<'_, KitsuneResult<RR>>;
}

struct FramedReaderInner {
    sub: Box<dyn futures::io::AsyncRead + 'static + Send + Unpin>,
    local_buf: [u8; 4096],
}

/// Efficiently read framed data from a sub AsyncRead instance.
pub struct FramedReader(Option<FramedReaderInner>);

fn read_size(b: &[u8]) -> usize {
    let mut bytes = [0_u8; MSG_SIZE_BYTES];
    bytes.copy_from_slice(&b[..MSG_SIZE_BYTES]);
    u32::from_le_bytes(bytes) as usize
}

fn read_msg_id(b: &[u8]) -> MsgId {
    let mut bytes = [0_u8; MSG_ID_BYTES];
    bytes.copy_from_slice(&b[..MSG_ID_BYTES]);
    u64::from_le_bytes(bytes).into()
}

impl FramedReader {
    /// Create a new FramedReader instance.
    pub fn new(sub: Box<dyn futures::io::AsyncRead + 'static + Send + Unpin>) -> Self {
        Self(Some(FramedReaderInner {
            sub,
            local_buf: [0; 4096],
        }))
    }
}

impl AsFramedReader for FramedReader {
    fn read(&mut self, timeout: KitsuneTimeout) -> BoxFuture<'_, KitsuneResult<RR>> {
        async move {
            let mut inner = match self.0.take() {
                None => return Err(KitsuneErrorKind::Closed.into()),
                Some(inner) => inner,
            };

            let read_result = timeout
                .mix("FramedReader::read", async {
                    let mut read = 0;
                    let want = MSG_SIZE_BYTES + MSG_ID_BYTES;
                    while read < want {
                        let sub_read = inner
                            .sub
                            .read(&mut inner.local_buf[read..MSG_SIZE_BYTES + MSG_ID_BYTES])
                            .await
                            .map_err(KitsuneError::other)?;
                        if sub_read == 0 {
                            return Err(KitsuneErrorKind::Closed.into());
                        }
                        read += sub_read;
                    }

                    let want_size = read_size(&inner.local_buf[..MSG_SIZE_BYTES])
                        - MSG_SIZE_BYTES
                        - MSG_ID_BYTES;
                    let msg_id = read_msg_id(
                        &inner.local_buf[MSG_SIZE_BYTES..MSG_SIZE_BYTES + MSG_ID_BYTES],
                    );

                    let mut buf = PoolBuf::new();
                    buf.reserve(want_size);
                    while buf.len() < want_size {
                        let to_read = std::cmp::min(inner.local_buf.len(), want_size - buf.len());
                        read = match inner
                            .sub
                            .read(&mut inner.local_buf[..to_read])
                            .await
                            .map_err(KitsuneError::other)
                        {
                            Err(e) => return Err(e),
                            Ok(read) => read,
                        };
                        if read == 0 {
                            return Err(KitsuneErrorKind::Closed.into());
                        }
                        buf.extend_from_slice(&inner.local_buf[..read]);
                    }

                    Ok((msg_id, buf))
                })
                .await;
            let out = match read_result {
                Err(e) => {
                    return Err(e);
                }
                Ok(out) => out,
            };

            self.0 = Some(inner);
            Ok(out)
        }
        .boxed()
    }
}

/// Efficiently write framed data.
#[cfg_attr(feature = "test_utils", mockall::automock)]
pub trait AsFramedWriter: 'static + Send + Unpin {
    /// Write a frame of data to this FramedWriter instance.
    /// If timeout is exceeded, a timeout error is returned,
    /// and the stream is closed.
    fn write(
        &mut self,
        msg_id: MsgId,
        data: PoolBuf,
        timeout: KitsuneTimeout,
    ) -> BoxFuture<'_, KitsuneResult<()>>;
}

struct FramedWriterInner {
    sub: Box<dyn futures::io::AsyncWrite + 'static + Send + Unpin>,
}

/// Efficiently write framed data to a sub AsyncWrite instance.
pub struct FramedWriter(Option<FramedWriterInner>);

impl FramedWriter {
    /// Create a new FramedWriter instance.
    pub fn new(sub: Box<dyn futures::io::AsyncWrite + 'static + Send + Unpin>) -> Self {
        Self(Some(FramedWriterInner { sub }))
    }
}

impl AsFramedWriter for FramedWriter {
    /// Write a frame of data to this FramedWriter instance.
    /// If timeout is exceeded, a timeout error is returned,
    /// and the stream is closed.
    fn write(
        &mut self,
        msg_id: MsgId,
        mut data: PoolBuf,
        timeout: KitsuneTimeout,
    ) -> BoxFuture<'_, KitsuneResult<()>> {
        async move {
            let mut inner = match self.0.take() {
                None => return Err(KitsuneErrorKind::Closed.into()),
                Some(inner) => inner,
            };

            if let Err(e) = timeout
                .mix("FramedWriter::write", async {
                    let total = (data.len() + MSG_SIZE_BYTES + MSG_ID_BYTES) as u32;

                    data.reserve_front(MSG_SIZE_BYTES + MSG_ID_BYTES);
                    data.prepend_from_slice(&msg_id.inner().to_le_bytes()[..]);
                    data.prepend_from_slice(&total.to_le_bytes()[..]);

                    inner
                        .sub
                        .write_all(&data)
                        .await
                        .map_err(KitsuneError::other)?;

                    Ok(())
                })
                .await
            {
                tracing::error!(?e, "writer closing due to error");
                let _ = inner.sub.close().await;
                return Err(e);
            }

            self.0 = Some(inner);
            Ok(())
        }
        .boxed()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_msgid() {
        let req = MsgId::new(1);
        println!("{:?}", req);

        // make sure it starts out as a req
        assert!(!req.is_notify());
        assert!(req.is_req());
        assert!(!req.is_res());
        assert_eq!(1, req.as_id());

        // make sure as_req doesn't toggle
        let req = req.as_req();
        assert!(!req.is_notify());
        assert!(req.is_req());
        assert!(!req.is_res());
        assert_eq!(1, req.as_id());

        // make sure as_res works
        let res = req.as_res();
        println!("{:?}", res);

        assert!(!res.is_notify());
        assert!(res.is_res());
        assert!(!res.is_req());
        assert_eq!(1, res.as_id());

        // make sure as_res doesn't toggle
        let res = res.as_res();
        assert!(!res.is_notify());
        assert!(res.is_res());
        assert!(!res.is_req());
        assert_eq!(1, res.as_id());

        // make sure as_req works
        let req = res.as_req();
        assert!(!req.is_notify());
        assert!(req.is_req());
        assert!(!req.is_res());
        assert_eq!(1, req.as_id());

        // make sure new_notify works
        let not = MsgId::new_notify();
        println!("{:?}", not);
        assert!(not.is_notify());
        assert!(!not.is_req());
        assert!(!not.is_res());
        assert_eq!(0, not.as_id());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_framed() {
        let t = KitsuneTimeout::from_millis(5000);

        let (send, recv) = bound_async_mem_channel(4096, None);
        let mut send = FramedWriter::new(send);
        let mut recv = FramedReader::new(recv);

        let wt = metric_task(async move {
            let mut buf = PoolBuf::new();
            buf.extend_from_slice(&[0xd0; 512]);
            send.write(1.into(), buf, t).await.unwrap();
            let mut buf = PoolBuf::new();
            buf.extend_from_slice(&[0xd1; 8000]);
            send.write(2.into(), buf, t).await.unwrap();
            KitsuneResult::Ok(())
        });

        for _ in 0..2 {
            let (msg_id, data) = recv.read(t).await.unwrap();
            println!("got {} - {} bytes", msg_id.as_id(), data.len());
        }

        wt.await.unwrap().unwrap();
    }

    #[tokio::test]
    #[cfg(feature = "test_utils")]
    async fn test_mock_framed() {
        let mut f = MockAsFramedReader::new();
        f.expect_read().returning(|_t| {
            async move {
                let mut buf = PoolBuf::new();
                buf.extend_from_slice(b"test");
                Ok((0.into(), buf))
            }
            .boxed()
        });
        let (_, buf) = f.read(KitsuneTimeout::from_millis(100)).await.unwrap();
        assert_eq!(b"test", buf.as_ref());

        let mut f = MockAsFramedWriter::new();
        f.expect_write().returning(|_, buf, _| {
            assert_eq!(b"test2", buf.as_ref());
            async move { Ok(()) }.boxed()
        });
        let mut buf = PoolBuf::new();
        buf.extend_from_slice(b"test2");
        f.write(0.into(), buf, KitsuneTimeout::from_millis(100))
            .await
            .unwrap();
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/latency.rs
================================================
use once_cell::sync::Lazy;

/// this is a reference instance in time
/// all latency info is encoded relative to this
static LOC_EPOCH: Lazy<tokio::time::Instant> = Lazy::new(tokio::time::Instant::now);

/// this tag identifies that a latency marker will follow
/// as a little endian ieee f64, this will decode to NaN.
const LAT_TAG: &[u8; 8] = &[0xff, 0xff, 0xff, 0xfe, 0xfe, 0xff, 0xff, 0xff];

/// Fill a buffer with data that is readable as latency information.
/// Note, the minimum message size to get the timing data across is 16 bytes.
pub fn fill_with_latency_info(buf: &mut [u8]) {
    if buf.is_empty() {
        return;
    }

    // make sure we call this first, so we don't go back in time
    let epoch = *LOC_EPOCH;

    let now = tokio::time::Instant::now();
    let now = now.duration_since(epoch).as_secs_f64();

    // create a pattern of tag/marker
    let mut pat = [0_u8; 16];
    pat[0..8].copy_from_slice(LAT_TAG);
    pat[8..16].copy_from_slice(&now.to_le_bytes());

    // copy the tag/marker pattern repeatedly into the buffer
    let mut offset = 0;
    while offset < buf.len() {
        let len = std::cmp::min(pat.len(), buf.len() - offset);
        buf[offset..offset + len].copy_from_slice(&pat[..len]);
        offset += len;
    }
}

/// Return the duration since the time encoded in a latency info buffer.
/// Returns a unit error if we could not parse the buffer into time data.
#[allow(clippy::result_unit_err)]
pub fn parse_latency_info(buf: &[u8]) -> Result<std::time::Duration, ()> {
    // if the buffer is smaller than 16 bytes, we cannot decode it
    if buf.len() < 16 {
        return Err(());
    }

    // look for a tag, read the next bytes as latency info
    for i in 0..buf.len() - 15 {
        if &buf[i..i + 8] == LAT_TAG {
            let mut time = [0; 8];
            time.copy_from_slice(&buf[i + 8..i + 16]);
            let time = f64::from_le_bytes(time);
            let now = tokio::time::Instant::now();
            let now = now.duration_since(*LOC_EPOCH).as_secs_f64();
            let time = std::time::Duration::from_secs_f64(now - time);
            return Ok(time);
        }
    }
    Err(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_bad_latency_buffer_sizes() {
        for i in 0..16 {
            let mut buf = vec![0; i];
            fill_with_latency_info(&mut buf);
            assert!(parse_latency_info(&buf).is_err());
        }
    }

    #[test]
    fn test_bad_latency_buffer_data() {
        assert!(parse_latency_info(&[0; 64]).is_err());
    }

    #[test]
    fn test_good_latency_buffers() {
        for i in 16..64 {
            let mut buf = vec![0; i];
            fill_with_latency_info(&mut buf);
            let val = parse_latency_info(&buf).unwrap();
            assert!(val.as_micros() < 10_000);
        }
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/logic_chan.rs
================================================
use crate::tx_utils::*;
use crate::*;
use futures::future::{BoxFuture, FutureExt};
use futures::stream::futures_unordered::FuturesUnordered;
use futures::stream::Stream;
use tokio::sync::{OwnedSemaphorePermit, Semaphore};

enum LType<E: 'static + Send> {
    Event(E),
    Logic(OwnedSemaphorePermit, BoxFuture<'static, ()>),
}
type LTypeSend<E> = TSender<LType<E>>;
type LTypeRecv<E> = TReceiver<LType<E>>;

struct LogicChanInner<E: 'static + Send> {
    send: LTypeSend<E>,
    logic_limit: Arc<Semaphore>,
}

/// Handle to a logic_chan instance.
/// A clone of a LogicChanHandle is `Eq` to its origin.
/// A clone of a LogicChanHandle will `Hash` the same as its origin.
pub struct LogicChanHandle<E: 'static + Send>(Share<LogicChanInner<E>>);

impl<E: 'static + Send> Clone for LogicChanHandle<E> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<E: 'static + Send> PartialEq for LogicChanHandle<E> {
    fn eq(&self, oth: &Self) -> bool {
        self.0.eq(&oth.0)
    }
}

impl<E: 'static + Send> Eq for LogicChanHandle<E> {}

impl<E: 'static + Send> std::hash::Hash for LogicChanHandle<E> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.0.hash(state);
    }
}

impl<E: 'static + Send> LogicChanHandle<E> {
    /// Cause the logic_chan to emit an event.
    pub fn emit(
        &self,
        e: E,
    ) -> impl std::future::Future<Output = KitsuneResult<()>> + 'static + Send {
        let e = LType::Event(e);
        let send = self.0.share_mut(|i, _| Ok(i.send.clone()));
        async move {
            send?
                .send(e)
                .await
                .map_err(|_| KitsuneError::from(KitsuneErrorKind::Closed))?;
            Ok(())
        }
    }

    /// Capture new logic into the logic_chan.
    /// The passed future can capture other async objects such as streams,
    /// that will be polled as a part of the main logic_chan stream,
    /// without introducing any executor tasks.
    /// Be careful calling `capture_logic()` from within previously captured
    /// logic. While there may be reason to do this, it can lead to
    /// deadlock when approaching the capture_bound.
    pub fn capture_logic<L>(
        &self,
        l: L,
    ) -> impl std::future::Future<Output = KitsuneResult<()>> + 'static + Send
    where
        L: std::future::Future<Output = ()> + 'static + Send,
    {
        let l = futures::future::FutureExt::boxed(l);
        let r = self
            .0
            .share_mut(|i, _| Ok((i.logic_limit.clone(), i.send.clone())));
        async move {
            let (limit, send) = r?;
            let permit = limit.acquire_owned().await.map_err(KitsuneError::other)?;
            let l = LType::Logic(permit, l);
            send.send(l)
                .await
                .map_err(|_| KitsuneError::from(KitsuneErrorKind::Closed))?;
            Ok(())
        }
    }

    /// Check if this logic_chan was closed.
    pub fn is_closed(&self) -> bool {
        self.0.is_closed()
    }

    /// Close this logic_chan.
    pub fn close(&self) {
        let _ = self.0.share_mut(|i, c| {
            *c = true;
            i.send.close_channel();
            Ok(())
        });
    }
}

/// A logic channel.
/// Capture a handle to the logic_chan.
/// Fill the LogicChan with async logic.
/// Report events to the handle in the async logic.
/// Treat the LogicChan as a stream, collecting the events.
///
/// # Example
///
/// ```
/// # #[tokio::main(flavor = "multi_thread")]
/// # async fn main() {
/// # use kitsune_p2p_types::tx_utils::*;
/// # use futures::stream::StreamExt;
/// let chan = <LogicChan<&'static str>>::new(32);
/// let hnd = chan.handle().clone();
/// hnd.clone().capture_logic(async move {
///     hnd.emit("apple").await.unwrap();
///     hnd.emit("banana").await.unwrap();
///     hnd.close();
/// }).await.unwrap();
///
/// let res = chan.collect::<Vec<_>>().await;
/// assert_eq!(
///     &["apple", "banana"][..],
///     res.as_slice(),
/// );
/// # }
/// ```
pub struct LogicChan<E: 'static + Send> {
    recv: LTypeRecv<E>,
    hnd: LogicChanHandle<E>,
    logic: FuturesUnordered<BoxFuture<'static, ()>>,
}

impl<E: 'static + Send> LogicChan<E> {
    /// Create a new LogicChan instance.
    pub fn new(capture_bound: usize) -> Self {
        let (send, recv) = t_chan(capture_bound);
        let logic_limit = Arc::new(Semaphore::new(capture_bound));
        let inner = LogicChanInner { send, logic_limit };
        let hnd = LogicChanHandle(Share::new(inner));
        let logic = FuturesUnordered::new();
        Self { recv, hnd, logic }
    }

    /// A handle to this logic_chan. You can clone this.
    pub fn handle(&self) -> &LogicChanHandle<E> {
        &self.hnd
    }
}

impl<E: 'static + Send> LogicChan<E> {
    /// if there is any pending logic, poll it to pending
    fn poll_logic(mut self: std::pin::Pin<&mut Self>, cx: &mut std::task::Context<'_>) {
        loop {
            if self.logic.is_empty() {
                return;
            }

            let l = &mut self.logic;
            futures::pin_mut!(l);
            match Stream::poll_next(l, cx) {
                std::task::Poll::Pending => return,
                std::task::Poll::Ready(None) => return,
                _ => continue,
            }
        }
    }
}

impl<E: 'static + Send> futures::stream::Stream for LogicChan<E> {
    type Item = E;

    fn poll_next(
        mut self: std::pin::Pin<&mut Self>,
        cx: &mut std::task::Context<'_>,
    ) -> std::task::Poll<Option<Self::Item>> {
        loop {
            // always poll logic to start
            // note, we could make this more efficient by differentiating
            // a custom waker that identified whether logic or stream
            // was woken.
            Self::poll_logic(std::pin::Pin::new(&mut *self), cx);

            // always poll in stream to start
            // (see efficiency note above)
            // this accepts:
            //   - new incoming logic (queued to be polled as we continue loop)
            //   - new events to emit (emitted right away)
            let (permit, new_logic) = {
                match self.recv.poll_recv(cx) {
                    std::task::Poll::Ready(Some(t)) => match t {
                        LType::Event(e) => return std::task::Poll::Ready(Some(e)),
                        LType::Logic(permit, logic) => (permit, logic),
                    },
                    std::task::Poll::Ready(None) => return std::task::Poll::Ready(None),
                    std::task::Poll::Pending => return std::task::Poll::Pending,
                }
            };

            // queue the new logic
            // capture the permit such that it will drop
            // when the logic completes.
            self.logic.push(
                async move {
                    let _permit = permit;
                    new_logic.await;
                }
                .boxed(),
            );
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic;

    #[tokio::test(flavor = "multi_thread")]
    async fn doc_example() {
        use futures::stream::StreamExt;

        let chan = <LogicChan<&'static str>>::new(32);
        let hnd = chan.handle().clone();
        hnd.clone()
            .capture_logic(async move {
                hnd.emit("apple").await.unwrap();
                hnd.emit("banana").await.unwrap();
                hnd.close();
            })
            .await
            .unwrap();

        let res = chan.collect::<Vec<_>>().await;
        assert_eq!(&["apple", "banana"][..], res.as_slice(),);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_util_logic_chan() {
        let mut logic_chan = <LogicChan<&'static str>>::new(32);
        let h = logic_chan.handle().clone();
        let a = logic_chan.handle().clone();

        let count = Arc::new(atomic::AtomicUsize::new(0));

        let count2 = count.clone();
        let rt = metric_task(async move {
            while let Some(_res) = futures::stream::StreamExt::next(&mut logic_chan).await {
                count2.fetch_add(1, atomic::Ordering::SeqCst);
            }
            KitsuneResult::Ok(())
        });

        let wt = metric_task(async move {
            a.emit("a1").await.unwrap();
            let b = a.clone();
            a.capture_logic(async move {
                b.emit("b1").await.unwrap();
                tokio::time::sleep(std::time::Duration::from_millis(10)).await;
                b.emit("b2").await.unwrap();
            })
            .await
            .unwrap();
            tokio::time::sleep(std::time::Duration::from_millis(10)).await;
            a.emit("a2").await.unwrap();
            KitsuneResult::Ok(())
        });

        tokio::time::sleep(std::time::Duration::from_millis(100)).await;
        h.close();

        wt.await.unwrap().unwrap();
        rt.await.unwrap().unwrap();

        assert_eq!(4, count.load(atomic::Ordering::SeqCst));
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/mem_chan.rs
================================================
use crate::tx_utils::*;
use crate::*;
use futures::io::{Error, ErrorKind};

/// Construct a bound async read/write memory channel
pub fn bound_async_mem_channel(
    max_bytes: usize,
    maybe_active: Option<&Active>,
) -> (
    Box<dyn futures::io::AsyncWrite + 'static + Send + Unpin>,
    Box<dyn futures::io::AsyncRead + 'static + Send + Unpin>,
) {
    let buf = Vec::with_capacity(max_bytes);

    let inner = Arc::new(Share::new(MemInner {
        buf,
        max_bytes,
        closed: false,
        want_read_waker: None,
        want_write_waker: None,
    }));

    if let Some(active) = maybe_active {
        let k_inner = inner.clone();
        active.register_kill_cb(move || {
            let _ = k_inner.share_mut(|i, c| {
                *c = true;
                if let Some(waker) = i.want_read_waker.take() {
                    waker.wake();
                }
                if let Some(waker) = i.want_write_waker.take() {
                    waker.wake();
                }
                Ok(())
            });
        });
    }

    (Box::new(MemWrite(inner.clone())), Box::new(MemRead(inner)))
}

struct MemInner {
    buf: Vec<u8>,
    max_bytes: usize,
    closed: bool,
    want_read_waker: Option<std::task::Waker>,
    want_write_waker: Option<std::task::Waker>,
}

/// close this channel from the writer side
fn write_close(inner: &Arc<Share<MemInner>>) {
    let _ = inner.share_mut(|i, _| {
        i.closed = true;
        if let Some(waker) = i.want_read_waker.take() {
            waker.wake();
        }
        Ok(())
    });
}

/// close this channel from the reader side
fn read_close(inner: &Arc<Share<MemInner>>) {
    let _ = inner.share_mut(|i, c| {
        *c = true;
        if let Some(waker) = i.want_write_waker.take() {
            waker.wake();
        }
        Ok(())
    });
}

/// the writer side of the channel
struct MemWrite(Arc<Share<MemInner>>);

impl Drop for MemWrite {
    fn drop(&mut self) {
        write_close(&self.0);
    }
}

impl futures::io::AsyncWrite for MemWrite {
    fn poll_write(
        self: std::pin::Pin<&mut Self>,
        cx: &mut std::task::Context<'_>,
        buf: &[u8],
    ) -> std::task::Poll<Result<usize, futures::io::Error>> {
        // we cannot handle zero buffers
        if buf.is_empty() {
            return std::task::Poll::Ready(Err(Error::new(
                ErrorKind::InvalidInput,
                "AmbiguousZeroBuffer",
            )));
        }

        self.0
            .share_mut(|i, _| {
                // exit early if we are already write-side closed
                if i.closed {
                    return Ok(std::task::Poll::Ready(Err(Error::new(
                        ErrorKind::Other,
                        "PreviouslyClosed",
                    ))));
                }

                // how much can we write to the buffer?
                let amount = std::cmp::min(
                    4096, //
                    std::cmp::min(
                        buf.len(),                 //
                        i.max_bytes - i.buf.len(), //
                    ),
                );

                // if we cannot write, schedule a waker / return pending
                if amount == 0 {
                    i.want_write_waker = Some(cx.waker().clone());
                    return Ok(std::task::Poll::Pending);
                }

                // write the amout we decided
                i.buf.extend_from_slice(&buf[..amount]);

                // wake the reader side if pending
                if let Some(waker) = i.want_read_waker.take() {
                    waker.wake();
                }

                Ok(std::task::Poll::Ready(Ok(amount)))
            })
            .unwrap_or_else(|_| {
                std::task::Poll::Ready(Err(Error::new(ErrorKind::Other, "PreviouslyClosed")))
            })
    }

    fn poll_flush(
        self: std::pin::Pin<&mut Self>,
        _cx: &mut std::task::Context<'_>,
    ) -> std::task::Poll<Result<(), futures::io::Error>> {
        std::task::Poll::Ready(Ok(()))
    }

    fn poll_close(
        self: std::pin::Pin<&mut Self>,
        _cx: &mut std::task::Context<'_>,
    ) -> std::task::Poll<Result<(), futures::io::Error>> {
        write_close(&self.0);
        std::task::Poll::Ready(Ok(()))
    }
}

/// the reader side of the channel
struct MemRead(Arc<Share<MemInner>>);

impl Drop for MemRead {
    fn drop(&mut self) {
        read_close(&self.0);
    }
}

impl futures::io::AsyncRead for MemRead {
    fn poll_read(
        self: std::pin::Pin<&mut Self>,
        cx: &mut std::task::Context<'_>,
        buf: &mut [u8],
    ) -> std::task::Poll<Result<usize, futures::io::Error>> {
        // we cannot handle zero buffers
        if buf.is_empty() {
            return std::task::Poll::Ready(Err(Error::new(
                ErrorKind::InvalidInput,
                "AmbiguousZeroBuffer",
            )));
        }

        self.0
            .share_mut(|i, c| {
                // if the read buffer is empty...
                if i.buf.is_empty() {
                    if i.closed {
                        // if we are writer-side closed, close reader side too
                        *c = true;
                        if let Some(waker) = i.want_write_waker.take() {
                            waker.wake();
                        }
                        return Ok(std::task::Poll::Ready(Ok(0)));
                    } else {
                        // otherwise record waker / return pending
                        i.want_read_waker = Some(cx.waker().clone());
                        return Ok(std::task::Poll::Pending);
                    }
                }

                // determine how much we can read
                let amount = std::cmp::min(
                    4096, //
                    std::cmp::min(
                        buf.len(),   //
                        i.buf.len(), //
                    ),
                );

                // read that amount
                buf[..amount].copy_from_slice(&i.buf[..amount]);

                if i.buf.len() > amount {
                    // if there is more that "could" be read...
                    // move that data to the front of our buf / truncate
                    i.buf.copy_within(amount.., 0);
                    let new_len = i.buf.len() - amount;
                    i.buf.truncate(new_len);
                } else {
                    // otherwise we can more cheaply clear the buf
                    i.buf.clear()
                }

                // notify the writer that maybe more can be written
                if let Some(waker) = i.want_write_waker.take() {
                    waker.wake();
                }

                Ok(std::task::Poll::Ready(Ok(amount)))
            })
            .unwrap_or_else(|_| {
                std::task::Poll::Ready(Err(Error::new(ErrorKind::Other, "PreviouslyClosed")))
            })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    async fn _inner_test_async_bound_mem_channel(bind_size: usize, buf_size: usize) {
        let (mut send, mut recv) = bound_async_mem_channel(bind_size, None);

        let rt = metric_task(async move {
            let mut read_buf = vec![0_u8; buf_size];
            use futures::io::AsyncReadExt;
            recv.read_exact(&mut read_buf).await.unwrap();
            println!(
                "mem_chan(bind-{},buf-{}) in: {} us",
                bind_size,
                buf_size,
                parse_latency_info(&read_buf).unwrap().as_micros()
            );
            KitsuneResult::Ok(())
        });

        use futures::io::AsyncWriteExt;
        let mut write_buf = vec![0_u8; buf_size];
        fill_with_latency_info(&mut write_buf);
        send.write_all(&write_buf).await.unwrap();

        rt.await.unwrap().unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bound_mem_channel_sm_buf() {
        _inner_test_async_bound_mem_channel(15, 4096).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bound_mem_channel_lg_buf() {
        _inner_test_async_bound_mem_channel(4096 * 3, 4096 * 4).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bound_mem_channel_disparity() {
        _inner_test_async_bound_mem_channel(4096, 1024 * 1024 * 8).await;
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/notify_all.rs
================================================
#![allow(clippy::blocks_in_conditions)]

use crate::tx_utils::*;
use crate::*;

/// Sync callback signature to be invoked on notify()
type NotifySyncCb = Box<dyn FnOnce() + 'static + Send>;

struct Inner {
    wakers: Vec<std::task::Waker>,
    cbs: Vec<NotifySyncCb>,
}

type InnerWrap = Arc<Share<Inner>>;

struct WaitFut(InnerWrap, Option<usize>);

impl std::future::Future for WaitFut {
    type Output = ();

    fn poll(
        mut self: std::pin::Pin<&mut Self>,
        cx: &mut std::task::Context<'_>,
    ) -> std::task::Poll<Self::Output> {
        let mut index = self.1.take();
        let waker_update_result = self.0.share_mut(|i, _| {
            if let Some(idx) = index {
                i.wakers[idx].clone_from(cx.waker());
                index = Some(idx);
            } else {
                index = Some(i.wakers.len());
                i.wakers.push(cx.waker().clone());
            }
            Ok(())
        });
        if waker_update_result.is_err() {
            return std::task::Poll::Ready(());
        }
        self.1 = index;
        std::task::Poll::Pending
    }
}

fn do_notify(inner: &InnerWrap) {
    if let Ok((wakers, cbs)) = inner.share_mut(|i, c| {
        *c = true;
        Ok((
            i.wakers.drain(..).collect::<Vec<_>>(),
            i.cbs.drain(..).collect::<Vec<_>>(),
        ))
    }) {
        for waker in wakers {
            waker.wake();
        }
        for cb in cbs {
            cb();
        }
    }
}

#[derive(Clone)]
struct NotifyOnDrop(InnerWrap);

impl Drop for NotifyOnDrop {
    fn drop(&mut self) {
        do_notify(&self.0)
    }
}

/// Many tasks can await on this notify struct.
/// They will all be notified once notify is called.
#[derive(Clone)]
pub struct NotifyAll(InnerWrap, #[allow(dead_code)] Arc<NotifyOnDrop>);

impl Default for NotifyAll {
    fn default() -> Self {
        Self::new()
    }
}

impl NotifyAll {
    /// Construct a new NotifyAll instance.
    pub fn new() -> Self {
        let inner = Inner {
            wakers: Vec::new(),
            cbs: Vec::new(),
        };
        let wrap = Arc::new(Share::new(inner));
        let notify_on_drop = Arc::new(NotifyOnDrop(wrap.clone()));
        Self(wrap, notify_on_drop)
    }

    /// Register a sync cb to be invoked on notify
    /// (will be invoked immediately if did_notify())
    pub fn wait_cb<F>(&self, sync_cb: F)
    where
        F: FnOnce() + 'static + Send,
    {
        let mut maybe_sync_cb: Option<NotifySyncCb> = Some(Box::new(sync_cb));

        // if we have not already notified, take it to be notified later
        let _ = self.0.share_mut(|i, _| {
            i.cbs.push(maybe_sync_cb.take().unwrap());
            Ok(())
        });

        // if the cb was not taken, we have already notified, so call it now
        if let Some(sync_cb) = maybe_sync_cb {
            sync_cb();
        }
    }

    /// Wait on this NotifyAll instance.
    pub fn wait(&self) -> impl std::future::Future<Output = ()> + 'static + Send {
        WaitFut(self.0.clone(), None)
    }

    /// Trigger all waiters.
    pub fn notify(&self) {
        do_notify(&self.0)
    }

    /// Has this notify all already been triggered?
    pub fn did_notify(&self) -> bool {
        self.0.is_closed()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_notify_all_on_drop() {
        let count = Arc::new(atomic::AtomicUsize::new(0));

        let t = {
            let n = NotifyAll::new();
            let c2 = count.clone();
            n.wait_cb(move || {
                c2.fetch_add(1, atomic::Ordering::Relaxed);
            });
            let c3 = count.clone();
            let not = n.wait();
            let t = metric_task(async move {
                not.await;
                c3.fetch_add(1, atomic::Ordering::Relaxed);
                KitsuneResult::Ok(())
            });

            tokio::time::sleep(std::time::Duration::from_millis(10)).await;

            assert_eq!(0, count.load(atomic::Ordering::Relaxed));

            t
        };

        t.await.unwrap().unwrap();

        assert_eq!(2, count.load(atomic::Ordering::Relaxed));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_notify_all_sync() {
        let count = Arc::new(atomic::AtomicUsize::new(0));

        let n = NotifyAll::new();

        let c2 = count.clone();
        n.wait_cb(move || {
            c2.fetch_add(1, atomic::Ordering::Relaxed);
        });
        let c3 = count.clone();
        n.wait_cb(move || {
            c3.fetch_add(1, atomic::Ordering::Relaxed);
        });

        n.notify();

        assert_eq!(2, count.load(atomic::Ordering::Relaxed));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_notify_all() {
        let count = Arc::new(atomic::AtomicUsize::new(0));

        let n = NotifyAll::new();

        let mut all = Vec::new();
        for _ in 0..10 {
            let not = n.wait();
            let count = count.clone();
            all.push(metric_task(async move {
                not.await;
                count.fetch_add(1, atomic::Ordering::Relaxed);
                KitsuneResult::Ok(())
            }));
        }

        tokio::time::sleep(std::time::Duration::from_millis(10)).await;

        assert_eq!(0, count.load(atomic::Ordering::Relaxed));

        n.notify();

        futures::future::try_join_all(all).await.unwrap();

        assert_eq!(10, count.load(atomic::Ordering::Relaxed));
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/pool_buf.rs
================================================
use std::cell::RefCell;

// MAYBE - expirement with these values for efficiency.

/// The max capacity of in-pool stored PoolBufs per thread.
/// See how this affects max mem usage in doc of POOL_BUF_SHRINK_TO_CAPACITY.
pub(crate) const POOL_MAX_CAPACITY: usize = 1024;

/// Returned PoolBufs will be shrunk to this capacity when returned.
/// for an 8 core system (using 8 tokio threads),
/// this results in a max of 1024 * 16_384 * 8 = 128MiB of memory overhead.
pub(crate) const POOL_BUF_SHRINK_TO_CAPACITY: usize = 16_384;

/// PoolBufs will be allocated/reset with this byte count BEFORE
/// the readable buffer to make prepending frame info more efficient.
pub(crate) const POOL_BUF_PRE_WRITE_SPACE: usize = 128;

/// A buffer that will return to a pool after use.
///
/// When working with network code, we try to avoid two slow things:
/// - Allocation
/// - Initialization
///
/// We avoid allocation by returning used buffers to a pool for later re-use.
///
/// We avoid initialization by using `extend_from_slice()`.
#[derive(Clone, PartialEq, Eq)]
pub struct PoolBuf(Option<(usize, Vec<u8>)>);

impl std::fmt::Debug for PoolBuf {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let inner = self.0.as_ref().unwrap();
        let byte_count = inner.1.len() - inner.0;
        f.debug_struct("PoolBuf")
            .field("byte_count", &byte_count)
            .finish()
    }
}

impl std::io::Write for PoolBuf {
    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        std::io::Write::write(&mut self.0.as_mut().unwrap().1, buf)
    }

    fn flush(&mut self) -> std::io::Result<()> {
        std::io::Write::flush(&mut self.0.as_mut().unwrap().1)
    }
}

impl serde::Serialize for PoolBuf {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        serializer.serialize_bytes(self.as_ref())
    }
}

struct VisitBytes;

impl serde::de::Visitor<'_> for VisitBytes {
    type Value = PoolBuf;

    fn expecting(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(fmt, "raw bytes")
    }

    fn visit_bytes<E>(self, v: &[u8]) -> Result<Self::Value, E>
    where
        E: serde::de::Error,
    {
        let mut out = PoolBuf::new();
        out.extend_from_slice(v);
        Ok(out)
    }
}

impl<'de> serde::Deserialize<'de> for PoolBuf {
    fn deserialize<D>(deserializer: D) -> Result<PoolBuf, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        // we might be tempted to deserialize_byte_buf here...
        // but that may cause decoders to clone when there was no need.
        deserializer.deserialize_bytes(VisitBytes)
    }
}

thread_local! {
    pub(crate) static BUF_POOL: RefCell<Vec<Vec<u8>>> = RefCell::new(Vec::with_capacity(POOL_MAX_CAPACITY));
}

impl Drop for PoolBuf {
    fn drop(&mut self) {
        if let Some((_, mut inner)) = self.0.take() {
            BUF_POOL.with(|p| {
                let mut p = p.borrow_mut();
                if p.len() < POOL_MAX_CAPACITY {
                    reset(&mut inner, true);
                    p.push(inner);
                }
            });
        }
    }
}

/// reset used both for requeuing into thread local, and for clear()
fn reset(v: &mut Vec<u8>, do_truncate: bool) {
    crate::metrics::metric_push_pool_buf_release_size(v.capacity() as u64);
    if do_truncate && v.capacity() > POOL_BUF_SHRINK_TO_CAPACITY {
        v.truncate(POOL_BUF_SHRINK_TO_CAPACITY);
        v.shrink_to_fit();
    }
    v.resize(POOL_BUF_PRE_WRITE_SPACE, 0);
}

impl PoolBuf {
    /// Create a new PoolBuf.
    pub fn new() -> Self {
        let inner = BUF_POOL.with(|p| {
            let mut p = p.borrow_mut();
            if p.is_empty() {
                vec![0; POOL_BUF_PRE_WRITE_SPACE]
            } else {
                p.remove(0)
            }
        });
        Self(Some((POOL_BUF_PRE_WRITE_SPACE, inner)))
    }

    /// Reset this buffer
    pub fn clear(&mut self) {
        let inner = self.0.as_mut().unwrap();
        reset(&mut inner.1, false);
        inner.0 = inner.1.len();
    }

    /// Like `drain(..len)` but without the iterator trappings.
    /// Note, this actually copies the memory, leaving start the same.
    /// perhaps you want `cheap_move_start()`?
    pub fn shift_data_forward(&mut self, len: usize) {
        if len == 0 {
            return;
        }

        let inner = self.0.as_mut().unwrap();

        let start = inner.0;
        let data_len = inner.1.len() - start;

        if len >= data_len {
            reset(&mut inner.1, false);
            inner.0 = POOL_BUF_PRE_WRITE_SPACE;
            return;
        }

        let r = len + start..inner.1.len();
        inner.1.copy_within(r, start);
        inner.1.truncate(inner.1.len() - len);
    }

    /// Like `drain(..len)` but without the iterator trappings.
    /// Note, this just moves the start pointer forward,
    /// if you actually want to reclaim space,
    /// perhaps you want `shift_data_forward()`?
    pub fn cheap_move_start(&mut self, len: usize) {
        if len == 0 {
            return;
        }

        let inner = self.0.as_mut().unwrap();

        let start = inner.0;
        let data_len = inner.1.len() - start;

        if len >= data_len {
            reset(&mut inner.1, false);
            inner.0 = POOL_BUF_PRE_WRITE_SPACE;
            return;
        }

        inner.0 += len;
    }

    /// Reserve desired capacity. Prefer doing this once at the beginning
    /// of an operation to avoid the time cost of allocation.
    pub fn reserve(&mut self, want_size: usize) {
        let inner = self.0.as_mut().unwrap();
        inner.1.reserve(want_size + inner.0);
    }

    /// Extend this buffer with data from src.
    pub fn extend_from_slice(&mut self, src: &[u8]) {
        let inner = self.0.as_mut().unwrap();
        inner.1.extend_from_slice(src);
    }

    #[allow(clippy::uninit_vec)]
    /// Ensure we have enough front space to prepend the given byte count.
    /// If not, shift all data over to the right, making more prepend space.
    pub fn reserve_front(&mut self, mut len: usize) {
        let inner = self.0.as_mut().unwrap();

        if len < inner.0 {
            // we already have enough space, return early
            return;
        }

        // we don't have enough space - allocate a little extra
        len += POOL_BUF_PRE_WRITE_SPACE;

        let prev_len = inner.1.len();
        let new_len = prev_len + len;

        inner.1.reserve(new_len);

        // any way to work around this unsafe without needlessly
        // initializing this data we're going to overwrite?
        unsafe {
            inner.1.set_len(new_len);
        }

        inner.1.copy_within(inner.0..prev_len, inner.0 + len);
        inner.0 += len;
    }

    /// Efficiently copy data *before* the current data.
    pub fn prepend_from_slice(&mut self, src: &[u8]) {
        self.reserve_front(src.len());

        let inner = self.0.as_mut().unwrap();
        inner.1[inner.0 - src.len()..inner.0].copy_from_slice(src);
        inner.0 -= src.len();
    }
}

impl Default for PoolBuf {
    fn default() -> Self {
        Self::new()
    }
}

impl std::ops::Deref for PoolBuf {
    type Target = [u8];

    fn deref(&self) -> &Self::Target {
        self.as_ref()
    }
}

impl AsRef<[u8]> for PoolBuf {
    fn as_ref(&self) -> &[u8] {
        let inner = self.0.as_ref().unwrap();
        &inner.1[inner.0..]
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn pool_buf_prepend() {
        let mut b = PoolBuf::new();
        b.extend_from_slice(b"World!");
        b.prepend_from_slice(b"Hello ");
        assert_eq!("Hello World!", String::from_utf8_lossy(&b));
    }

    #[test]
    fn pool_buf_prepend_large() {
        const D: [u8; 512] = [0xdb; 512];
        let mut b = PoolBuf::new();
        b.extend_from_slice(b"apple");
        b.prepend_from_slice(&D[..]);
        b.prepend_from_slice(b"banana");
        assert_eq!(b"banana", &b[0..6]);
        assert_eq!(b"apple", &b[518..523]);
        assert_eq!(&D[..], &b[6..518]);
        assert_eq!(523, b.len());
    }

    #[test]
    fn pool_buf_grow_shrink_reset_reuse() {
        let mut b = PoolBuf::new();
        b.extend_from_slice(b"bar");
        assert_eq!(b"bar", &*b);
        b.prepend_from_slice(b"foo");
        assert_eq!(b"foobar", &*b);
        b.cheap_move_start(3);
        assert_eq!(b"bar", &*b);
        b.clear();
        b.extend_from_slice(b"ab");
        assert_eq!(b"ab", &*b);
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/proxy_url.rs
================================================
//! Utilities for dealing with proxy urls.

use crate::*;
use base64::Engine;

use derive_more::{AsRef, Deref, Display};

/// Utility for dealing with proxy urls.
/// Proxy URLs are like super-urls... they need to be able to
/// compose a sub or base-transport url, while adding a new scheme and
/// a tls certificate digest, without shadowing any info.
///
/// We could do this by percent encoding the base-url into a
/// query string or path segment, but that is not very user-friendly looking.
///
/// Instead, we extract some info from the base-url into path
/// segments, and include everything else after a special path segment marker
/// `--`.
///
/// Optional extracted items (order matters):
///  - `h` - host: `/h/[host-name-here]`
///  - `p` - port: `/p/[port-here]`
///  - `u` - username: `/u/[user-name-here]`
///  - `w` - password: `/w/[password-here]`
#[derive(Debug, Display, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Deref, AsRef)]
#[display(fmt = "{}", full)]
pub struct ProxyUrl {
    #[deref]
    #[as_ref]
    full: url2::Url2,
    base: url2::Url2,
}

impl ProxyUrl {
    /// Create a new proxy url from a full url str.
    pub fn from_full(full: &str) -> KitsuneResult<Self> {
        macro_rules! err {
            ($h:literal) => {
                KitsuneError::from(format!(
                    "Invalid Proxy Url({}): {}: at: {}:{}",
                    $h,
                    full,
                    file!(),
                    line!()
                ))
            };
        }
        let full = url2::try_url2!("{}", full).map_err(|_| err!("parse"))?;
        if full.scheme() == "wss" || full.scheme() == "ws" {
            return Ok(Self {
                full: full.clone(),
                base: full,
            });
        }
        let base_scheme = match full.path_segments() {
            None => return Err(err!("read scheme")),
            Some(mut s) => match s.next() {
                None => return Err(err!("read scheme")),
                Some(s) => s,
            },
        };
        let mut base = url2::try_url2!("{}://", base_scheme).map_err(KitsuneError::other)?;
        {
            let mut path = full.path_segments().ok_or_else(|| err!("read base"))?;
            path.next();
            let mut found_base_path_marker = false;
            loop {
                let key = match path.next() {
                    None => break,
                    Some(key) => key,
                };
                if key == "--" {
                    found_base_path_marker = true;
                    continue;
                }
                if found_base_path_marker {
                    base.path_segments_mut()
                        .map_err(|_| err!("read marker"))?
                        .push(key);
                } else {
                    let val = match path.next() {
                        None => break,
                        Some(val) => val,
                    };
                    match key {
                        "h" => base.set_host(Some(val)).map_err(|_| err!("read host"))?,
                        "p" => base
                            .set_port(Some(val.parse().map_err(|_| err!("read port"))?))
                            .map_err(|_| err!("read port"))?,
                        "u" => base.set_username(val).map_err(|_| err!("read username"))?,
                        "w" => base
                            .set_password(Some(val))
                            .map_err(|_| err!("read password"))?,
                        _ => return Err(err!("read base")),
                    }
                }
            }
        }
        base.set_query(full.query());
        base.set_fragment(full.fragment());
        Ok(Self { full, base })
    }

    /// Create a new proxy url from a base + tls cert digest.
    pub fn new(base: &str, cert_digest: CertDigest) -> KitsuneResult<Self> {
        let base = url2::try_url2!("{}", base).map_err(KitsuneError::other)?;
        let tls = base64::prelude::BASE64_URL_SAFE_NO_PAD.encode(&cert_digest[..]);
        let mut full = url2::try_url2!("kitsune-proxy://{}", tls).map_err(KitsuneError::other)?;
        {
            let mut path = full
                .path_segments_mut()
                .map_err(|_| KitsuneError::from(""))?;
            path.push(base.scheme());
            if let Some(h) = base.host_str() {
                path.push("h");
                path.push(h);
            }
            if let Some(p) = base.port() {
                path.push("p");
                path.push(&format!("{}", p));
            }
            if !base.username().is_empty() {
                path.push("u");
                path.push(base.username());
            }
            if let Some(w) = base.password() {
                path.push("w");
                path.push(w);
            }
            path.push("--");
            if let Some(s) = base.path_segments() {
                for s in s {
                    path.push(s);
                }
            }
        }
        full.set_query(base.query());
        full.set_fragment(base.fragment());
        Ok(Self { full, base })
    }

    /// Extract the cert digest from the url
    pub fn digest(&self) -> KitsuneResult<CertDigest> {
        let scheme = self.full.scheme();
        if scheme == "wss" || scheme == "ws" {
            // override for tx5
            if let Some(mut i) = self.full.path_segments() {
                if let Some(u) = i.next() {
                    let digest = base64::prelude::BASE64_URL_SAFE_NO_PAD
                        .decode(u)
                        .map_err(KitsuneError::other)?;
                    return Ok(CertDigest::from_slice(&digest));
                }
            }
        }
        let digest = base64::prelude::BASE64_URL_SAFE_NO_PAD
            .decode(
                self.full
                    .host_str()
                    .ok_or_else(|| KitsuneError::other("no host string in URL"))?,
            )
            .map_err(KitsuneError::other)?;
        Ok(CertDigest::from_slice(&digest))
    }

    /// Get a short-hash / first six characters of tls digest for logging
    pub fn short(&self) -> &str {
        let h = self.full.host_str().unwrap();
        &h[..std::cmp::min(h.chars().count(), 6)]
    }

    /// Get the base url this proxy is addressable at.
    pub fn as_base(&self) -> &url2::Url2 {
        &self.base
    }

    /// Get the base url this proxy is addressable at as a &str reference.
    pub fn as_base_str(&self) -> &str {
        self.base.as_str()
    }

    /// Convert this proxy url instance into a base url.
    pub fn into_base(self) -> url2::Url2 {
        self.base
    }

    /// Get the full url referencing this proxy.
    pub fn as_full(&self) -> &url2::Url2 {
        &self.full
    }

    /// Get the full url referencing this proxy as a &str reference.
    pub fn as_full_str(&self) -> &str {
        self.full.as_str()
    }

    /// Convert this proxy url instance into a full url.
    pub fn into_full(self) -> url2::Url2 {
        self.full
    }

    /// Convert this proxy url instance into a (BaseUrl, FullUrl) tuple.
    pub fn into_inner(self) -> (url2::Url2, url2::Url2) {
        (self.base, self.full)
    }
}

macro_rules! q_from {
    ($($t1:ty => $t2:ty, | $i:ident | {$e:expr},)*) => {$(
        impl From<$t1> for $t2 {
            fn from($i: $t1) -> Self {
                $e
            }
        }
    )*};
}

q_from! {
       ProxyUrl => (url2::Url2, url2::Url2), |url| { url.into_inner() },
      &ProxyUrl => (url2::Url2, url2::Url2), |url| { url.clone().into_inner() },
       ProxyUrl => url2::Url2, |url| { url.into_full() },
      &ProxyUrl => url2::Url2, |url| { url.as_full().clone() },
         String => ProxyUrl, |url| { ProxyUrl::from_full(&url).unwrap() },
        &String => ProxyUrl, |url| { ProxyUrl::from_full(url).unwrap() },
           &str => ProxyUrl, |url| { ProxyUrl::from_full(url).unwrap() },
     url2::Url2 => ProxyUrl, |url| { ProxyUrl::from_full(url.as_str()).unwrap() },
    &url2::Url2 => ProxyUrl, |url| { ProxyUrl::from_full(url.as_str()).unwrap() },
}

impl AsRef<str> for ProxyUrl {
    fn as_ref(&self) -> &str {
        self.as_full_str()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    const TEST_CERT: &str = "VlyCSmL5WRKUTOLmF9wF0oFy5Jqbxy0I9KPeXqB_9Z4";
    const TEST_FULL: &str = "kitsune-proxy://VlyCSmL5WRKUTOLmF9wF0oFy5Jqbxy0I9KPeXqB_9Z4/kitsune-quic/h/1.2.3.4/p/443/u/bob/w/pass/--/yada1/yada2?c=bla&t=EugO96mIgrCph7QMpqJkkI5BPY5GuIP7JcCshnwh8FY&j=bla#bla";
    const TEST_BASE: &str = "kitsune-quic://bob:pass@1.2.3.4:443/yada1/yada2?c=bla&t=EugO96mIgrCph7QMpqJkkI5BPY5GuIP7JcCshnwh8FY&j=bla#bla";

    #[test]
    fn proxy_url_from_full() {
        let u = ProxyUrl::from_full(TEST_FULL).unwrap();
        assert_eq!(TEST_FULL, u.as_full_str());
        assert_eq!(TEST_BASE, u.as_base_str());
    }

    #[test]
    fn proxy_url_from_base() {
        let cert_digest = base64::prelude::BASE64_URL_SAFE_NO_PAD
            .decode(TEST_CERT)
            .unwrap();
        let digest = CertDigest::from_slice(&cert_digest);
        let u = ProxyUrl::new(TEST_BASE, digest).unwrap();
        assert_eq!(TEST_FULL, u.as_full_str());
        assert_eq!(TEST_BASE, u.as_base_str());
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/resource_bucket.rs
================================================
use crate::tx_utils::*;
use crate::*;

struct Inner<T: 'static + Send> {
    bucket: Vec<T>,
    notify: Arc<tokio::sync::Notify>,
}

/// Control efficient access to shared resource pool.
pub struct ResourceBucket<T: 'static + Send>(Arc<Share<Inner<T>>>);

impl<T: 'static + Send> Clone for ResourceBucket<T> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<T: 'static + Send> Default for ResourceBucket<T> {
    fn default() -> Self {
        Self::new()
    }
}

impl<T: 'static + Send> ResourceBucket<T> {
    /// Create a new resource bucket.
    pub fn new() -> Self {
        Self(Arc::new(Share::new(Inner {
            bucket: Vec::new(),
            notify: Arc::new(tokio::sync::Notify::new()),
        })))
    }

    /// Add a resource to the bucket.
    /// Could be a new resource, or a previously acquired resource.
    pub fn release(&self, t: T) {
        let _ = self.0.share_mut(move |i, _| {
            i.bucket.push(t);
            i.notify.notify_one();
            Ok(())
        });
    }

    /// Acquire a resource that is immediately available from the bucket
    /// or generate a new one.
    pub fn acquire_or_else<F>(&self, f: F) -> T
    where
        F: FnOnce() -> T + 'static + Send,
    {
        if let Ok(t) = self.0.share_mut(|i, _| {
            if !i.bucket.is_empty() {
                return Ok(i.bucket.remove(0));
            }
            Err(().into())
        }) {
            return t;
        }
        f()
    }

    /// Acquire a resource from the bucket.
    pub fn acquire(
        &self,
        timeout: Option<KitsuneTimeout>,
    ) -> impl std::future::Future<Output = KitsuneResult<T>> + 'static + Send {
        let inner = self.0.clone();
        async move {
            // first, see if there is a resource to return immediately
            // or, capture a notifier for when there might be again
            let bucket_result = inner.share_mut(|i, _| {
                if !i.bucket.is_empty() {
                    return Ok((Some(i.bucket.remove(0)), None));
                }
                Ok((None, Some(i.notify.clone())))
            });
            let notify = match bucket_result {
                Err(e) => return Err(e),
                Ok((Some(t), None)) => return Ok(t),
                Ok((None, Some(notify))) => notify,
                _ => unreachable!(),
            };

            loop {
                // capture the notifier future
                let n = notify.notified();

                // mix with timeout if appropriate
                match timeout {
                    Some(timeout) => {
                        timeout
                            .mix("ResourceBucket::acquire", async move {
                                n.await;
                                Ok(())
                            })
                            .await
                    }
                    None => {
                        n.await;
                        Ok(())
                    }
                }?;

                // we've been notified, see if there is data
                let bucket_result = inner.share_mut(|i, _| {
                    if !i.bucket.is_empty() {
                        return Ok(Some(i.bucket.remove(0)));
                    }
                    Ok(None)
                });
                match bucket_result {
                    Err(e) => return Err(e),
                    Ok(Some(t)) => return Ok(t),
                    _ => (),
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bucket_timeout() {
        let t = Some(KitsuneTimeout::from_millis(10));
        let bucket = <ResourceBucket<&'static str>>::new();
        let j1 = metric_task(bucket.acquire(t));
        let j2 = metric_task(bucket.acquire(t));
        assert!(j1.await.unwrap().is_err());
        assert!(j2.await.unwrap().is_err());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bucket() {
        let bucket = <ResourceBucket<&'static str>>::new();
        let j1 = metric_task(bucket.acquire(None));
        let j2 = metric_task(bucket.acquire(None));
        bucket.release("1");
        bucket.release("2");
        let j1 = j1.await.unwrap().unwrap();
        let j2 = j2.await.unwrap().unwrap();
        assert!((j1 == "1" && j2 == "2") || (j2 == "1" && j1 == "2"));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_async_bucket_acquire_or_else() {
        let bucket = <ResourceBucket<&'static str>>::new();
        let j1 = metric_task(bucket.acquire(None));
        let j2 = bucket.acquire_or_else(|| "2");
        bucket.release("1");
        let j1 = j1.await.unwrap().unwrap();
        assert_eq!(j1, "1");
        assert_eq!(j2, "2");
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/share.rs
================================================
use crate::*;

/// Synchronized droppable share-lock around internal state date.
pub struct Share<T: 'static + Send>(Arc<parking_lot::Mutex<Option<T>>>);

impl<T: 'static + Send> Clone for Share<T> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<T: 'static + Send + std::fmt::Debug> std::fmt::Debug for Share<T> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.share_ref(|s| Ok(f.debug_tuple("Share").field(s).finish()))
            .unwrap()
    }
}

impl<T: 'static + Send> PartialEq for Share<T> {
    fn eq(&self, oth: &Self) -> bool {
        Arc::ptr_eq(&self.0, &oth.0)
    }
}

impl<T: 'static + Send> Eq for Share<T> {}

impl<T: 'static + Send> std::hash::Hash for Share<T> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        Arc::as_ptr(&self.0).hash(state);
    }
}

impl<T: 'static + Send + Default> Default for Share<T> {
    fn default() -> Self {
        Self::new(T::default())
    }
}

impl<T: 'static + Send> Share<T> {
    /// Create a new share lock.
    pub fn new(t: T) -> Self {
        Self(Arc::new(parking_lot::Mutex::new(Some(t))))
    }

    /// Create a new closed share lock.
    pub fn new_closed() -> Self {
        Self(Arc::new(parking_lot::Mutex::new(None)))
    }

    /// Execute code with immutable access to the internal state.
    pub fn share_ref<R, F>(&self, f: F) -> KitsuneResult<R>
    where
        F: FnOnce(&T) -> KitsuneResult<R>,
    {
        let t = self.0.lock();
        if t.is_none() {
            return Err(KitsuneErrorKind::Closed.into());
        }
        f(t.as_ref().unwrap())
    }

    /// Attempt to unwrap the inner value, assuming this is the only instance.
    pub fn try_unwrap(self) -> Result<Option<T>, Self> {
        match Arc::try_unwrap(self.0) {
            Ok(inner) => Ok(inner.into_inner()),
            Err(inner) => Err(Self(inner)),
        }
    }

    /// Execute code with mut access to the internal state.
    /// The second param, if set to true, will drop the shared state,
    /// any further access will `Err(KitsuneError::Closed)`.
    /// E.g. `share.share_mut(|_state, close| *close = true).unwrap();`
    pub fn share_mut<R, F>(&self, f: F) -> KitsuneResult<R>
    where
        F: FnOnce(&mut T, &mut bool) -> KitsuneResult<R>,
    {
        let mut t = self.0.lock();
        if t.is_none() {
            return Err(KitsuneErrorKind::Closed.into());
        }
        let mut close = false;
        let r = f(t.as_mut().unwrap(), &mut close);
        if close {
            *t = None;
        }
        r
    }

    /// Returns true if the internal state has been dropped.
    pub fn is_closed(&self) -> bool {
        self.0.lock().is_none()
    }

    /// Explicity drop the internal state.
    pub fn close(&self) {
        *(self.0.lock()) = None;
    }
}

/// A version of Share which can never be closed, and thus every
/// share is infallible (no Err possible).
#[derive(PartialEq, Eq, Hash, Debug)]
pub struct ShareOpen<T: 'static + Send>(Share<T>);

impl<T: 'static + Send> Clone for ShareOpen<T> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<T: 'static + Send> ShareOpen<T> {
    /// Create a new share lock.
    pub fn new(t: T) -> Self {
        Self(Share::new(t))
    }

    /// Execute code with immutable access to the internal state.
    pub fn share_ref<R, F>(&self, f: F) -> R
    where
        F: FnOnce(&T) -> R,
    {
        self.0
            .share_ref(|s| Ok(f(s)))
            .expect("ShareOpen state is never dropped")
    }

    /// Execute code with mutable access to the internal state.
    pub fn share_mut<R, F>(&self, f: F) -> R
    where
        F: FnOnce(&mut T) -> R,
    {
        self.0
            .share_mut(|s, _| Ok(f(s)))
            .expect("ShareOpen state is never dropped")
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/t_chan.rs
================================================
use crate::tx_utils::*;
use crate::*;
use std::future::Future;

use futures::future::FutureExt;
use tokio::sync::mpsc::error::SendTimeoutError;
use tokio::sync::mpsc::{channel, Receiver, Sender};

/// tokio::sync::mpsc::Sender is cheaply clonable,
/// futures::channel::mpsc::Sender can be closed from the sender side.
/// We want both these things.
/// Produce a TChan - a wrapper around a tokio::sync::mpsc::channel.
/// Provides futures::stream::Stream impl on TReceive.
/// Allows channel close from sender side.
pub fn t_chan<T: 'static + Send>(bound: usize) -> (TSender<T>, TReceiver<T>) {
    let (s, r) = channel(bound);
    (TSender(Arc::new(Share::new(s))), TReceiver(r))
}

/// The sender side of a t_chan - this is cheaply clone-able.
pub struct TSender<T: 'static + Send>(Arc<Share<Sender<T>>>);

impl<T: 'static + Send> Clone for TSender<T> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<T: 'static + Send> PartialEq for TSender<T> {
    fn eq(&self, oth: &Self) -> bool {
        Arc::ptr_eq(&self.0, &oth.0)
    }
}

impl<T: 'static + Send> Eq for TSender<T> {}

impl<T: 'static + Send> std::hash::Hash for TSender<T> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        Arc::as_ptr(&self.0).hash(state);
    }
}

impl<T: 'static + Send> TSender<T> {
    /// Send a type instance to this channel sender's receiver.
    pub fn send(&self, t: T) -> impl Future<Output = Result<(), T>> + 'static + Send + Unpin {
        let sender = match self.0.share_mut(|i, _| Ok(i.clone())) {
            Err(_) => return async move { Err(t) }.boxed(),
            Ok(s) => s,
        };
        async move {
            sender
                .send_timeout(t, std::time::Duration::from_secs(30))
                .await
                .map_err(|e| match e {
                    SendTimeoutError::Timeout(e) => e,
                    SendTimeoutError::Closed(e) => e,
                })
        }
        .boxed()
    }

    /// Close this channel from the sender side.
    /// The receiver can accept all pending sends, and then will close.
    pub fn close_channel(&self) {
        let _ = self.0.share_mut(|_, c| {
            *c = true;
            Ok(())
        });
    }
}

/// The receiver side of a t_chan.
pub struct TReceiver<T: 'static + Send>(Receiver<T>);

impl<T: 'static + Send> TReceiver<T> {
    /// Async receive the next item in the stream.
    pub async fn recv(&mut self) -> Option<T> {
        self.0.recv().await
    }

    /// Poll function for implementing low-level futures streams.
    pub fn poll_recv(&mut self, cx: &mut std::task::Context<'_>) -> std::task::Poll<Option<T>> {
        self.0.poll_recv(cx)
    }
}

impl<T: 'static + Send> futures::stream::Stream for TReceiver<T> {
    type Item = T;

    fn poll_next(
        mut self: std::pin::Pin<&mut Self>,
        cx: &mut std::task::Context<'_>,
    ) -> std::task::Poll<Option<Self::Item>> {
        self.poll_recv(cx)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_t_chan() {
        let (s, r) = t_chan::<u8>(2);

        let f = futures::future::try_join_all(vec![s.send(2), s.send(1), s.send(3)]);

        let t = metric_task(async move {
            let f = futures::future::try_join_all(vec![s.send(5), s.send(4), s.send(6)]);
            s.close_channel();
            f.await
        });

        let r = metric_task(async move {
            use futures::stream::StreamExt;
            KitsuneResult::Ok(r.collect::<Vec<_>>().await)
        });

        f.await.unwrap();
        t.await.unwrap().unwrap();

        let mut r = r.await.unwrap().unwrap();
        r.sort();
        assert_eq!(&[1, 2, 3, 4, 5, 6], r.as_slice());
    }
}



================================================
File: crates/kitsune_p2p/types/src/tx_utils/tx_url.rs
================================================
//! Defines the TxUrl type

use std::sync::Arc;

use crate::{KitsuneError, KitsuneErrorKind, KitsuneResult};

/// New-type for sync ref-counted Urls
/// to make passing around transport layer implementation more efficient.
#[derive(Clone, serde::Serialize, serde::Deserialize, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct TxUrl(Arc<url2::Url2>);

impl TxUrl {
    /// reference this txurl as a Url2.
    pub fn as_url2(&self) -> &url2::Url2 {
        &self.0
    }

    /// Construct from a string which is known to be a valid URL.
    /// Panics if the URL is not parseable.
    pub fn from_str_panicking(s: &str) -> Self {
        url2::Url2::parse(s).into()
    }
}

impl std::ops::Deref for TxUrl {
    type Target = url::Url;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl std::fmt::Debug for TxUrl {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl std::fmt::Display for TxUrl {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl From<TxUrl> for Arc<url2::Url2> {
    fn from(u: TxUrl) -> Self {
        u.0
    }
}

impl From<TxUrl> for url2::Url2 {
    fn from(u: TxUrl) -> Self {
        (*u.0).clone()
    }
}

impl From<Arc<url2::Url2>> for TxUrl {
    fn from(r: Arc<url2::Url2>) -> Self {
        Self(r)
    }
}

impl From<url2::Url2> for TxUrl {
    fn from(r: url2::Url2) -> Self {
        Self(Arc::new(r))
    }
}

impl TryFrom<String> for TxUrl {
    type Error = KitsuneError;

    fn try_from(r: String) -> KitsuneResult<Self> {
        Ok(Self(Arc::new(url2::Url2::try_parse(r.clone()).map_err(
            |e| KitsuneError::from(KitsuneErrorKind::BadInput(Box::new(e), r)),
        )?)))
    }
}

impl TryFrom<&String> for TxUrl {
    type Error = KitsuneError;

    fn try_from(r: &String) -> KitsuneResult<Self> {
        Ok(Self(Arc::new(url2::Url2::try_parse(r).map_err(|e| {
            KitsuneError::from(KitsuneErrorKind::BadInput(Box::new(e), r.clone()))
        })?)))
    }
}

impl TryFrom<&str> for TxUrl {
    type Error = KitsuneError;

    fn try_from(r: &str) -> KitsuneResult<Self> {
        Ok(Self(Arc::new(url2::Url2::try_parse(r).map_err(|e| {
            KitsuneError::from(KitsuneErrorKind::BadInput(Box::new(e), r.to_string()))
        })?)))
    }
}



================================================
File: crates/mock_hdi/README.md
================================================

# Holochain Mock HDI

This is a simple utility crate that allows mocking the HDI.

# Examples

```rust
use hdi::prelude::*;

// Create the mock.
let mut mock_hdi = holochain_mock_hdi::MockHdiT::new();

// Create the a return type.
let empty_agent_key = AgentPubKey::from_raw_36(vec![0u8; 36]);

// Setup the expectation.
mock_hdi.expect_hash().once().returning({
    let empty_agent_key = empty_agent_key.clone();
    move |_| Ok(HashOutput::Entry(empty_agent_key.clone().into()))
});

// Set the HDI to use the mock.
set_hdi(mock_hdi);

// Create an input type.
let hash_input = HashInput::Entry(Entry::Agent(empty_agent_key.clone()));

// Call the HDI and the mock will run.
let hash_output = HDI.with(|i| i.borrow().hash(hash_input)).unwrap();

assert!(matches!(
    hash_output,
    HashOutput::Entry(output) if output == EntryHash::from(empty_agent_key)
));
```



================================================
File: crates/mock_hdi/Cargo.toml
================================================
[package]
name = "holochain_mock_hdi"
version = "0.0.1"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"
description = "A mock version of the holochain HDI"
license = "Apache-2.0"
documentation = "https://docs.rs/holochain_mock_hdi"

# reminder - do not use workspace deps
[dependencies]
hdi = { path = "../hdi" }
mockall = { version = "0.11.3" }

[lints]
workspace = true

[features]
unstable-functions = ["hdi/unstable-functions"]



================================================
File: crates/mock_hdi/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## Unreleased



================================================
File: crates/mock_hdi/src/lib.rs
================================================
//! # Holochain Mock HDI
//!
//! This is a simple utility crate that allows mocking the HDI.
//!
//! # Examples
//!
//! ```
//! use hdi::prelude::*;
//!
//! // Create the mock.
//! let mut mock_hdi = holochain_mock_hdi::MockHdiT::new();
//!
//! // Create the a return type.
//! let empty_agent_key = AgentPubKey::from_raw_36(vec![0u8; 36]);
//!
//! // Setup the expectation.
//! mock_hdi.expect_hash().once().returning({
//!     let empty_agent_key = empty_agent_key.clone();
//!     move |_| Ok(HashOutput::Entry(empty_agent_key.clone().into()))
//! });
//!
//! // Set the HDI to use the mock.
//! set_hdi(mock_hdi);
//!
//! // Create an input type.
//! let hash_input = HashInput::Entry(Entry::Agent(empty_agent_key.clone()));
//!
//! // Call the HDI and the mock will run.
//! let hash_output = HDI.with(|i| i.borrow().hash(hash_input)).unwrap();
//!
//! assert!(matches!(
//!     hash_output,
//!     HashOutput::Entry(output) if output == EntryHash::from(empty_agent_key)
//! ));
//! ```

use hdi::prelude::*;

::mockall::mock! {
    pub HdiT {}
    impl HdiT for HdiT {
        fn verify_signature(&self, verify_signature: VerifySignature) -> ExternResult<bool>;
        fn hash(&self, hash_input: HashInput) -> ExternResult<HashOutput>;
        fn must_get_entry(&self, must_get_entry_input: MustGetEntryInput) -> ExternResult<EntryHashed>;
        fn must_get_action(
            &self,
            must_get_action_input: MustGetActionInput,
        ) -> ExternResult<SignedActionHashed>;
        fn must_get_valid_record(
            &self,
            must_get_valid_record_input: MustGetValidRecordInput,
        ) -> ExternResult<Record>;
        fn must_get_agent_activity(
            &self,
            input: MustGetAgentActivityInput,
        ) -> ExternResult<Vec<RegisterAgentActivity>>;
        // DPKI
        #[cfg(feature = "unstable-functions")]
        fn is_same_agent(&self, key_1: AgentPubKey, key_2: AgentPubKey) -> ExternResult<bool>;
        // Info
        fn dna_info(&self, dna_info_input: ()) -> ExternResult<DnaInfo>;
        fn zome_info(&self, zome_info_input: ()) -> ExternResult<ZomeInfo>;
        // Trace
        fn trace(&self, trace_msg: TraceMsg) -> ExternResult<()>;
        // XSalsa20Poly1305
        fn x_salsa20_poly1305_decrypt(
            &self,
            x_salsa20_poly1305_decrypt: XSalsa20Poly1305Decrypt,
        ) -> ExternResult<Option<XSalsa20Poly1305Data>>;
        fn x_25519_x_salsa20_poly1305_decrypt(
            &self,
            x_25519_x_salsa20_poly1305_decrypt: X25519XSalsa20Poly1305Decrypt,
        ) -> ExternResult<Option<XSalsa20Poly1305Data>>;
        fn ed_25519_x_salsa20_poly1305_decrypt(
            &self,
            ed_25519_x_salsa20_poly1305_decrypt: Ed25519XSalsa20Poly1305Decrypt,
        ) -> ExternResult<XSalsa20Poly1305Data>;
    }

}



================================================
File: crates/mr_bundle/README.md
================================================
# mr_bundle

Library for collecting and packing resources into a bundle with a manifest
file which describes those resources.

A [`Bundle`](crate::Bundle) contains a [`Manifest`](crate::Manifest) as well as any number of arbitrary,
opaque resources in the form of [`ResourceBytes`](crate::ResourceBytes).
A Bundle can be serialized and written to a file.

A Bundle can also be [packed](Bundle::pack_yaml) and [unpacked](Bundle::unpack_yaml),
via the `"packing"` feature.
Bundle packing is performed by following the [`Location`](crate::Location)s specified in the
Manifest as "Bundled", and pulling them into the Bundle that way.
Unpacking is done by specifying a target directory and creating a new file
for each resource at a relative path specified by the Manifest.

License: Apache-2.0



================================================
File: crates/mr_bundle/Cargo.toml
================================================
[package]
name = "mr_bundle"
version = "0.5.0-dev.5"
authors = ["Michael Dougherty <maackle.d@gmail.com>"]
edition = "2021"
description = "Implements the un-/packing of bundles that either embed or reference a set of resources"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/mr_bundle"

# reminder - do not use workspace deps
[dependencies]
derive_more = "0.99"
flate2 = "1.0"
holochain_util = { version = "^0.5.0-dev.1", path = "../holochain_util" }
futures = "0.3"
reqwest = { version = "0.12", default-features = false, features = [
  "rustls-tls",
] }
rmp-serde = "=1.3.0"
serde = { version = "1.0", features = ["serde_derive", "derive"] }
serde_bytes = "0.11"
thiserror = "1.0"

proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }
test-strategy = { version = "0", optional = true }
serde_yaml = { version = "0.9", optional = true }

[dev-dependencies]
anyhow = "1.0"
matches = "0.1"
maplit = "1"
serde_yaml = "0.9"
tokio = { version = "1.36.0", features = ["full"] }
tempfile = "3"

[lints]
workspace = true

[features]

packing = ["serde_yaml", "holochain_util/tokio"]

fuzzing = ["proptest", "proptest-derive", "test-strategy"]



================================================
File: crates/mr_bundle/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.5

## 0.5.0-dev.4

- Use `rustls-tls` instead of `native-tls-vendored` in reqwest due to compatibility issue with Android platform

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.2

- **BREAKING CHANGE:** The `resources` field of bundles was not properly set up for efficient serialization. Bundles built before this change must now be rebuilt. [\#1723](https://github.com/holochain/holochain/pull/1723)
  - Where the actual bytes of the resource were previously specified by a simple sequence of numbers, now a byte array is expected. For instance, in JavaScript, this is the difference between an Array and a Buffer.

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.20

## 0.0.19

## 0.0.18

## 0.0.17

## 0.0.16

## 0.0.15

## 0.0.14

- Fix inconsistent bundle writting due to unordered map of bundle resources

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/mr_bundle/.gitignore
================================================
tests/fixtures



================================================
File: crates/mr_bundle/src/bundle.rs
================================================
use crate::{
    error::{BundleError, MrBundleResult},
    location::Location,
    manifest::Manifest,
    resource::ResourceBytes,
};
use holochain_util::ffs;
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use std::{
    borrow::Cow,
    collections::{BTreeMap, HashMap, HashSet},
    path::{Path, PathBuf},
};

pub type ResourceMap = BTreeMap<PathBuf, ResourceBytes>;

/// A Manifest bundled together, optionally, with the Resources that it describes.
/// This is meant to be serialized for standalone distribution, and deserialized
/// by the receiver.
///
/// The manifest may describe locations of resources not included in the Bundle.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct Bundle<M>
where
    M: Manifest,
{
    /// The manifest describing the resources that compose this bundle.
    #[serde(bound(deserialize = "M: DeserializeOwned"))]
    manifest: M,

    /// The full or partial resource data. Each entry must correspond to one
    /// of the Bundled Locations specified by the Manifest. Bundled Locations
    /// are always relative paths (relative to the root_dir).
    resources: ResourceMap,

    /// Since the Manifest may contain local paths referencing unbundled files,
    /// on the local filesystem, we must have an absolute path at runtime for
    /// normalizing those locations.
    ///
    /// Passing None is a runtime assertion that the manifest contains only
    /// absolute local paths. If this assertion fails,
    /// **resource resolution will panic!**
    //
    // MAYBE: Represent this with types more solidly, perhaps breaking this
    //        struct into two versions for each case.
    #[serde(skip)]
    root_dir: Option<PathBuf>,
}

impl<M> Bundle<M>
where
    M: Manifest,
{
    /// Creates a bundle containing a manifest and a collection of resources to
    /// be bundled together with the manifest.
    ///
    /// The paths paired with each resource must correspond to the set of
    /// `Location::Bundle`s specified in the `Manifest::location()`, or else
    /// this is not a valid bundle.
    ///
    /// A base directory must also be supplied so that relative paths can be
    /// resolved into absolute ones.
    pub fn new<R: IntoIterator<Item = (PathBuf, ResourceBytes)>>(
        manifest: M,
        resources: R,
        root_dir: PathBuf,
    ) -> MrBundleResult<Self> {
        Self::from_parts(manifest, resources, Some(root_dir))
    }

    /// Create a bundle, asserting that all paths in the Manifest are absolute.
    pub fn new_unchecked<R: IntoIterator<Item = (PathBuf, ResourceBytes)>>(
        manifest: M,
        resources: R,
    ) -> MrBundleResult<Self> {
        Self::from_parts(manifest, resources, None)
    }

    fn from_parts<R: IntoIterator<Item = (PathBuf, ResourceBytes)>>(
        manifest: M,
        resources: R,
        root_dir: Option<PathBuf>,
    ) -> MrBundleResult<Self> {
        let resources: ResourceMap = resources.into_iter().collect();
        let manifest_paths: HashSet<_> = manifest
            .locations()
            .into_iter()
            .filter_map(|loc| match loc {
                Location::Bundled(path) => Some(path),
                _ => None,
            })
            .collect();

        // Validate that each resource path is contained in the manifest
        for (resource_path, _) in resources.iter() {
            if !manifest_paths.contains(resource_path) {
                return Err(BundleError::BundledPathNotInManifest(resource_path.clone()).into());
            }
        }

        let resources = resources.into_iter().collect();
        Ok(Self {
            manifest,
            resources,
            root_dir,
        })
    }

    /// Accessor for the Manifest
    pub fn manifest(&self) -> &M {
        &self.manifest
    }

    /// Return a new Bundle with an updated manifest, subject to the same
    /// validation constraints as creating a new Bundle from scratch.
    pub fn update_manifest(self, manifest: M) -> MrBundleResult<Self> {
        Self::from_parts(manifest, self.resources, self.root_dir)
    }

    /// Load a Bundle into memory from a file
    pub async fn read_from_file(path: &Path) -> MrBundleResult<Self> {
        Self::decode(&ffs::read(path).await?)
    }

    /// Write a Bundle to a file
    pub async fn write_to_file(&self, path: &Path) -> MrBundleResult<()> {
        Ok(ffs::write(path, &self.encode()?).await?)
    }

    /// Retrieve the bytes for a resource at a Location, downloading it if
    /// necessary
    pub async fn resolve(&self, location: &Location) -> MrBundleResult<Cow<'_, ResourceBytes>> {
        let bytes = match &location.normalize(self.root_dir.as_ref())? {
            Location::Bundled(path) => Cow::Borrowed(
                self.resources
                    .get(path)
                    .ok_or_else(|| BundleError::BundledResourceMissing(path.clone()))?,
            ),
            Location::Path(path) => Cow::Owned(crate::location::resolve_local(path).await?),
            Location::Url(url) => Cow::Owned(crate::location::resolve_remote(url).await?),
        };
        Ok(bytes)
    }

    /// Return the full set of resources specified by this bundle's manifest.
    /// References to bundled resources can be returned directly, while all
    /// others will be fetched from the filesystem or the network.
    pub async fn resolve_all(&self) -> MrBundleResult<HashMap<Location, Cow<'_, ResourceBytes>>> {
        futures::future::join_all(
            self.manifest.locations().into_iter().map(|loc| async move {
                MrBundleResult::Ok((loc.clone(), self.resolve(&loc).await?))
            }),
        )
        .await
        .into_iter()
        .collect::<MrBundleResult<HashMap<Location, Cow<'_, ResourceBytes>>>>()
    }

    /// Resolve all resources, but with fully owned references
    pub async fn resolve_all_cloned(&self) -> MrBundleResult<HashMap<Location, ResourceBytes>> {
        Ok(self
            .resolve_all()
            .await?
            .into_iter()
            .map(|(k, v)| (k, v.into_owned()))
            .collect())
    }

    /// Access the map of resources included in this bundle
    /// Bundled resources are also accessible via `resolve` or `resolve_all`,
    /// but using this method prevents a Clone
    pub fn bundled_resources(&self) -> &ResourceMap {
        &self.resources
    }

    /// An arbitrary and opaque encoding of the bundle data into a byte array
    pub fn encode(&self) -> MrBundleResult<Vec<u8>> {
        crate::encode(self)
    }

    /// Decode bytes produced by [`encode`](Bundle::encode)
    pub fn decode(bytes: &[u8]) -> MrBundleResult<Self> {
        crate::decode(bytes)
    }

    /// Given that the Manifest is located at the given absolute `path`, find
    /// the absolute root directory for the "unpacked" Bundle directory.
    /// Useful when "imploding" a directory into a bundle to determine the
    /// default location of the generated Bundle file.
    ///
    /// This will only be different than the Manifest path itself if the
    /// Manifest::path impl specifies a nested path.
    ///
    /// Will return None if the `path` does not actually end with the
    /// manifest relative path, meaning that either the manifest file is
    /// misplaced within the unpacked directory, or an incorrect path was
    /// supplied.
    #[cfg(feature = "packing")]
    pub fn find_root_dir(&self, path: &Path) -> MrBundleResult<PathBuf> {
        crate::util::prune_path(path.into(), M::path()).map_err(Into::into)
    }
}

/// A manifest bundled together, optionally, with the Resources that it describes.
/// The manifest may be of any format. This is useful for deserializing a bundle of
/// an outdated format, so that it may be modified to fit the supported format.
#[derive(Debug, PartialEq, Eq, Deserialize)]
pub struct RawBundle<M> {
    /// The manifest describing the resources that compose this bundle.
    #[serde(bound(deserialize = "M: DeserializeOwned"))]
    pub manifest: M,

    /// The full or partial resource data. Each entry must correspond to one
    /// of the Bundled Locations specified by the Manifest. Bundled Locations
    /// are always relative paths (relative to the root_dir).
    pub resources: ResourceMap,
}

impl<M: serde::de::DeserializeOwned> RawBundle<M> {
    /// Load a Bundle into memory from a file
    pub async fn read_from_file(path: &Path) -> MrBundleResult<Self> {
        crate::decode(&ffs::read(path).await?)
    }
}

#[cfg(test)]
mod tests {
    use crate::error::MrBundleError;

    use super::*;

    #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
    struct TestManifest(Vec<Location>);

    impl Manifest for TestManifest {
        fn locations(&self) -> Vec<Location> {
            self.0.clone()
        }

        #[cfg(feature = "packing")]
        fn path() -> PathBuf {
            unimplemented!()
        }

        #[cfg(feature = "packing")]
        fn bundle_extension() -> &'static str {
            unimplemented!()
        }
    }

    #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
    struct Thing(u32);

    #[tokio::test]
    async fn bundle_validation() {
        let manifest = TestManifest(vec![
            Location::Bundled("1.thing".into()),
            Location::Bundled("2.thing".into()),
        ]);
        assert!(
            Bundle::new_unchecked(manifest.clone(), vec![("1.thing".into(), vec![1].into())])
                .is_ok()
        );

        matches::assert_matches!(
            Bundle::new_unchecked(manifest, vec![("3.thing".into(), vec![3].into())]),
            Err(MrBundleError::BundleError(BundleError::BundledPathNotInManifest(path))) if path == PathBuf::from("3.thing")
        );
    }
}



================================================
File: crates/mr_bundle/src/encoding.rs
================================================
use crate::error::MrBundleError;

use super::error::MrBundleResult;
use std::io::Read;
use std::io::Write;

/// Get compressed bytes from some serializable data
pub fn encode<T: serde::ser::Serialize>(data: &T) -> MrBundleResult<Vec<u8>> {
    let bytes = rmp_serde::to_vec_named(data)?;
    let mut enc = flate2::write::GzEncoder::new(Vec::new(), flate2::Compression::default());
    enc.write_all(&bytes)?;
    Ok(enc.finish()?)
}

/// Decompress and deserialize some bytes (inverse of `encode`)
pub fn decode<T: serde::de::DeserializeOwned>(compressed: &[u8]) -> MrBundleResult<T> {
    let mut gz = flate2::read::GzDecoder::new(compressed);
    let mut bytes = Vec::new();
    gz.read_to_end(&mut bytes)?;
    rmp_serde::from_slice(&bytes)
        .map_err(|e| MrBundleError::MsgpackDecodeError(std::any::type_name::<T>().to_string(), e))
}



================================================
File: crates/mr_bundle/src/error.rs
================================================
#![allow(missing_docs)]

use holochain_util::ffs::IoError;

/// Any error which can occur in this crate
#[derive(Debug, thiserror::Error)]
pub enum MrBundleError {
    #[error(transparent)]
    StdIoError(#[from] std::io::Error),

    #[error(transparent)]
    BundleError(#[from] BundleError),

    #[cfg(feature = "packing")]
    #[error(transparent)]
    UnpackingError(#[from] UnpackingError),

    #[cfg(feature = "packing")]
    #[error(transparent)]
    PackingError(#[from] PackingError),

    #[error("IO error: {0}")]
    IoError(#[from] IoError),

    #[error(transparent)]
    HttpError(#[from] reqwest::Error),

    #[error(transparent)]
    MsgpackEncodeError(#[from] rmp_serde::encode::Error),

    #[error("Failed to decode bundle to [{0}] due to a deserialization error: {1}")]
    MsgpackDecodeError(String, rmp_serde::decode::Error),

    #[error("This bundle failed to validate because: {0}")]
    BundleValidationError(String),
}
pub type MrBundleResult<T> = Result<T, MrBundleError>;

/// Errors which can occur while constructing a Bundle
#[derive(Debug, PartialEq, Eq, thiserror::Error)]
pub enum BundleError {
    #[error(
        "The bundled resource path '{0}' is not mentioned in the manifest.
        Make sure that Manifest::location returns this path as a Location::Bundled."
    )]
    BundledPathNotInManifest(std::path::PathBuf),

    #[error("Attempted to resolve a bundled resource not present in this bundle: {0}")]
    BundledResourceMissing(std::path::PathBuf),

    #[error(
        "Cannot use relative paths for local locations. The following local path is relative: {0}"
    )]
    RelativeLocalPath(std::path::PathBuf),
}
pub type BundleResult<T> = Result<T, BundleError>;

/// Errors which can occur while unpacking resources from a Bundle
#[cfg(feature = "packing")]
#[derive(Debug, thiserror::Error)]
pub enum UnpackingError {
    #[error(transparent)]
    StdIoError(#[from] std::io::Error),

    #[error("IO error: {0}")]
    IoError(#[from] IoError),

    #[error(transparent)]
    YamlError(#[from] serde_yaml::Error),

    #[error("The supplied path '{0}' has no parent directory.")]
    ParentlessPath(std::path::PathBuf),

    #[error("The target directory '{0}' already exists.")]
    DirectoryExists(std::path::PathBuf),

    #[error("When imploding a bundle directory, the absolute manifest path specified did not match the relative path expected by the manifest.
    Absolute path: '{0}'. Relative path: '{1}'.")]
    ManifestPathSuffixMismatch(std::path::PathBuf, std::path::PathBuf),
}
#[cfg(feature = "packing")]
pub type UnpackingResult<T> = Result<T, UnpackingError>;

/// Errors which can occur while packing resources into a Bundle
#[cfg(feature = "packing")]
#[derive(Debug, thiserror::Error)]
pub enum PackingError {
    #[error("Must supply the path to the manifest file inside a bundle directory to pack. You supplied: {0}. Original error: {1}")]
    BadManifestPath(std::path::PathBuf, std::io::Error),
}
#[cfg(feature = "packing")]
pub type PackingResult<T> = Result<T, PackingError>;



================================================
File: crates/mr_bundle/src/lib.rs
================================================
//! Library for collecting and packing resources into a bundle with a manifest
//! file which describes those resources.
//!
//! A [`Bundle`] contains a [`Manifest`] as well as any number of arbitrary,
//! opaque resources in the form of [`ResourceBytes`].
//! A Bundle can be serialized and written to a file.
//!
//! A Bundle can also be [packed](Bundle::pack_yaml) and [unpacked](Bundle::unpack_yaml),
//! via the `"packing"` feature.
//! Bundle packing is performed by following the [`Location`]s specified in the
//! Manifest as "Bundled", and pulling them into the Bundle that way.
//! Unpacking is done by specifying a target directory and creating a new file
//! for each resource at a relative path specified by the Manifest.

#![warn(missing_docs)]

mod bundle;
mod encoding;
pub mod error;
mod location;
mod manifest;
mod resource;
pub(crate) mod util;

#[cfg(feature = "packing")]
mod packing;

pub use bundle::{Bundle, RawBundle};
pub use encoding::{decode, encode};
pub use location::Location;
pub use manifest::Manifest;
pub use resource::ResourceBytes;



================================================
File: crates/mr_bundle/src/location.rs
================================================
use crate::{
    error::{BundleError, MrBundleResult},
    ResourceBytes,
};
use holochain_util::ffs;
use std::path::{Path, PathBuf};

/// Where to find a Resource.
///
/// This representation, with named fields, is chosen so that in the yaml config
/// either "path", "url", or "bundled" can be specified due to this field
/// being flattened.
#[derive(Clone, Debug, Hash, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
#[allow(missing_docs)]
pub enum Location {
    /// Expect file to be part of this bundle
    Bundled(PathBuf),

    /// Get file from local filesystem (not bundled)
    Path(PathBuf),

    /// Get file from URL
    Url(String),
}

impl Location {
    /// Make a relative Path absolute if possible, given the `root_dir`
    pub fn normalize(&self, root_dir: Option<&PathBuf>) -> MrBundleResult<Location> {
        if let Location::Path(path) = self {
            if path.is_relative() {
                if let Some(dir) = root_dir {
                    Ok(Location::Path(ffs::sync::canonicalize(dir.join(path))?))
                } else {
                    Err(BundleError::RelativeLocalPath(path.to_owned()).into())
                }
            } else {
                Ok(self.clone())
            }
        } else {
            Ok(self.clone())
        }
    }
}

#[cfg(feature = "fuzzing")]
impl proptest::arbitrary::Arbitrary for Location {
    type Parameters = ();
    type Strategy = proptest::strategy::BoxedStrategy<Self>;

    // XXX: this is a bad arbitrary impl, could be derived automatically when
    // https://github.com/proptest-rs/proptest/pull/362 lands
    fn arbitrary_with((): Self::Parameters) -> Self::Strategy {
        use proptest::strategy::Strategy;

        proptest::prelude::any::<String>()
            .prop_map(|s| Self::Path(s.into()))
            .boxed()
    }
}

pub(crate) async fn resolve_local(path: &Path) -> MrBundleResult<ResourceBytes> {
    Ok(ffs::read(path).await?.into())
}

pub(crate) async fn resolve_remote(url: &str) -> MrBundleResult<ResourceBytes> {
    Ok(reqwest::get(url)
        .await?
        .bytes()
        .await?
        .into_iter()
        .collect::<Vec<_>>()
        .into())
}

#[cfg(test)]
mod tests {

    use super::*;
    use serde::{Deserialize, Serialize};
    use serde_yaml::value::{Tag, TaggedValue};

    #[derive(Serialize, Deserialize)]
    struct TunaSalad {
        celery: Vec<Location>,

        #[serde(flatten)]
        mayo: Location,
    }

    /// Test that Location serializes in a convenient way suitable for
    /// human-readable manifests, e.g. YAML
    ///
    /// The YAML produced by this test looks like:
    /// ```yaml
    /// ---
    /// celery:
    ///   - !bundled: b
    ///   - !path: p
    /// url: "http://r.co"
    /// ```
    #[test]
    fn location_flattening() {
        use serde_yaml::Value;

        let tuna = TunaSalad {
            celery: vec![Location::Bundled("b".into()), Location::Path("p".into())],
            mayo: Location::Url("http://r.co".into()),
        };
        let val = serde_yaml::to_value(&tuna).unwrap();
        println!("yaml produced:\n{}", serde_yaml::to_string(&tuna).unwrap());

        assert_eq!(
            val["celery"][0],
            Value::Tagged(Box::new(TaggedValue {
                tag: Tag::new("!bundled"),
                value: Value::from("b")
            }))
        );
        assert_eq!(
            val["celery"][1],
            Value::Tagged(Box::new(TaggedValue {
                tag: Tag::new("!path"),
                value: Value::from("p")
            }))
        );
        assert_eq!(val["url"], Value::from("http://r.co"));
    }
}



================================================
File: crates/mr_bundle/src/manifest.rs
================================================
use std::path::PathBuf;

use crate::location::Location;

/// A Manifest describes the resources in a [`Bundle`](crate::Bundle) and how
/// to pack and unpack them.
///
/// Regardless of the format of your Manifest, it must contain a set of Locations
/// describing where to find resources, and this trait must implement `locations`
/// properly to match the data contained in the manifest.
///
/// You must also specify a relative path for the Manifest, and the extension
/// for the bundle file, if you are using the "packing" feature.
pub trait Manifest:
    Clone + Sized + PartialEq + Eq + serde::Serialize + serde::de::DeserializeOwned
{
    /// The list of Locations referenced in the manifest data. This must be
    /// correctly implemented to enable resource resolution.
    fn locations(&self) -> Vec<Location>;

    /// When unpacking the bundle into a directory structure, this becomes
    /// the relative path of the manifest file.
    #[cfg(feature = "packing")]
    fn path() -> PathBuf;

    /// When packing a bundle from a directory structure, the bundle file gets
    /// this extension.
    #[cfg(feature = "packing")]
    fn bundle_extension() -> &'static str;

    /// Get only the Bundled locations
    fn bundled_paths(&self) -> Vec<PathBuf> {
        self.locations()
            .into_iter()
            .filter_map(|loc| {
                if let Location::Bundled(path) = loc {
                    Some(path)
                } else {
                    None
                }
            })
            .collect()
    }
}



================================================
File: crates/mr_bundle/src/packing.rs
================================================
use super::Bundle;
use crate::{
    bundle::ResourceMap,
    error::{MrBundleResult, PackingError, UnpackingError, UnpackingResult},
    util::prune_path,
    Manifest, RawBundle,
};
use holochain_util::ffs;
use std::path::Path;

impl<M: Manifest> Bundle<M> {
    /// Create a directory which contains the manifest as a YAML file,
    /// and each resource written to its own file (as raw bytes)
    /// The paths of the resources are specified by the paths of the bundle,
    /// and the path of the manifest file is specified by the `Manifest::path`
    /// trait method implementation of the `M` type.
    pub async fn unpack_yaml(&self, base_path: &Path, force: bool) -> MrBundleResult<()> {
        unpack_yaml(
            self.manifest(),
            self.bundled_resources(),
            base_path,
            M::path().as_ref(),
            force,
        )
        .await
        .map_err(Into::into)
    }

    /// Reconstruct a `Bundle<M>` from a previously unpacked directory.
    /// The manifest file itself must be specified, since it may have an arbitrary
    /// path relative to the unpacked directory root.
    pub async fn pack_yaml(manifest_path: &Path) -> MrBundleResult<Self> {
        let manifest_path = ffs::canonicalize(manifest_path).await?;
        let manifest_yaml = ffs::read_to_string(&manifest_path).await.map_err(|err| {
            PackingError::BadManifestPath(manifest_path.clone(), err.into_inner())
        })?;
        let manifest: M = serde_yaml::from_str(&manifest_yaml).map_err(UnpackingError::from)?;
        let manifest_relative_path = M::path();
        let base_path = prune_path(manifest_path.clone(), &manifest_relative_path)?;
        let resources = futures::future::join_all(manifest.bundled_paths().into_iter().map(
            |relative_path| async {
                let resource_path = ffs::canonicalize(base_path.join(&relative_path)).await?;
                ffs::read(&resource_path)
                    .await
                    .map(|resource| (relative_path, resource.into()))
            },
        ))
        .await
        .into_iter()
        .collect::<Result<Vec<_>, _>>()?;
        Bundle::new(manifest, resources, base_path)
    }
}

impl<M: serde::Serialize> RawBundle<M> {
    /// Create a directory which contains the manifest as a YAML file,
    /// and each resource written to its own file (as raw bytes)
    /// The paths of the resources are specified by the paths of the bundle,
    /// and the path of the manifest file is specified by the manifest_path parameter.
    pub async fn unpack_yaml(
        &self,
        base_path: &Path,
        manifest_path: &Path,
        force: bool,
    ) -> MrBundleResult<()> {
        unpack_yaml(
            &self.manifest,
            &self.resources,
            base_path,
            manifest_path,
            force,
        )
        .await
        .map_err(Into::into)
    }
}

async fn unpack_yaml<M: serde::Serialize>(
    manifest: &M,
    resources: &ResourceMap,
    base_path: &Path,
    manifest_path: &Path,
    force: bool,
) -> UnpackingResult<()> {
    if !force && base_path.exists() {
        return Err(UnpackingError::DirectoryExists(base_path.to_owned()));
    }
    ffs::create_dir_all(&base_path).await?;
    for (relative_path, resource) in resources {
        let path = base_path.join(relative_path);
        let path_clone = path.clone();
        let parent = path_clone
            .parent()
            .ok_or_else(|| UnpackingError::ParentlessPath(path.clone()))?;
        ffs::create_dir_all(&parent).await?;
        ffs::write(&path, resource).await?;
    }
    let yaml_str = serde_yaml::to_string(manifest)?;
    let manifest_path = base_path.join(manifest_path);
    ffs::write(&manifest_path, yaml_str.as_bytes()).await?;
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    fn test_pruning() {
        let path = PathBuf::from("/a/b/c/d");
        assert_eq!(
            prune_path(path.clone(), "d").unwrap(),
            PathBuf::from("/a/b/c")
        );
        assert_eq!(
            prune_path(path.clone(), "b/c/d").unwrap(),
            PathBuf::from("/a")
        );
        matches::assert_matches!(
            prune_path(path.clone(), "a/c"),
            Err(UnpackingError::ManifestPathSuffixMismatch(abs, rel))
            if abs == path && rel == PathBuf::from("a/c")
        );
    }
}



================================================
File: crates/mr_bundle/src/resource.rs
================================================
/// Arbitrary opaque bytes representing a Resource in a [`Bundle`](crate::Bundle)
#[derive(
    Clone,
    PartialEq,
    Eq,
    Hash,
    serde::Serialize,
    serde::Deserialize,
    derive_more::From,
    derive_more::Deref,
)]
pub struct ResourceBytes(#[serde(with = "serde_bytes")] Vec<u8>);

impl ResourceBytes {
    /// Accessor
    pub fn inner(&self) -> &[u8] {
        self.0.as_slice()
    }

    /// Convert to raw vec
    pub fn into_inner(self) -> Vec<u8> {
        self.0
    }
}

impl std::fmt::Debug for ResourceBytes {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!(
            "mr_bundle::ResourceBytes({})",
            &holochain_util::hex::many_bytes_string(self.0.as_slice())
        ))
    }
}



================================================
File: crates/mr_bundle/src/util.rs
================================================
#[cfg(feature = "packing")]
use std::path::{Path, PathBuf};

#[cfg(feature = "packing")]
use crate::error::{UnpackingError, UnpackingResult};

/// Removes a subpath suffix from a path
#[cfg(feature = "packing")]
pub fn prune_path<P: AsRef<Path>>(mut path: PathBuf, subpath: P) -> UnpackingResult<PathBuf> {
    if path.ends_with(&subpath) {
        for _ in subpath.as_ref().components() {
            let _ = path.pop();
        }
        Ok(path)
    } else {
        Err(UnpackingError::ManifestPathSuffixMismatch(
            path,
            subpath.as_ref().to_owned(),
        ))
    }
}



================================================
File: crates/mr_bundle/tests/integration.rs
================================================
use mr_bundle::{Bundle, Location, Manifest, ResourceBytes};
use std::{collections::HashSet, path::PathBuf};

#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "manifest_version")]
#[allow(missing_docs)]
enum TestManifest {
    #[serde(rename = "1")]
    #[serde(alias = "\"1\"")]
    V1(ManifestV1),
}

impl Manifest for TestManifest {
    fn locations(&self) -> Vec<Location> {
        match self {
            Self::V1(mani) => mani.things.iter().map(|b| b.location.clone()).collect(),
        }
    }

    #[cfg(feature = "packing")]
    fn path() -> PathBuf {
        "test-manifest.yaml".into()
    }

    #[cfg(feature = "packing")]
    fn bundle_extension() -> &'static str {
        unimplemented!()
    }
}

#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
struct ManifestV1 {
    name: String,
    things: Vec<ThingManifest>,
}

#[derive(Clone, Debug, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
struct ThingManifest {
    #[serde(flatten)]
    location: Location,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
struct Thing(String);

#[tokio::test]
async fn resource_resolution() {
    let dir = tempfile::tempdir().unwrap();

    // Write a ResourceBytes to disk
    let local_thing = Thing("local".into());
    let local_thing_encoded = mr_bundle::encode(&local_thing).unwrap().into();
    let local_path = dir.path().join("deeply/nested/local.thing");
    std::fs::create_dir_all(local_path.parent().unwrap()).unwrap();
    std::fs::write(&local_path, mr_bundle::encode(&local_thing).unwrap()).unwrap();

    let bundled_thing = Thing("bundled".into());
    let bundled_thing_encoded: ResourceBytes = mr_bundle::encode(&bundled_thing).unwrap().into();
    let bundled_path = PathBuf::from("another/nested/bundled.thing");

    // Create a Manifest that references these resources
    let bundled_location = Location::Bundled(bundled_path.clone());
    let local_location = Location::Path(local_path.clone());
    let manifest = TestManifest::V1(ManifestV1 {
        name: "name".to_string(),
        things: vec![
            ThingManifest {
                location: bundled_location.clone(),
            },
            ThingManifest {
                location: local_location.clone(),
            },
        ],
    });

    // Put the bundled resource into a Bundle (excluding the local resource)
    let bundle = Bundle::new_unchecked(
        manifest,
        vec![(bundled_path.clone(), bundled_thing_encoded.clone())],
    )
    .unwrap();
    assert_eq!(
        bundle
            .bundled_resources()
            .iter()
            .collect::<HashSet<(&PathBuf, &ResourceBytes)>>(),
        maplit::hashset![(&bundled_path, &bundled_thing_encoded)]
    );

    assert_eq!(
        bundle
            .resolve_all_cloned()
            .await
            .unwrap()
            .into_iter()
            .collect::<HashSet<(Location, ResourceBytes)>>(),
        maplit::hashset![
            (bundled_location, bundled_thing_encoded),
            (local_location, local_thing_encoded)
        ]
    );

    // Ensure that the bundle is serializable and writable
    let bundled_path = dir.path().join("test.bundle");
    let bundle_bytes = bundle.encode().unwrap();
    std::fs::write(&bundled_path, bundle_bytes).unwrap();

    // Ensure that it is also readable and deserializable
    let decoded_bundle: Bundle<_> = Bundle::decode(&std::fs::read(bundled_path).unwrap()).unwrap();
    assert_eq!(bundle, decoded_bundle);

    // Ensure that bundle writing and reading are inverses
    bundle
        .write_to_file(&dir.path().join("bundle.bundle"))
        .await
        .unwrap();
    let bundle_file = Bundle::read_from_file(&dir.path().join("bundle.bundle"))
        .await
        .unwrap();
    assert_eq!(bundle, bundle_file);
}

#[cfg(feature = "packing")]
#[tokio::test]
#[cfg_attr(target_os = "windows", ignore = "unc path mismatch - use dunce")]
async fn unpack_roundtrip() {
    let dir = tempfile::tempdir().unwrap();
    let dir_path = dir.path().canonicalize().unwrap();

    // Write a ResourceBytes to disk
    let local_thing = Thing("local".into());
    let local_path = dir_path.join("deeply/nested/local.thing");
    std::fs::create_dir_all(local_path.parent().unwrap()).unwrap();
    std::fs::write(&local_path, mr_bundle::encode(&local_thing).unwrap()).unwrap();

    let bundled_thing = Thing("bundled".into());
    let bundled_thing_encoded: ResourceBytes = mr_bundle::encode(&bundled_thing).unwrap().into();
    let bundled_path = PathBuf::from("another/nested/bundled.thing");

    // Create a Manifest that references these resources
    let bundled_location = Location::Bundled(bundled_path.clone());
    let local_location = Location::Path(local_path.clone());
    let manifest = TestManifest::V1(ManifestV1 {
        name: "name".to_string(),
        things: vec![
            ThingManifest {
                location: bundled_location.clone(),
            },
            ThingManifest {
                location: local_location.clone(),
            },
        ],
