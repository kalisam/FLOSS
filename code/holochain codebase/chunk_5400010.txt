    let sender = channel_factory
        .create_channel::<HolochainP2p>()
        .await
        .unwrap();

    tokio::task::spawn(builder.spawn(StubNetwork));

    sender
}

fixturator!(
    HolochainP2pDna;
    curve Empty {
        tokio_helper::block_forever_on(async {
            let holochain_p2p = crate::test::stub_network().await;
            holochain_p2p.to_dna(
                DnaHashFixturator::new(Empty).next().unwrap(),
                None
            )
        })
    };
    curve Unpredictable {
        HolochainP2pDnaFixturator::new(Empty).next().unwrap()
    };
    curve Predictable {
        HolochainP2pDnaFixturator::new(Empty).next().unwrap()
    };
);

#[cfg(test)]
mod tests {
    use crate::HolochainP2pSender;
    use crate::*;
    use ::fixt::prelude::*;
    use futures::future::FutureExt;
    use ghost_actor::GhostControlSender;
    use holo_hash::fixt::AgentPubKeyFixturator;
    use holo_hash::fixt::DhtOpHashFixturator;
    use holochain_keystore::test_keystore;
    use holochain_types::prelude::*;
    use kitsune_p2p::dht::prelude::Topology;
    use kitsune_p2p::dht::{ArqStrat, PeerView, PeerViewQ};
    use kitsune_p2p::*;
    use kitsune_p2p_types::config::KitsuneP2pConfig;
    use kitsune_p2p_types::dependencies::lair_keystore_api::dependencies::sodoken::sign;
    use kitsune_p2p_types::tls::TlsConfig;
    use std::sync::Mutex;

    macro_rules! newhash {
        ($p:ident, $c:expr) => {
            holo_hash::$p::from_raw_36([$c; HOLO_HASH_UNTYPED_LEN].to_vec())
        };
    }

    fn test_setup() -> (
        holo_hash::DnaHash,
        holo_hash::AgentPubKey,
        holo_hash::AgentPubKey,
        holo_hash::AgentPubKey,
    ) {
        holochain_trace::test_run();
        (
            newhash!(DnaHash, b's'),
            fixt!(AgentPubKey, Predictable, 0),
            newhash!(AgentPubKey, b'2'),
            newhash!(AgentPubKey, b'3'),
        )
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_call_remote_workflow() {
        let (dna, a1, a2, _) = test_setup();
        let keystore = test_keystore();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let (p2p, mut evt) = spawn_holochain_p2p(
            KitsuneP2pConfig::from_signal_addr(signal_url),
            TlsConfig::new_ephemeral().await.unwrap(),
            kitsune_p2p::HostStub::new(),
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    CallRemote { respond, .. } => {
                        respond.r(Ok(
                            async move { Ok(UnsafeBytes::from(b"yada".to_vec()).into()) }
                                .boxed()
                                .into(),
                        ));
                    }
                    SignNetworkData { respond, .. } => {
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    _ => {}
                }
            }
        });

        p2p.join(dna.clone(), a1.clone(), None, None).await.unwrap();
        p2p.join(dna.clone(), a2.clone(), None, None).await.unwrap();

        let zome_name: ZomeName = "".into();
        let fn_name: FunctionName = "".into();
        let nonce = Nonce256Bits::from([0; 32]);
        let cap_secret = None;
        let payload = ExternIO::encode(b"yippo").unwrap();
        let expires_at = (Timestamp::now() + std::time::Duration::from_secs(10)).unwrap();

        let (bytes, bytes_hash) = ZomeCallParams {
            provenance: a1.clone(),
            cell_id: CellId::new(dna.clone(), a2.clone()),
            zome_name: zome_name.clone(),
            fn_name: fn_name.clone(),
            cap_secret,
            payload: payload.clone(),
            nonce,
            expires_at,
        }
        .serialize_and_hash()
        .unwrap();
        let signature = a1.sign_raw(&keystore, bytes_hash.into()).await.unwrap();

        let res = p2p
            .call_remote(dna, a2, ExternIO(bytes), signature)
            .await
            .unwrap();
        let res: Vec<u8> = UnsafeBytes::from(res).into();

        assert_eq!(b"yada".to_vec(), res);

        p2p.ghost_actor_shutdown().await.unwrap();
        r_task.await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_call_remote_interface() {
        let (dna, alice, bob, _) = test_setup();
        let keystore = test_keystore();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let (p2p, mut evt) = spawn_holochain_p2p(
            KitsuneP2pConfig::from_signal_addr(signal_url),
            TlsConfig::new_ephemeral().await.unwrap(),
            kitsune_p2p::HostStub::new(),
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    CallRemote { respond, .. } => {
                        println!("incoming call remote");
                        respond.r(Ok(
                            async move { Ok(UnsafeBytes::from(b"yada".to_vec()).into()) }
                                .boxed()
                                .into(),
                        ));
                    }
                    SignNetworkData { respond, .. } => {
                        println!("incoming sign network data");
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        println!("incoming put agent info signed");
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        println!("incoming query peer density");
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    _ => {}
                }
            }
        });

        p2p.join(dna.clone(), alice.clone(), None, None)
            .await
            .unwrap();
        p2p.join(dna.clone(), bob.clone(), None, None)
            .await
            .unwrap();

        let zome_name: ZomeName = "".into();
        let fn_name: FunctionName = "".into();
        let nonce = Nonce256Bits::from([0; 32]);
        let cap_secret = None;
        let payload = ExternIO::encode(b"yippo").unwrap();
        let expires_at = (Timestamp::now() + std::time::Duration::from_secs(10)).unwrap();

        let (bytes, bytes_hash) = ZomeCallParams {
            provenance: alice.clone(),
            cell_id: CellId::new(dna.clone(), bob.clone()),
            zome_name: zome_name.clone(),
            fn_name: fn_name.clone(),
            cap_secret,
            payload: payload.clone(),
            nonce,
            expires_at,
        }
        .serialize_and_hash()
        .unwrap();
        let signature = alice.sign_raw(&keystore, bytes_hash.into()).await.unwrap();

        let res = p2p
            .call_remote(dna, bob, ExternIO(bytes), signature)
            .await
            .unwrap();
        let res: Vec<u8> = UnsafeBytes::from(res).into();
        println!("received res {res:?}");

        assert_eq!(b"yada".to_vec(), res);

        p2p.ghost_actor_shutdown().await.unwrap();
        r_task.await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_send_validation_receipt_workflow() {
        let (dna, a1, a2, _) = test_setup();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let (p2p, mut evt): (HolochainP2pRef, _) = spawn_holochain_p2p(
            KitsuneP2pConfig::from_signal_addr(signal_url),
            TlsConfig::new_ephemeral().await.unwrap(),
            kitsune_p2p::HostStub::new(),
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    ValidationReceiptsReceived {
                        respond, receipts, ..
                    } => {
                        assert_eq!(1, receipts.into_iter().count());
                        respond.r(Ok(async move { Ok(()) }.boxed().into()));
                    }
                    SignNetworkData { respond, .. } => {
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    _ => {}
                }
            }
        });

        p2p.join(dna.clone(), a1.clone(), None, None).await.unwrap();
        p2p.join(dna.clone(), a2.clone(), None, None).await.unwrap();

        let receipts = vec![SignedValidationReceipt {
            receipt: ValidationReceipt {
                dht_op_hash: fixt!(DhtOpHash),
                validation_status: ValidationStatus::Valid,
                validators: vec![],
                when_integrated: Timestamp::now(),
            },
            validators_signatures: vec![],
        }];
        p2p.send_validation_receipts(dna, a1, receipts.into())
            .await
            .unwrap();

        p2p.ghost_actor_shutdown().await.unwrap();
        r_task.await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_publish_workflow() {
        let (dna, a1, a2, a3) = test_setup();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let mut tuning_params =
            kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
        tuning_params.gossip_strategy = "none".to_string();
        let tuning_params = Arc::new(tuning_params);
        let mut config = KitsuneP2pConfig::from_signal_addr(signal_url);
        config.tuning_params = tuning_params;

        let host_list = Arc::new(Mutex::new(Vec::new()));
        let test_host = {
            let host_list = host_list.clone();
            HostStub::with_check_op_data(Box::new(move |space, list, ctx| {
                host_list
                    .lock()
                    .unwrap()
                    .push(format!("{:?}:{:?}:{:?}", space, list, ctx,));
                async move { Ok(list.into_iter().map(|_| false).collect()) }
                    .boxed()
                    .into()
            }))
        };
        //let test_host = TestHost::default();

        let (p2p, mut evt) = spawn_holochain_p2p(
            config,
            TlsConfig::new_ephemeral().await.unwrap(),
            test_host,
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    Publish { respond, .. } => {
                        respond.r(Ok(async move { Ok(()) }.boxed().into()));
                    }
                    SignNetworkData { respond, .. } => {
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryAgentInfoSignedNearBasis { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    oth => {
                        tracing::warn!(?oth, "@@@");
                    }
                }
            }
        });

        p2p.join(dna.clone(), a1.clone(), None, None).await.unwrap();
        p2p.join(dna.clone(), a2.clone(), None, None).await.unwrap();
        p2p.join(dna.clone(), a3.clone(), None, None).await.unwrap();

        let action_hash = holo_hash::OpBasis::from_raw_36_and_type(
            b"eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee".to_vec(),
            holo_hash::hash_type::AnyLinkable::Action,
        );

        // this will fail because we can't reach any remote nodes
        // but, it still published locally, so our test will work
        let _ = p2p
            .publish(
                dna,
                true,
                false,
                action_hash,
                a1.clone(),
                vec![],
                Some(200),
                None,
            )
            .await;

        assert_eq!(
            "KitsuneSpace(0x737373737373737373737373737373737373737373737373737373737373737373737373):[]:Some(FetchContext(1))",
            host_list.lock().unwrap()[0],
        );

        p2p.ghost_actor_shutdown().await.unwrap();
        r_task.await.unwrap();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_get_workflow() {
        holochain_trace::test_run();

        let (dna, a1, a2, _a3) = test_setup();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let cert = TlsConfig::new_ephemeral().await.unwrap();

        let mut params =
            kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
        params.default_rpc_multi_remote_agent_count = 1;
        params.default_rpc_multi_remote_request_grace_ms = 100;
        let mut config = KitsuneP2pConfig::from_signal_addr(signal_url);
        config.tuning_params = Arc::new(params);
        let (p2p, mut evt) = spawn_holochain_p2p(
            config,
            cert,
            kitsune_p2p::HostStub::new(),
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let test_1 = WireOps::Record(WireRecordOps {
            action: Some(Judged::valid(SignedAction::new(
                fixt!(Action),
                fixt!(Signature),
            ))),
            deletes: vec![],
            updates: vec![],
            entry: None,
        });
        let test_2 = WireOps::Record(WireRecordOps {
            action: Some(Judged::valid(SignedAction::new(
                fixt!(Action),
                fixt!(Signature),
            ))),
            deletes: vec![],
            updates: vec![],
            entry: None,
        });

        let mut respond_queue = vec![test_1.clone(), test_2.clone()];
        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    Get { respond, .. } => {
                        let resp = if let Some(h) = respond_queue.pop() {
                            h
                        } else {
                            panic!("too many requests!")
                        };
                        tracing::info!("test - get respond");
                        respond.r(Ok(async move { Ok(resp) }.boxed().into()));
                    }
                    SignNetworkData { respond, .. } => {
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryOpHashes { respond, .. } => {
                        respond.r(Ok(async move { Ok(None) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    evt => tracing::trace!("unhandled: {:?}", evt),
                }
            }
        });

        tracing::info!("test - join1");
        p2p.join(dna.clone(), a1.clone(), None, None).await.unwrap();
        tracing::info!("test - join2");
        p2p.join(dna.clone(), a2.clone(), None, None).await.unwrap();

        let hash = holo_hash::AnyDhtHash::from_raw_36_and_type(
            b"eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee".to_vec(),
            holo_hash::hash_type::AnyDht::Action,
        );

        tracing::info!("test - get");
        let res = p2p
            .get(dna, hash, crate::actor::GetOptions::default())
            .await
            .unwrap();

        tracing::info!("test - check res");
        assert_eq!(1, res.len());

        for r in res {
            assert!(r == test_1 || r == test_2);
        }

        tracing::info!("test - end of test shutdown p2p");
        p2p.ghost_actor_shutdown().await.unwrap();
        tracing::info!("test - end of test await task end");
        r_task.await.unwrap();
        tracing::info!("test - end of test - final done.");
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_get_links_workflow() {
        let (dna, a1, a2, _) = test_setup();
        let (signal_url, _signal_srv_handle) = kitsune_p2p::test_util::start_signal_srv().await;

        let mut params =
            kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams::default();
        params.default_rpc_multi_remote_agent_count = 1;
        params.default_rpc_multi_remote_request_grace_ms = 100;
        let mut config = KitsuneP2pConfig::from_signal_addr(signal_url);
        config.tuning_params = Arc::new(params);

        let (p2p, mut evt) = spawn_holochain_p2p(
            config,
            TlsConfig::new_ephemeral().await.unwrap(),
            kitsune_p2p::HostStub::new(),
            NetworkCompatParams::default(),
        )
        .await
        .unwrap();

        let test_1 = WireLinkOps {
            creates: vec![WireCreateLink::condense(
                fixt!(CreateLink),
                fixt!(Signature),
                ValidationStatus::Valid,
            )],
            deletes: vec![WireDeleteLink::condense(
                fixt!(DeleteLink),
                fixt!(Signature),
                ValidationStatus::Valid,
            )],
        };

        let test_1_clone = test_1.clone();
        let r_task = tokio::task::spawn(async move {
            use tokio_stream::StreamExt;
            while let Some(evt) = evt.next().await {
                let test_1_clone = test_1_clone.clone();
                use crate::types::event::HolochainP2pEvent::*;
                match evt {
                    GetLinks { respond, .. } => {
                        respond.r(Ok(async move { Ok(test_1_clone) }.boxed().into()));
                    }
                    SignNetworkData { respond, .. } => {
                        respond.r(Ok(async move { Ok([0; 64].into()) }.boxed().into()));
                    }
                    PutAgentInfoSigned { respond, .. } => {
                        respond.r(Ok(async move { Ok(vec![]) }.boxed().into()));
                    }
                    QueryPeerDensity { respond, .. } => {
                        let view = test_peer_view();
                        respond.r(Ok(async move { Ok(view) }.boxed().into()));
                    }
                    _ => {}
                }
            }
        });

        p2p.join(dna.clone(), a1.clone(), None, None).await.unwrap();
        p2p.join(dna.clone(), a2.clone(), None, None).await.unwrap();

        let hash = holo_hash::EntryHash::from_raw_36_and_type(
            b"eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee".to_vec(),
            holo_hash::hash_type::Entry,
        );
        let link_key = WireLinkKey {
            base: hash.into(),
            type_query: LinkTypeFilter::single_dep(0.into()),
            tag: None,
            after: None,
            before: None,
            author: None,
        };

        let res = p2p
            .get_links(dna, link_key, crate::actor::GetLinksOptions::default())
            .await
            .unwrap();

        assert_eq!(1, res.len());

        for r in res {
            assert_eq!(r, test_1);
        }

        p2p.ghost_actor_shutdown().await.unwrap();
        r_task.await.unwrap();
    }

    fn test_peer_view() -> PeerView {
        PeerViewQ::new(Topology::standard_epoch_full(), ArqStrat::default(), vec![]).into()
    }
}



================================================
File: crates/holochain_p2p/src/types.rs
================================================
/// Type to mock a Holochain P2p network using [`crate::MockHolochainP2pDnaT`].
pub type GenericNetwork = Arc<dyn HolochainP2pDnaT>;

/// Error type for Holochain P2p.
#[derive(Debug, thiserror::Error)]
#[non_exhaustive]
pub enum HolochainP2pError {
    /// GhostError
    #[error(transparent)]
    GhostError(#[from] ghost_actor::GhostError),

    /// RoutingDnaError
    #[error("Routing Dna Error: {0}")]
    RoutingDnaError(holo_hash::DnaHash),

    /// RoutingAgentError
    #[error("Routing Agent Error: {0}")]
    RoutingAgentError(holo_hash::AgentPubKey),

    /// OtherKitsuneP2pError
    #[error(transparent)]
    OtherKitsuneP2pError(kitsune_p2p::KitsuneP2pError),

    /// SerializedBytesError
    #[error(transparent)]
    SerializedBytesError(#[from] holochain_serialized_bytes::SerializedBytesError),

    /// Invalid P2p Message
    #[error("InvalidP2pMessage: {0}")]
    InvalidP2pMessage(String),

    /// Other
    #[error("Other: {0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),

    /// Chain Head Coordination error
    #[error(transparent)]
    ChcError(#[from] holochain_chc::ChcError),
}

impl HolochainP2pError {
    /// promote a custom error type to a TransportError
    pub fn other(e: impl Into<Box<dyn std::error::Error + Send + Sync>>) -> Self {
        Self::Other(e.into())
    }

    /// construct an invalid p2p message error variant
    pub fn invalid_p2p_message(s: String) -> Self {
        Self::InvalidP2pMessage(s)
    }
}

// do some manual type translation so we get better error displays
impl From<kitsune_p2p::KitsuneP2pError> for HolochainP2pError {
    fn from(e: kitsune_p2p::KitsuneP2pError) -> Self {
        use kitsune_p2p::KitsuneP2pError::*;
        match e {
            RoutingSpaceError(space) => {
                Self::RoutingDnaError(holo_hash::DnaHash::from_kitsune(&space))
            }
            RoutingAgentError(agent) => {
                Self::RoutingAgentError(holo_hash::AgentPubKey::from_kitsune(&agent))
            }
            _ => Self::OtherKitsuneP2pError(e),
        }
    }
}

impl From<HolochainP2pError> for kitsune_p2p::KitsuneP2pError {
    fn from(e: HolochainP2pError) -> Self {
        use HolochainP2pError::*;
        match e {
            RoutingDnaError(dna) => Self::RoutingSpaceError(dna.to_kitsune()),
            RoutingAgentError(agent) => Self::RoutingAgentError(agent.to_kitsune()),
            OtherKitsuneP2pError(e) => e,
            _ => Self::other(e),
        }
    }
}

impl From<String> for HolochainP2pError {
    fn from(s: String) -> Self {
        #[derive(Debug, thiserror::Error)]
        struct OtherError(String);
        impl std::fmt::Display for OtherError {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                write!(f, "{}", self.0)
            }
        }

        HolochainP2pError::other(OtherError(s))
    }
}

impl From<&str> for HolochainP2pError {
    fn from(s: &str) -> Self {
        s.to_string().into()
    }
}

/// Turn an [`AgentKey`](holo_hash::AgentPubKey) into a [`KitsuneAgent`](kitsune_p2p::KitsuneAgent)
pub fn agent_holo_to_kit(a: holo_hash::AgentPubKey) -> kitsune_p2p::KitsuneAgent {
    a.into_kitsune_raw()
}

/// Turn a [`DnaHash`](holo_hash::DnaHash) into a [`KitsuneSpace`](kitsune_p2p::KitsuneSpace)
pub fn space_holo_to_kit(d: holo_hash::DnaHash) -> kitsune_p2p::KitsuneSpace {
    d.into_kitsune_raw()
}

pub mod actor;
pub mod event;

pub(crate) mod wire;

use std::sync::Arc;

pub use wire::WireDhtOpData;
pub use wire::WireMessage;

use crate::HolochainP2pDnaT;

macro_rules! to_and_from_kitsune {
    ($($i:ident<$h:ty> -> $k:ty,)*) => {
        $(
            /// Extension trait for holo/kitsune conversion
            pub trait $i: ::std::clone::Clone + Sized {
                /// convert into `Arc<Kitsune>` type
                fn into_kitsune(self) -> ::std::sync::Arc<$k>;

                /// convert into Kitsune type
                fn into_kitsune_raw(self) -> $k;

                /// to `Arc<Kitsune>` type
                fn to_kitsune(&self) -> ::std::sync::Arc<$k> {
                    self.clone().into_kitsune()
                }

                /// from Kitsune type
                fn from_kitsune(k: &::std::sync::Arc<$k>) -> Self;

                /// from Kitsune type
                fn from_kitsune_raw(k: $k) -> Self;
            }

            impl $i for $h {
                fn into_kitsune(self) -> ::std::sync::Arc<$k> {
                    ::std::sync::Arc::new(self.into_kitsune_raw())
                }

                fn into_kitsune_raw(self) -> $k {
                    <$k as kitsune_p2p::KitsuneBinType>::new(self.get_raw_36().to_vec())
                }

                fn from_kitsune(k: &::std::sync::Arc<$k>) -> Self {
                    <$h>::from_raw_36((**k).clone().into()).into()
                }

                fn from_kitsune_raw(k: $k) -> Self {
                    <$h>::from_raw_36(k.into()).into()
                }
            }
        )*
    };
}

to_and_from_kitsune! {
    DnaHashExt<holo_hash::DnaHash> -> kitsune_p2p::KitsuneSpace,
    AgentPubKeyExt<holo_hash::AgentPubKey> -> kitsune_p2p::KitsuneAgent,
    DhtOpHashExt<holo_hash::DhtOpHash> -> kitsune_p2p::KitsuneOpHash,
}

macro_rules! to_kitsune {
    ($($i:ident<$h:ty> -> $k:ty,)*) => {
        $(
            /// Extension trait for holo/kitsune conversion
            pub trait $i: ::std::clone::Clone + Sized {
                /// convert into `Arc<Kitsune>` type
                fn into_kitsune(self) -> ::std::sync::Arc<$k>;

                /// convert into Kitsune type
                fn into_kitsune_raw(self) -> $k;

                /// to `Arc<Kitsune>` type
                fn to_kitsune(&self) -> ::std::sync::Arc<$k> {
                    self.clone().into_kitsune()
                }
            }

            impl $i for $h {
                fn into_kitsune(self) -> ::std::sync::Arc<$k> {
                    ::std::sync::Arc::new(self.into_kitsune_raw())
                }

                fn into_kitsune_raw(self) -> $k {
                    <$k as kitsune_p2p::KitsuneBinType>::new(self.get_raw_36().to_vec())
                }
            }
        )*
    };
}

to_kitsune! {
    AnyDhtHashExt<holo_hash::AnyDhtHash> -> kitsune_p2p::KitsuneBasis,
    AnyLinkableHashExt<holo_hash::AnyLinkableHash> -> kitsune_p2p::KitsuneBasis,
}



================================================
File: crates/holochain_p2p/src/spawn/actor.rs
================================================
#![allow(clippy::too_many_arguments)]
use crate::actor::*;
use crate::event::*;
use crate::*;

use futures::future::FutureExt;
use kitsune_p2p::actor::BroadcastData;
use kitsune_p2p::dependencies::kitsune_p2p_fetch;
use kitsune_p2p::dht::Arq;
use kitsune_p2p::event::*;
use kitsune_p2p::gossip::sharded_gossip::KitsuneDiagnostics;
use kitsune_p2p::KOp;
use kitsune_p2p::KitsuneOpData;
use kitsune_p2p::PreflightUserData;
use kitsune_p2p_fetch::FetchContext;

use crate::types::AgentPubKeyExt;

use ghost_actor::dependencies::tracing;
use ghost_actor::dependencies::tracing_futures::Instrument;

use kitsune2_api::DhtArc;
use kitsune_p2p::actor::KitsuneP2pSender;
use kitsune_p2p::agent_store::AgentInfoSigned;
use kitsune_p2p_types::bootstrap::AgentInfoPut;
use std::collections::{HashMap, HashSet};
use std::future::Future;
use std::iter;

macro_rules! timing_trace {
    ($netaudit:literal, $code:block $($rest:tt)*) => {{
        let __start = std::time::Instant::now();
        let __out = $code;
        async move {
            let __out = __out.await;
            let __elapsed_s = __start.elapsed().as_secs_f64();
            if __elapsed_s >= 5.0 {
                if $netaudit {
                    tracing::warn!( target: "NETAUDIT", m = "holochain_p2p", elapsed_s = %__elapsed_s $($rest)* );
                } else {
                    tracing::warn!( elapsed_s = %__elapsed_s $($rest)* );
                }
            } else {
                if $netaudit {
                    tracing::trace!( target: "NETAUDIT", m = "holochain_p2p", elapsed_s = %__elapsed_s $($rest)* );
                } else {
                    tracing::trace!( elapsed_s = %__elapsed_s $($rest)* );
                }
            }
            __out
        }
    }};
}

#[derive(Clone)]
struct WrapEvtSender(futures::channel::mpsc::Sender<HolochainP2pEvent>);

impl WrapEvtSender {
    pub fn put_agent_info_signed(
        &self,
        dna_hash: DnaHash,
        peer_data: Vec<AgentInfoSigned>,
    ) -> impl Future<Output = HolochainP2pResult<Vec<AgentInfoPut>>> + 'static + Send {
        timing_trace!(
            false,
            { self.0.put_agent_info_signed(dna_hash, peer_data) },
            a = "recv_put_agent_info_signed",
        )
    }

    fn query_gossip_agents(
        &self,
        dna_hash: DnaHash,
        agents: Option<Vec<AgentPubKey>>,
        kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
        since_ms: u64,
        until_ms: u64,
        arc_set: Arc<kitsune_p2p_types::dht_arc::DhtArcSet>,
    ) -> impl Future<Output = HolochainP2pResult<Vec<AgentInfoSigned>>> + 'static + Send {
        timing_trace!(
            false,
            {
                self.0.query_gossip_agents(
                    dna_hash,
                    agents,
                    kitsune_space,
                    since_ms,
                    until_ms,
                    arc_set,
                )
            },
            a = "recv_query_gossip_agents",
        )
    }

    fn query_agent_info_signed(
        &self,
        dna_hash: DnaHash,
        agents: Option<HashSet<Arc<kitsune_p2p::KitsuneAgent>>>,
        kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
    ) -> impl Future<Output = HolochainP2pResult<Vec<AgentInfoSigned>>> + 'static + Send {
        timing_trace!(
            false,
            {
                self.0
                    .query_agent_info_signed(dna_hash, agents, kitsune_space)
            },
            a = "recv_query_agent_info_signed",
        )
    }

    fn query_agent_info_signed_near_basis(
        &self,
        dna_hash: DnaHash,
        kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
        basis_loc: u32,
        limit: u32,
    ) -> impl Future<Output = HolochainP2pResult<Vec<AgentInfoSigned>>> + 'static + Send {
        timing_trace!(
            false,
            {
                self.0
                    .query_agent_info_signed_near_basis(dna_hash, kitsune_space, basis_loc, limit)
            },
            a = "recv_query_agent_info_signed_near_basis",
        )
    }

    fn query_peer_density(
        &self,
        dna_hash: DnaHash,
        kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
        dht_arc: kitsune_p2p_types::dht_arc::DhtArc,
    ) -> impl Future<Output = HolochainP2pResult<kitsune_p2p_types::dht::PeerView>> + 'static + Send
    {
        timing_trace!(
            false,
            { self.0.query_peer_density(dna_hash, kitsune_space, dht_arc) },
            a = "recv_query_peer_density",
        )
    }

    fn call_remote(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> impl Future<Output = HolochainP2pResult<SerializedBytes>> + 'static + Send {
        let byte_count = zome_call_params_serialized.0.len();
        timing_trace!(
            true,
            {
                self.0.call_remote(
                    dna_hash, // from,
                    to_agent,
                    zome_call_params_serialized,
                    signature,
                )
            },
            byte_count,
            a = "recv_call_remote",
        )
    }

    fn publish(
        &self,
        dna_hash: DnaHash,
        request_validation_receipt: bool,
        countersigning_session: bool,
        ops: Vec<holochain_types::dht_op::DhtOp>,
    ) -> impl Future<Output = HolochainP2pResult<()>> + 'static + Send {
        let op_count = ops.len();
        timing_trace!(
            true,
            {
                self.0.publish(dna_hash, request_validation_receipt, countersigning_session, ops)
            }, %op_count, a = "recv_publish")
    }

    fn get(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetOptions,
    ) -> impl Future<Output = HolochainP2pResult<WireOps>> + 'static + Send {
        timing_trace!(
            true,
            { self.0.get(dna_hash, to_agent, dht_hash, options) },
            a = "recv_get",
        )
    }

    fn get_meta(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetMetaOptions,
    ) -> impl Future<Output = HolochainP2pResult<MetadataSet>> + 'static + Send {
        timing_trace!(
            true,
            { self.0.get_meta(dna_hash, to_agent, dht_hash, options) },
            a = "recv_get_meta",
        )
    }

    fn get_links(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        link_key: WireLinkKey,
        options: event::GetLinksOptions,
    ) -> impl Future<Output = HolochainP2pResult<WireLinkOps>> + 'static + Send {
        timing_trace!(
            true,
            { self.0.get_links(dna_hash, to_agent, link_key, options) },
            a = "recv_get_links",
        )
    }

    fn count_links(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        query: WireLinkQuery,
    ) -> impl Future<Output = HolochainP2pResult<CountLinksResponse>> + 'static + Send {
        timing_trace!(
            true,
            { self.0.count_links(dna_hash, to_agent, query) },
            a = "recv_count_links"
        )
    }

    fn get_agent_activity(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: event::GetActivityOptions,
    ) -> impl Future<Output = HolochainP2pResult<AgentActivityResponse>> + 'static + Send {
        timing_trace!(
            true,
            {
                self.0
                    .get_agent_activity(dna_hash, to_agent, agent, query, options)
            },
            a = "recv_get_agent_activity",
        )
    }

    fn must_get_agent_activity(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> impl Future<Output = HolochainP2pResult<MustGetAgentActivityResponse>> + 'static + Send
    {
        timing_trace!(
            true,
            {
                self.0
                    .must_get_agent_activity(dna_hash, to_agent, agent, filter)
            },
            a = "recv_must_get_agent_activity",
        )
    }

    fn validation_receipts_received(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> impl Future<Output = HolochainP2pResult<()>> + 'static + Send {
        timing_trace!(
            false,
            {
                self.0
                    .validation_receipts_received(dna_hash, to_agent, receipts)
            },
            a = "recv_validation_receipt_received",
        )
    }

    fn query_op_hashes(
        &self,
        dna_hash: DnaHash,
        arc_set: kitsune_p2p::dht_arc::DhtArcSet,
        window: TimeWindow,
        max_ops: usize,
        include_limbo: bool,
    ) -> impl Future<
        Output = HolochainP2pResult<Option<(Vec<holo_hash::DhtOpHash>, TimeWindowInclusive)>>,
    >
           + 'static
           + Send {
        timing_trace!(
            false,
            {
                self.0
                    .query_op_hashes(dna_hash, arc_set, window, max_ops, include_limbo)
            },
            a = "recv_query_op_hashes",
        )
    }

    fn fetch_op_data(
        &self,
        dna_hash: DnaHash,
        query: FetchOpDataQuery,
    ) -> impl Future<
        Output = HolochainP2pResult<Vec<(holo_hash::DhtOpHash, holochain_types::dht_op::DhtOp)>>,
    >
           + 'static
           + Send {
        timing_trace!(
            false,
            { self.0.fetch_op_data(dna_hash, query) },
            a = "recv_fetch_op_data",
        )
    }

    fn sign_network_data(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        data: Vec<u8>,
    ) -> impl Future<Output = HolochainP2pResult<Signature>> + 'static + Send {
        let byte_count = data.len();
        timing_trace!(
            false,
            { self.0.sign_network_data(dna_hash, to_agent, data) },
            %byte_count,
            a = "recv_sign_network_data",
        )
    }

    fn countersigning_session_negotiation(
        &self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        message: event::CountersigningSessionNegotiationMessage,
    ) -> impl Future<Output = HolochainP2pResult<()>> + 'static + Send {
        timing_trace!(
            false,
            {
                self.0
                    .countersigning_session_negotiation(dna_hash, to_agent, message)
            },
            a = "recv_countersigning_session_negotiation"
        )
    }
}

pub(crate) struct HolochainP2pActor {
    config: kitsune_p2p_types::config::KitsuneP2pConfig,
    evt_sender: WrapEvtSender,
    kitsune_p2p: ghost_actor::GhostSender<kitsune_p2p::actor::KitsuneP2p>,
    host: kitsune_p2p::HostApi,
}

impl ghost_actor::GhostControlHandler for HolochainP2pActor {
    fn handle_ghost_actor_shutdown(
        self,
    ) -> ghost_actor::dependencies::must_future::MustBoxFuture<'static, ()> {
        use ghost_actor::GhostControlSender;
        async move {
            let _ = self.kitsune_p2p.ghost_actor_shutdown_immediate().await;
        }
        .boxed()
        .into()
    }
}

impl HolochainP2pActor {
    /// constructor
    pub async fn new(
        config: kitsune_p2p_types::config::KitsuneP2pConfig,
        tls_config: kitsune_p2p_types::tls::TlsConfig,
        channel_factory: ghost_actor::actor_builder::GhostActorChannelFactory<Self>,
        evt_sender: futures::channel::mpsc::Sender<HolochainP2pEvent>,
        host: kitsune_p2p::HostApi,
        compat: NetworkCompatParams,
    ) -> HolochainP2pResult<Self> {
        let mut bytes = vec![];
        kitsune_p2p_types::codec::rmp_encode(&mut bytes, &compat)
            .map_err(HolochainP2pError::other)?;

        let preflight_user_data = PreflightUserData {
            bytes: bytes.clone(),
            comparator: Box::new(move |url, mut recvd_bytes| {
                if bytes.as_slice() != recvd_bytes {
                    let common = "Cannot complete preflight handshake with peer because network compatibility params don't match";
                    Err(
                        match kitsune_p2p_types::codec::rmp_decode::<_, NetworkCompatParams>(
                            &mut recvd_bytes,
                        ) {
                            Ok(theirs) => {
                                format!("{common}. ours={compat:?}, theirs={theirs:?}, url={url}")
                            }
                            Err(err) => {
                                format!(
                                "{common}. (Can't decode peer's sent hash.) url={url}, err={err}"
                            )
                            }
                        },
                    )
                } else {
                    Ok(())
                }
            }),
        };

        let (kitsune_p2p, kitsune_p2p_events) = kitsune_p2p::spawn_kitsune_p2p(
            config.clone(),
            tls_config,
            host.clone(),
            preflight_user_data,
        )
        .await?;

        channel_factory.attach_receiver(kitsune_p2p_events).await?;

        Ok(Self {
            config,
            evt_sender: WrapEvtSender(evt_sender),
            kitsune_p2p,
            host,
        })
    }

    /// receiving an incoming request from a remote node
    #[allow(clippy::too_many_arguments)]
    fn handle_incoming_call_remote(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender
                .call_remote(dna_hash, to_agent, zome_call_params_serialized, signature)
                .await;
            res.map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming get request from a remote node
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, dna_hash, to_agent, dht_hash, options), level = "trace")
    )]
    fn handle_incoming_get(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetOptions,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender.get(dna_hash, to_agent, dht_hash, options).await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .instrument(tracing::debug_span!("incoming_get_task"))
        .boxed()
        .into())
    }

    /// receiving an incoming get_meta request from a remote node
    fn handle_incoming_get_meta(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetMetaOptions,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender
                .get_meta(dna_hash, to_agent, dht_hash, options)
                .await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming get_links request from a remote node
    fn handle_incoming_get_links(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        link_key: WireLinkKey,
        options: event::GetLinksOptions,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender
                .get_links(dna_hash, to_agent, link_key, options)
                .await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    fn handle_incoming_count_links(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        query: WireLinkQuery,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender.count_links(dna_hash, to_agent, query).await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming get_links request from a remote node
    fn handle_incoming_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: event::GetActivityOptions,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender
                .get_agent_activity(dna_hash, to_agent, agent, query, options)
                .await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming must_get_agent_activity request from a remote node
    fn handle_incoming_must_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<Vec<u8>> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let res = evt_sender
                .must_get_agent_activity(dna_hash, to_agent, agent, filter)
                .await;
            res.and_then(|r| Ok(SerializedBytes::try_from(r)?))
                .map_err(kitsune_p2p::KitsuneP2pError::from)
                .map(|res| UnsafeBytes::from(res).into())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming publish from a remote node
    fn handle_incoming_publish(
        &mut self,
        dna_hash: DnaHash,
        request_validation_receipt: bool,
        countersigning_session: bool,
        ops: Vec<holochain_types::dht_op::DhtOp>,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<()> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            evt_sender
                .publish(
                    dna_hash,
                    request_validation_receipt,
                    countersigning_session,
                    ops,
                )
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    /// receiving an incoming validation receipt from a remote node
    fn handle_incoming_validation_receipt(
        &mut self,
        dna_hash: DnaHash,
        agent_pub_key: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<()> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            evt_sender
                .validation_receipts_received(dna_hash, agent_pub_key, receipts)
                .await?;

            // validation receipts don't need a response
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_incoming_countersigning_session_negotiation(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        message: CountersigningSessionNegotiationMessage,
    ) -> kitsune_p2p::actor::KitsuneP2pHandlerResult<()> {
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            evt_sender
                .countersigning_session_negotiation(dna_hash, to_agent, message)
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }
}

impl ghost_actor::GhostHandler<kitsune_p2p::event::KitsuneP2pEvent> for HolochainP2pActor {}

impl kitsune_p2p::event::KitsuneP2pEventHandler for HolochainP2pActor {
    /// We need to store signed agent info.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_put_agent_info_signed(
        &mut self,
        input: kitsune_p2p::event::PutAgentInfoSignedEvt,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<Vec<AgentInfoPut>> {
        let kitsune_p2p::event::PutAgentInfoSignedEvt { peer_data } = input;

        let put_requests = peer_data
            .into_iter()
            .map(|agent| (DnaHash::from_kitsune(&agent.space), agent))
            .fold(
                HashMap::<DnaHash, Vec<AgentInfoSigned>>::new(),
                |mut acc, (dna, agent)| {
                    acc.entry(dna).or_default().push(agent);
                    acc
                },
            );

        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            Ok(futures::future::join_all(
                iter::repeat_with(|| evt_sender.clone())
                    .zip(put_requests.into_iter())
                    .map(|(evt_sender, (dna, agents))| async move {
                        evt_sender.put_agent_info_signed(dna, agents).await
                    }),
            )
            .await
            .into_iter()
            .collect::<HolochainP2pResult<Vec<Vec<AgentInfoPut>>>>()?
            .into_iter()
            .flatten()
            .collect())
        }
        .boxed()
        .into())
    }

    /// We need to get previously stored agent info. A single kitusne agent query
    /// can take one of three Holochain agent query paths. We do "duck typing"
    /// on the query object to determine which query path to take. The reason for
    /// this is that Holochain is optimized for these three query types, while
    /// kitsune has a more general interface.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_query_agents(
        &mut self,
        input: kitsune_p2p::event::QueryAgentsEvt,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<Vec<AgentInfoSigned>> {
        let kitsune_p2p::event::QueryAgentsEvt {
            space,
            agents,
            window,
            arq_set,
            near_basis,
            limit,
        } = input;

        let h_space = DnaHash::from_kitsune(&space);
        let evt_sender = self.evt_sender.clone();

        Ok(async move {
            let agents = match (agents, window, arq_set, near_basis, limit) {
                // If only basis and limit are set, this is a "near basis" query
                (None, None, None, Some(basis), Some(limit)) => {
                    evt_sender
                        .query_agent_info_signed_near_basis(h_space, space, basis.as_u32(), limit)
                        .await?
                }

                // If arc_set is set, this is a "gossip agents" query
                (agents, window, Some(arq_set), None, None) => {
                    let window = window.unwrap_or_else(full_time_window);
                    let h_agents =
                        agents.map(|agents| agents.iter().map(AgentPubKey::from_kitsune).collect());
                    let since_ms = window.start.as_millis().max(0) as u64;
                    let until_ms = window.end.as_millis().max(0) as u64;
                    evt_sender
                        .query_gossip_agents(
                            h_space,
                            h_agents,
                            space,
                            since_ms,
                            until_ms,
                            arq_set.to_dht_arc_set_std().into(),
                        )
                        .await?
                }

                // Otherwise, do a simple agent query with optional agent filter
                (agents, None, None, None, None) => {
                    evt_sender
                        .query_agent_info_signed(h_space, agents, space)
                        .await?
                }

                // If none of the above match, we have no implementation for such a query
                // and must fail
                tuple => unimplemented!(
                    "Holochain cannot interpret the QueryAgentsEvt data as given: {:?}",
                    tuple
                ),
            };
            Ok(agents)
        }
        .boxed()
        .into())
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_query_peer_density(
        &mut self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        dht_arc: kitsune_p2p_types::dht_arc::DhtArc,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView> {
        let h_space = DnaHash::from_kitsune(&space);
        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            Ok(evt_sender
                .query_peer_density(h_space, space, dht_arc)
                .await?)
        }
        .boxed()
        .into())
    }

    /// Handle an incoming call.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, space, to_agent, payload), level = "trace")
    )]
    fn handle_call(
        &mut self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        to_agent: Arc<kitsune_p2p::KitsuneAgent>,
        payload: Vec<u8>,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<Vec<u8>> {
        let space = DnaHash::from_kitsune(&space);
        let to_agent = AgentPubKey::from_kitsune(&to_agent);

        let request =
            crate::wire::WireMessage::decode(payload.as_ref()).map_err(HolochainP2pError::from)?;

        match request {
            crate::wire::WireMessage::CallRemote {
                to_agent,
                zome_call_params_serialized,
                signature,
            } => self.handle_incoming_call_remote(
                space,to_agent,  zome_call_params_serialized,signature,
            ),
            crate::wire::WireMessage::CallRemoteMulti {
                to_agents,
            } => {
                match to_agents
                    .into_iter()
                    .find(|( agent,_zome_call_payload, _signature)| agent == &to_agent)
                {
                    Some((to_agent, zome_call_payload, signature)) => self.handle_incoming_call_remote(
                        space, to_agent,zome_call_payload, signature
                    ),
                    None => Err(HolochainP2pError::RoutingAgentError(to_agent).into()),
                }
            }
            crate::wire::WireMessage::Get { dht_hash, options } => {
                self.handle_incoming_get(space, to_agent, dht_hash, options)
            }
            crate::wire::WireMessage::GetMeta { dht_hash, options } => {
                self.handle_incoming_get_meta(space, to_agent, dht_hash, options)
            }
            crate::wire::WireMessage::GetLinks { link_key, options } => {
                self.handle_incoming_get_links(space, to_agent, link_key, options)
            }
            WireMessage::CountLinks { query } => {
                self.handle_incoming_count_links(space, to_agent, query)
            }
            crate::wire::WireMessage::GetAgentActivity {
                agent,
                query,
                options,
            } => self.handle_incoming_get_agent_activity(space, to_agent, agent, query, options),
            crate::wire::WireMessage::MustGetAgentActivity { agent, filter } => {
                self.handle_incoming_must_get_agent_activity(space, to_agent, agent, filter)
            }
            crate::wire::WireMessage::ValidationReceipts { .. } => {
                Err(HolochainP2pError::invalid_p2p_message(
                    "invalid: validation receipts are now notifications rather than requests, please upgrade".to_string(),
                )
                    .into())
            }
            // holochain_p2p only broadcasts this message.
            crate::wire::WireMessage::CountersigningSessionNegotiation { .. }
            | crate::wire::WireMessage::PublishCountersign { .. } => {
                Err(HolochainP2pError::invalid_p2p_message(
                    "invalid: countersigning messages are broadcast, not requests".to_string(),
                )
                .into())
            }
        }
    }

    /// Handle an incoming notify.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_notify(
        &mut self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        to_agent: Arc<kitsune_p2p::KitsuneAgent>,
        payload: Vec<u8>,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<()> {
        let space = DnaHash::from_kitsune(&space);
        let to_agent = AgentPubKey::from_kitsune(&to_agent);

        let request =
            crate::wire::WireMessage::decode(payload.as_ref()).map_err(HolochainP2pError::from)?;

        match request {
            // error on these call type messages
            crate::wire::WireMessage::Get { .. }
            | crate::wire::WireMessage::GetMeta { .. }
            | crate::wire::WireMessage::GetLinks { .. }
            | crate::wire::WireMessage::CountLinks { .. }
            | crate::wire::WireMessage::GetAgentActivity { .. }
            | crate::wire::WireMessage::MustGetAgentActivity { .. } => {
                Err(HolochainP2pError::invalid_p2p_message(
                    "invalid call type message in a notify".to_string(),
                )
                .into())
            }
            crate::wire::WireMessage::CallRemote {
                to_agent,
                zome_call_params_serialized,
                signature,
            } => {
                let fut = self.handle_incoming_call_remote(
                    space,
                    to_agent,
                    zome_call_params_serialized,
                    signature,
                );
                Ok(async move {
                    let _ = fut?.await?;
                    Ok(())
                }
                .boxed()
                .into())
            }
            crate::wire::WireMessage::CallRemoteMulti { to_agents } => {
                match to_agents
                    .into_iter()
                    .find(|(agent, _zome_call_payload, _signature)| agent == &to_agent)
                {
                    Some((to_agent, zome_call_payload, signature)) => {
                        let fut = self.handle_incoming_call_remote(
                            space,
                            to_agent,
                            zome_call_payload,
                            signature,
                        );
                        Ok(async move {
                            let _ = fut?.await?;
                            Ok(())
                        }
                        .boxed()
                        .into())
                    }
                    None => Err(HolochainP2pError::RoutingAgentError(to_agent).into()),
                }
            }
            WireMessage::ValidationReceipts { receipts } => {
                self.handle_incoming_validation_receipt(space, to_agent, receipts)
            }
            crate::wire::WireMessage::CountersigningSessionNegotiation { message } => {
                self.handle_incoming_countersigning_session_negotiation(space, to_agent, message)
            }
            crate::wire::WireMessage::PublishCountersign { flag, op } => {
                self.handle_incoming_publish(space, false, flag, vec![op])
            }
        }
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_receive_ops(
        &mut self,
        space: Arc<kitsune_p2p::KitsuneSpace>,
        ops: Vec<KOp>,
        context: Option<FetchContext>,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<()> {
        let space = DnaHash::from_kitsune(&space);

        let ops = ops
            .into_iter()
            .map(|op_data| {
                let op = crate::wire::WireDhtOpData::decode(op_data.0.clone())
                    .map_err(HolochainP2pError::from)?
                    .op_data;

                Ok(op)
            })
            .collect::<Result<_, HolochainP2pError>>()?;
        if let Some(context) = context {
            self.handle_incoming_publish(
                space,
                context.has_request_validation_receipt(),
                context.has_countersigning_session(),
                ops,
            )
        } else {
            self.handle_incoming_publish(space, false, false, ops)
        }
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_query_op_hashes(
        &mut self,
        input: kitsune_p2p::event::QueryOpHashesEvt,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<
        Option<(Vec<Arc<kitsune_p2p::KitsuneOpHash>>, TimeWindowInclusive)>,
    > {
        let kitsune_p2p::event::QueryOpHashesEvt {
            space,
            arc_set,
            window,
            max_ops,
            include_limbo,
        } = input;
        let space = DnaHash::from_kitsune(&space);

        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            Ok(evt_sender
                .query_op_hashes(space, arc_set, window, max_ops, include_limbo)
                .await?
                .map(|(h, time)| (h.into_iter().map(|h| h.into_kitsune()).collect(), time)))
        }
        .boxed()
        .into())
    }

    #[allow(clippy::needless_collect)]
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_fetch_op_data(
        &mut self,
        input: kitsune_p2p::event::FetchOpDataEvt,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<Vec<(Arc<kitsune_p2p::KitsuneOpHash>, KOp)>>
    {
        let kitsune_p2p::event::FetchOpDataEvt { space, query } = input;
        let space = DnaHash::from_kitsune(&space);
        let query = FetchOpDataQuery::from_kitsune(query);

        let evt_sender = self.evt_sender.clone();
        Ok(async move {
            let mut out = vec![];
            for (op_hash, dht_op) in evt_sender.fetch_op_data(space.clone(), query).await? {
                out.push((
                    op_hash.into_kitsune(),
                    KitsuneOpData::new(
                        crate::wire::WireDhtOpData { op_data: dht_op }
                            .encode()
                            .map_err(kitsune_p2p::KitsuneP2pError::other)?,
                    ),
                ));
            }
            Ok(out)
        }
        .boxed()
        .into())
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_sign_network_data(
        &mut self,
        input: kitsune_p2p::event::SignNetworkDataEvt,
    ) -> kitsune_p2p::event::KitsuneP2pEventHandlerResult<kitsune_p2p::KitsuneSignature> {
        let space = DnaHash::from_kitsune(&input.space);
        let agent = AgentPubKey::from_kitsune(&input.agent);
        let fut = self
            .evt_sender
            .sign_network_data(space, agent, input.data.to_vec());
        Ok(async move {
            let sig = fut.await?.0;
            Ok(sig.to_vec().into())
        }
        .boxed()
        .into())
    }
}

macro_rules! timing_trace_out {
    ($code:expr, $($rest:tt)*) => {{
        let __start = std::time::Instant::now();
        let __out = $code;
        Ok(async move {
            let __out = __out.await;
            let __elapsed_s = __start.elapsed().as_secs_f64();
            match &__out {
                Ok(_) => {
                    tracing::trace!(
                        target: "NETAUDIT",
                        m = "holochain_p2p",
                        r = "ok",
                        elapsed_s = __elapsed_s,
                        $($rest)*
                    );
                }
                Err(err) => {
                    tracing::trace!(
                        target: "NETAUDIT",
                        m = "holochain_p2p",
                        ?err,
                        elapsed_s = __elapsed_s,
                        $($rest)*
                    );
                }
            }
            __out
        }
        .boxed()
        .into())
    }};
}

impl ghost_actor::GhostHandler<HolochainP2p> for HolochainP2pActor {}

impl HolochainP2pHandler for HolochainP2pActor {
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_join(
        &mut self,
        dna_hash: DnaHash,
        agent_pub_key: AgentPubKey,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<Arq>,
    ) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();
        let agent = agent_pub_key.into_kitsune();

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            Ok(kitsune_p2p
                .join(space, agent, maybe_agent_info, initial_arq)
                .await?)
        }
        .boxed()
        .into())
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_leave(
        &mut self,
        dna_hash: DnaHash,
        agent_pub_key: AgentPubKey,
    ) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();
        let agent = agent_pub_key.into_kitsune();

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move { Ok(kitsune_p2p.leave(space, agent).await?) }
            .boxed()
            .into())
    }

    /// Dispatch an outgoing remote call.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_call_remote(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> HolochainP2pHandlerResult<SerializedBytes> {
        let space = dna_hash.into_kitsune();
        let to_agent_kitsune = to_agent.clone().into_kitsune();

        let byte_count = zome_call_params_serialized.0.len();

        let req =
            crate::wire::WireMessage::call_remote(to_agent, zome_call_params_serialized, signature)
                .encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        timing_trace_out!(
            async move {
                let result: Vec<u8> = kitsune_p2p
                    .rpc_single(space, to_agent_kitsune, req, None)
                    .await?;
                Ok(UnsafeBytes::from(result).into())
            },
            byte_count,
            a = "send_call_remote"
        )
    }

    /// Dispatch an outgoing signal.
    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_send_remote_signal(
        &mut self,
        dna_hash: DnaHash,
        to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
    ) -> HolochainP2pHandlerResult<()> {
        let byte_count = to_agent_list
            .first()
            .map(|to_agent| to_agent.1 .0.len())
            .unwrap_or_else(|| 0);
        let space = dna_hash.into_kitsune();

        let req = crate::wire::WireMessage::call_remote_multi(to_agent_list.clone()).encode()?;

        let timeout = self.config.tuning_params.implicit_timeout();

        let to_agents = to_agent_list
            .iter()
            .map(|(agent, _zome_call_payload, _signature)| agent.clone().into_kitsune())
            .collect();
        let kitsune_p2p = self.kitsune_p2p.clone();
        timing_trace_out!(
            async move {
                kitsune_p2p
                    .targeted_broadcast(space, to_agents, timeout, req, true)
                    .await?;
                Ok(())
            },
            byte_count,
            a = "send_remote_signal"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_publish(
        &mut self,
        dna_hash: DnaHash,
        request_validation_receipt: bool,
        countersigning_session: bool,
        basis_hash: holo_hash::OpBasis,
        source: AgentPubKey,
        op_hash_list: Vec<OpHashSized>,
        timeout_ms: Option<u64>,
        reflect_ops: Option<Vec<DhtOp>>,
    ) -> HolochainP2pHandlerResult<()> {
        let op_hash_count = op_hash_list.len();

        use kitsune_p2p_types::KitsuneTimeout;

        let source = source.into_kitsune();
        let space = dna_hash.clone().into_kitsune();
        let basis = basis_hash.to_kitsune();
        let timeout = match timeout_ms {
            Some(ms) => KitsuneTimeout::from_millis(ms),
            None => self.config.tuning_params.implicit_timeout(),
        };

        let fetch_context = FetchContext::default()
            .with_request_validation_receipt(request_validation_receipt)
            .with_countersigning_session(countersigning_session);

        let kitsune_p2p = self.kitsune_p2p.clone();
        let host = self.host.clone();
        let evt_sender = self.evt_sender.clone();
        timing_trace_out!(
            async move {
                if let Some(reflect_ops) = reflect_ops {
                    let _ = evt_sender
                        .publish(
                            dna_hash,
                            request_validation_receipt,
                            countersigning_session,
                            reflect_ops,
                        )
                        .await;
                }

                // little awkward, but we need the side-effects of reporting
                // the context back to the host api here:
                if let Err(err) = host
                    .check_op_data(
                        space.clone(),
                        op_hash_list.iter().map(|x| x.data()).collect(),
                        Some(fetch_context),
                    )
                    .await
                {
                    tracing::warn!(?err);
                }

                kitsune_p2p
                    .broadcast(
                        space.clone(),
                        basis.clone(),
                        timeout,
                        BroadcastData::Publish {
                            source,
                            transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                            op_hash_list,
                            context: fetch_context,
                        },
                    )
                    .await?;
                Ok(())
            },
            op_hash_count,
            a = "send_publish"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_publish_countersign(
        &mut self,
        dna_hash: DnaHash,
        flag: bool,
        basis_hash: holo_hash::OpBasis,
        op: DhtOp,
    ) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();
        let basis = basis_hash.to_kitsune();
        let timeout = self.config.tuning_params.implicit_timeout();

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            let payload = crate::wire::WireMessage::publish_countersign(flag, op).encode()?;

            kitsune_p2p
                .broadcast(space, basis, timeout, BroadcastData::User(payload))
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self, dna_hash, dht_hash, options), level = "trace")
    )]
    fn handle_get(
        &mut self,
        dna_hash: DnaHash,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetOptions,
    ) -> HolochainP2pHandlerResult<Vec<WireOps>> {
        let space = dna_hash.into_kitsune();
        let basis = dht_hash.to_kitsune();
        let r_options: event::GetOptions = (&options).into();

        let payload = crate::wire::WireMessage::get(dht_hash, r_options).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                let result = kitsune_p2p
                    .rpc_multi(input)
                    .instrument(tracing::debug_span!("rpc_multi"))
                    .await?;

                let mut out = Vec::new();
                for item in result {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = item;
                    out.push(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?);
                }

                Ok(out)
            },
            a = "send_get"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_get_meta(
        &mut self,
        dna_hash: DnaHash,
        dht_hash: holo_hash::AnyDhtHash,
        options: actor::GetMetaOptions,
    ) -> HolochainP2pHandlerResult<Vec<MetadataSet>> {
        let space = dna_hash.into_kitsune();
        let basis = dht_hash.to_kitsune();
        let r_options: event::GetMetaOptions = (&options).into();

        let payload = crate::wire::WireMessage::get_meta(dht_hash, r_options).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                let result = kitsune_p2p.rpc_multi(input).await?;

                let mut out = Vec::new();
                for item in result {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = item;
                    out.push(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?);
                }

                Ok(out)
            },
            a = "send_get_meta"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_get_links(
        &mut self,
        dna_hash: DnaHash,
        link_key: WireLinkKey,
        options: actor::GetLinksOptions,
    ) -> HolochainP2pHandlerResult<Vec<WireLinkOps>> {
        let space = dna_hash.into_kitsune();
        let basis = link_key.base.to_kitsune();
        let r_options: event::GetLinksOptions = (&options).into();

        let payload = crate::wire::WireMessage::get_links(link_key, r_options).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let mut input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                // NOTE - We're just targeting a single remote node for now
                //        without doing any pagination / etc...
                //        Setting up RpcMulti to act like RpcSingle
                input.max_remote_agent_count = 1;
                let result = kitsune_p2p.rpc_multi(input).await?;

                let mut out = Vec::new();
                for item in result {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = item;
                    out.push(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?);
                }

                Ok(out)
            },
            a = "send_get_links"
        )
    }

    fn handle_count_links(
        &mut self,
        dna_hash: DnaHash,
        query: WireLinkQuery,
    ) -> HolochainP2pHandlerResult<CountLinksResponse> {
        let space = dna_hash.into_kitsune();
        let basis = query.base.to_kitsune();

        let payload = WireMessage::count_links(query).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let mut input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                input.max_remote_agent_count = 1;
                let result = kitsune_p2p.rpc_multi(input).await?;

                if let Some(result) = result.into_iter().next() {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = result;
                    Ok(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?)
                } else {
                    Err(HolochainP2pError::from(
                        "Failed to fetch link count from a peer",
                    ))
                }
            },
            a = "send_count_links"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: actor::GetActivityOptions,
    ) -> HolochainP2pHandlerResult<Vec<AgentActivityResponse>> {
        let space = dna_hash.into_kitsune();
        // Convert the agent key to an any dht hash so that it can be used
        // as the basis for sending this request
        let agent_hash: AnyDhtHash = agent.clone().into();
        let basis = agent_hash.to_kitsune();
        let r_options: event::GetActivityOptions = (&options).into();

        let payload =
            crate::wire::WireMessage::get_agent_activity(agent, query, r_options).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let mut input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                // TODO - We're just targeting a single remote node for now
                //        without doing any pagination / etc...
                //        Setting up RpcMulti to act like RpcSingle
                input.max_remote_agent_count = 1;
                let result = kitsune_p2p.rpc_multi(input).await?;

                let mut out = Vec::new();
                for item in result {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = item;
                    out.push(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?);
                }

                Ok(out)
            },
            a = "send_get_agent_activity"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_must_get_agent_activity(
        &mut self,
        dna_hash: DnaHash,
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> HolochainP2pHandlerResult<Vec<MustGetAgentActivityResponse>> {
        let space = dna_hash.into_kitsune();
        // Convert the agent key to an any dht hash so it can be used
        // as the basis for sending this request
        let agent_hash: AnyDhtHash = agent.clone().into();
        let basis = agent_hash.to_kitsune();

        let payload = crate::wire::WireMessage::must_get_agent_activity(agent, filter).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        let tuning_params = self.config.tuning_params.clone();
        timing_trace_out!(
            async move {
                let mut input =
                    kitsune_p2p::actor::RpcMulti::new(&tuning_params, space, basis, payload);
                // TODO - We're just targeting a single remote node for now
                //        without doing any pagination / etc...
                //        Setting up RpcMulti to act like RpcSingle
                input.max_remote_agent_count = 1;
                let result = kitsune_p2p.rpc_multi(input).await?;

                let mut out = Vec::new();
                for item in result {
                    let kitsune_p2p::actor::RpcMultiResponse { response, .. } = item;
                    out.push(SerializedBytes::from(UnsafeBytes::from(response)).try_into()?);
                }

                Ok(out)
            },
            a = "send_must_get_agent_activity"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_send_validation_receipts(
        &mut self,
        dna_hash: DnaHash,
        to_agent: AgentPubKey,
        receipts: ValidationReceiptBundle,
    ) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();
        let to_agent = to_agent.into_kitsune();

        let req = crate::wire::WireMessage::validation_receipts(receipts).encode()?;

        let timeout = self.config.tuning_params.implicit_timeout();

        let kitsune_p2p = self.kitsune_p2p.clone();
        timing_trace_out!(
            async move {
                kitsune_p2p
                    .targeted_broadcast(space, vec![to_agent], timeout, req, false)
                    .await?;
                Ok(())
            },
            a = "send_validation_receipts"
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_new_integrated_data(&mut self, dna_hash: DnaHash) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(
            async move { Ok(kitsune_p2p.new_integrated_data(space).await?) }
                .boxed()
                .into(),
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_authority_for_hash(
        &mut self,
        dna_hash: DnaHash,
        basis_hash: OpBasis,
    ) -> HolochainP2pHandlerResult<bool> {
        let space = dna_hash.into_kitsune();
        let basis = basis_hash.to_kitsune();

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(
            async move { Ok(kitsune_p2p.authority_for_hash(space, basis).await?) }
                .boxed()
                .into(),
        )
    }

    #[cfg_attr(
        feature = "instrument",
        tracing::instrument(skip(self), level = "trace")
    )]
    fn handle_countersigning_session_negotiation(
        &mut self,
        dna_hash: DnaHash,
        agents: Vec<AgentPubKey>,
        message: CountersigningSessionNegotiationMessage,
    ) -> HolochainP2pHandlerResult<()> {
        let space = dna_hash.into_kitsune();
        let agents = agents.into_iter().map(|a| a.into_kitsune()).collect();

        let timeout = self.config.tuning_params.implicit_timeout();

        let payload =
            crate::wire::WireMessage::countersigning_session_negotiation(message).encode()?;

        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            kitsune_p2p
                .targeted_broadcast(space, agents, timeout, payload, false)
                .await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_dump_network_metrics(
        &mut self,
        dna_hash: Option<DnaHash>,
    ) -> HolochainP2pHandlerResult<String> {
        let space = dna_hash.map(|h| h.into_kitsune());
        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            serde_json::to_string_pretty(&kitsune_p2p.dump_network_metrics(space).await?)
                .map_err(HolochainP2pError::other)
        }
        .boxed()
        .into())
    }

    fn handle_dump_network_stats(&mut self) -> HolochainP2pHandlerResult<String> {
        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            serde_json::to_string_pretty(&kitsune_p2p.dump_network_stats().await?)
                .map_err(HolochainP2pError::other)
        }
        .boxed()
        .into())
    }

    fn handle_get_diagnostics(
        &mut self,
        dna_hash: DnaHash,
    ) -> HolochainP2pHandlerResult<KitsuneDiagnostics> {
        let space = dna_hash.into_kitsune();
        let kitsune_p2p = self.kitsune_p2p.clone();
        Ok(async move {
            kitsune_p2p
                .get_diagnostics(space)
                .await
                .map_err(HolochainP2pError::other)
        }
        .boxed()
        .into())
    }

    fn handle_storage_arcs(&mut self, dna_hash: DnaHash) -> HolochainP2pHandlerResult<Vec<DhtArc>> {
        let space = dna_hash.into_kitsune();
        let kitsune_p2p = self.kitsune_p2p.clone();

        Ok(async move {
            kitsune_p2p
                .storage_arcs(space)
                .await
                .map_err(HolochainP2pError::other)
        }
        .boxed()
        .into())
    }
}



================================================
File: crates/holochain_p2p/src/types/actor.rs
================================================
//! Module containing the HolochainP2p actor definition.
#![allow(clippy::too_many_arguments)]

use crate::event::GetRequest;
use crate::*;
use holochain_chc::ChcImpl;
use holochain_types::activity::AgentActivityResponse;
use holochain_types::prelude::ValidationReceiptBundle;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::FetchContext;
use kitsune_p2p::dependencies::kitsune_p2p_fetch::OpHashSized;
use kitsune_p2p::gossip::sharded_gossip::KitsuneDiagnostics;
use kitsune_p2p_types::agent_info::AgentInfoSigned;

/// Holochain-specific FetchContext extension trait.
pub trait FetchContextExt {
    /// Applies the "request_validation_receipt" flag *if* the param is true
    /// otherwise, leaves the flag unchanged.
    fn with_request_validation_receipt(&self, request_validation_receipt: bool) -> Self;

    /// Returns true if the "request_validation_receipt" flag is set.
    fn has_request_validation_receipt(&self) -> bool;

    /// Applies the "countersigning_session" flag *if* the param is true
    /// otherwise, leaves the flag unchanged.
    fn with_countersigning_session(&self, countersigning_session: bool) -> Self;

    /// Returns true if the "countersigning_session" flag is set.
    fn has_countersigning_session(&self) -> bool;
}

const FLAG_REQ_VAL_RCPT: u32 = 1 << 0;
const FLAG_CNTR_SSN: u32 = 1 << 1;

impl FetchContextExt for FetchContext {
    fn with_request_validation_receipt(&self, request_validation_receipt: bool) -> Self {
        if request_validation_receipt {
            FetchContext(self.0 | FLAG_REQ_VAL_RCPT)
        } else {
            *self
        }
    }

    fn has_request_validation_receipt(&self) -> bool {
        self.0 & FLAG_REQ_VAL_RCPT > 0
    }

    fn with_countersigning_session(&self, countersigning_session: bool) -> Self {
        if countersigning_session {
            FetchContext(self.0 | FLAG_CNTR_SSN)
        } else {
            *self
        }
    }

    fn has_countersigning_session(&self) -> bool {
        self.0 & FLAG_CNTR_SSN > 0
    }
}

#[derive(Clone, Debug)]
/// Get options help control how the get is processed at various levels.
/// Fields tagged with ```[Network]``` are network-level controls.
/// Fields tagged with ```[Remote]``` are controls that will be forwarded to the
/// remote agent processing this `Get` request.
pub struct GetOptions {
    /// ```[Network]```
    /// How many remote nodes should we make requests of / aggregate.
    /// Set to `None` for a default "best-effort".
    pub remote_agent_count: Option<u8>,

    /// ```[Network]```
    /// Timeout to await responses for aggregation.
    /// Set to `None` for a default "best-effort".
    /// Note - if all requests time-out you will receive an empty result,
    /// not a timeout error.
    pub timeout_ms: Option<u64>,

    /// ```[Network]```
    /// We are interested in speed. If `true` and we have any results
    /// when `race_timeout_ms` is expired, those results will be returned.
    /// After `race_timeout_ms` and before `timeout_ms` the first result
    /// received will be returned.
    pub as_race: bool,

    /// ```[Network]```
    /// See `as_race` for details.
    /// Set to `None` for a default "best-effort" race.
    pub race_timeout_ms: Option<u64>,

    /// ```[Remote]```
    /// Whether the remote-end should follow redirects or just return the
    /// requested entry.
    pub follow_redirects: bool,

    /// ```[Remote]```
    /// Return all live actions even if there is deletes.
    /// Useful for metadata calls.
    pub all_live_actions_with_metadata: bool,

    /// ```[Remote]```
    /// The type of data this get request requires.
    pub request_type: GetRequest,
}

impl Default for GetOptions {
    fn default() -> Self {
        Self {
            remote_agent_count: None,
            timeout_ms: None,
            as_race: true,
            race_timeout_ms: None,
            follow_redirects: true,
            all_live_actions_with_metadata: false,
            request_type: Default::default(),
        }
    }
}

impl GetOptions {
    /// Using defaults is dangerous in a must_get as it can undermine determinism.
    /// We want refactors to explicitly consider this.
    pub fn must_get_options() -> Self {
        Self {
            remote_agent_count: None,
            timeout_ms: None,
            as_race: true,
            race_timeout_ms: None,
            // Never redirect as the returned value must always match the hash.
            follow_redirects: false,
            all_live_actions_with_metadata: false,
            // Redundant with retrieve_entry internals.
            request_type: GetRequest::Pending,
        }
    }
}

impl From<holochain_zome_types::entry::GetOptions> for GetOptions {
    fn from(_: holochain_zome_types::entry::GetOptions) -> Self {
        Self::default()
    }
}

/// Get metadata from the DHT.
/// Fields tagged with ```[Network]``` are network-level controls.
/// Fields tagged with ```[Remote]``` are controls that will be forwarded to the
/// remote agent processing this `GetLinks` request.
#[derive(Clone, Debug)]
pub struct GetMetaOptions {
    /// ```[Network]```
    /// How many remote nodes should we make requests of / aggregate.
    /// Set to `None` for a default "best-effort".
    pub remote_agent_count: Option<u8>,

    /// ```[Network]```
    /// Timeout to await responses for aggregation.
    /// Set to `None` for a default "best-effort".
    /// Note - if all requests time-out you will receive an empty result,
    /// not a timeout error.
    pub timeout_ms: Option<u64>,

    /// ```[Network]```
    /// We are interested in speed. If `true` and we have any results
    /// when `race_timeout_ms` is expired, those results will be returned.
    /// After `race_timeout_ms` and before `timeout_ms` the first result
    /// received will be returned.
    pub as_race: bool,

    /// ```[Network]```
    /// See `as_race` for details.
    /// Set to `None` for a default "best-effort" race.
    pub race_timeout_ms: Option<u64>,

    /// ```[Remote]```
    /// Tells the remote-end which metadata to return
    pub metadata_request: MetadataRequest,
}

impl Default for GetMetaOptions {
    fn default() -> Self {
        Self {
            remote_agent_count: None,
            timeout_ms: None,
            as_race: true,
            race_timeout_ms: None,
            metadata_request: MetadataRequest::default(),
        }
    }
}

#[derive(Debug, Clone, Default)]
/// Get links from the DHT.
/// Fields tagged with ```[Network]``` are network-level controls.
/// Fields tagged with ```[Remote]``` are controls that will be forwarded to the
/// remote agent processing this `GetLinks` request.
pub struct GetLinksOptions {
    /// ```[Network]```
    /// Timeout to await responses for aggregation.
    /// Set to `None` for a default "best-effort".
    /// Note - if all requests time-out you will receive an empty result,
    /// not a timeout error.
    pub timeout_ms: Option<u64>,
    /// Whether to fetch links from the network or return only
    /// locally available links. Defaults to fetching links from network.
    pub get_options: holochain_zome_types::entry::GetOptions,
}

#[derive(Debug, Clone)]
/// Get agent activity from the DHT.
/// Fields tagged with ```[Network]``` are network-level controls.
/// Fields tagged with ```[Remote]``` are controls that will be forwarded to the
/// remote agent processing this `GetLinks` request.
pub struct GetActivityOptions {
    /// ```[Network]```
    /// Timeout to await responses for aggregation.
    /// Set to `None` for a default "best-effort".
    /// Note - if all requests time-out you will receive an empty result,
    /// not a timeout error.
    pub timeout_ms: Option<u64>,
    /// Number of times to retry getting records in parallel.
    /// For a small dht a large parallel get can overwhelm a single
    /// agent and it can be worth retrying the records that didn't
    /// get found.
    pub retry_gets: u8,
    /// ```[Remote]```
    /// Include the all valid activity actions in the response.
    /// If this is false the call becomes a lightweight response with
    /// just the chain status and highest observed action.
    /// This is useful when you want to ask an authority about the
    /// status of a chain but do not need all the actions.
    pub include_valid_activity: bool,
    /// Include any rejected actions in the response.
    pub include_rejected_activity: bool,
    /// Include warrants for this agent
    pub include_warrants: bool,
    /// Include the full signed records in the response, instead of just the hashes.
    pub include_full_records: bool,
    /// Configure how the data should be fetched.
    pub get_options: holochain_zome_types::entry::GetOptions,
}

impl Default for GetActivityOptions {
    fn default() -> Self {
        Self {
            timeout_ms: None,
            retry_gets: 0,
            include_valid_activity: true,
            include_rejected_activity: false,
            include_warrants: true,
            include_full_records: false,
            get_options: Default::default(),
        }
    }
}

type MaybeDnaHash = Option<DnaHash>;

ghost_actor::ghost_chan! {
    /// The HolochainP2pSender struct allows controlling the HolochainP2p
    /// actor instance.
    pub chan HolochainP2p<HolochainP2pError> {
        /// The p2p module must be informed at runtime which dna/agent pairs it should be tracking.
        fn join(dna_hash: DnaHash, agent_pub_key: AgentPubKey, maybe_agent_info: Option<AgentInfoSigned>, initial_arq: Option<crate::dht::Arq>) -> ();

        /// If a cell is disabled, we'll need to \"leave\" the network module as well.
        fn leave(dna_hash: DnaHash, agent_pub_key: AgentPubKey) -> ();

        /// Invoke a zome function on a remote node (if you have been granted the capability).
        fn call_remote(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            zome_call_params_serialized: ExternIO,
            signature: Signature,
        ) -> SerializedBytes;

        /// Invoke a zome function on a remote node (if you have been granted the capability).
        /// This is a fire-and-forget operation, a best effort will be made
        /// to forward the signal, but if the conductor network is overworked
        /// it may decide not to deliver some of the signals.
        fn send_remote_signal(
            dna_hash: DnaHash,
            to_agent_list: Vec<(AgentPubKey, ExternIO, Signature)>,
        ) -> ();

        /// Publish data to the correct neighborhood.
        fn publish(
            dna_hash: DnaHash,
            request_validation_receipt: bool,
            countersigning_session: bool,
            basis_hash: holo_hash::OpBasis,
            source: AgentPubKey,
            op_hash_list: Vec<OpHashSized>,
            timeout_ms: Option<u64>,
            reflect_ops: Option<Vec<DhtOp>>,
        ) -> ();

        /// Publish a countersigning op.
        fn publish_countersign(
            dna_hash: DnaHash,
            flag: bool,
            basis_hash: holo_hash::OpBasis,
            op: DhtOp,
        ) -> ();

        /// Get an entry from the DHT.
        fn get(
            dna_hash: DnaHash,
            dht_hash: holo_hash::AnyDhtHash,
            options: GetOptions,
        ) -> Vec<WireOps>;

        /// Get metadata from the DHT.
        fn get_meta(
            dna_hash: DnaHash,
            dht_hash: holo_hash::AnyDhtHash,
            options: GetMetaOptions,
        ) -> Vec<MetadataSet>;

        /// Get links from the DHT.
        fn get_links(
            dna_hash: DnaHash,
            link_key: WireLinkKey,
            options: GetLinksOptions,
        ) -> Vec<WireLinkOps>;

        /// Get a count of links from the DHT.
        fn count_links(
            dna_hash: DnaHash,
            query: WireLinkQuery,
        ) -> CountLinksResponse;

        /// Get agent activity from the DHT.
        fn get_agent_activity(
            dna_hash: DnaHash,
            agent: AgentPubKey,
            query: ChainQueryFilter,
            options: GetActivityOptions,
        ) -> Vec<AgentActivityResponse>;

        /// A remote node is requesting agent activity from us.
        fn must_get_agent_activity(
            dna_hash: DnaHash,
            author: AgentPubKey,
            filter: holochain_zome_types::chain::ChainFilter,
        ) -> Vec<MustGetAgentActivityResponse>;

        /// Send a validation receipt to a remote node.
        fn send_validation_receipts(dna_hash: DnaHash, to_agent: AgentPubKey, receipts: ValidationReceiptBundle) -> ();

        /// New data has been integrated and is ready for gossiping.
        fn new_integrated_data(dna_hash: DnaHash) -> ();

        /// Check if any local agent in this space is an authority for a hash.
        fn authority_for_hash(dna_hash: DnaHash, basis: OpBasis) -> bool;

        /// Messages between agents negotiation a countersigning session.
        fn countersigning_session_negotiation(
            dna_hash: DnaHash,
            agents: Vec<AgentPubKey>,
            message: event::CountersigningSessionNegotiationMessage,
        ) -> ();

        /// Dump network metrics.
        fn dump_network_metrics(
            dna_hash: MaybeDnaHash,
        ) -> String;

        /// Dump network stats.
        fn dump_network_stats() -> String;

        /// Get struct for diagnostic data
        fn get_diagnostics(dna_hash: DnaHash) -> KitsuneDiagnostics;

        /// Get the storage arcs of the agents currently in this space.
        fn storage_arcs(dna_hash: DnaHash) -> Vec<kitsune2_api::DhtArc>;
    }
}

/// Convenience type for referring to the HolochainP2p GhostSender
pub type HolochainP2pRef = ghost_actor::GhostSender<HolochainP2p>;

/// Extension trait for converting `GhostSender<HolochainP2p>` into HolochainP2pDna
pub trait HolochainP2pRefToDna {
    /// Partially apply dna_hash && agent_pub_key to this sender,
    /// binding it to a specific dna context.
    fn into_dna(self, dna_hash: DnaHash, chc: Option<ChcImpl>) -> crate::HolochainP2pDna;

    /// Clone and partially apply dna_hash && agent_pub_key to this sender,
    /// binding it to a specific dna context.
    fn to_dna(&self, dna_hash: DnaHash, chc: Option<ChcImpl>) -> crate::HolochainP2pDna;
}

impl HolochainP2pRefToDna for HolochainP2pRef {
    fn into_dna(self, dna_hash: DnaHash, chc: Option<ChcImpl>) -> crate::HolochainP2pDna {
        crate::HolochainP2pDna {
            sender: self,
            dna_hash: Arc::new(dna_hash),
            chc,
        }
    }

    fn to_dna(&self, dna_hash: DnaHash, chc: Option<ChcImpl>) -> crate::HolochainP2pDna {
        self.clone().into_dna(dna_hash, chc)
    }
}



================================================
File: crates/holochain_p2p/src/types/event.rs
================================================
#![allow(clippy::too_many_arguments)]
//! Module containing incoming events from the HolochainP2p actor.

use crate::*;
use holochain_zome_types::signature::Signature;
use kitsune_p2p::{agent_store::AgentInfoSigned, dht::region::RegionBounds, event::*};

#[derive(Debug, serde::Serialize, serde::Deserialize, Clone)]
/// The data required for a get request.
#[derive(Default)]
pub enum GetRequest {
    /// Get all the integrated data.
    #[default]
    All,
    /// Get only the integrated content.
    Content,
    /// Get only the metadata.
    /// If you already have the content this is all you need.
    Metadata,
    /// Get the content even if it's still pending.
    Pending,
}

/// Get options help control how the get is processed at various levels.
#[derive(Debug, serde::Serialize, serde::Deserialize, Clone)]
pub struct GetOptions {
    /// Whether the remote-end should follow redirects or just return the
    /// requested entry.
    pub follow_redirects: bool,
    /// Return all live actions even if there is deletes.
    /// Useful for metadata calls.
    pub all_live_actions_with_metadata: bool,
    /// The type of data this get request requires.
    pub request_type: GetRequest,
}

impl From<&actor::GetOptions> for GetOptions {
    fn from(a: &actor::GetOptions) -> Self {
        Self {
            follow_redirects: a.follow_redirects,
            all_live_actions_with_metadata: a.all_live_actions_with_metadata,
            request_type: a.request_type.clone(),
        }
    }
}

/// GetMeta options help control how the get is processed at various levels.
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub struct GetMetaOptions {}

impl From<&actor::GetMetaOptions> for GetMetaOptions {
    fn from(_a: &actor::GetMetaOptions) -> Self {
        Self {}
    }
}

/// GetLinks options help control how the get is processed at various levels.
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub struct GetLinksOptions {}

impl From<&actor::GetLinksOptions> for GetLinksOptions {
    fn from(_a: &actor::GetLinksOptions) -> Self {
        Self {}
    }
}

/// Get agent activity options help control how the get is processed at various levels.
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct GetActivityOptions {
    /// Include the activity actions in the response
    pub include_valid_activity: bool,
    /// Include any rejected actions in the response.
    pub include_rejected_activity: bool,
    /// Include warrants in the response.
    pub include_warrants: bool,
    /// Include the full records, instead of just the hashes.
    pub include_full_records: bool,
}

impl Default for GetActivityOptions {
    fn default() -> Self {
        Self {
            include_valid_activity: true,
            include_warrants: true,
            include_rejected_activity: false,
            include_full_records: false,
        }
    }
}

impl From<&actor::GetActivityOptions> for GetActivityOptions {
    fn from(a: &actor::GetActivityOptions) -> Self {
        Self {
            include_valid_activity: a.include_valid_activity,
            include_warrants: a.include_warrants,
            include_rejected_activity: a.include_rejected_activity,
            include_full_records: a.include_full_records,
        }
    }
}

/// Message between agents actively driving/negotiating a countersigning session.
#[derive(Debug, serde::Serialize, serde::Deserialize)]
pub enum CountersigningSessionNegotiationMessage {
    /// An authority has a complete set of signed actions and is responding with
    /// them back to the counterparties.
    AuthorityResponse(Vec<SignedAction>),
    /// Counterparties are sending their signed action to an enzyme instead of
    /// authorities as part of an enzymatic session.
    EnzymePush(Box<ChainOp>),
}

/// Multiple ways to fetch op data
#[derive(Debug, derive_more::From)]
pub enum FetchOpDataQuery {
    /// Fetch all ops with the hashes specified
    Hashes {
        /// list of ops to fetch
        op_hash_list: Vec<holo_hash::DhtOpHash>,

        /// should we include limbo ops
        include_limbo: bool,
    },

    /// Fetch all ops within the time and space bounds specified
    Regions(Vec<RegionBounds>),
}

impl FetchOpDataQuery {
    /// Convert from the kitsune form of this query
    pub fn from_kitsune(kit: FetchOpDataEvtQuery) -> Self {
        match kit {
            FetchOpDataEvtQuery::Hashes {
                op_hash_list,
                include_limbo,
            } => Self::Hashes {
                op_hash_list: op_hash_list
                    .into_iter()
                    .map(|h| DhtOpHash::from_kitsune(&h))
                    .collect::<Vec<_>>(),
                include_limbo,
            },
            FetchOpDataEvtQuery::Regions(coords) => Self::Regions(coords),
        }
    }
}

ghost_actor::ghost_chan! {
    /// The HolochainP2pEvent stream allows handling events generated from
    /// the HolochainP2p actor.
    pub chan HolochainP2pEvent<super::HolochainP2pError> {

        /// We need to store signed agent info.
        fn put_agent_info_signed(dna_hash: DnaHash, peer_data: Vec<AgentInfoSigned>) -> Vec<kitsune_p2p_types::bootstrap::AgentInfoPut>;

        /// We need to get previously stored agent info.
        /// The optional `agents` parameter is an include filter. This can be thought of as a way to filter a held list of agents against the current state of the store.
        fn query_agent_info_signed(dna_hash: DnaHash, agents: Option<std::collections::HashSet<Arc<kitsune_p2p::KitsuneAgent>>>, kitsune_space: Arc<kitsune_p2p::KitsuneSpace>) -> Vec<AgentInfoSigned>;

        /// We need to get agents that fit into an arc set for gossip.
        fn query_gossip_agents(
            dna_hash: DnaHash,
            agents: Option<Vec<AgentPubKey>>,
            kitsune_space: Arc<kitsune_p2p::KitsuneSpace>,
            since_ms: u64,
            until_ms: u64,
            arc_set: Arc<kitsune_p2p_types::dht_arc::DhtArcSet>,
        ) -> Vec<AgentInfoSigned>;

        /// query agent info in order of closeness to a basis location.
        fn query_agent_info_signed_near_basis(dna_hash: DnaHash, kitsune_space: Arc<kitsune_p2p::KitsuneSpace>, basis_loc: u32, limit: u32) -> Vec<AgentInfoSigned>;

        /// Query the peer density of a space for a given [`DhtArc`].
        fn query_peer_density(dna_hash: DnaHash, kitsune_space: Arc<kitsune_p2p::KitsuneSpace>, dht_arc: kitsune_p2p_types::dht_arc::DhtArc) -> kitsune_p2p_types::dht::PeerView;

        /// A remote node is attempting to make a remote call on us.
        fn call_remote(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            zome_call_params_serialized: ExternIO,
            signature: Signature,
        ) -> SerializedBytes;

        /// A remote node is publishing data in a range we claim to be holding.
        fn publish(
            dna_hash: DnaHash,
            request_validation_receipt: bool,
            countersigning_session: bool,
            ops: Vec<holochain_types::dht_op::DhtOp>,
        ) -> ();

        /// A remote node is requesting entry data from us.
        fn get(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            dht_hash: holo_hash::AnyDhtHash,
            options: GetOptions,
        ) -> WireOps;

        /// A remote node is requesting metadata from us.
        fn get_meta(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            dht_hash: holo_hash::AnyDhtHash,
            options: GetMetaOptions,
        ) -> MetadataSet;

        /// A remote node is requesting link data from us.
        fn get_links(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            link_key: WireLinkKey,
            options: GetLinksOptions,
        ) -> WireLinkOps;

        /// A remote node is requesting a link count from us.
        fn count_links(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            query: WireLinkQuery,
        ) -> CountLinksResponse;

        /// A remote node is requesting agent activity from us.
        fn get_agent_activity(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            agent: AgentPubKey,
            query: ChainQueryFilter,
            options: GetActivityOptions,
        ) -> AgentActivityResponse;

        /// A remote node is requesting agent activity from us.
        fn must_get_agent_activity(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            author: AgentPubKey,
            filter: holochain_zome_types::chain::ChainFilter,
        ) -> MustGetAgentActivityResponse;

        /// A remote node has sent us a validation receipt.
        fn validation_receipts_received(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            receipts: ValidationReceiptBundle,
        ) -> ();

        /// The p2p module wishes to query our DhtOpHash store.
        /// Gets all ops from a set of agents within a time window
        /// and max number of ops.
        /// Returns the actual time window of returned ops as well.
        fn query_op_hashes(
            dna_hash: DnaHash,
            arc_set: kitsune_p2p::dht_arc::DhtArcSet,
            window: TimeWindow,
            max_ops: usize,
            include_limbo: bool,
        ) -> Option<(Vec<holo_hash::DhtOpHash>, TimeWindowInclusive)>;

        /// The p2p module needs access to the content for a given set of DhtOpHashes.
        fn fetch_op_data(
            dna_hash: DnaHash,
            query: FetchOpDataQuery,
        ) -> Vec<(holo_hash::DhtOpHash, holochain_types::dht_op::DhtOp)>;

        /// P2p operations require cryptographic signatures and validation.
        fn sign_network_data(
            // The dna_hash / space_hash context.
            dna_hash: DnaHash,
            // The agent_id / agent_pub_key context.
            to_agent: AgentPubKey,
            // The data to sign.
            data: Vec<u8>,
        ) -> Signature;

        /// Messages between agents that drive a countersigning session.
        fn countersigning_session_negotiation(
            dna_hash: DnaHash,
            to_agent: AgentPubKey,
            message: CountersigningSessionNegotiationMessage,
        ) -> ();
    }
}

/// utility macro to make it more ergonomic to access the enum variants
macro_rules! match_p2p_evt {
    ($h:ident => |$i:ident| { $($t:tt)* }, { $($t2:tt)* }) => {
        match $h {
            HolochainP2pEvent::CallRemote { $i, .. } => { $($t)* }
            HolochainP2pEvent::Get { $i, .. } => { $($t)* }
            HolochainP2pEvent::GetMeta { $i, .. } => { $($t)* }
            HolochainP2pEvent::GetLinks { $i, .. } => { $($t)* }
            HolochainP2pEvent::CountLinks { $i, .. } => { $($t)* }
            HolochainP2pEvent::GetAgentActivity { $i, .. } => { $($t)* }
            HolochainP2pEvent::MustGetAgentActivity { $i, .. } => { $($t)* }
            HolochainP2pEvent::ValidationReceiptsReceived { $i, .. } => { $($t)* }
            HolochainP2pEvent::SignNetworkData { $i, .. } => { $($t)* }
            HolochainP2pEvent::CountersigningSessionNegotiation { $i, .. } => { $($t)* }
            $($t2)*
        }
    };
}

impl HolochainP2pEvent {
    /// The dna_hash associated with this network p2p event.
    pub fn dna_hash(&self) -> &DnaHash {
        match_p2p_evt!(self => |dna_hash| { dna_hash }, {
            HolochainP2pEvent::Publish { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::FetchOpData { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::QueryOpHashes { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::QueryAgentInfoSigned { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::QueryAgentInfoSignedNearBasis { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::QueryGossipAgents { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::PutAgentInfoSigned { dna_hash, .. } => { dna_hash }
            HolochainP2pEvent::QueryPeerDensity { dna_hash, .. } => { dna_hash }
        })
    }

    /// The agent_pub_key associated with this network p2p event.
    pub fn target_agents(&self) -> &AgentPubKey {
        match_p2p_evt!(self => |to_agent| { to_agent }, {
            HolochainP2pEvent::Publish { .. } => { unimplemented!("There is no single agent target for Publish") }
            HolochainP2pEvent::FetchOpData { .. } => { unimplemented!("There is no single agent target for FetchOpData") }
            HolochainP2pEvent::QueryOpHashes { .. } => { unimplemented!("There is no single agent target for QueryOpHashes") }
            HolochainP2pEvent::QueryAgentInfoSigned { .. } => { unimplemented!("There is no single agent target for QueryAgentInfoSigned") },
            HolochainP2pEvent::QueryAgentInfoSignedNearBasis { .. } => { unimplemented!("There is no single agent target for QueryAgentInfoSignedNearBasis") },
            HolochainP2pEvent::QueryGossipAgents { .. } => { unimplemented!("There is no single agent target for QueryGossipAgents") },
            HolochainP2pEvent::PutAgentInfoSigned { .. } => { unimplemented!("There is no single agent target for PutAgentInfoSigned") },
            HolochainP2pEvent::QueryPeerDensity { .. } => { unimplemented!() },
        })
    }
}

/// Receiver type for incoming holochain p2p events.
pub type HolochainP2pEventReceiver = futures::channel::mpsc::Receiver<HolochainP2pEvent>;



================================================
File: crates/holochain_p2p/src/types/wire.rs
================================================
use crate::*;

#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
/// Struct for encoding DhtOp as bytes.
pub struct WireDhtOpData {
    /// The dht op.
    pub op_data: holochain_types::dht_op::DhtOp,
}

impl WireDhtOpData {
    /// Encode as bytes.
    pub fn encode(self) -> Result<Vec<u8>, SerializedBytesError> {
        Ok(UnsafeBytes::from(SerializedBytes::try_from(self)?).into())
    }

    /// Decode from bytes.
    pub fn decode(data: Vec<u8>) -> Result<Self, SerializedBytesError> {
        let request: SerializedBytes = UnsafeBytes::from(data).into();
        request.try_into()
    }
}

#[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
#[serde(tag = "type", content = "content")]
#[allow(missing_docs)]
pub enum WireMessage {
    CallRemote {
        to_agent: AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    },
    CallRemoteMulti {
        to_agents: Vec<(holo_hash::AgentPubKey, ExternIO, Signature)>,
    },
    ValidationReceipts {
        receipts: ValidationReceiptBundle,
    },
    Get {
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetOptions,
    },
    GetMeta {
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetMetaOptions,
    },
    GetLinks {
        link_key: WireLinkKey,
        options: event::GetLinksOptions,
    },
    CountLinks {
        query: WireLinkQuery,
    },
    GetAgentActivity {
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: event::GetActivityOptions,
    },
    MustGetAgentActivity {
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    },
    CountersigningSessionNegotiation {
        message: event::CountersigningSessionNegotiationMessage,
    },
    PublishCountersign {
        flag: bool,
        op: DhtOp,
    },
}

#[allow(missing_docs)]
impl WireMessage {
    pub fn encode(&self) -> Result<Vec<u8>, SerializedBytesError> {
        holochain_serialized_bytes::encode(&self)
    }

    pub fn decode(data: &[u8]) -> Result<Self, SerializedBytesError> {
        holochain_serialized_bytes::decode(&data)
    }

    pub fn publish_countersign(flag: bool, op: DhtOp) -> WireMessage {
        Self::PublishCountersign { flag, op }
    }

    /// For an outgoing remote call.
    pub fn call_remote(
        to_agent: holo_hash::AgentPubKey,
        zome_call_params_serialized: ExternIO,
        signature: Signature,
    ) -> WireMessage {
        Self::CallRemote {
            to_agent,
            zome_call_params_serialized,
            signature,
        }
    }

    #[allow(clippy::too_many_arguments)]
    pub fn call_remote_multi(
        to_agents: Vec<(holo_hash::AgentPubKey, ExternIO, Signature)>,
    ) -> WireMessage {
        Self::CallRemoteMulti { to_agents }
    }

    pub fn validation_receipts(receipts: ValidationReceiptBundle) -> WireMessage {
        Self::ValidationReceipts { receipts }
    }

    pub fn get(dht_hash: holo_hash::AnyDhtHash, options: event::GetOptions) -> WireMessage {
        Self::Get { dht_hash, options }
    }

    pub fn get_meta(
        dht_hash: holo_hash::AnyDhtHash,
        options: event::GetMetaOptions,
    ) -> WireMessage {
        Self::GetMeta { dht_hash, options }
    }

    pub fn get_links(link_key: WireLinkKey, options: event::GetLinksOptions) -> WireMessage {
        Self::GetLinks { link_key, options }
    }

    pub fn count_links(query: WireLinkQuery) -> WireMessage {
        Self::CountLinks { query }
    }

    pub fn get_agent_activity(
        agent: AgentPubKey,
        query: ChainQueryFilter,
        options: event::GetActivityOptions,
    ) -> WireMessage {
        Self::GetAgentActivity {
            agent,
            query,
            options,
        }
    }

    pub fn must_get_agent_activity(
        agent: AgentPubKey,
        filter: holochain_zome_types::chain::ChainFilter,
    ) -> WireMessage {
        Self::MustGetAgentActivity { agent, filter }
    }

    pub fn countersigning_session_negotiation(
        message: event::CountersigningSessionNegotiationMessage,
    ) -> WireMessage {
        Self::CountersigningSessionNegotiation { message }
    }
}



================================================
File: crates/holochain_p2p/tests/peer_meta_store.rs
================================================
use bytes::Bytes;
use holochain_p2p::HolochainPeerMetaStore;
use holochain_sqlite::db::{DbKindPeerMetaStore, DbWrite, ReadAccess};
use holochain_sqlite::error::DatabaseResult;
use kitsune2_api::{PeerMetaStore, SpaceId, Timestamp, Url};
use std::sync::Arc;

#[tokio::test]
async fn peer_meta_crd() {
    let db = DbWrite::test_in_mem(DbKindPeerMetaStore(Arc::new(SpaceId::from(
        Bytes::from_static("test".as_bytes()),
    ))))
    .unwrap();

    let store = HolochainPeerMetaStore::create(db).await.unwrap();

    let peer_url = Url::from_str("ws://test:80/1").unwrap();
    let key = "test".to_string();

    store
        .put(
            peer_url.clone(),
            key.clone(),
            Bytes::from_static("test".as_bytes()),
            None,
        )
        .await
        .unwrap();

    let value = store.get(peer_url.clone(), key.clone()).await.unwrap();

    assert!(value.is_some());
    assert_eq!(Bytes::from_static("test".as_bytes()), value.unwrap());

    store.delete(peer_url.clone(), key.clone()).await.unwrap();

    let value = store.get(peer_url, key).await.unwrap();

    assert!(value.is_none());
}

#[tokio::test]
async fn prune_on_create() {
    let db = DbWrite::test_in_mem(DbKindPeerMetaStore(Arc::new(SpaceId::from(
        Bytes::from_static("test".as_bytes()),
    ))))
    .unwrap();

    {
        let store = HolochainPeerMetaStore::create(db.clone()).await.unwrap();

        let peer_url = Url::from_str("ws://test:80/1").unwrap();
        let key = "test".to_string();

        store
            .put(
                peer_url,
                key,
                Bytes::from_static("test".as_bytes()),
                Some(Timestamp::from_micros(0)),
            )
            .await
            .unwrap();

        let count = db
            .read_async(|txn| -> DatabaseResult<u32> {
                let count = txn.query_row("SELECT COUNT(*) FROM peer_meta", [], |row| {
                    row.get::<_, u32>(0)
                })?;
                Ok(count)
            })
            .await
            .unwrap();

        assert_eq!(1, count);
    }

    // Setting up a new store should clear expired values
    HolochainPeerMetaStore::create(db.clone()).await.unwrap();

    let count = db
        .read_async(|txn| -> DatabaseResult<u32> {
            let count = txn.query_row("SELECT COUNT(*) FROM peer_meta", [], |row| {
                row.get::<_, u32>(0)
            })?;
            Ok(count)
        })
        .await
        .unwrap();

    assert_eq!(0, count);
}



================================================
File: crates/holochain_secure_primitive/README.md
================================================
# holochain_secure_primitive

License: CAL-1.0



================================================
File: crates/holochain_secure_primitive/Cargo.toml
================================================
[package]
name = "holochain_secure_primitive"
description = "Crate for the secure primitive macros"
version = "0.5.0-dev.1"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_secure_primitive"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
serde = { version = "1.0", features = ["derive"] }
paste = "1.0"

# TODO: Figure out if we can keep this dependency behind a feature flag,
#       or limit it to test-only code, to reduce Wasm code bloat
subtle = "2"

[lints]
workspace = true



================================================
File: crates/holochain_secure_primitive/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.1

- Prevent TODO comments from being rendered in cargo docs.

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0-dev.0

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

- Change the license from CAL-1.0 to Apache-2.0.

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

- New package to extract secure primitive macros from `holochain_integrity_types`



================================================
File: crates/holochain_secure_primitive/src/lib.rs
================================================
pub use paste;
pub use serde;
pub use subtle;

mod types;
pub use types::*;

#[macro_export]
/// Serialization for fixed arrays is generally not available in a way that can be derived.
/// Being able to wrap fixed size arrays is important e.g. for crypto safety etc. so this is a
/// simple way to implement serialization so that we can send these types between the host/guest.
macro_rules! fixed_array_serialization {
    ($t:ty, $len:expr) => {
        $crate::paste::paste! {
            impl $crate::serde::ser::Serialize for $t {
                fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
                where
                    S: $crate::serde::ser::Serializer,
                {
                    serializer.serialize_bytes(&self.0)
                }
            }

            struct [<Visitor$t>];

            impl<'de> $crate::serde::de::Visitor<'de> for [<Visitor$t>] {
                type Value = [u8; $len];

                fn expecting(&self, formatter: &mut std::fmt::Formatter) -> std::fmt::Result {
                    formatter.write_str(format!("a byte array of length {}", $len).as_str())
                }

                fn visit_bytes<E>(self, value: &[u8]) -> Result<Self::Value, E>
                where
                    E: $crate::serde::de::Error,
                {
                    if value.len() == $len {
                        let mut bytes = [0 as u8; $len];
                        bytes.clone_from_slice(value);
                        Ok(bytes)
                    } else {
                        let error_message = format!("{} bytes, got {} bytes", $len, value.len());
                        Err(E::invalid_value(
                            $crate::serde::de::Unexpected::Bytes(value),
                            &error_message.as_str(),
                        ))
                    }
                }

                fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>
                where
                    A: $crate::serde::de::SeqAccess<'de>,
                {
                    let mut vec = Vec::with_capacity(seq.size_hint().unwrap_or(0));

                    while let Some(b) = seq.next_element()? {
                        vec.push(b);
                    }

                    self.visit_bytes(&vec)
                }
            }

            impl<'de> $crate::serde::de::Deserialize<'de> for $t {
                fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
                where
                    D: $crate::serde::de::Deserializer<'de>,
                {
                    let bytes = deserializer.deserialize_bytes([<Visitor$t>])?;
                    Ok(Self(bytes))
                }
            }
        }
    };
}

#[macro_export]
/// Cryptographic secrets are fiddly at the best of times.
///
/// In wasm it is somewhat impossible to have true secrets because wasm memory is not secure.
///
///  - The host can always read wasm memory so any vulnerability in the host compromises the guest.
///  - The host/rust generally doesn't guarantee to immediately wipe/zero out freed memory, either
///    when a zome call is running or after a wasm instance is thrown away.
///
/// Most of the time we should just try to minimise the interaction between wasm and secret data.
///
/// For example, lair keeps all our private keys internal and we can only send it signing requests
/// associated with public keys.
///
/// In other contexts it is more difficult, such as when generating secrets from raw cryptographic
/// random bytes and sending them to peers directly.
///
/// The best we can do here is try to protect ourselves against third parties across the network.
/// e.g. We don't want other machines to simply `remote_call` a successful timing attack.
///
/// MITM attacks are mitigated by the networking implementation itself.
///
// @todo given how impossible it is for wasm to protect its memory from the host, it would make
// more sense to:
//
//  - use key exchange protocols like libsodium kx <https://libsodium.gitbook.io/doc/key_exchange>.
//  - keep secrets inside lair with all algorithms behind an API, wasm only has access to opaque
//    references to the secret data.
//
// @todo implement explicit zeroing, moving and copying of memory for sensitive data.
//       - e.g. the secrecy crate <https://crates.io/crates/secrecy>
macro_rules! secure_primitive {
    ($t:ty, $len:expr) => {
        $crate::fixed_array_serialization!($t, $len);

        /// Constant time equality check.
        /// This mitigates timing attacks where a remote agent can reverse engineer data by
        /// measuring tiny changes in latency associated with optimised equality checks.
        /// More matching bytes = more latency = vulnerability.
        /// This type of attack has been successfully demonstrated over a network despite varied latencies.
        impl PartialEq for $t {
            fn eq(&self, other: &Self) -> bool {
                use $crate::subtle::ConstantTimeEq;
                self.0.ct_eq(&other.0).into()
            }
        }

        impl Eq for $t {}

        /// The only meaningful debug information for a cryptograhpic secret is the literal bytes.
        /// Also, encodings like base64 are not constant time so debugging could open some weird
        /// side channel issue trying to be 'human friendly'.
        /// It seems better to never try to encode secrets.
        ///
        /// Note that when using this crate with feature "subtle-encoding", a hex
        /// representation will be used.
        //
        // @todo maybe we want something like **HIDDEN** by default and putting the actual bytes
        //       behind a feature flag?
        #[cfg(not(feature = "subtle-encoding"))]
        impl std::fmt::Debug for $t {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                std::fmt::Debug::fmt(&self.0.to_vec(), f)
            }
        }

        #[cfg(feature = "subtle-encoding")]
        impl std::fmt::Debug for $t {
            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                let str = String::from_utf8(subtle_encoding::hex::encode(self.0.to_vec()))
                    .unwrap_or_else(|_| "<unparseable signature>".into());
                f.write_str(&str)
            }
        }

        /// Trivial new type derivation.
        /// Secrets should have private interiors and be constructed directly from fixed length
        /// arrays of known length.
        impl From<[u8; $len]> for $t {
            fn from(b: [u8; $len]) -> Self {
                Self(b)
            }
        }

        impl core::convert::TryFrom<&[u8]> for $t {
            type Error = $crate::SecurePrimitiveError;
            fn try_from(slice: &[u8]) -> Result<Self, Self::Error> {
                if slice.len() == $len {
                    let mut inner = [0; $len];
                    inner.copy_from_slice(slice);
                    Ok(inner.into())
                } else {
                    Err($crate::SecurePrimitiveError::BadSize)
                }
            }
        }

        impl core::convert::TryFrom<Vec<u8>> for $t {
            type Error = $crate::SecurePrimitiveError;
            fn try_from(v: Vec<u8>) -> Result<Self, Self::Error> {
                Self::try_from(v.as_ref())
            }
        }

        impl AsRef<[u8]> for $t {
            fn as_ref(&self) -> &[u8] {
                &self.0
            }
        }
    };
}



================================================
File: crates/holochain_secure_primitive/src/types.rs
================================================
/// Errors related to the secure primitive macro.
#[derive(Debug)]
pub enum SecurePrimitiveError {
    /// We have the wrong number of bytes.
    BadSize,
}
impl std::error::Error for SecurePrimitiveError {}
impl core::fmt::Display for SecurePrimitiveError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            SecurePrimitiveError::BadSize => write!(f, "Bad sized secure primitive."),
        }
    }
}



================================================
File: crates/holochain_sqlite/README.md
================================================
# holochain_sqlite

[![Project](https://img.shields.io/badge/project-holochain-blue.svg?style=flat-square)](http://holochain.org/)
[![Forum](https://img.shields.io/badge/chat-forum%2eholochain%2enet-blue.svg?style=flat-square)](https://forum.holochain.org)
[![Chat](https://img.shields.io/badge/chat-chat%2eholochain%2enet-blue.svg?style=flat-square)](https://chat.holochain.org)

[![Twitter Follow](https://img.shields.io/twitter/follow/holochain.svg?style=social&label=Follow)](https://twitter.com/holochain)
License: [![License: CAL 1.0](https://img.shields.io/badge/License-CAL%201.0-blue.svg)](https://github.com/holochain/cryptographic-autonomy-license)

Current version: 0.0.1

## Building blocks for persisted Holochain state

### History

Originally, this crate was written to target LMDB. After it had already stabilized, we completely refactored it to naively use SQLite as a key-value store, instead. This is in preparation for using more intelligently and fully, using carefully chosen indexes and queries. However, for now, the structure of this crate can only be understood in the context of this major recent refactor.

### Backend: SQLite

Persistence is not generalized for different backends: it is targeted specifically for SQLite. In the future, if we have to change backends (again), or if we have to support something like IndexedDb, we will generalize the interface just enough to cover both.

### Buffered Stores

The unit of persisted Holochain state is the [BufferedStore]. This interface groups three things together:

- A reference to a SQLite database
- A reference to a read-only transaction (shared by other stores)
- A "scratch space", which is a HashMap into which write operations get staged (the buffer)

The purpose of the scratch space is to prevent the need for opening a read-write transaction, of which there can be only one at a time. With the buffer of the scratch space, store references can live for a more leisurely length of time, accumulating changes, and then the buffer can be flushed all at once in a short-lived read-write transaction.

Note that a BufferedStore includes a reference to a read-only transaction, which means that the store acts as a snapshot of the persisted data at the moment it was constructed. Changes to the underlying persistence will not be seen by this BufferedStore.

See the [buffer] crate for implementations.

#### Strong typing

All BufferedStores are strongly typed. All keys and values must be de/serializable, and so de/serialization happens automatically when getting and putting items into stores. As a consequence, the source chain CAS is split into two separate DBs: one for Entries, and one for Actions.

### Workspaces

The intention is that Holochain code never deals with individual data stores directly, individually. BufferedStores are always grouped into a Workspace, which is a collection of stores that's been put together for a specific purpose. A workspace may choose to provide open access to the underlying stores, or it may protect them behind a purpose-built interface.

The stores in a Workspace are all provided a common read-only transaction, so their snapshots are all consistent with each other at the moment in time the workspace was constructed. The workspace provides its own interface for interacting with the stores. Once changes have been accumulated in the BufferedStores, the Workspace itself can be committed, which uses a fresh read-write transaction to flush the changes from each store and commit them to disk. Committing consumes the Workspace.

Workspaces themselves are implemented in the `holochain` crate

### Building blocks

The `holochain_sqlite` crate provides three buffered KV store abstractions as well as a simple CAS abstraction:

- [KvBuf]: a normal KV store
- [KvIntBuf]: a KV store where keys must be integers (this was significant when using LMDB, but not any more)
- [KvvBuf]: a KV store with multiple values per key, with per-key iteration
- [CasBuf]: a [KvBuf] which enforces that keys must be the "address" of the values (content)

The `holochain` crate composes these building blocks together to build more purpose-specific BufferedStore implementations

See [this hackmd](https://holo.hackmd.io/@holochain/SkuVLpqEL) for a diagram explaining the relationships between these building blocks and the higher abstractions

## Contribute
Holochain is an open source project.  We welcome all sorts of participation and are actively working on increasing surface area to accept it.  Please see our [contributing guidelines](/CONTRIBUTING.md) for our general practices and protocols on participating in the community, as well as specific expectations around things like code formatting, testing practices, continuous integration, etc.

* Connect with us on our [forum](https://forum.holochain.org)

## License
 [![License: CAL 1.0](https://img.shields.io/badge/License-CAL-1.0-blue.svg)](https://github.com/holochain/cryptographic-autonomy-license)

Copyright (C) 2019 - 2024, Holochain Foundation

This program is free software: you can redistribute it and/or modify it under the terms of the license
provided in the LICENSE file (CAL-1.0).  This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE.



================================================
File: crates/holochain_sqlite/build.rs
================================================
#![allow(dead_code)]

use std::path::PathBuf;

/// The location of all our sql scrips
const SQL_DIR: &str = "./src/sql";

/// An env var that will trigger a SQL check.
const CHK_SQL_FMT: Option<&str> = option_env!("CHK_SQL_FMT");

/// An env var that will trigger a SQL format.
const FIX_SQL_FMT: Option<&str> = option_env!("FIX_SQL_FMT");

fn chk_sql_fmt() -> bool {
    if let Some(csf) = CHK_SQL_FMT {
        !csf.is_empty()
    } else {
        false
    }
}

fn fix_sql_fmt() -> bool {
    if let Some(fsf) = FIX_SQL_FMT {
        !fsf.is_empty()
    } else {
        false
    }
}

fn find_sql(path: &std::path::Path) -> Vec<std::path::PathBuf> {
    let mut out = Vec::new();
    for e in std::fs::read_dir(path)
        .unwrap_or_else(|e| panic!("Path doesn't exist: {:?}. Error: {}", path, e))
    {
        let e = e.unwrap();
        let path = e.path();
        let t = e.file_type().unwrap();
        if t.is_dir() {
            out.append(&mut find_sql(&path));
            continue;
        }
        if !t.is_file() {
            continue;
        }
        if path.extension() != Some(std::ffi::OsStr::new("sql")) {
            continue;
        }
        out.push(path);
    }
    out
}

const FIX_MSG: &str = "-- `FIX_SQL_FMT=1 cargo build` to fix --";
fn panic_on_diff(path: &std::path::Path, s1: &str, s2: &str) {
    let s1 = s1.split('\n').collect::<Vec<_>>();
    let s2 = s2.split('\n').collect::<Vec<_>>();
    pretty_assertions::assert_eq!((s1, path, FIX_MSG), (s2, path, FIX_MSG));
}

fn check_fmt(path: &std::path::Path) {
    let src_sql = std::fs::read_to_string(path).unwrap();
    let src_sql = src_sql.trim();

    if src_sql.contains("no-sql-format") {
        return;
    }

    let opt = sqlformat::FormatOptions {
        indent: sqlformat::Indent::Spaces(2),
        uppercase: false,
        lines_between_queries: 2,
    };

    let fmt_sql = sqlformat::format(src_sql, &sqlformat::QueryParams::None, opt);

    let fmt_sql = fmt_sql.trim();

    if fix_sql_fmt() {
        if src_sql != fmt_sql {
            std::fs::write(path, format!("{}\n", fmt_sql)).unwrap();
            println!(
                "cargo:warning=FIX_SQL_FMT--fixing: {}",
                path.to_string_lossy()
            );
        }
    } else if chk_sql_fmt() {
        panic_on_diff(path, src_sql, fmt_sql);
    }
}

fn _check_migrations() {
    let root = PathBuf::from(SQL_DIR);

    for dir in [
        root.join("cell/schema"),
        root.join("conductor/schema"),
        root.join("p2p_agent_store/schema"),
        root.join("p2p_metrics/schema"),
        root.join("wasm/schema"),
    ] {
        for _path in find_sql(&dir) {
            // TODO: ensure that each schema migration script not introduced "recently"
            // (for some value of "recently") has not changed. We don't ever
            // want these to change, we only want to add new ones.
            // Probably the best way to accomplish this is through a git commit hook or something.
        }
    }
}

fn main() {
    println!("cargo:rerun-if-env-changed=CHK_SQL_FMT");
    println!("cargo:rerun-if-env-changed=FIX_SQL_FMT");
    let all_sql = find_sql(std::path::Path::new(SQL_DIR));
    for sql in all_sql {
        println!("cargo:rerun-if-changed={}", sql.to_string_lossy());
        check_fmt(&sql);
    }
}



================================================
File: crates/holochain_sqlite/Cargo.toml
================================================
[package]
name = "holochain_sqlite"
version = "0.5.0-dev.19"
description = "Abstractions for persistence of Holochain state via SQLite"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/holochain_sqlite"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
async-trait = "0.1"
anyhow = "1.0"
base64 = "0.22"
derive_more = "0.99"
fallible-iterator = "0.3.0"
futures = "0.3"
holo_hash = { path = "../holo_hash", version = "^0.5.0-dev.7" }
holochain_serialized_bytes = "=0.0.55"
holochain_util = { version = "^0.5.0-dev.1", path = "../holochain_util", features = [
  "backtrace",
  "time",
], optional = true }
holochain_zome_types = { version = "^0.5.0-dev.17", path = "../holochain_zome_types" }
holochain_nonce = { version = "^0.5.0-dev.2", path = "../holochain_nonce" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../kitsune_p2p/bin_data" }
kitsune_p2p_dht_arc = { version = "^0.5.0-dev.2", path = "../kitsune_p2p/dht_arc" }
kitsune_p2p_dht = { version = "^0.5.0-dev.3", path = "../kitsune_p2p/dht", features = [
  "test_utils",
] }
holochain_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../kitsune_p2p/types", optional = true }
nanoid = "0.4"
once_cell = "1.4.1"
num_cpus = "1.13.0"
parking_lot = "0.12"
r2d2 = "0.8"
r2d2_sqlite = { version = "0.25.0", package = "hc_r2d2_sqlite" }
rmp-serde = "=1.3.0"
scheduled-thread-pool = "0.2"
serde = "1.0"
serde_json = { version = "1.0.51", features = ["preserve_order"] }
shrinkwraprs = "0.3.0"
sodoken = "=0.0.11"
tempfile = "3.3"
thiserror = "1.0.22"
tokio = { version = "1.27", features = [
  "macros",
  "rt-multi-thread",
  "io-util",
  "sync",
  "time",
] }
tracing = "0.1.18"
getrandom = "0.2.7"
opentelemetry_api = { version = "=0.20.0", features = ["metrics"] }
schemars = "0.8.21"

kitsune2_api = "0.0.1-alpha.1"
bytes = "1.10"

rusqlite = { version = "0.32.1", features = [
  "blob",      # better integration with blob types (Read, Write, etc)
  "backup",
  "trace",
  "functions", # rust scalar / agg / window functions
  #"chrono",      # integration with chrono crate
  #"hooks",       # sqlite notification callbacks
  #"serde_json",  # integration with serde_json crate
  #"url",         # integration with url crate
  #"uuid",        # integration with uuid crate
] }

[dev-dependencies]
holochain_sqlite = { path = ".", features = ["test_utils", "slow_tests"] }
holochain_trace = { version = "^0.5.0-dev.1", path = "../holochain_trace" }
nanoid = "0.4.0"
rand = "0.8.5"
walkdir = "2.5.0"

[build-dependencies]
pretty_assertions = "1.4"
sqlformat = "=0.2.6"

[target.'cfg(loom)'.dev-dependencies]
loom = { version = "0.7", features = ["futures", "checkpoint"] }
holochain_sqlite = { path = ".", default-features = false, features = [
  "test_utils",
] }

[lints]
workspace = true

[features]
default = ["sqlite", "kitsune_p2p_types", "holochain_util"]

test_utils = []

slow_tests = []

# Use at-rest encryption of databases
sqlite-encrypted = [
  "rusqlite/bundled-sqlcipher-vendored-openssl",
  "r2d2_sqlite/bundled-sqlcipher-vendored-openssl",
  "holo_hash/sqlite-encrypted",
  "holochain_zome_types/sqlite-encrypted",
  "kitsune_p2p_bin_data/sqlite-encrypted",
  "kitsune_p2p_types/sqlite-encrypted",
  "kitsune_p2p_dht_arc/sqlite-encrypted",
  "holochain_timestamp/sqlite-encrypted",
]

# Compile SQLite from source rather than depending on a library
sqlite = [
  "rusqlite/bundled",
  "r2d2_sqlite/bundled",
  "holo_hash/sqlite",
  "holochain_zome_types/sqlite",
  "kitsune_p2p_bin_data/sqlite",
  "kitsune_p2p_dht_arc/sqlite",
  "kitsune_p2p_types/sqlite",
  "holochain_timestamp/sqlite",
]

# Enables tracing instrumentation 
# (we experience segfaults in some tests if there is too much instrumentation)
instrument = []



================================================
File: crates/holochain_sqlite/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.19

## 0.5.0-dev.18

## 0.5.0-dev.17

- Prevent TODO comments from being rendered in cargo docs.

- Add new database type `DbKindPeerMetaStore`

## 0.5.0-dev.16

## 0.5.0-dev.15

## 0.5.0-dev.14

## 0.5.0-dev.13

## 0.5.0-dev.12

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.23

## 0.4.0-dev.22

## 0.4.0-dev.21

## 0.4.0-dev.20

## 0.4.0-dev.19

## 0.4.0-dev.18

## 0.4.0-dev.17

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

- **Breaking**: Shorten paths to database files by removing the name prefix where that prefix matches the directory name. The file extensions have also been removed. For example, an authored database would have been stored at `<root-dir>/authored/authored-<dna-hash>-<agent-key>.sqlite` and will now be stored at `<root-dir>/authored/<dna-hash>-<agent-key>`. This also affects the DHT, cache, agent store and metrics databases. You can work around this being a breaking change by renaming your database files to match the new expected names.

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.43

## 0.3.0-beta-dev.42

## 0.3.0-beta-dev.41

## 0.3.0-beta-dev.40

## 0.3.0-beta-dev.39

## 0.3.0-beta-dev.38

## 0.3.0-beta-dev.37

## 0.3.0-beta-dev.36

## 0.3.0-beta-dev.35

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

- Provide a mechanism to automatically encrypt databases which are currently unencrypted. This is useful if you are switching from a Holochain built with the `sqlite` feature, to a Holochain built with `sqlite-encrypted`. In order to enable this mechanism you will need to set the environment variable `HOLOCHAIN_MIGRATE_UNENCRYPTED=true`. *DANGER*: If you switch your Holochain without this environment variable then on first startup it will recognise your cache, dht, peer and kitsune metrics databases will be recognised as corrupt and automatically wiped. These databases may be rebuilt, assuming that the same data is still available from other peers, but please consider making a backup before attempting to make the switch.

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.6

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.66

## 0.0.65

## 0.0.64

## 0.0.63

## 0.0.62

## 0.0.61

## 0.0.60

## 0.0.59

## 0.0.58

## 0.0.57

## 0.0.56

## 0.0.55

## 0.0.54

## 0.0.53

## 0.0.52

## 0.0.51

## 0.0.50

## 0.0.49

## 0.0.48

## 0.0.47

## 0.0.46

## 0.0.45

## 0.0.44

## 0.0.43

## 0.0.42

## 0.0.41

## 0.0.40

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

## 0.0.21

## 0.0.20

## 0.0.19

- Adds `basis_hash` index to `DhtOp` table. This makes get queries faster. [\#1143](https://github.com/holochain/holochain/pull/1143)

## 0.0.18

## 0.0.17

- **BREAKING CHANGES**: All DHT data for the same DNA space is now shared in the same database. All authored data for the same DNA space is also now shared in another database. This requires no changes however data must be manually migrated from the old databases to the new databases. [\#1130](https://github.com/holochain/holochain/pull/1130)

## 0.0.16

## 0.0.15

- Fixes: Bug where database connections would timeout and return `DatabaseError(DbConnectionPoolError(Error(None)))`. [\#1097](https://github.com/holochain/holochain/pull/1097).

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

- Update to rusqlite 0.26.0 [\#1023](https://github.com/holochain/holochain/pull/1023)
  - provides `bundled-sqlcipher-vendored-openssl` to ease build process on non-windows systems (windows is still using `bundled` which doesnt provide at-rest encryption).

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/holochain_sqlite/loom_tests.sh
================================================
#!/usr/bin/env bash
RUSTFLAGS="--cfg loom" cargo test --test loom --no-default-features



================================================
File: crates/holochain_sqlite/src/error.rs
================================================
//! All possible errors when working with SQLite databases

// missing_docs allowed here since the errors already have self-descriptive strings
#![allow(missing_docs)]

use holochain_serialized_bytes::SerializedBytesError;
use holochain_zome_types::prelude::*;
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum DatabaseError {
    #[error("A database's database map was initialized more than once: {0}")]
    EnvironmentDoubleInitialized(PathBuf),

    #[error("database directory does not exist at configured path: {0}")]
    DatabaseMissing(PathBuf),

    #[error(
        "Attempted to access a private entry in a context where no private database is specified: {0}"
    )]
    NoPrivateDb(String),

    #[error("Error encoding to MsgPack: {0}")]
    MsgPackEncodeError(#[from] rmp_serde::encode::Error),

    #[error("Error decoding to MsgPack: {0}")]
    MsgPackDecodeError(#[from] rmp_serde::decode::Error),

    #[error("SerializedBytes error when attempting to interact with SQLite: {0}")]
    SerializedBytes(#[from] SerializedBytesError),

    #[error(transparent)]
    Other(#[from] anyhow::Error),

    #[error(transparent)]
    SqliteError(#[from] rusqlite::Error),

    #[error("Failure to remove directory")]
    DirectoryError(#[from] std::io::Error),

    #[error(transparent)]
    DbConnectionPoolError(#[from] r2d2::Error),

    #[error("Empty keys cannot be used with SQLite")]
    EmptyKey,

    #[error("Key range must be not empty and start < end")]
    InvalidKeyRange,

    #[error("Unable to construct a value key")]
    KeyConstruction,

    #[error("transparent")]
    FailedToJoinBlocking(#[from] tokio::task::JoinError),

    #[error(transparent)]
    Timestamp(TimestampError),

    #[error(transparent)]
    GetRandom(getrandom::Error),

    #[error(transparent)]
    Timeout(tokio::time::error::Elapsed),
}

impl From<TimestampError> for DatabaseError {
    fn from(timestamp_error: TimestampError) -> Self {
        Self::Timestamp(timestamp_error)
    }
}

impl From<getrandom::Error> for DatabaseError {
    fn from(getrandom_error: getrandom::Error) -> Self {
        Self::GetRandom(getrandom_error)
    }
}

impl PartialEq for DatabaseError {
    fn eq(&self, other: &Self) -> bool {
        self.to_string() == other.to_string()
    }
}

pub type DatabaseResult<T> = Result<T, DatabaseError>;



================================================
File: crates/holochain_sqlite/src/exports.rs
================================================
//! A few imports from `rkv`, to avoid consumers needing to import `rkv` explicitly

pub use fallible_iterator::FallibleIterator;



================================================
File: crates/holochain_sqlite/src/fatal.rs
================================================
//! Sometimes we have fatal errors, and need to halt the system.
//! This module provides standards for showing these messages to the user.

/// Macro for standard handling of fatal errors
#[macro_export]
macro_rules! fatal {
    ($($t:tt)*) => {{
        let m = format!($($t)*);

        // human_panic is going to eat the text of our fatal error
        // so we need to duplicate it with a direct eprintln!
        eprintln!("{}", &m);

        // now panic
        panic!("{}", m);
    }};
}

/// Macro for standard handling of db deserialization fatal errors
#[macro_export]
macro_rules! fatal_db_hash_construction_check {
    ($hint:expr, $hash:expr, $res:expr,) => {
        fatal_db_hash_construction_check!($hint, $hash, $res);
    };
    ($hint:expr, $hash:expr, $res:expr) => {{
        match $res {
            Ok(res) => res,
            Err(e) => {
                $crate::fatal!(
                    r#"Holochain detected database corruption.

Corrupt module: {}
Expected hash: {:?}
Deserialization Error: {:?}

We are shutting down as a precaution to prevent further corruption."#,
                    $hint,
                    $hash,
                    e,
                );
            }
        }
    }};
}

/// Macro for standard handling of db hash integrity check failures
#[macro_export]
macro_rules! fatal_db_hash_integrity_check {
    ($hint:expr, $expected_hash:expr, $actual_hash:expr, $content:expr $(,)?) => {
        if *$expected_hash != *$actual_hash {
            $crate::fatal!(
                r#"Holochain detected database corruption.

Corrupt module: {}
Expected hash: {:?}
Actual hash: {:?}
Content: {:?}

We are shutting down as a precaution to prevent further corruption."#,
                $hint,
                $expected_hash,
                $actual_hash,
                $content,
            );
        }
    };
}



================================================
File: crates/holochain_sqlite/src/functions.rs
================================================
use kitsune_p2p_dht::{
    hash::{hash_slice_32, Hash32},
    region::slice_xor,
};
use rusqlite::{functions::*, types::ValueRef, *};

pub fn add_custom_functions(conn: &Connection) -> Result<()> {
    conn.create_aggregate_function(
        "REDUCE_XOR",
        -1,
        FunctionFlags::SQLITE_DETERMINISTIC | FunctionFlags::SQLITE_DIRECTONLY,
        AggregateXor,
    )?;

    Ok(())
}

pub struct AggregateXor;

impl Aggregate<Hash32, Vec<u8>> for AggregateXor {
    fn init(&self, _ctx: &mut Context<'_>) -> Result<Hash32> {
        Ok([0; 32])
    }

    fn step(&self, ctx: &mut Context<'_>, v: &mut Hash32) -> Result<()> {
        let blob: &[u8] = match ctx.get_raw(0) {
            ValueRef::Blob(b) => Ok(b),
            v => Err(rusqlite::Error::InvalidFunctionParameterType(
                0,
                v.data_type(),
            )),
        }?;
        let len = blob.len();
        if len == 39 {
            slice_xor(v, hash_slice_32(blob));
            Ok(())
        } else {
            Err(Error::UserFunctionError(
                format!(
                    "REDUCE_XOR can only handle BLOBs of 39 bytes, but encountered one of {} bytes",
                    len
                )
                .into(),
            ))
        }
    }

    fn finalize(&self, _ctx: &mut Context<'_>, v: Option<Hash32>) -> Result<Vec<u8>> {
        Ok(v.unwrap_or([0; 32]).to_vec())
    }
}



================================================
File: crates/holochain_sqlite/src/lib.rs
================================================
//! # Building blocks for persisted Holochain state
//!
//! See crate README for more info.
//!
//! See [this hackmd](https://holo.hackmd.io/@holochain/SkuVLpqEL) for a diagram explaining the relationships between these building blocks and the higher abstractions
//!
//! ### Connecting to Encrypted Databases
//!
//! Ubuntu doesn't ship with the correct version of the sqlcipher utility.
//! We're going to need to build it ourselves.
//!
//! As of this writing, we are using rusqlite 0.32.1. You can find the sqlcipher
//! version used here: <https://github.com/rusqlite/rusqlite/blob/v0.32.1/libsqlite3-sys/upgrade_sqlcipher.sh#L11> -- `4.5.7`.
//!
//! #### Building `sqlcipher`
//!
//! Download the source from here: <https://github.com/sqlcipher/sqlcipher/releases/tag/v4.5.7>
//!
//! Unpack and run the build commands per the README.md:
//!
//! ```sh
//! ./configure --enable-tempstore=yes CFLAGS="-DSQLITE_HAS_CODEC" LDFLAGS="-lcrypto"
//! make
//! ```
//!
//! Now you have a compatible sqlcipher cli utility: `./sqlcipher`, but we
//! need the secrets used to encrypt the database.
//!
//! #### Getting the database secrets out of holochain.
//!
//! Holochain stores secrets in a file named `db.key` in the configured
//! `data_root_path`. If you print out the file, it will just be base64:
//!
//! ```sh
//! $ cat /tmp/bob/databases/db.key
//! RXfUEZzCURLrG8hJVcUP4A6T1qY_gql0Fata5PxEgbV7P5IuKoeTu8hyCo9MYdH3vZTU8Loprip22YmRk0vdd_Lcuz3lfKx5FeB_0pskegI_6Zsb4zcTZA
//! ```
//!
//! To decrypt this, we will need the passphrase. We can use a cli flag
//! on holochain, `--danger-print-db-secrets`, which will print the secrets
//! out on stderr:
//!
//! ```sh
//! $ holochain --danger-print-db-secrets -c ~/conductor-config.yaml
//! Initialising log output formatting with option Log
//! # passphrase>
//! # lair-keystore connection_url # unix:///tmp/bob/ks/socket?k=aq19xrSyPaDZbL-Keb8WHhaZ2xbxN07yYztfwqpNAxs #
//! # lair-keystore running #
//! --beg-db-secrets--
//! PRAGMA key = "x'6D71B0A31666195576242A41129FE9387ECA216DA241C98F92A18A01557A8199'";
//! PRAGMA cipher_salt = "x'15E07FD29B247A023FE99B1BE3371364'";
//! PRAGMA cipher_compatibility = 4;
//! PRAGMA cipher_plaintext_header_size = 32;
//! --end-db-secrets--
//!
//! ###HOLOCHAIN_SETUP###
//! ###HOLOCHAIN_SETUP_END###
//! Conductor ready.
//! ```
//!
//! Note the `PRAGMA` directives printed out between the `--beg-db-secrets--`
//! and `--end-db-secrets--` markers.
//!
//! #### Connect to your encrypted holochain database via sqlcipher
//!
//! ```sh
//! ./sqlcipher /tmp/bob/databases/conductor/conductor
//! ```
//!
//! At the `sqlite>` prompt, input your key:
//!
//! ```text
//! PRAGMA key = "x'6D71B0A31666195576242A41129FE9387ECA216DA241C98F92A18A01557A8199'";
//! PRAGMA cipher_salt = "x'15E07FD29B247A023FE99B1BE3371364'";
//! PRAGMA cipher_compatibility = 4;
//! PRAGMA cipher_plaintext_header_size = 32;
//! ```
//!
//! It should print out `ok` for the `key` pragma, and nothing for the other
//! three lines.
//!
//! You should now be able to make sqlite queries:
//!
//! ```text
//! select count(id) from ConductorState;
//! ```

pub mod db;
pub mod error;
pub mod exports;
pub mod fatal;
pub mod functions;
#[cfg(not(loom))]
pub mod nonce;
pub mod prelude;
pub mod schema;
#[cfg(not(loom))]
pub mod sql;
pub mod stats;
#[cfg(not(loom))]
pub mod store;
pub mod swansong;

mod table;

// Re-export rusqlite for use with `impl_to_sql_via_as_ref!` macro
pub use ::rusqlite;



================================================
File: crates/holochain_sqlite/src/nonce.rs
================================================
//! nonce sql logic
use crate::prelude::*;
use crate::sql::sql_conductor;
use holo_hash::AgentPubKey;
use holochain_nonce::Nonce256Bits;
use holochain_zome_types::prelude::Timestamp;
use rusqlite::*;

pub fn nonce_already_seen(
    txn: &Transaction<'_>,
    agent: &AgentPubKey,
    nonce: Nonce256Bits,
    now: Timestamp,
) -> DatabaseResult<bool> {
    let mut statement = txn
        .prepare(sql_conductor::SELECT_NONCE)
        .map_err(|e| rusqlite::Error::ToSqlConversionFailure(e.into()))?;

    Ok(statement
        .query_row(
            named_params! {":agent": agent, ":nonce": nonce.into_inner(), ":now": now },
            |row| Ok(row.get_ref(0)?.as_i64()?),
        )
        .optional()?
        .is_some())
}



================================================
File: crates/holochain_sqlite/src/prelude.rs
================================================
//! Common types, especially traits, which we'd like to import en masse

pub use crate::db::*;
pub use crate::error::*;
pub use crate::exports::*;
#[cfg(not(loom))]
pub use crate::store::*;

pub use rusqlite::{OptionalExtension, Transaction};



================================================
File: crates/holochain_sqlite/src/schema.rs
================================================
//! Schema and migration definitions
//!
//! To create a new migration, add a new [`Migration`] object to the `migrations`
//! vec for a particular schema, and bump the `current_index` by 1.
//! The `Migration` must specify the actual forward migration script, as well as
//! an updated schema defining the result of running the migration.
//!
//! Currently, the updated schema only serves as a point of reference for examining
//! the current schema. In the future, we should find a way to compare the actual
//! schema resulting from migrations with the schema provided, to make sure they match.
//!
//! Note that there is code in `build.rs` which fails the build if any schema or migration
//! file has a change according to `git diff`. This will hopefully help prevent accidental
//! modification of schemas, which should never be committed.

use once_cell::sync::Lazy;
use rusqlite::{Connection, Transaction};

use crate::db::DbKind;

pub static SCHEMA_CELL: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![
        M::initial(include_str!("sql/cell/schema/0.sql")),
        M {
            forward: include_str!("sql/cell/schema/1-up.sql").into(),
            _schema: include_str!("sql/cell/schema/1.sql").into(),
        },
        M {
            forward: include_str!("sql/cell/schema/2-up.sql").into(),
            _schema: include_str!("sql/cell/schema/2.sql").into(),
        },
        M {
            forward: include_str!("sql/cell/schema/3-up.sql").into(),
            _schema: include_str!("sql/cell/schema/3.sql").into(),
        },
        M {
            forward: include_str!("sql/cell/schema/4-up.sql").into(),
            _schema: include_str!("sql/cell/schema/4.sql").into(),
        },
    ],
});

pub static SCHEMA_CONDUCTOR: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![
        M::initial(include_str!("sql/conductor/schema/0.sql")),
        M {
            forward: include_str!("sql/conductor/schema/1-up.sql").into(),
            _schema: "".into(),
        },
    ],
});

pub static SCHEMA_WASM: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![M::initial(include_str!("sql/wasm/schema/0.sql"))],
});

pub static SCHEMA_P2P_STATE: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![M::initial(include_str!("sql/p2p_agent_store/schema/0.sql"))],
});

pub static SCHEMA_P2P_METRICS: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![M::initial(include_str!("sql/p2p_metrics/schema/0.sql"))],
});

pub static SCHEMA_PEER_META_STORE: Lazy<Schema> = Lazy::new(|| Schema {
    migrations: vec![M::initial(include_str!("sql/peer_meta_store/schema/0.sql"))],
});

pub struct Schema {
    migrations: Vec<Migration>,
}

impl Schema {
    /// Determine if any database migrations need to run, and run them if so.
    /// The decision is based on the difference between this Schema's
    /// current_index and the user_version pragma value in the database itself.
    /// NB: The current_index is 0-based, and the user_version is 1-based.
    pub fn initialize(
        &self,
        conn: &mut Connection,
        db_kind: Option<DbKind>,
    ) -> rusqlite::Result<()> {
        let user_version: u16 = conn.pragma_query_value(None, "user_version", |row| row.get(0))?;
        let db_kind = db_kind
            .as_ref()
            .map(ToString::to_string)
            .unwrap_or_else(|| "<no name>".to_string());

        let migrations_applied = user_version as usize;
        let num_migrations = self.migrations.len();
        match migrations_applied.cmp(&(num_migrations)) {
            std::cmp::Ordering::Less => {
                let mut txn = conn.transaction()?;

                // run forward migrations
                for v in migrations_applied..num_migrations {
                    self.migrations[v].run_forward(&mut txn)?;
                    // set the DB user_version so that next time we don't run
                    // the same migration
                    txn.pragma_update(None, "user_version", v + 1)?;
                }
                txn.commit()?;
                tracing::info!(
                    "database forward migrated: {} from {} to {}",
                    db_kind,
                    migrations_applied,
                    num_migrations - 1,
                );
            }
            std::cmp::Ordering::Equal => {
                tracing::debug!(
                    "database needed no migration or initialization, good to go: {}",
                    db_kind
                );
            }
            std::cmp::Ordering::Greater => {
                unimplemented!("backward migrations unimplemented");
            }
        }

        Ok(())
    }
}

#[derive(Clone, Debug)]
pub struct Migration {
    _schema: Sql,
    forward: Sql,
}

impl Migration {
    /// The initial migration's forward migration is the entire schema
    pub fn initial(schema: &str) -> Self {
        Self {
            _schema: schema.into(),
            forward: schema.into(),
        }
    }

    pub fn run_forward(&self, txn: &mut Transaction) -> rusqlite::Result<()> {
        txn.execute_batch(&self.forward)?;
        Ok(())
    }
}
type M = Migration;

type Sql = String;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_migrations_initial() {
        let schema = Schema {
            migrations: vec![
                M::initial("CREATE TABLE Numbers (num INTEGER);"),
                M {
                    forward: "CREATE TABLE Names (name TEXT);".into(),
                    _schema: "n/a".into(),
                },
            ],
        };

        let mut conn = Connection::open_in_memory().unwrap();

        // The Names table doesn't exist yet, since the current_index is set to 0.
        schema.initialize(&mut conn, None).unwrap();
        assert_eq!(
            conn.execute("INSERT INTO Numbers (num) VALUES (1)", ())
                .unwrap(),
            1
        );
        assert_eq!(
            conn.execute("INSERT INTO Names (name) VALUES ('Mike')", ())
                .unwrap(),
            1
        );
    }

    #[test]
    fn test_migrations_sequential() {
        let mut schema = Schema {
            migrations: vec![M::initial("CREATE TABLE Numbers (num INTEGER);")],
        };

        let mut conn = Connection::open_in_memory().unwrap();

        // The Names table doesn't exist yet, since the current_index is set to 0.
        schema.initialize(&mut conn, None).unwrap();
        assert_eq!(
            conn.execute("INSERT INTO Numbers (num) VALUES (1)", ())
                .unwrap(),
            1
        );
        assert!(conn
            .execute("INSERT INTO Names (name) VALUES ('Mike')", ())
            .is_err());

        // This initialization will run only the second migration and create the Names table.
        schema.migrations = vec![
            M::initial("This bad SQL won't run, phew!"),
            M {
                forward: "CREATE TABLE Names (name TEXT);".into(),
                _schema: "n/a".into(),
            },
        ];
        schema.initialize(&mut conn, None).unwrap();
        assert_eq!(
            conn.execute("INSERT INTO Numbers (num) VALUES (1)", ())
                .unwrap(),
            1
        );
        assert_eq!(
            conn.execute("INSERT INTO Names (name) VALUES ('Mike')", ())
                .unwrap(),
            1
        );
    }
}



================================================
File: crates/holochain_sqlite/src/sql.rs
================================================
pub mod sql_cell {
    pub const UPDATE_INTEGRATE_DEP_ACTIVITY: &str =
        include_str!("sql/cell/update_dep_activity.sql");
    pub const ACTIVITY_INTEGRATED_UPPER_BOUND: &str =
        include_str!("sql/cell/activity_integrated_upper_bound.sql");
    pub const ACTION_HASH_BY_PREV: &str = include_str!("sql/cell/action_hash_by_prev.sql");
    pub const ALL_ACTIVITY_AUTHORS: &str = include_str!("sql/cell/all_activity_authors.sql");
    pub const ALL_READY_ACTIVITY: &str = include_str!("sql/cell/all_ready_activity.sql");
    pub const DELETE_ACTIONS_AFTER_SEQ: &str =
        include_str!("sql/cell/delete_actions_after_seq.sql");
    pub const UPDATE_INTEGRATE_DEP_STORE_RECORD: &str =
        include_str!("sql/cell/update_dep_store_record.sql");
    pub const UPDATE_INTEGRATE_DEP_STORE_ENTRY: &str =
        include_str!("sql/cell/update_dep_store_entry.sql");
    pub const SET_ADD_LINK_OPS_TO_INTEGRATED: &str =
        include_str!("sql/cell/set_add_link_ops_to_integrated.sql");
    pub const SET_DELETE_LINK_OPS_TO_INTEGRATED: &str =
        include_str!("sql/cell/set_delete_link_ops_to_integrated.sql");
    pub const SET_CHAIN_INTEGRITY_WARRANT_OPS_TO_INTEGRATED: &str =
        include_str!("sql/cell/set_chain_integrity_warrant_ops_to_integrated.sql");

    pub const UPDATE_INTEGRATE_STORE_RECORD: &str =
        include_str!("sql/cell/update_store_record.sql");
    pub const UPDATE_INTEGRATE_STORE_ENTRY: &str = include_str!("sql/cell/update_store_entry.sql");

    pub const SELECT_VALID_AGENT_PUB_KEY: &str =
        include_str!("sql/cell/select_valid_agent_pub_key.sql");

    pub const FETCH_OP_HASHES_P1: &str =
        include_str!("sql/cell/fetch_hashes/fetch_op_hashes_p1.sql");
    pub const FETCH_OP_HASHES_P2: &str =
        include_str!("sql/cell/fetch_hashes/fetch_op_hashes_p2.sql");

    pub const FETCH_OP_REGION: &str = include_str!("sql/cell/fetch_op_region.sql");
    pub const FETCH_OPS_BY_REGION: &str = include_str!("sql/cell/fetch_ops_by_region.sql");
    pub const FETCH_REGION_OP_HASHES: &str = include_str!("sql/cell/fetch_region_op_hashes.sql");

    pub const FETCH_PUBLISHABLE_OP: &str = include_str!("sql/cell/fetch_publishable_op.sql");

    pub const SUM_OF_RECEIVED_BYTES_SINCE_TIMESTAMP: &str =
        include_str!("sql/cell/sum_of_received_bytes_since_timestamp.sql");

    pub mod must_get_agent_activity {
        pub const MUST_GET_AGENT_ACTIVITY: &str =
            include_str!("sql/cell/agent_activity/must_get_agent_activity.sql");
        pub const ACTION_HASH_TO_SEQ: &str =
            include_str!("sql/cell/agent_activity/action_hash_to_seq.sql");
    }

    pub mod schedule {
        pub const UPDATE: &str = include_str!("sql/cell/schedule/update.sql");
        pub const DELETE: &str = include_str!("sql/cell/schedule/delete.sql");
        pub const EXPIRED: &str = include_str!("sql/cell/schedule/expired.sql");
        pub const DELETE_ALL_EPHEMERAL: &str =
            include_str!("sql/cell/schedule/delete_all_ephemeral.sql");
        pub const DELETE_LIVE_EPHEMERAL: &str =
            include_str!("sql/cell/schedule/delete_live_ephemeral.sql");
    }
    pub mod state_dump {
        pub const DHT_OPS_IN_INTEGRATION_LIMBO: &str =
            include_str!("sql/cell/state_dump/dht_ops_in_integration_limbo.sql");
        pub const DHT_OPS_INTEGRATED: &str =
            include_str!("sql/cell/state_dump/dht_ops_integrated.sql");
        pub const DHT_OPS_IN_VALIDATION_LIMBO: &str =
            include_str!("sql/cell/state_dump/dht_ops_in_validation_limbo.sql");
        pub const DHT_OPS_ROW_ID: &str = include_str!("sql/cell/state_dump/dht_ops_row_id.sql");
    }
}

pub mod sql_conductor {
    pub(crate) const SELECT_NONCE: &str = include_str!("sql/conductor/nonce_already_seen.sql");
    pub const DELETE_EXPIRED_NONCE: &str = include_str!("sql/conductor/delete_expired_nonce.sql");
    pub const FROM_BLOCK_SPAN_WHERE_OVERLAPPING: &str =
        include_str!("sql/conductor/from_block_span_where_overlapping.sql");
    pub const IS_BLOCKED: &str = include_str!("sql/conductor/is_blocked.sql");
    pub const SELECT_VALID_CAP_GRANT_FOR_CAP_SECRET: &str =
        include_str!("sql/conductor/select_valid_cap_grant_for_cap_secret.sql");
    pub const SELECT_VALID_UNRESTRICTED_CAP_GRANT: &str =
        include_str!("sql/conductor/select_valid_unrestricted_cap_grant.sql");
}

pub(crate) mod sql_p2p_agent_store {
    pub(crate) const INSERT: &str = include_str!("sql/p2p_agent_store/insert.sql");
    pub(crate) const SELECT_ALL: &str = include_str!("sql/p2p_agent_store/select_all.sql");
    pub(crate) const DELETE: &str = include_str!("sql/p2p_agent_store/delete.sql");
    pub(crate) const EXTRAPOLATED_COVERAGE: &str =
        include_str!("sql/p2p_agent_store/extrapolated_coverage.sql");
    pub(crate) const PRUNE: &str = include_str!("sql/p2p_agent_store/prune.sql");
}

pub(crate) mod sql_p2p_metrics {
    pub(crate) const INSERT: &str = include_str!("sql/p2p_metrics/insert.sql");
    pub(crate) const PRUNE: &str = include_str!("sql/p2p_metrics/prune.sql");
}

pub(crate) mod sql_wasm {}

pub mod sql_peer_meta_store {
    pub const PRUNE: &str = include_str!("sql/peer_meta_store/prune.sql");

    pub const INSERT: &str = include_str!("sql/peer_meta_store/insert.sql");

    pub const GET: &str = include_str!("sql/peer_meta_store/get.sql");

    pub const DELETE: &str = include_str!("sql/peer_meta_store/delete.sql");
}



================================================
File: crates/holochain_sqlite/src/stats.rs
================================================
use crate::{
    db::{DbKindT, Txn},
    error::DatabaseError,
};
use rusqlite::OptionalExtension;

pub fn get_size_on_disk<K: DbKindT>(txn: &Txn<K>) -> Result<usize, DatabaseError> {
    Ok(txn
        .query_row("select sum(pgsize) from dbstat", (), |r| r.get(0))
        .optional()
        .map_err(DatabaseError::SqliteError)?
        .unwrap_or_default())
}

pub fn get_used_size<K: DbKindT>(txn: &Txn<K>) -> Result<usize, DatabaseError> {
    Ok(txn
        .query_row("select sum(pgsize - unused) from dbstat", (), |r| r.get(0))
        .optional()
        .map_err(DatabaseError::SqliteError)?
        .unwrap_or_default())
}



================================================
File: crates/holochain_sqlite/src/swansong.rs
================================================
/// A shrinkwrapped type with a Drop impl provided as a simple closure
#[derive(shrinkwraprs::Shrinkwrap)]
#[shrinkwrap(mutable, unsafe_ignore_visibility)]
pub struct SwanSong<'a, T> {
    #[shrinkwrap(main_field)]
    inner: T,
    #[allow(clippy::type_complexity)]
    song: Option<Box<dyn FnOnce(&mut T) + 'a>>,
}

impl<T> Drop for SwanSong<'_, T> {
    fn drop(&mut self) {
        self.song.take().unwrap()(&mut self.inner);
    }
}

impl<'a, T> SwanSong<'a, T> {
    pub fn new<F: FnOnce(&mut T) + 'a>(inner: T, song: F) -> Self {
        Self {
            inner,
            song: Some(Box::new(song)),
        }
    }
}



================================================
File: crates/holochain_sqlite/src/table.rs
================================================
//! Functionality for safely accessing databases.

use rusqlite::Connection;

use crate::db::DbKind;

/// Enumeration of all databases needed by Holochain
pub(crate) fn initialize_database(conn: &mut Connection, db_kind: DbKind) -> rusqlite::Result<()> {
    match db_kind {
        DbKind::Dht(_) => {
            crate::schema::SCHEMA_CELL.initialize(conn, Some(db_kind))?;
        }
        DbKind::Authored(_) => {
            crate::schema::SCHEMA_CELL.initialize(conn, Some(db_kind))?;
        }
        DbKind::Conductor => {
            crate::schema::SCHEMA_CONDUCTOR.initialize(conn, Some(db_kind))?;
        }
        DbKind::Wasm => {
            crate::schema::SCHEMA_WASM.initialize(conn, Some(db_kind))?;
        }
        DbKind::P2pAgentStore(_) => {
            crate::schema::SCHEMA_P2P_STATE.initialize(conn, Some(db_kind))?;
        }
        DbKind::P2pMetrics(_) => {
            crate::schema::SCHEMA_P2P_METRICS.initialize(conn, Some(db_kind))?;
        }
        DbKind::Cache(_) => {
            crate::schema::SCHEMA_CELL.initialize(conn, Some(db_kind))?;
        }
        DbKind::PeerMetaStore(_) => {
            crate::schema::SCHEMA_PEER_META_STORE.initialize(conn, Some(db_kind))?;
        }
        #[cfg(feature = "test_utils")]
        DbKind::Test(_) => {
            // Nothing to do
        }
    }
    Ok(())
}



================================================
File: crates/holochain_sqlite/src/db/access.rs
================================================
use crate::db::conn::PConn;
use crate::db::databases::DATABASE_HANDLES;
use crate::db::guard::{PConnGuard, PTxnGuard};
use crate::db::kind::{DbKind, DbKindT};
use crate::db::pool::{
    initialize_connection, new_connection_pool, num_read_threads, ConnectionPool, PoolConfig,
};
use crate::error::{DatabaseError, DatabaseResult};
use derive_more::Into;
use holochain_util::log_elapsed;
use parking_lot::Mutex;
use rusqlite::*;
use shrinkwraprs::Shrinkwrap;
use std::marker::PhantomData;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use std::time::Instant;
use std::{collections::HashMap, path::Path};
use std::{path::PathBuf, sync::atomic::AtomicUsize};
use tokio::sync::{OwnedSemaphorePermit, Semaphore};
use tracing::Instrument;

use super::metrics::{create_connection_use_time_metric, create_pool_usage_metric, UseTimeMetric};

static ACQUIRE_TIMEOUT_MS: AtomicU64 = AtomicU64::new(10_000);
static THREAD_ACQUIRE_TIMEOUT_MS: AtomicU64 = AtomicU64::new(30_000);

/// Wrapper around a Transaction reference which is typed by database kind.
///
/// This allows us to write functions which can only operate on a specific database kind,
/// or sets of database kinds (with the introduction of a new trait that covers those kinds).
#[derive(derive_more::Deref, derive_more::DerefMut, derive_more::Into)]
pub struct Txn<'a, 'txn, D: DbKindT> {
    #[deref]
    #[deref_mut]
    #[into]
    txn: &'a mut Transaction<'txn>,
    db_kind: std::marker::PhantomData<D>,
}

impl<'a, 'txn, D: DbKindT> From<&'a mut Transaction<'txn>> for Txn<'a, 'txn, D> {
    fn from(txn: &'a mut Transaction<'txn>) -> Self {
        Txn {
            txn,
            db_kind: PhantomData,
        }
    }
}

#[async_trait::async_trait]
/// A trait for being generic over [`DbWrite`] and [`DbRead`] that
/// both implement read access.
pub trait ReadAccess<Kind: DbKindT>: Clone + Into<DbRead<Kind>> {
    /// Run an async read transaction on a background thread.
    async fn read_async<E, R, F>(&self, f: F) -> Result<R, E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static;

    /// Access the kind of database.
    fn kind(&self) -> &Kind;
}

#[async_trait::async_trait]
impl<Kind: DbKindT> ReadAccess<Kind> for DbWrite<Kind> {
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(kind = ?self.kind)))]
    async fn read_async<E, R, F>(&self, f: F) -> Result<R, E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static,
    {
        let db: &DbRead<Kind> = self.as_ref();
        DbRead::read_async(db, f).await
    }

    fn kind(&self) -> &Kind {
        self.0.kind()
    }
}

#[async_trait::async_trait]
impl<Kind: DbKindT> ReadAccess<Kind> for DbRead<Kind> {
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(kind = ?self.kind)))]
    async fn read_async<E, R, F>(&self, f: F) -> Result<R, E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static,
    {
        DbRead::read_async(self, f).await
    }

    fn kind(&self) -> &Kind {
        &self.kind
    }
}

/// A read-only version of [DbWrite].
/// This environment can only generate read-only transactions, never read-write.
#[derive(Clone)]
pub struct DbRead<Kind: DbKindT> {
    kind: Kind,
    path: PathBuf,
    connection_pool: ConnectionPool,
    write_semaphore: Arc<Semaphore>,
    read_semaphore: Arc<Semaphore>,
    long_read_semaphore: Arc<Semaphore>,
    statement_trace_fn: Option<fn(&str)>,
    max_readers: usize,
    num_readers: Arc<AtomicUsize>,
    use_time_metric: UseTimeMetric,
}

impl<Kind: DbKindT> std::fmt::Debug for DbRead<Kind> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("DbRead")
            .field("kind", &self.kind)
            .field("path", &self.path)
            .field("max_readers", &self.max_readers)
            .field("num_readers", &self.num_readers)
            .finish()
    }
}

impl<Kind: DbKindT> DbRead<Kind> {
    /// Accessor for the [DbKindT] of the DbWrite
    pub fn kind(&self) -> &Kind {
        &self.kind
    }

    /// The environment's path
    pub fn path(&self) -> &PathBuf {
        &self.path
    }

    /// Execute a read closure on the database by acquiring a connection from the pool, starting a new transaction and
    /// running the closure with that transaction.
    ///
    /// Note that it is not enforced that your closure runs read-only operations or that it finishes quickly so it is
    /// up to the caller to use this function as intended.
    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(kind = ?self.kind)))]
    pub async fn read_async<E, R, F>(&self, f: F) -> Result<R, E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static,
    {
        let mut conn = self
            .checkout_connection(self.read_semaphore.clone())
            .await?;

        let start = tokio::time::Instant::now();
        let span = tracing::info_span!("spawn_blocking");

        // Once sync code starts in the spawn_blocking it cannot be cancelled BUT if we've run out of threads to execute blocking work on then
        // this timeout should prevent the caller being blocked by this await that may not finish.
        tokio::time::timeout(std::time::Duration::from_millis(THREAD_ACQUIRE_TIMEOUT_MS.load(Ordering::Acquire)), tokio::task::spawn_blocking(move || {
                let _s = span.enter();
                log_elapsed!([10, 100, 1000], start, "read_async:before-closure");
                let r = conn.execute_in_read_txn(|mut txn| f(&Txn::from(&mut txn)));
                log_elapsed!([10, 100, 1000], start, "read_async:after-closure");
                r
            }).in_current_span()).in_current_span().await.map_err(|e| {
                tracing::error!("Failed to claim a thread to run the database read transaction. It's likely that the program is out of threads.");
                DatabaseError::Timeout(e)
            })?.map_err(DatabaseError::from)?
    }

    /// Intended to be used for transactions that need to be kept open for a longer period of time than just running a
    /// sequence of reads using `read_async`. You should default to `read_async` and only call this if you have a good
    /// reason.
    ///
    /// A valid reason for this is holding read transactions across multiple databases as part of a cascade query.
    #[cfg_attr(feature = "instrument", tracing::instrument)]
    pub async fn get_read_txn(&self) -> DatabaseResult<PTxnGuard> {
        let conn = self
            .checkout_connection(self.long_read_semaphore.clone())
            .await?;
        Ok(conn.into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument)]
    async fn checkout_connection(&self, semaphore: Arc<Semaphore>) -> DatabaseResult<PConnGuard> {
        // TODO: use semaphore for this message
        let waiting = self.num_readers.fetch_add(1, Ordering::Relaxed);
        if waiting > self.max_readers {
            let s = tracing::info_span!("holochain_perf", kind = ?self.kind().kind());
            s.in_scope(|| {
                tracing::info!(
                    "Database read connection is saturated. Util {:.2}%",
                    waiting as f64 / self.max_readers as f64 * 100.0
                )
            });
        } else {
            tracing::trace!("checkout_connection ready to acquire semaphore");
        }

        let permit = acquire_semaphore_permit(semaphore).await?;

        self.num_readers.fetch_sub(1, Ordering::Relaxed);

        let mut conn = self.get_connection_from_pool()?;
        if self.statement_trace_fn.is_some() {
            conn.trace(self.statement_trace_fn);
        }

        Ok(PConnGuard::new(conn, permit, self.use_time_metric.clone()))
    }

    /// Get a connection from the pool.
    /// TODO: We should eventually swap this for an async solution.
    #[cfg_attr(feature = "instrument", tracing::instrument)]
    fn get_connection_from_pool(&self) -> DatabaseResult<PConn> {
        let now = Instant::now();
        let r = Ok(PConn::new(self.connection_pool.get()?));
        let el = now.elapsed();
        if el.as_millis() > 20 {
            // TODO Convert to a metric
            tracing::info!("Connection pool took {:?} to be freed", el);
        } else {
            tracing::trace!("Got connection");
        }
        r
    }

    #[cfg(all(any(test, feature = "test_utils"), not(loom)))]
    pub fn test_read<R, F>(&self, f: F) -> R
    where
        F: FnOnce(&Txn<Kind>) -> R + Send + 'static,
        R: Send + 'static,
    {
        holochain_util::tokio_helper::block_forever_on(async {
            self.read_async(move |txn| -> DatabaseResult<R> { Ok(f(txn)) })
                .await
                .unwrap()
        })
    }
}

/// The canonical representation of a (singleton) database.
/// The wrapper contains methods for managing transactions
/// and database connections,
#[derive(Clone, Debug, Shrinkwrap, Into)]
pub struct DbWrite<Kind: DbKindT>(DbRead<Kind>);

impl<Kind: DbKindT + Send + Sync + 'static> DbWrite<Kind> {
    pub fn open_with_pool_config(
        path_prefix: &Path,
        kind: Kind,
        pool_config: PoolConfig,
    ) -> DatabaseResult<Self> {
        DATABASE_HANDLES.get_or_insert(&kind, path_prefix, |kind| {
            Self::new(Some(path_prefix), kind, pool_config, None)
        })
    }

    pub fn new(
        path_prefix: Option<&Path>,
        kind: Kind,
        pool_config: PoolConfig,
        statement_trace_fn: Option<fn(&str)>,
    ) -> DatabaseResult<Self> {
        let path = match path_prefix {
            Some(path_prefix) => {
                let path = path_prefix.join(kind.filename());
                let parent = path
                    .parent()
                    .ok_or_else(|| DatabaseError::DatabaseMissing(path_prefix.to_owned()))?;
                if !parent.is_dir() {
                    std::fs::create_dir_all(parent)
                        .map_err(|_e| DatabaseError::DatabaseMissing(parent.to_owned()))?;
                }
                // Check if the database is valid and take the appropriate
                // action if it isn't.
                match Self::check_database_file(&path, &pool_config) {
                    Ok(path) => path,
                    Err(err) => {
                        // Check if the database might be unencrypted.
                        if "true"
                            == std::env::var("HOLOCHAIN_MIGRATE_UNENCRYPTED")
                                .unwrap_or_default()
                                .as_str()
                        {
                            #[cfg(feature = "sqlite-encrypted")]
                            encrypt_unencrypted_database(&path, &pool_config)?;
                        }
                        // Check if this database kind requires wiping.
                        else if kind.if_corrupt_wipe() {
                            std::fs::remove_file(&path)?;
                        } else {
                            // If we don't wipe we need to return an error.
                            return Err(err.into());
                        }

                        // Now that we've taken the appropriate action we can try again.
                        match Self::check_database_file(&path, &pool_config) {
                            Ok(path) => path,
                            Err(e) => return Err(e.into()),
                        }
                    }
                }
            }
            None => None,
        };

        // Now we know the database file is valid we can open a connection pool.
        let pool = new_connection_pool(path.as_ref().map(|p| p.as_ref()), pool_config);
        let mut conn = pool.get()?;
        // set to faster write-ahead-log mode
        conn.pragma_update(None, "journal_mode", "WAL".to_string())?;
        crate::table::initialize_database(&mut conn, kind.kind())?;

        let use_time_metric = create_connection_use_time_metric(kind.kind());

        let db_read = DbRead {
            write_semaphore: Self::get_write_semaphore(kind.kind()),
            read_semaphore: Self::get_read_semaphore(kind.kind()),
            long_read_semaphore: Self::get_long_read_semaphore(kind.kind()),
            max_readers: num_read_threads() * 2,
            num_readers: Arc::new(AtomicUsize::new(0)),
            kind: kind.clone(),
            path: path.unwrap_or_default(),
            connection_pool: pool,
            statement_trace_fn,
            use_time_metric,
        };

        create_pool_usage_metric(
            kind.kind(),
            vec![
                db_read.write_semaphore.clone(),
                db_read.read_semaphore.clone(),
                db_read.long_read_semaphore.clone(),
            ],
        );

        Ok(DbWrite(db_read))
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(kind = ?self.kind)))]
    pub async fn write_async<E, R, F>(&self, f: F) -> Result<R, E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&mut Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static,
    {
        let permit = acquire_semaphore_permit(self.0.write_semaphore.clone()).await?;
        self.write_async_with_permit(permit, f)
            .await
            .map(|(r, _permit)| r)
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all, fields(kind = ?self.kind)))]
    pub async fn write_async_with_permit<E, R, F>(
        &self,
        permit: OwnedSemaphorePermit,
        f: F,
    ) -> Result<(R, OwnedSemaphorePermit), E>
    where
        E: From<DatabaseError> + Send + 'static,
        F: FnOnce(&mut Txn<Kind>) -> Result<R, E> + Send + 'static,
        R: Send + 'static,
    {
        let mut conn = self.get_connection_from_pool()?;

        let start = tokio::time::Instant::now();
        let span = tracing::info_span!("spawn_blocking");

        // Once sync code starts in the spawn_blocking it cannot be cancelled BUT if we've run out of threads to execute blocking work on then
        // this timeout should prevent the caller being blocked by this await that may not finish.
        tokio::time::timeout(std::time::Duration::from_millis(THREAD_ACQUIRE_TIMEOUT_MS.load(Ordering::Acquire)), tokio::task::spawn_blocking(move || {
            let _s = span.enter();
            log_elapsed!([10, 100, 1000], start, "write_async:before-closure");
            let r = conn.execute_in_exclusive_rw_txn(|txn| f(&mut Txn::from(txn)));
            log_elapsed!([10, 100, 1000], start, "write_async:after-closure");
            r.map(|r| (r, permit))
        }).in_current_span()).in_current_span().await.map_err(|e| {
            tracing::error!("Failed to claim a thread to run the database write transaction. It's likely that the program is out of threads.");
            DatabaseError::Timeout(e)
        })?.map_err(DatabaseError::from)?
    }

    /// Acquire the single write permit for the database.
    ///
    /// This will prevent any other writes from proceeding until the semaphore is released.
    /// It can be used to ensure that a read, followed by other operations, followed by another
    /// write, will not be interleaved with some other write.
    pub async fn acquire_write_permit(&self) -> DatabaseResult<OwnedSemaphorePermit> {
        acquire_semaphore_permit(self.0.write_semaphore.clone()).await
    }

    pub fn available_writer_count(&self) -> usize {
        self.write_semaphore.available_permits()
    }

    pub fn available_reader_count(&self) -> usize {
        self.read_semaphore.available_permits()
    }

    fn get_write_semaphore(kind: DbKind) -> Arc<Semaphore> {
        static MAP: once_cell::sync::Lazy<Mutex<HashMap<DbKind, Arc<Semaphore>>>> =
            once_cell::sync::Lazy::new(|| Mutex::new(HashMap::new()));
        MAP.lock()
            .entry(kind)
            .or_insert_with(|| Arc::new(Semaphore::new(1)))
            .clone()
    }

    fn get_read_semaphore(kind: DbKind) -> Arc<Semaphore> {
        static MAP: once_cell::sync::Lazy<Mutex<HashMap<DbKind, Arc<Semaphore>>>> =
            once_cell::sync::Lazy::new(|| Mutex::new(HashMap::new()));
        MAP.lock()
            .entry(kind)
            .or_insert_with(|| Arc::new(Semaphore::new(num_read_threads())))
            .clone()
    }

    fn get_long_read_semaphore(kind: DbKind) -> Arc<Semaphore> {
        static MAP: once_cell::sync::Lazy<Mutex<HashMap<DbKind, Arc<Semaphore>>>> =
            once_cell::sync::Lazy::new(|| Mutex::new(HashMap::new()));
        MAP.lock()
            .entry(kind)
            .or_insert_with(|| Arc::new(Semaphore::new(num_read_threads())))
            .clone()
    }

    fn check_database_file(
        path: &Path,
        pool_config: &PoolConfig,
    ) -> rusqlite::Result<Option<PathBuf>> {
        Connection::open(path)
            // For some reason calling pragma_update is necessary to prove the database file is valid.
            .and_then(|mut c| {
                initialize_connection(&mut c, pool_config)?;
                c.pragma_update(None, "synchronous", "0".to_string())?;
                Ok(c.path().map(PathBuf::from))
            })
    }

    /// Create a unique db in a temp dir with no static management of the
    /// connection pool, useful for testing.
    #[cfg(any(test, feature = "test_utils"))]
    pub fn test(path: &Path, kind: Kind) -> DatabaseResult<Self> {
        Self::new(Some(path), kind, PoolConfig::default(), None)
    }

    #[cfg(any(test, feature = "test_utils"))]
    pub fn test_in_mem(kind: Kind) -> DatabaseResult<Self> {
        Self::new(None, kind, PoolConfig::default(), None)
    }

    #[cfg(all(any(test, feature = "test_utils"), not(loom)))]
    pub fn test_write<R, F>(&self, f: F) -> R
    where
        F: FnOnce(&mut Txn<Kind>) -> R + Send + 'static,
        R: Send + 'static,
    {
        holochain_util::tokio_helper::block_forever_on(async {
            self.write_async(|txn| -> DatabaseResult<R> { Ok(f(txn)) })
                .await
                .unwrap()
        })
    }
}

// The method for this function is taken from https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868
#[cfg(feature = "sqlite-encrypted")]
pub fn encrypt_unencrypted_database(path: &Path, pool_config: &PoolConfig) -> DatabaseResult<()> {
    // e.g. conductor/conductor.sqlite3 -> conductor/conductor-encrypted.sqlite3
    let encrypted_path = path
        .parent()
        .ok_or_else(|| DatabaseError::DatabaseMissing(path.to_owned()))?
        .join(
            path.file_stem()
                .and_then(|s| s.to_str())
                .ok_or_else(|| DatabaseError::DatabaseMissing(path.to_owned()))?
                .to_string()
                + "-encrypted",
        );

    tracing::warn!(
        "Attempting encryption of unencrypted database: {:?} -> {:?}",
        path,
        encrypted_path
    );

    // Migrate the database
    {
        let conn = Connection::open(path)?;

        // Ensure everything in the WAL is written to the main database
        conn.execute("VACUUM", ())?;

        // Start an exclusive transaction to avoid anybody writing to the database while we're migrating it
        conn.execute("BEGIN EXCLUSIVE", ())?;

        {
            let lock = pool_config.key.unlocked.read_lock();
            conn.execute(
                "ATTACH DATABASE :db_name AS encrypted KEY :key",
                rusqlite::named_params! {
                    ":db_name": encrypted_path.to_str(),
                    // we have to pull out the hex encoded key
                    // with the x'..' but NOT the surrounding
                    // double quotes.
                    ":key": &lock[15..82],
                },
            )?;
        }

        let mut batch = "PRAGMA encrypted.cipher_salt = \"x'".to_string();
        for b in &*pool_config.key.salt.read_lock() {
            batch.push_str(&format!("{b:02X}"));
        }
        batch.push_str("'\";\n");
        batch.push_str("PRAGMA encrypted.cipher_compatibility = 4;\n");
        batch.push_str("PRAGMA encrypted.cipher_plaintext_header_size = 32;\n");

        conn.execute_batch(&batch)?;

        conn.query_row("SELECT sqlcipher_export('encrypted')", (), |_| Ok(0))?;

        conn.execute("COMMIT", ())?;

        conn.execute("DETACH DATABASE encrypted", ())?;
        conn.close().map_err(|(_, err)| err)?;
    }

    // Swap the databases over
    std::fs::remove_file(path)?;
    std::fs::rename(encrypted_path, path)?;

    Ok(())
}

#[cfg(feature = "test_utils")]
pub fn set_acquire_timeout(timeout_ms: u64) {
    ACQUIRE_TIMEOUT_MS.store(timeout_ms, Ordering::Relaxed);
}

#[cfg_attr(feature = "instrument", tracing::instrument)]
