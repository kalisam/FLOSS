    }

    fn handle_query_op_hashes(
        &mut self,
        _input: QueryOpHashesEvt,
    ) -> KitsuneP2pEventHandlerResult<Option<(Vec<Arc<super::KitsuneOpHash>>, TimeWindowInclusive)>>
    {
        let hashes: Vec<Arc<super::KitsuneOpHash>> = self.gossip_store.keys().cloned().collect();
        let slug_hashes: Vec<Slug> = hashes.iter().map(|h| h.into()).collect();
        tracing::trace!(?slug_hashes, "FETCH_OP_HASHES");
        Ok(
            async move { Ok(Some((hashes, full_time_window_inclusive()))) }
                .boxed()
                .into(),
        )
    }

    fn handle_fetch_op_data(
        &mut self,
        input: FetchOpDataEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<(Arc<super::KitsuneOpHash>, KOp)>> {
        let mut out = Vec::new();
        match input.query {
            FetchOpDataEvtQuery::Hashes {
                op_hash_list: hashes,
                ..
            } => {
                for hash in hashes {
                    if let Some(op) = self.gossip_store.get(&hash) {
                        let data = KitsuneOpData::new(op.clone().into_bytes());
                        out.push((hash.clone(), data));
                    }
                }
            }
            FetchOpDataEvtQuery::Regions(_coords) => unimplemented!(),
        }
        Ok(async move { Ok(out) }.boxed().into())
    }

    fn handle_sign_network_data(
        &mut self,
        input: SignNetworkDataEvt,
    ) -> KitsuneP2pEventHandlerResult<KitsuneSignature> {
        let sig = sodoken::BufWriteSized::new_no_lock();
        let fut = sodoken::sign::detached(sig.clone(), input.data.to_vec(), self.priv_key.clone());
        Ok(async move {
            fut.await.map_err(KitsuneP2pError::other)?;
            let sig = sig.read_lock().to_vec();
            Ok(sig.into())
        }
        .boxed()
        .into())
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util/harness_event.rs
================================================
use super::*;
use futures::sink::SinkExt;

/// a small debug representation of another type
#[derive(Clone, PartialEq)]
pub struct Slug(String);

impl std::fmt::Debug for Slug {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

macro_rules! q_slug_from {
    ($($t:ty => |$i:ident| $c:block,)*) => {$(
        impl From<$t> for Slug {
            fn from(f: $t) -> Self {
                Slug::from(&f)
            }
        }

        impl From<&$t> for Slug {
            fn from(f: &$t) -> Self {
                let $i = f;
                Self($c)
            }
        }
    )*};
}

q_slug_from! {
    Arc<KitsuneSpace> => |s| {
        let f = format!("{:?}", s);
        format!("s{}", &f[13..25])
    },
    Arc<KitsuneAgent> => |s| {
        let f = format!("{:?}", s);
        format!("a{}", &f[13..25])
    },
    Arc<KitsuneOpHash> => |s| {
        let f = format!("{:?}", s);
        format!("o{}", &f[13..25])
    },
}

/// an event type for an event emitted by the test suite harness
#[derive(Clone, Debug)]
pub enum HarnessEventType {
    Close,
    Join {
        space: Slug,
        agent: Slug,
    },
    StoreAgentInfo {
        agent: Slug,
        agent_info: Arc<AgentInfoSigned>,
    },
    Call {
        space: Slug,
        to_agent: Slug,
        payload: String,
    },
    Notify {
        space: Slug,
        to_agent: Slug,
        payload: String,
    },
    Gossip {
        op_hash: Slug,
        op_data: String,
    },
}

/// an event emitted by the test suite harness
#[derive(Clone, Debug)]
pub struct HarnessEvent {
    /// the nickname of the node emitting the event
    pub nick: Arc<String>,

    /// the event type
    pub ty: HarnessEventType,
}

/// a harness event channel prioritizing use ergonomics over efficiency
/// this one struct is either sender / receiver depending on what
/// fns you invoke : ) ... clone all you like
#[derive(Clone)]
pub struct HarnessEventChannel {
    nick: Arc<String>,
    chan: tokio::sync::broadcast::Sender<HarnessEvent>,
}

impl HarnessEventChannel {
    /// constructor for a new harness event channel
    pub fn new(nick: impl AsRef<str>) -> Self {
        let (chan, mut trace_recv) = tokio::sync::broadcast::channel(10);

        // we need an active dummy recv or the sends will error
        tokio::task::spawn(async move {
            while let Ok(evt) = trace_recv.recv().await {
                let HarnessEvent { nick, ty } = evt;
                const T: &str = "HARNESS_EVENT";
                tracing::debug!(
                    %T,
                    %nick,
                    ?ty,
                );
                if let HarnessEventType::Close = ty {
                    return;
                }
            }
        });

        Self {
            nick: Arc::new(nick.as_ref().to_string()),
            chan,
        }
    }

    /// clone this channel, but append a nickname segment to the messages
    pub fn sub_clone(&self, sub_nick: impl AsRef<str>) -> Self {
        let mut new_nick = (*self.nick).clone();
        if !new_nick.is_empty() {
            new_nick.push('.');
        }
        new_nick.push_str(sub_nick.as_ref());
        Self {
            nick: Arc::new(new_nick),
            chan: self.chan.clone(),
        }
    }

    /// close this channel.
    pub fn close(&self) {
        self.publish(HarnessEventType::Close);
    }

    /// break off a broadcast receiver. this receiver will not get historical
    /// messages... only those that are emitted going forward
    pub fn receive(&self) -> impl tokio_stream::Stream<Item = HarnessEvent> {
        let (mut s, r) = futures::channel::mpsc::channel(10);
        let mut chan = self.chan.subscribe();
        tokio::task::spawn(async move {
            while let Ok(msg) = chan.recv().await {
                let is_close = matches!(&msg.ty, HarnessEventType::Close);
                if s.send(msg).await.is_err() {
                    break;
                }
                if is_close {
                    break;
                }
            }
            s.close_channel();
        });
        r
    }

    /// publish a harness event to all receivers
    pub fn publish(&self, ty: HarnessEventType) {
        let _ = self.chan.send(HarnessEvent {
            nick: self.nick.clone(),
            ty,
        });
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/test_util/scenario_def_local.rs
================================================
use crate::*;
use std::sync::Arc;

/// Concise representation of data held by various Agents on a single Node,
/// without having to refer to explicit op hashes or locations.
///
/// This type is a simplification of `ScenarioDef`, making the definition of
/// single-node scenarios even more concise. Rather than referring to op
/// locations directly, this type refers to ops (and their locations) by *index*.
/// The ops are generated by [`mock_agent_persistence`] and guaranteed to be
/// in ascending order by DHT location. Therefore this type is agnostic of the
/// actual location of anything, and only cares about "arc topology".
///
/// It's expected that we'll eventually have a small library of these scenarios,
/// defined in terms of this type.
///
/// See [`mock_agent_persistence`] for usage detail.
///
/// A bit of a historical note: this type was created before `ScenarioDef`, and
/// if it were the other way around, perhaps this type wouldn't exist. But it's
/// still nice how concise it is.
// TODO: can this somehow be unified with `ScenarioDef`?
pub struct LocalScenarioDef {
    /// Total number of op hashes to be generated
    pub total_ops: usize,
    /// Declares arcs and ownership in terms of indices into a vec of generated op hashes.
    pub agents: Vec<LocalScenarioDefAgent>,
}

impl LocalScenarioDef {
    /// Construct this type from a compact "untagged" format using
    /// tuples instead of structs. This is intended to be the canonical constructor.
    pub fn from_compact(total_ops: usize, v: Vec<LocalScenarioDefAgentCompact>) -> Self {
        Self {
            total_ops,
            agents: v
                .into_iter()
                .map(|(agent, arc_indices, hash_indices)| LocalScenarioDefAgent {
                    agent,
                    arc_indices,
                    hash_indices,
                })
                .collect(),
        }
    }
}

/// Declares arcs and ownership in terms of indices into a vec of generated op hashes.
pub struct LocalScenarioDefAgent {
    /// The agent in question
    pub agent: Arc<KitsuneAgent>,
    /// The start and end op indices of the arc for this agent
    pub arc_indices: (usize, usize),
    /// The indices of ops to consider as owned
    pub hash_indices: Vec<usize>,
}

/// Same as [`LocalScenarioDefAgent`], but using a tuple instead of a struct.
/// It's just more compact.
pub type LocalScenarioDefAgentCompact = (Arc<KitsuneAgent>, (usize, usize), Vec<usize>);



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types/actor.rs
================================================
//! Definitions related to the KitsuneP2p peer-to-peer / dht communications actor.

use kitsune_p2p_types::config::KitsuneP2pTuningParams;
use kitsune_p2p_types::KitsuneTimeout;
use std::sync::Arc;

use crate::gossip::sharded_gossip::KitsuneDiagnostics;

/// Make a request to multiple destination agents - awaiting/aggregating the responses.
/// The remote sides will see these messages as "RequestEvt" events.
#[derive(Clone, Debug)]
pub struct RpcMulti {
    /// The "space" context.
    pub space: Arc<super::KitsuneSpace>,

    /// The "basis" hash/coordinate of destination neigborhood.
    pub basis: Arc<super::KitsuneBasis>,

    /// Request data.
    pub payload: Vec<u8>,

    /// Max number of remote requests to make
    pub max_remote_agent_count: u8,

    /// Max timeout for aggregating response data
    pub max_timeout: KitsuneTimeout,

    /// Remote request grace period.
    /// If we already have results from other sources,
    /// but made any additional outgoing remote requests,
    /// we'll wait at least this long for additional responses.
    pub remote_request_grace_ms: u64,
}

impl RpcMulti {
    /// Construct a new RpcMulti input struct
    /// with timing defaults specified by tuning_params.
    pub fn new(
        tuning_params: &KitsuneP2pTuningParams,
        space: Arc<super::KitsuneSpace>,
        basis: Arc<super::KitsuneBasis>,
        payload: Vec<u8>,
    ) -> Self {
        Self {
            space,
            basis,
            payload,
            max_remote_agent_count: tuning_params.default_rpc_multi_remote_agent_count,
            max_timeout: tuning_params.implicit_timeout(),
            remote_request_grace_ms: tuning_params.default_rpc_multi_remote_request_grace_ms,
        }
    }
}

/// A response type helps indicate what agent gave what response.
#[derive(Clone, Debug)]
pub struct RpcMultiResponse {
    /// The agent that gave this response.
    pub agent: Arc<super::KitsuneAgent>,
    /// Response data.
    pub response: Vec<u8>,
}

/// Data to broadcast to the remote.
#[derive(Debug, Clone, PartialEq, Eq, serde::Deserialize, serde::Serialize)]
#[serde(tag = "type", content = "value", rename_all = "camelCase")]
pub enum BroadcastData {
    /// User broadcast.
    User(#[serde(with = "serde_bytes")] Vec<u8>),

    /// Agent info.
    AgentInfo(kitsune_p2p_types::agent_info::AgentInfoSigned),

    /// Publish broadcast.
    Publish {
        /// Source (origin) agent that sent this publish.
        source: Arc<super::KitsuneAgent>,

        /// Method used to send this publish.
        transfer_method: kitsune_p2p_fetch::TransferMethod,

        /// List of hashes being published.
        op_hash_list: Vec<kitsune_p2p_fetch::OpHashSized>,

        /// Context associated with this publish.
        context: kitsune_p2p_fetch::FetchContext,
    },
}

type KSpace = Arc<super::KitsuneSpace>;
type KSpaceOpt = Option<Arc<super::KitsuneSpace>>;
type KAgent = Arc<super::KitsuneAgent>;
type KAgents = Vec<Arc<super::KitsuneAgent>>;
type KBasis = Arc<super::KitsuneBasis>;
type Payload = Vec<u8>;
type OptU64 = Option<u64>;
type OptArq = Option<crate::dht::Arq>;

ghost_actor::ghost_chan! {
    /// The KitsuneP2pSender allows async remote-control of the KitsuneP2p actor.
    pub chan KitsuneP2p<super::KitsuneP2pError> {
        /// Announce a space/agent pair on this network.
        fn join(space: KSpace, agent: KAgent, maybe_agent_info: Option<kitsune_p2p_types::agent_info::AgentInfoSigned>, initial_arq: OptArq) -> ();

        /// Withdraw this space/agent pair from this network.
        fn leave(space: KSpace, agent: KAgent) -> ();

        /// Make a request of a single remote agent, expecting a response.
        /// The remote side will receive a "Call" event.
        fn rpc_single(space: KSpace, to_agent: KAgent, payload: Payload, timeout_ms: OptU64) -> Vec<u8>;

        /// Make a request to multiple destination agents - awaiting/aggregating the responses.
        /// The remote sides will see these messages as "Call" events.
        /// NOTE: We've currently disabled the "multi" part of this.
        /// It will still pick appropriate peers by basis, but will only
        /// make requests one at a time, returning the first success.
        fn rpc_multi(input: RpcMulti) -> Vec<RpcMultiResponse>;

        /// Publish data to a "neighborhood" of remote nodes surrounding the
        /// "basis" hash. This is a multi-step fire-and-forget algorithm.
        /// An Ok(()) result only means that we were able to establish at
        /// least one connection with a node in the target neighborhood.
        /// The remote sides will see these messages as "Notify" events.
        fn broadcast(
            space: KSpace,
            basis: KBasis,
            timeout: KitsuneTimeout,
            data: BroadcastData,
        ) -> ();

        /// Broadcast data to a specific set of agents without
        /// expecting a response.
        /// An Ok(()) result only means that we were able to establish at
        /// least one connection with a node in the agent set.
        fn targeted_broadcast(
            space: KSpace,
            agents: KAgents,
            timeout: KitsuneTimeout,
            payload: Payload,

            // If we have reached the maximum concurrent notify requests limit
            // (specified by tuning param `concurrent_limit_per_thread`)
            // This message will be dropped / not sent, but still return an
            // Ok() response.
            drop_at_limit: bool,
        ) -> ();

        /// New data has been integrated and is ready for gossiping.
        fn new_integrated_data(space: KSpace) -> ();

        /// Check if an agent is an authority for a hash.
        fn authority_for_hash(
            space: KSpace,
            basis: KBasis,
        ) -> bool;

        /// dump network metrics
        fn dump_network_metrics(
            space: KSpaceOpt,
        ) -> serde_json::Value;

        /// dump network stats
        fn dump_network_stats() -> serde_json::Value;

        /// Get data for diagnostics
        fn get_diagnostics(space: KSpace) -> KitsuneDiagnostics;

        /// Get the storage arcs for the agents currently in this space
        fn storage_arcs(space: KSpace) -> Vec<kitsune2_api::DhtArc>;
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types/event.rs
================================================
//! Definitions for events emited from the KitsuneP2p actor.

use crate::dht::prelude::ArqSet;
use crate::types::agent_store::AgentInfoSigned;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::{
    bin_types::KOp,
    dht::region::RegionBounds,
    dht_arc::{DhtArcSet, DhtLocation},
    KOpHash,
};
use std::{collections::HashSet, sync::Arc};

/// Gather a list of op-hashes from our implementor that meet criteria.
/// Also get the start and end times for ops within a time window
/// up to a maximum number.
#[derive(Debug, Clone)]
pub struct QueryOpHashesEvt {
    /// The "space" context.
    pub space: KSpace,
    /// The DhtArcSet to filter by.
    pub arc_set: DhtArcSet,
    /// The time window to search within.
    pub window: TimeWindow,
    /// Maximum number of ops to return.
    pub max_ops: usize,
    /// Include ops that are still in limbo (not yet validated or integrated).
    pub include_limbo: bool,
}

/// Gather all op-hash data for a list of op-hashes from our implementor.
#[derive(Debug, Clone)]
pub struct FetchOpDataEvt {
    /// The "space" context.
    pub space: KSpace,
    /// The criteria to query by
    pub query: FetchOpDataEvtQuery,
}

/// Multiple ways to fetch op data
#[derive(Debug, derive_more::From, Clone)]
pub enum FetchOpDataEvtQuery {
    /// Fetch all ops with the hashes specified
    Hashes {
        /// list of ops to fetch
        op_hash_list: Vec<KOpHash>,

        /// should we include limbo ops
        include_limbo: bool,
    },

    /// Fetch all ops within the time and space bounds specified
    Regions(Vec<RegionBounds>),
}

/// Request that our implementor sign some data on behalf of an agent.
#[derive(Debug, Clone)]
pub struct SignNetworkDataEvt {
    /// The "space" context.
    pub space: KSpace,
    /// The "agent" context.
    pub agent: KAgent,
    /// The data to sign.
    #[allow(clippy::rc_buffer)]
    pub data: Arc<Vec<u8>>,
}

/// Store the AgentInfo as signed by the agent themselves.
#[derive(Debug, Clone)]
pub struct PutAgentInfoSignedEvt {
    /// A batch of signed agent info. Possibly from multiple spaces, see the space included
    /// on each agent.
    pub peer_data: Vec<AgentInfoSigned>,
}

/// Get agent info for a single agent, as previously signed and put.
#[derive(Debug)]
pub struct GetAgentInfoSignedEvt {
    /// The "space" context.
    pub space: KSpace,
    /// The "agent" context.
    pub agent: KAgent,
}

/// Get agent info which satisfies a query.
#[derive(Debug, Clone)]
pub struct QueryAgentsEvt {
    /// The "space" context.
    pub space: KSpace,
    /// Optional set of agents to filter by.
    pub agents: Option<HashSet<KAgent>>,
    /// Optional time range to filter by.
    pub window: Option<TimeWindow>,
    /// Optional arcset to intersect by.
    pub arq_set: Option<ArqSet>,
    /// If set, results are ordered by proximity to the specified location
    pub near_basis: Option<DhtLocation>,
    /// Limit to the number of results returned
    pub limit: Option<u32>,
}

// NB: if we want to play it safer, rather than providing these fine-grained
//     builder methods, we could provide only the three "flavors" of query that
//     Holochain supports, which would still provide us the full expressivity to
//     implement Kitsune.
impl QueryAgentsEvt {
    /// Constructor. Every query needs to know what space it's for.
    pub fn new(space: KSpace) -> Self {
        Self {
            space,
            agents: None,
            window: None,
            arq_set: None,
            near_basis: None,
            limit: None,
        }
    }

    /// Add in an agent list query
    pub fn by_agents<A: IntoIterator<Item = KAgent>>(mut self, agents: A) -> Self {
        self.agents = Some(agents.into_iter().collect());
        self
    }

    /// Add in an a time window query
    pub fn by_window(mut self, window: TimeWindow) -> Self {
        self.window = Some(window);
        self
    }

    /// Add in an an arcset query
    pub fn by_arq_set(mut self, arq_set: ArqSet) -> Self {
        self.arq_set = Some(arq_set);
        self
    }

    /// Specify that the results should be ordered by proximity to this basis
    pub fn near_basis(mut self, basis: DhtLocation) -> Self {
        self.near_basis = Some(basis);
        self
    }

    /// Limit the number of results
    pub fn limit(mut self, limit: u32) -> Self {
        self.limit = Some(limit);
        self
    }
}

/// An exclusive range of timestamps, measured in microseconds
pub type TimeWindow = std::ops::Range<Timestamp>;

/// An inclusive range of timestamps, measured in microseconds
pub type TimeWindowInclusive = std::ops::RangeInclusive<Timestamp>;

/// A time window which covers all of recordable time
pub fn full_time_window() -> TimeWindow {
    Timestamp::MIN..Timestamp::MAX
}

/// A time window which inclusively covers all of recordable time
pub fn full_time_window_inclusive() -> TimeWindowInclusive {
    Timestamp::MIN..=Timestamp::MAX
}

type KSpace = Arc<super::KitsuneSpace>;
type KAgent = Arc<super::KitsuneAgent>;
pub(crate) type Payload = Vec<u8>;
type Ops = Vec<KOp>;
type MaybeContext = Option<kitsune_p2p_fetch::FetchContext>;

ghost_actor::ghost_chan! {
    /// The KitsuneP2pEvent stream allows handling events generated from the
    /// KitsuneP2p actor.
    pub chan KitsuneP2pEvent<super::KitsuneP2pError> {

        /// We need to store signed agent info.
        fn put_agent_info_signed(input: PutAgentInfoSignedEvt) -> Vec<kitsune_p2p_types::bootstrap::AgentInfoPut>;

        /// We need to get previously stored agent info.
        fn query_agents(input: QueryAgentsEvt) -> Vec<crate::types::agent_store::AgentInfoSigned>;

        /// Query the peer density of a space for a given [`DhtArc`].
        fn query_peer_density(space: KSpace, dht_arc: kitsune_p2p_types::dht_arc::DhtArc) -> kitsune_p2p_types::dht::PeerView;

        /// We are receiving a request from a remote node.
        fn call(space: KSpace, to_agent: KAgent, payload: Payload) -> Vec<u8>;

        /// We are receiving a notification from a remote node.
        fn notify(space: KSpace, to_agent: KAgent, payload: Payload) -> ();

        /// We have received ops to be integrated,
        /// either through gossip or publish.
        fn receive_ops(
            space: KSpace,
            ops: Ops,
            context: MaybeContext,
        ) -> ();

        /// Gather a list of op-hashes from our implementor that meet criteria.
        /// Get the oldest and newest times for ops within a time window and max number of ops.
        // maackle: do we really need to *individually* wrap all these op hashes in Arcs?
        fn query_op_hashes(input: QueryOpHashesEvt) -> Option<(Vec<KOpHash>, TimeWindowInclusive)>;

        /// Gather all op-hash data for a list of op-hashes from our implementor.
        fn fetch_op_data(input: FetchOpDataEvt) -> Vec<(KOpHash, KOp)>;

        /// Request that our implementor sign some data on behalf of an agent.
        fn sign_network_data(input: SignNetworkDataEvt) -> super::KitsuneSignature;
    }
}

/// Receiver type for incoming connection events.
pub type KitsuneP2pEventReceiver = futures::channel::mpsc::Receiver<KitsuneP2pEvent>;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types/gossip.rs
================================================
use crate::meta_net::*;
use crate::metrics::*;
use crate::types::*;
use crate::HostApiLegacy;
use kitsune_p2p_fetch::FetchPool;
use kitsune_p2p_types::config::*;
use kitsune_p2p_types::*;
use std::sync::Arc;

/// The type of gossip module running this gossip.
#[derive(Clone, Debug, Copy, serde::Serialize, serde::Deserialize, PartialEq, Eq, Hash)]
pub enum GossipModuleType {
    /// Recent sharded gossip.
    ShardedRecent,
    /// Historical sharded gossip.
    ShardedHistorical,
}

/// Represents an interchangeable gossip strategy module
pub trait AsGossipModule: 'static + Send + Sync {
    fn close(&self);
    fn incoming_gossip(
        &self,
        con: crate::meta_net::MetaNetCon,
        remote_url: String,
        gossip_data: Box<[u8]>,
    ) -> KitsuneResult<()>;
    fn local_agent_join(&self, a: Arc<KitsuneAgent>);
    fn local_agent_leave(&self, a: Arc<KitsuneAgent>);
    fn new_integrated_data(&self) {}
}

#[derive(Clone)]
pub struct GossipModule(pub Arc<dyn AsGossipModule>);

impl GossipModule {
    pub fn close(&self) {
        self.0.close()
    }

    pub fn incoming_gossip(
        &self,
        con: crate::meta_net::MetaNetCon,
        remote_url: String,
        gossip_data: Box<[u8]>,
    ) -> KitsuneResult<()> {
        self.0.incoming_gossip(con, remote_url, gossip_data)
    }

    pub fn local_agent_join(&self, a: Arc<KitsuneAgent>) {
        self.0.local_agent_join(a);
    }

    pub fn local_agent_leave(&self, a: Arc<KitsuneAgent>) {
        self.0.local_agent_leave(a);
    }

    /// New data has been integrated and is ready for gossiping.
    pub fn new_integrated_data(&self) {
        self.0.new_integrated_data();
    }
}

impl std::fmt::Debug for GossipModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("GossipModule").finish()
    }
}

/// Represents an interchangeable gossip strategy module factory
pub trait AsGossipModuleFactory: 'static + Send + Sync {
    #[allow(clippy::too_many_arguments)]
    fn spawn_gossip_task(
        &self,
        config: Arc<KitsuneP2pConfig>,
        space: Arc<KitsuneSpace>,
        ep_hnd: MetaNet,
        host: HostApiLegacy,
        metrics: MetricsSync,
        fetch_pool: FetchPool,
    ) -> GossipModule;
}

pub struct GossipModuleFactory(pub Arc<dyn AsGossipModuleFactory>);

impl GossipModuleFactory {
    #[allow(clippy::too_many_arguments)]
    pub fn spawn_gossip_task(
        &self,
        config: Arc<KitsuneP2pConfig>,
        space: Arc<KitsuneSpace>,
        ep_hnd: MetaNet,
        host: HostApiLegacy,
        metrics: MetricsSync,
        fetch_pool: FetchPool,
    ) -> GossipModule {
        self.0
            .spawn_gossip_task(config, space, ep_hnd, host, metrics, fetch_pool)
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types/metrics.rs
================================================
use ghost_actor::dependencies::tracing;

holochain_trace::metrics!(
    KitsuneMetrics,
    Failure,
    Call,
    CallResp,
    Notify,
    NotifyResp,
    Gossip,
    PeerGet,
    PeerGetResp,
    PeerQuery,
    PeerQueryResp
);

/// Print all metrics as tracing events
#[cfg_attr(feature = "instrument", tracing::instrument)]
#[allow(dead_code)]
pub fn print_all_metrics() {
    if holochain_trace::metrics::is_enabled() {
        use std::fmt::Write;
        use KitsuneMetrics::*;
        let mut out = String::new();
        writeln!(
            out,
            "\n**************************\n* Kitsune Metrics Report *\n**************************\n",
        )
        .expect("Failed to print metrics");
        for (metric, count) in KitsuneMetrics::iter() {
            match metric {
                Call | Notify | Gossip | PeerGet | PeerQuery => {
                    writeln!(
                        out,
                        "metric: {:?} {}Bytes {:.4}MB",
                        metric,
                        count,
                        count as f64 / 1_000_000.0,
                    )
                    .expect("Failed to print metrics");
                }
                Failure | CallResp | NotifyResp | PeerGetResp | PeerQueryResp => {
                    writeln!(
                        out,
                        "metric: {:?} {}Bytes {:.4}MB",
                        metric,
                        count,
                        count as f64 / 1_000_000.0,
                    )
                    .expect("Failed to print metrics");
                }
            }
        }
        tracing::trace!(metric = %out);
    }
}

/// Turn on metrics if `KITSUNE_METRICS=ON`
pub fn init() {
    if let Some(km) = std::env::var_os("KITSUNE_METRICS") {
        if km == "ON" {
            holochain_trace::metrics::init();
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/types/wire.rs
================================================
//! KitsuneP2p Wire Protocol Encoding Decoding

use crate::actor::BroadcastData;
use crate::agent_store::AgentInfoSigned;
use crate::types::*;
use derive_more::*;
use kitsune_p2p_fetch::FetchKey;
use kitsune_p2p_types::dht_arc::DhtLocation;
use std::sync::Arc;

/// Type used for content data of wire messages.
#[derive(
    Clone, PartialEq, Eq, Hash, Deref, AsRef, From, Into, serde::Serialize, serde::Deserialize,
)]
pub struct WireData(#[serde(with = "serde_bytes")] pub Vec<u8>);

impl std::fmt::Debug for WireData {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "<{} bytes>", self.0.len())
    }
}

/// Enum containing the individual metric exchange messages used by clients
#[derive(Debug, Clone, PartialEq, Eq, serde::Deserialize, serde::Serialize)]
#[serde(tag = "type", rename_all = "camelCase")]
pub enum MetricExchangeMsg {
    /// To start off, let's use a naive single message sending
    /// everything we care about.
    V1UniBlast {
        /// The extrapolated coverage calculated by this node
        /// note this is NOT the aggregate the node has collected,
        /// just the direct extrapolation based on known peer infos.
        extrap_cov_f32_le: WireData,
    },

    /// Future proof by having an unknown message catch-all variant
    /// that we can ignore for any future variants that are added
    #[serde(other)]
    UnknownMessage,
}

/// An individual op item within a "PushOpData" wire message.
#[derive(Debug, Clone, PartialEq, Eq, serde::Deserialize, serde::Serialize)]
#[serde(tag = "type", rename_all = "camelCase")]
pub struct PushOpItem {
    /// The payload of this op.
    pub op_data: Arc<KitsuneOpData>,

    /// If this op is a response to a "region" request,
    /// includes the region coords and a bool that, if true,
    /// indicates this is the final op in the region list.
    /// NOTE: we may want to just ignore this bool, as out-of-order
    /// messages could lead us to ignore valid ops coming in for the region.
    pub region: Option<(dht::prelude::RegionCoords, bool)>,
}

kitsune_p2p_types::write_codec_enum! {
    /// KitsuneP2p Wire Protocol Top-Level Enum.
    codec Wire {
        /// Failure
        Failure(0x00) {
            reason.0: String,
        },

        /// "Call" to the remote.
        Call(0x10) {
            space.0: Arc<KitsuneSpace>,
            to_agent.1: Arc<KitsuneAgent>,
            data.2: WireData,
        },

        /// "Call" response from the remote.
        CallResp(0x11) {
            data.0: WireData,
        },

        /// "DelegateBroadcast" to the remote.
        /// Remote should in turn connect to nodes in neighborhood,
        /// and call "Notify" per broadcast algorithm.
        /// uses low-level notify, not request
        DelegateBroadcast(0x22) {
            space.0: Arc<KitsuneSpace>,
            basis.1: Arc<KitsuneBasis>,
            to_agent.2: Arc<KitsuneAgent>,

            /// If `tgt_agent.get_loc() % mod_cnt == mod_idx`,
            /// we are responsible for broadcasting to tgt_agent.
            mod_idx.3: u32,

            /// see mod_idx description
            mod_cnt.4: u32,

            data.5: BroadcastData,
        },

        /// Fire-and-forget broadcast message.
        /// uses low-level notify, not request
        Broadcast(0x23) {
            space.0: Arc<KitsuneSpace>,
            to_agent.1: Arc<KitsuneAgent>,
            data.2: BroadcastData,
        },

        /// Gossip op with opaque data section,
        /// to be forwarded to gossip module.
        /// uses low-level notify, not request
        Gossip(0x42) {
            space.0: Arc<KitsuneSpace>,
            data.1: WireData,
            module.2: gossip::GossipModuleType,
        },

        /// Ask a remote node if they know about a specific agent
        PeerGet(0x50) {
            space.0: Arc<KitsuneSpace>,
            agent.1: Arc<KitsuneAgent>,
        },

        /// Response to a peer get. If the agent isn't known, None will be returned.
        PeerGetResp(0x51) {
            agent_info_signed.0: Option<AgentInfoSigned>,
        },

        /// Query a remote node for peers holding
        /// or nearest to holding a u32 location.
        PeerQuery(0x52) {
            space.0: Arc<KitsuneSpace>,
            basis_loc.1: DhtLocation,
        },

        /// Response to a peer query. May be empty if no matching agents are known.
        PeerQueryResp(0x53) {
            peer_list.0: Vec<AgentInfoSigned>,
        },

        /// Nodes can just send peer info without prompting.
        /// Notably they may want to send their own peer info to prevent being
        /// inadvertantly blocked.
        PeerUnsolicited(0x54) {
            peer_list.0: Vec<AgentInfoSigned>,
        },

        /// Request the peer send op data.
        /// This is sent as a fire-and-forget Notify message.
        /// The "response" is "PushOpData" below.
        FetchOp(0x60) {
            fetch_list.0: Vec<(Arc<KitsuneSpace>, Vec<FetchKey>)>,
        },

        /// This is a fire-and-forget "response" to the
        /// fire-and-forget "FetchOp" request, also sent via Notify.
        PushOpData(0x61) {
            op_data_list.0: Vec<(Arc<KitsuneSpace>, Vec<PushOpItem>)>,
        },

        /// MetricsExchangeMessage
        MetricExchange(0xa0) {
            space.0: Arc<KitsuneSpace>,
            msgs.1: Vec<MetricExchangeMsg>,
        },
    }
}

impl Wire {
    pub fn maybe_space(&self) -> Option<Arc<KitsuneSpace>> {
        match self {
            Wire::Call(Call { space, .. })
            | Wire::DelegateBroadcast(DelegateBroadcast { space, .. })
            | Wire::Broadcast(Broadcast { space, .. })
            | Wire::Gossip(Gossip { space, .. })
            | Wire::PeerGet(PeerGet { space, .. })
            | Wire::PeerQuery(PeerQuery { space, .. })
            | Wire::MetricExchange(MetricExchange { space, .. }) => Some(space.clone()),
            Wire::Failure(_)
            | Wire::CallResp(_)
            | Wire::PeerGetResp(_)
            | Wire::PeerQueryResp(_)
            | Wire::FetchOp(_)
            | Wire::PushOpData(_)
            | Wire::PeerUnsolicited(_) => None,
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/peer_restart.rs
================================================
mod common;

use std::sync::Arc;

use base64::Engine;
use serde_json::Value;

use common::*;
use fixt::prelude::*;
use kitsune_p2p::actor::KitsuneP2pSender;
use kitsune_p2p::fixt::KitsuneSpaceFixturator;

// When Kitsune restarts, it will create a new connection to the signal server. That means a new
// peer URL will be distributed with the agent info. The connection via the old peer URL should be
// closed by other peers when they get the new agent info.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn connection_close_on_peer_restart() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_online = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr);

    let sender_online = harness_online
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));
    let agent_online = harness_online.create_agent().await;

    sender_online
        .join(space.clone(), agent_online.clone(), None, None)
        .await
        .unwrap();

    let mut harness_restart = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr);

    let sender_restart = harness_restart
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_restart = harness_restart.create_agent().await;

    sender_restart
        .join(space.clone(), agent_restart.clone(), None, None)
        .await
        .unwrap();

    // Sent a message to make sure that the connection is established
    wait_for_connected(sender_online.clone(), agent_restart.clone(), space.clone()).await;

    // Wait until the connection is found
    tokio::time::timeout(std::time::Duration::from_secs(15), {
        let sender_online = sender_online.clone();
        async move {
            loop {
                let dump = sender_online.dump_network_stats().await.unwrap();

                let connection_count = connection_ids_from_dump(&dump).len();
                if connection_count == 1 {
                    break;
                }

                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    let dump = sender_online.dump_network_stats().await.unwrap();
    let initial_connection_ids = connection_ids_from_dump(&dump);
    assert_eq!(1, initial_connection_ids.len());

    let initial_connection_id = initial_connection_ids[0].clone();

    // Restart the node
    let sender_restart = harness_restart
        .simulated_restart(sender_restart)
        .await
        .expect("should be able to spawn node");

    // Rejoin the space following the restart
    sender_restart
        .join(space.clone(), agent_restart.clone(), None, None)
        .await
        .unwrap();

    // Wait until there is a new connection and the old one is closed
    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let initial_connection_id = initial_connection_id.clone();
        let sender_online = sender_online.clone();
        async move {
            loop {
                let dump = sender_online.dump_network_stats().await.unwrap();

                let connection_ids = connection_ids_from_dump(&dump);

                let find_existing = connection_ids
                    .iter()
                    .find(|&id| id == &initial_connection_id);
                let connection_count = connection_ids.len();
                if find_existing.is_none() && connection_count == 1 {
                    break;
                }

                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    let dump = sender_online.dump_network_stats().await.unwrap();
    let connection_ids = connection_ids_from_dump(&dump);
    assert_eq!(1, connection_ids.len());
    assert_ne!(initial_connection_id, connection_ids[0]);
}

fn connection_ids_from_dump(dump: &Value) -> Vec<String> {
    let stats: tx5::stats::Stats =
        serde_json::from_str(&serde_json::to_string(dump).unwrap()).unwrap();
    stats
        .connection_list
        .into_iter()
        .map(|c| base64::engine::general_purpose::URL_SAFE_NO_PAD.encode(c.pub_key))
        .collect()
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/performance.rs
================================================
mod common;

use common::*;
use fixt::prelude::*;
use kitsune_p2p::actor::BroadcastData;
use kitsune_p2p::actor::KitsuneP2pSender;
use kitsune_p2p::fixt::KitsuneSpaceFixturator;
use kitsune_p2p::KitsuneBinType;
use kitsune_p2p_bin_data::KitsuneBasis;
use kitsune_p2p_fetch::FetchContext;
use kitsune_p2p_types::KitsuneTimeout;
use std::sync::Arc;

/*
 * This test runs two Kitsune nodes and has each run multiple spaces. Data is published to some of the spaces
 * on each node so that there is some activity to gossip about. The number of host calls for agent info is tracked
 * and asserted at the end of the test. This isn't entirely predictable but we can assert that we're close to a known value.
 * The idea here is to prevent the call count increasing and have some way to measure when we reduce it. The host is a limited
 * resource and we don't want to keep it busy giving back the same information over and over when it has other work to do.
 */
#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky in CI"]
async fn minimise_p2p_agent_store_host_calls() {
    holochain_trace::test_run();

    let num_spaces = 10;

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;
    let agent_b = harness_b.create_agent().await;

    // Create and join multiple spaces
    let mut all_spaces = vec![];
    for _ in 0..num_spaces {
        let space = Arc::new(fixt!(KitsuneSpace));
        all_spaces.push(space.clone());

        sender_a
            .join(space.clone(), agent_a.clone(), None, None)
            .await
            .unwrap();

        sender_b
            .join(space.clone(), agent_b.clone(), None, None)
            .await
            .unwrap();
    }

    // Wait for the nodes to discover each other before publishing
    wait_for_connected(sender_a.clone(), agent_b.clone(), all_spaces[0].clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), all_spaces[0].clone()).await;

    let basis = Arc::new(KitsuneBasis::new(vec![0; 32]));

    // Create some test data, roughly partitioned across the spaces as 'no data', 'data from node a', 'data from node b'
    for i in 0..100 {
        match i % 3 {
            0 => (), // Skip, don't create data in every space
            1 => {
                let use_space = &all_spaces[i % 10];
                let test_data = TestHostOp::new(use_space.clone());
                harness_a.op_store().write().push(test_data.clone());

                sender_a
                    .broadcast(
                        use_space.clone(),
                        basis.clone(),
                        KitsuneTimeout::from_millis(5_000),
                        BroadcastData::Publish {
                            source: agent_a.clone(),
                            transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                            op_hash_list: vec![test_data.into()],
                            context: FetchContext::default(),
                        },
                    )
                    .await
                    .unwrap();
            }
            2 => {
                let use_space = &all_spaces[i % 10];
                let test_data = TestHostOp::new(use_space.clone());
                harness_b.op_store().write().push(test_data.clone());

                sender_b
                    .broadcast(
                        use_space.clone(),
                        basis.clone(),
                        KitsuneTimeout::from_millis(5_000),
                        BroadcastData::Publish {
                            source: agent_b.clone(),
                            transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                            op_hash_list: vec![test_data.into()],
                            context: FetchContext::default(),
                        },
                    )
                    .await
                    .unwrap();
            }
            _ => {
                unreachable!("because maths");
            }
        }
    }

    // Wait for 30s to allow gossip to happen so we can measure the number of host calls
    tokio::time::sleep(std::time::Duration::from_secs(30)).await;

    let drained_events = harness_a.drain_legacy_host_events().await;

    let put_agent_info_signed_count = drained_events
        .iter()
        .filter(|e| matches!(e, RecordedKitsuneP2pEvent::PutAgentInfoSigned { .. }))
        .count();

    put_agent_info_signed_count.assert_close_to(100, 50);

    let query_agents_count = drained_events
        .iter()
        .filter(|e| matches!(e, RecordedKitsuneP2pEvent::QueryAgents { .. }))
        .count();

    query_agents_count.assert_close_to(1400, 100);

    let query_peer_density_count = drained_events
        .iter()
        .filter(|e| matches!(e, RecordedKitsuneP2pEvent::QueryPeerDensity { .. }))
        .count();

    query_peer_density_count.assert_close_to(10, 2);

    println!("total calls: {:?}", drained_events.len());
}

trait CloseToAssertion<T> {
    fn assert_close_to(&self, expected: T, tolerance: T);
}

impl CloseToAssertion<usize> for usize {
    fn assert_close_to(&self, expected: usize, tolerance: usize) {
        let diff = if self > &expected {
            self - expected
        } else {
            expected - self
        };

        assert!(
            diff <= tolerance,
            "Expected {} to be within {} of {}",
            self,
            tolerance,
            expected
        );
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/sharding.rs
================================================
use crate::common::{
    start_bootstrap, start_signal_srv, wait_for_connected, KitsuneTestHarness, TestHostOp,
};
use fixt::fixt;
use ghost_actor::GhostSender;
use kitsune_p2p::actor::{BroadcastData, KitsuneP2p, KitsuneP2pSender};
use kitsune_p2p::dht::arq::LocalStorageConfig;
use kitsune_p2p::dht::prelude::SpaceDimension;
use kitsune_p2p::dht_arc::DhtLocation;
use kitsune_p2p_bin_data::fixt::KitsuneSpaceFixturator;
use kitsune_p2p_bin_data::{KitsuneAgent, KitsuneBasis, KitsuneBinType, KitsuneSpace};
use kitsune_p2p_fetch::FetchContext;
use kitsune_p2p_types::config::tuning_params_struct;
use kitsune_p2p_types::dht::{Arq, ArqStrat};
use kitsune_p2p_types::{KAgent, KitsuneTimeout};
use num_traits::AsPrimitive;
use std::collections::HashSet;
use std::net::SocketAddr;
use std::sync::Arc;

mod common;

type AgentCtx = Vec<(KitsuneTestHarness, GhostSender<KitsuneP2p>, Arq, KAgent)>;

#[cfg(not(feature = "unstable-sharding"))]
#[tokio::test(flavor = "multi_thread")]
async fn gossip_arc_clamping_required() {
    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    // Try to set up a network with gossip_arc_clamping set to None
    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        params.gossip_arc_clamping = "none".to_string();
        params
    };

    let mut harness = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(tuner);

    let result = harness.spawn().await;
    match result {
        Ok(_) => panic!("Should not have been able to spawn node"),
        Err(kitsune_p2p::KitsuneP2pError::Other(e)) => {
            assert_eq!("gossip_arc_clamping must be set".to_string(), e.to_string(),);
        }
        Err(e) => panic!("Unexpected error type: {e:?}"),
    }
}

#[cfg(not(feature = "unstable-sharding"))]
#[tokio::test(flavor = "multi_thread")]
async fn gossip_arc_clamping_permits_empty() {
    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    // Try to set up a network with gossip_arc_clamping set to "empty"
    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        params.gossip_arc_clamping = "empty".to_string();
        params
    };

    let mut harness = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(tuner);

    harness.spawn().await.unwrap();
}

#[cfg(not(feature = "unstable-sharding"))]
#[tokio::test(flavor = "multi_thread")]
#[should_panic]
async fn gossip_arc_clamping_required_when_junk_config_provided() {
    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    // Try to set up a network with gossip_arc_clamping set to a junk value
    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        params.gossip_arc_clamping = "winkle".to_string();
        params
    };

    let mut harness = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(tuner);

    harness.spawn().await.unwrap();
}

/// Test scenario steps:
///   1. Set up 5 nodes, each with one agent.
///   2. Assign a DHT arc to each agent such that their start location is inside the previous agent's arc.
///   3. Connect each agent to the previous agent (circular), so that we know they are aware of each other.
///   4. Publish an op with a basis location set to the location of the 4th agent. This should also be visible to the 3rd agent by 2. above.
///   5. Wait for the 3rd agent to receive the data.
///   6. Assert that the op was never published to the 1st, 2nd, or 5th agents. (Note that we cannot check if we sent it to ourselves because the op was already in our store)
#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn publish_to_basis_from_inside() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let space = Arc::new(fixt!(KitsuneSpace));

    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        #[cfg(feature = "unstable-sharding")]
        {
            params.gossip_arc_clamping = "none".to_string();
        }
        #[cfg(not(feature = "unstable-sharding"))]
        {
            params.gossip_arc_clamping = "full".to_string();
        }
        #[cfg(feature = "unstable-sharding")]
        {
            // Don't update the arcs dynamically, use the initial value
            params.gossip_dynamic_arcs = false;
        }
        params.disable_recent_gossip = true;
        params.disable_historical_gossip = true;
        params
    };

    let sender_idx = 3;
    let should_recv_idx = 2;

    let agents = setup_overlapping_agents(
        signal_url,
        bootstrap_addr,
        space.clone(),
        tuner,
        Box::new(move |agents| {
            let basis = basis_from_agent(&agents[sender_idx].3);

            for (i, agent) in agents.iter().enumerate() {
                let should_this_agent_hold_the_op =
                    agent.2.to_dht_arc_std().contains(basis.get_loc());

                // Another agent ended up with the op location in their arc, don't want this!
                if should_this_agent_hold_the_op && (i != sender_idx && i != should_recv_idx) {
                    return false;
                }
            }

            true
        }),
    )
    .await;

    // If the location was copied correctly then the basis location should be the same as the sender
    // location. Due to the logic above, the receiver should have the sender's location in its arc.
    let basis = basis_from_agent(&agents[sender_idx].3);
    assert_eq!(agents[sender_idx].3.get_loc(), basis.get_loc());

    let test_op = TestHostOp::new(space.clone());

    agents[sender_idx]
        .0
        .op_store()
        .write()
        .push(test_op.clone());

    agents[sender_idx]
        .1
        .broadcast(
            space.clone(),
            basis.clone(),
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agents[sender_idx].3.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_op.into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let op_store_recv = agents[should_recv_idx].0.op_store().clone();
        async move {
            loop {
                if !op_store_recv.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, agents[should_recv_idx].0.op_store().read().len());

    check_op_receivers(&agents, basis, &[sender_idx, should_recv_idx]);
}

/// Very similar to the test above except the publisher does not have the basis in their arc.
/// This is a valid scenario because any hash might be produced by creating data and the publish
/// should still go to the correct agents. It also stays with the publisher, so we need to account
/// for that when checking the op stores at the end of the test.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn publish_to_basis_from_outside() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let space = Arc::new(fixt!(KitsuneSpace));

    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        #[cfg(feature = "unstable-sharding")]
        {
            params.gossip_arc_clamping = "none".to_string();
        }
        #[cfg(not(feature = "unstable-sharding"))]
        {
            params.gossip_arc_clamping = "full".to_string();
        }
        #[cfg(feature = "unstable-sharding")]
        {
            // Don't update the arcs dynamically, use the initial value
            params.gossip_dynamic_arcs = false;
        }
        params.disable_recent_gossip = true;
        params.disable_historical_gossip = true;
        params
    };

    let sender_idx = 0;
    let should_recv_idx_1 = 3;
    let should_recv_idx_2 = 2;

    let agents = setup_overlapping_agents(
        signal_url,
        bootstrap_addr,
        space.clone(),
        tuner,
        Box::new(move |agents| {
            let basis = basis_from_agent(&agents[should_recv_idx_1].3);

            for (i, agent) in agents.iter().enumerate() {
                let should_this_agent_hold_the_op =
                    agent.2.to_dht_arc_std().contains(basis.get_loc());

                // Another agent ended up with the op location in their arc, don't want this!
                if should_this_agent_hold_the_op
                    && (i != sender_idx && i != should_recv_idx_1 && i != should_recv_idx_2)
                {
                    return false;
                }
            }

            true
        }),
    )
    .await;

    // If the location was copied correctly then the basis location should be the same as the sender
    // location. Due to the logic above, the receiver should have the sender's location in its arc.
    let basis = basis_from_agent(&agents[should_recv_idx_1].3);
    assert_eq!(agents[should_recv_idx_1].3.get_loc(), basis.get_loc());

    let test_op = TestHostOp::new(space.clone());

    agents[sender_idx]
        .0
        .op_store()
        .write()
        .push(test_op.clone());

    agents[sender_idx]
        .1
        .broadcast(
            space.clone(),
            basis.clone(),
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agents[sender_idx].3.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_op.into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let op_store_recv = agents[should_recv_idx_1].0.op_store().clone();
        async move {
            loop {
                if !op_store_recv.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, agents[should_recv_idx_1].0.op_store().read().len());

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let op_store_recv = agents[should_recv_idx_2].0.op_store().clone();
        async move {
            loop {
                if !op_store_recv.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, agents[should_recv_idx_2].0.op_store().read().len());

    check_op_receivers(
        &agents,
        basis,
        &[sender_idx, should_recv_idx_1, should_recv_idx_2],
    );
}

/// Test scenario steps:
///   1. Set up 5 nodes, each with one agent.
///   2. Assign a DHT arc to each agent such their start location is inside the previous agent's arc.
///   3. Connect each agent to the previous agent (circular), so that we know they are aware of each other.
///   4. The 4th agent creates an op and places it in their store. This should be gossipped to the 3rd agent by 2. above.
///   5. Wait for the 3rd agent to receive the data.
///   6. Assert that the op was never gossipped to the 1st, 2nd, or 5th agents.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky--this is flaky both with and without unstable-sharding feature"]
async fn gossip_to_basis_from_inside() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let space = Arc::new(fixt!(KitsuneSpace));

    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        #[cfg(feature = "unstable-sharding")]
        {
            params.gossip_arc_clamping = "none".to_string();
        }
        #[cfg(not(feature = "unstable-sharding"))]
        {
            params.gossip_arc_clamping = "full".to_string();
        }
        #[cfg(feature = "unstable-sharding")]
        {
            // Don't update the arcs dynamically, use the initial value
            params.gossip_dynamic_arcs = false;
        }
        params.disable_historical_gossip = true;
        params.disable_publish = true;
        params.gossip_loop_iteration_delay_ms = 100;
        params.gossip_peer_on_success_next_gossip_delay_ms = 1_000;

        // This needs to be set because the first connection can fail and that would put the remote on a 5-minute cooldown
        // which we obviously don't want in a test.
        params.gossip_peer_on_error_next_gossip_delay_ms = 1_000;

        params
    };

    let sender_idx = 3;
    let should_recv_idx = 2;

    let agents = setup_overlapping_agents(
        signal_url,
        bootstrap_addr,
        space.clone(),
        tuner,
        Box::new(move |agents| {
            let basis = basis_from_agent(&agents[sender_idx].3);

            for (i, agent) in agents.iter().enumerate() {
                let should_this_agent_hold_the_op =
                    agent.2.to_dht_arc_std().contains(basis.get_loc());

                // Another agent ended up with the op location in their arc, don't want this!
                if should_this_agent_hold_the_op && (i != sender_idx && i != should_recv_idx) {
                    return false;
                }
            }

            true
        }),
    )
    .await;

    // If the location was copied correctly then the basis location should be the same as the sender
    // location. Due to the logic above, the receiver should have the sender's location in its arc.
    let basis = basis_from_agent(&agents[sender_idx].3);
    assert_eq!(agents[sender_idx].3.get_loc(), basis.get_loc());

    let test_op = TestHostOp::new(space.clone()).with_forced_location(basis.get_loc());
    assert_eq!(test_op.location(), basis.get_loc());

    agents[sender_idx]
        .0
        .op_store()
        .write()
        .push(test_op.clone());

    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let op_store_recv = agents[should_recv_idx].0.op_store().clone();
        async move {
            loop {
                if !op_store_recv.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .expect("Timed out waiting for op to be received");

    assert_eq!(1, agents[should_recv_idx].0.op_store().read().len());

    check_op_receivers(&agents, basis, &[sender_idx, should_recv_idx]);
}

/// Similar to the test above except that we create the op for an agent outside the arc that it belongs in.
/// By never publishing the op and having no overlap with the arc that the op does belong in, the op should
/// never be gossipped to anyone.
///
/// This is important because, while publish should cross arcs, gossip should not. If we gossip with everyone
/// on a network then we could go a long time between talking to each node and maintain a lot of connections.
#[tokio::test(flavor = "multi_thread")]
async fn no_gossip_to_basis_from_outside() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let space = Arc::new(fixt!(KitsuneSpace));

    let tuner = |mut params: tuning_params_struct::KitsuneP2pTuningParams| {
        #[cfg(feature = "unstable-sharding")]
        {
            params.gossip_arc_clamping = "none".to_string();
        }
        #[cfg(not(feature = "unstable-sharding"))]
        {
            params.gossip_arc_clamping = "full".to_string();
        }
        #[cfg(feature = "unstable-sharding")]
        {
            // Don't update the arcs dynamically, use the initial value
            params.gossip_dynamic_arcs = false;
        }
        params.disable_historical_gossip = true;
        params.disable_publish = true;
        params.gossip_loop_iteration_delay_ms = 100;
        params.gossip_peer_on_success_next_gossip_delay_ms = 100;

        // This needs to be set because the first connection can fail and that would put the remote on a 5-minute cooldown
        // which we obviously don't want in a test.
        params.gossip_peer_on_error_next_gossip_delay_ms = 100;

        params
    };

    let sender_idx = 0;

    let agents = setup_overlapping_agents(
        signal_url,
        bootstrap_addr,
        space.clone(),
        tuner,
        Box::new(|_agents| {
            // Any agent setup will do
            true
        }),
    )
    .await;

    // If the location was copied correctly then the basis location should be the same as the sender
    // location. Due to the logic above, the receiver should have the sender's location in its arc.
    let target_idx = 3;
    let basis = basis_from_agent(&agents[target_idx].3);
    assert_eq!(agents[target_idx].3.get_loc(), basis.get_loc());

    let test_op = TestHostOp::new(space.clone()).with_forced_location(basis.get_loc());
    assert_eq!(test_op.location(), basis.get_loc());

    agents[sender_idx]
        .0
        .op_store()
        .write()
        .push(test_op.clone());

    tokio::time::sleep(std::time::Duration::from_secs(2)).await;

    for (i, agent) in agents.iter().enumerate() {
        if i == sender_idx {
            continue;
        }

        // None of the other agents should have received the op.
        let store_lock = agent.0.op_store();
        let store = store_lock.read();
        assert!(
            store.is_empty(),
            "Agent {} should not have received any data but has {} ops. Ops store: {:?}",
            i,
            store.len(),
            store,
        );
    }
}

async fn setup_overlapping_agents(
    signal_url: SocketAddr,
    bootstrap_addr: SocketAddr,
    space: Arc<KitsuneSpace>,
    kitsune_tuner: fn(
        tuning_params_struct::KitsuneP2pTuningParams,
    ) -> tuning_params_struct::KitsuneP2pTuningParams,
    verify_agent_setup: Box<dyn Fn(&AgentCtx) -> bool>,
) -> Vec<(KitsuneTestHarness, GhostSender<KitsuneP2p>, Arq, KAgent)> {
    // Arcs are this long by default, with an adjustment to ensure overlap.
    let base_len = u32::MAX / 5;

    let dim = SpaceDimension::standard();

    let mut agents = Vec::new();
    let mut accepted_agent_setup = false;

    'agent_setup: for _ in 0..10 {
        agents.clear();
        for i in 0..5 {
            let mut harness = KitsuneTestHarness::try_new("")
                .await
                .expect("Failed to setup test harness")
                .configure_tx5_network(signal_url)
                .use_bootstrap_server(bootstrap_addr)
                .update_tuning_params(kitsune_tuner);

            let sender = harness.spawn().await.expect("should be able to spawn node");

            let mut agent = harness.create_agent().await;
            let mut found_loc = false;
            for _ in 0..1000 {
                let loc = agent.get_loc().as_();
                // Search from the start of our agent, up to halfway through it. Agents that are in the
                // upper part of their range are less likely to overlap with the previous agent.
                if (loc) > base_len * i && (loc as f64) < base_len as f64 * (i as f64 + 0.5) {
                    found_loc = true;
                    break;
                }

                // If we didn't find a location in the right range, try again
                agent = harness.create_agent().await;
            }

            assert!(
                found_loc,
                "Failed to find a location in the right range after 1000 tries"
            );

            // Distance to the end of the segment, plus the length of the next segment. Likely to
            // overlap with the next agent and not the one after that.
            // Because of arc quantisation, the layout won't be perfect, but we can expect overlap at
            // the start of the agent's arc, with the previous agent.
            let len =
                DhtLocation::new(base_len * (i + 1)) - agent.get_loc() + DhtLocation::new(base_len);

            let arc = Arq::from_start_and_half_len_approximate(
                dim,
                &ArqStrat::standard(LocalStorageConfig::default(), 2.0),
                agent.get_loc(),
                len.as_() / 2 + 1,
            );

            agents.push((harness, sender, arc, agent));
        }

        if !verify_agent_setup(&agents) {
            continue 'agent_setup;
        }

        accepted_agent_setup = true;
        break;
    }

    assert!(
        accepted_agent_setup,
        "Failed to find a setup that meets the test's requirements"
    );

    for agent in &agents {
        agent
            .1
            .join(space.clone(), agent.3.clone(), None, Some(agent.2))
            .await
            .unwrap();
    }

    // Each agent should be connected to the previous agent because that's how the arcs were set up
    // above.
    for i in (1..=4).rev() {
        // A circular `next` so that the last agent is connected to the first agent
        let prev = (i - 1) % 5;

        wait_for_connected(agents[i].1.clone(), agents[prev].3.clone(), space.clone()).await
    }

    agents
}

fn basis_from_agent(agent: &KAgent) -> Arc<KitsuneBasis> {
    let agent_location = &agent.0[32..];
    let mut kitsune_basis = KitsuneBasis::new(vec![0; 36]);
    kitsune_basis.0[32..].copy_from_slice(agent_location);
    Arc::new(kitsune_basis)
}

fn check_op_receivers(
    agents: &[(KitsuneTestHarness, GhostSender<KitsuneP2p>, Arq, KAgent)],
    basis: Arc<KitsuneBasis>,
    should_recv_idx: &[usize],
) {
    let should_recv_idx = should_recv_idx.iter().copied().collect::<HashSet<_>>();

    for (i, agent) in agents.iter().enumerate() {
        if should_recv_idx.contains(&i) {
            continue;
        }

        // We've filtered out the sender and the receiver, who are expected to have the data.
        // Now we check that the agent at the current index does not have the basis that the op was
        // published to in its arc. That would make the test wrong, not Kitsune, so fail here!
        let should_this_agent_hold_the_op =
            should_agent_hold_op_at_basis(&agent.0, agent.3.clone(), basis.clone());

        assert!(
            !should_this_agent_hold_the_op,
            "Agent {i} should not have received the data"
        );

        // Now make the important assertion that the agent at index `i` did not receive the data! If it's not in the agents arc
        // (which we just asserted above) then it should not have been received.
        let store_lock = agents[i].0.op_store();
        let store = store_lock.read();
        assert!(
            store.is_empty(),
            "Agent {} should not have received any data but has {} ops. Ops store: {:?}",
            i,
            store.len(),
            store,
        );
    }
}

fn should_agent_hold_op_at_basis(
    kitsune_test_harness: &KitsuneTestHarness,
    agent: Arc<KitsuneAgent>,
    basis: Arc<KitsuneBasis>,
) -> bool {
    // Find the agent info for the given agent
    let agent_store = kitsune_test_harness.agent_store();
    let agent_store_lock = agent_store.read();
    let agent_info = agent_store_lock
        .iter()
        .find(|info| info.agent == agent)
        .unwrap();

    // let range = agent_info.storage_arq.to_dht_arc_range_std();
    // range.contains(&basis.get_loc())

    // TODO Why is this different to doing the commented lines above?
    agent_info.storage_arc().contains(basis.get_loc())
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/tests.rs
================================================
mod common;

use common::*;
use fixt::prelude::*;
use kitsune_p2p::actor::BroadcastData;
use kitsune_p2p::actor::KitsuneP2pSender;
use kitsune_p2p::fixt::KitsuneAgentFixturator;
use kitsune_p2p::fixt::KitsuneSpaceFixturator;
use kitsune_p2p::KitsuneBinType;
use kitsune_p2p_bin_data::KitsuneBasis;
use kitsune_p2p_fetch::FetchContext;
use kitsune_p2p_types::KitsuneTimeout;
use std::sync::Arc;

// Test that two nodes can discover each other and connect. This checks that peer discovery
// works and that networking works well enough for a request reply.
#[tokio::test(flavor = "multi_thread")]
async fn two_agents_on_same_host_rpc_single() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr);

    let sender = harness.spawn().await.expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));
    let agent_a = harness.create_agent().await;

    sender
        .join(space.clone(), agent_a, None, None)
        .await
        .unwrap();

    let agent_b = harness.create_agent().await;

    sender
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    let resp = tokio::time::timeout(std::time::Duration::from_secs(10), async move {
        loop {
            match sender
                .rpc_single(
                    space.clone(),
                    agent_b.clone(),
                    "Hello from agent a".as_bytes().to_vec(),
                    Some(std::time::Duration::from_secs(10).as_millis() as u64),
                )
                .await
            {
                Ok(resp) => {
                    return resp;
                }
                Err(_) => {
                    tokio::time::sleep(std::time::Duration::from_millis(100)).await;
                }
            }
        }
    })
    .await
    .unwrap();

    // Assumes that the KitsuneP2pEvent::Call handler echoes the request
    assert_eq!("Hello from agent a".as_bytes().to_vec(), resp);
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn two_nodes_publish_and_fetch() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));
    let agent_a = harness_a.create_agent().await;

    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_b = harness_b.create_agent().await;

    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for the nodes to discover each other before publishing
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    // TODO This requires host code, does it make sense to construct valid values here?
    let basis = Arc::new(KitsuneBasis::new(vec![0; 32]));

    let test_data = TestHostOp::new(space.clone());
    harness_a.op_store().write().push(test_data.clone());

    sender_a
        .broadcast(
            space.clone(),
            basis,
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agent_a.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_data.into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let op_store_b = harness_b.op_store().clone();
        async move {
            loop {
                if !op_store_b.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, harness_b.op_store().read().len());
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "Takes nearly 5-10 minutes to run locally, that is far too slow for CI. Should it run quicker?"]
async fn two_nodes_publish_and_fetch_large_number_of_ops() {
    holochain_trace::test_run();

    // Must be larger than ShardedGossipLocal::UPPER_HASHES_BOUND, to encourage batching. But I'm wondering if that's even useful because each op is
    // actually sent individually.
    let num_ops = 30_000;

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    // TODO This requires host code, does it make sense to construct valid values here?
    let basis = Arc::new(KitsuneBasis::new(vec![0; 32]));
    let space = Arc::new(fixt!(KitsuneSpace));

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    {
        for _ in 0..num_ops {
            harness_a
                .op_store()
                .write()
                .push(TestHostOp::new(space.clone()));
        }
    }

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;

    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_b = harness_b.create_agent().await;

    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for the nodes to discover each other before publishing
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    let op_hash_list = harness_a
        .op_store()
        .read()
        .iter()
        .map(|o| o.clone().into())
        .collect();

    sender_a
        .broadcast(
            space.clone(),
            basis,
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agent_a.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list,
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(600), {
        let op_store_b = harness_b.op_store();
        async move {
            loop {
                if op_store_b.read().len() >= num_ops {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            }
        }
    })
    .await
    .expect("Expected B to get all ops but this hasn't happened");

    assert_eq!(num_ops, harness_b.op_store().read().len());

    let events = harness_b
        .drain_legacy_host_events()
        .await
        .into_iter()
        .filter_map(|e| match e {
            RecordedKitsuneP2pEvent::ReceiveOps { ops, .. } => Some(ops),
            _ => None,
        })
        .collect::<Vec<_>>();

    // Expect at least one event per op
    assert!(events.len() >= num_ops);

    // The total of receieved ops must be the same as the total published. This prevents the test from quietly receiving duplicates.
    assert_eq!(num_ops, harness_b.op_store().read().len());

    // TODO Can't use this assertion, duplicate ops are usually sent during this test.
    //      The `incoming_dht_ops_workflow` is what would deal with this problem in the Holochain host implementation but it'd be a nice guarantee if
    //      Kitsune didn't hand the host ops that it already has. It's not a lot of overhead to check later but the ghost actor queues are a limited
    //      resource.
    // assert_eq!(0, harness_b.duplicate_ops_received_count());
}

// This is expected to test that agent info is broadcast to current peers when a new agent joins
#[tokio::test(flavor = "multi_thread")]
#[ignore = "Flaky on CI"]
async fn two_nodes_broadcast_agent_info() {
    holochain_trace::test_run();

    let (bootstrap_addr, mut bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));
    let agent_a = harness_a.create_agent().await;

    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_b = harness_b.create_agent().await;

    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for the nodes to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    // Kill the bootstrap server so the new agents can't be found that way
    bootstrap_handle.abort();

    assert_eq!(2, harness_b.agent_store().read().len());

    let agent_c = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_c.clone(), None, None)
        .await
        // This will error because it can't connect to the bootstrap server but it won't roll back the other join actions
        // and that is enough for this test to continue.
        .unwrap_err();
    let agent_d = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_d.clone(), None, None)
        .await
        // This will error because it can't connect to the bootstrap server but it won't roll back the other join actions
        // and that is enough for this test to continue.
        .unwrap_err();

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let agent_store_b = harness_b.agent_store();
        async move {
            loop {
                if agent_store_b.read().len() == 4 {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(4, harness_b.agent_store().read().len());
}

// This is expected to test that agent info is gossiped to a new peer when it finds one peer who knows
// about peers that are unkown to the new peer.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "Flaky on CI"]
async fn two_nodes_gossip_agent_info() {
    holochain_trace::test_run();

    let (bootstrap_addr, mut bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_a_info = harness_a.agent_store().read().first().unwrap().clone();

    let agent_c = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_c.clone(), None, None)
        .await
        .unwrap();
    let agent_d = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_d.clone(), None, None)
        .await
        .unwrap();

    // Kill the bootstrap server so the new agent can't find anyone that way
    bootstrap_handle.abort();

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        // This will error because it can't connect to the bootstrap server but it won't roll back the other join actions
        // and that is enough for this test to continue.
        .unwrap_err();

    // Add agent_a to agent_b's store so these two nodes can gossip
    harness_b.agent_store().write().push(agent_a_info);

    // Wait for the nodes to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let agent_store_b = harness_b.agent_store();
        async move {
            loop {
                if agent_store_b.read().len() == 4 {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .ok(); // Don't fail on timeout, let the assertion report the issue

    assert_eq!(4, harness_b.agent_store().read().len());
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn gossip_stops_when_agent_leaves_space() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let space = Arc::new(fixt!(KitsuneSpace));
    harness_a
        .op_store()
        .write()
        .push(TestHostOp::new(space.clone()));

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for the nodes to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let op_store_b = harness_b.op_store();
        async move {
            loop {
                if op_store_b.read().len() == 1 {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    // Don't shut down node A or B but have the only agent leave for each. This should stop gossip.
    sender_a.leave(space.clone(), agent_a).await.unwrap();
    sender_b.leave(space.clone(), agent_b).await.unwrap();

    // Now start up a new node and join an agent for the same space. This should not receive gossip.
    let mut harness_c = KitsuneTestHarness::try_new("host_c")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_c = harness_c
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_c = harness_c.create_agent().await;
    sender_c
        .join(space.clone(), agent_c.clone(), None, None)
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(5), {
        let op_store_c = harness_c.op_store();
        async move {
            loop {
                if !op_store_c.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    // Expect this to time out because there are no agents to gossip with and so C's op store should stay empty
    .unwrap_err();

    assert!(harness_c.op_store().read().is_empty());
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn gossip_historical_ops() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let space = Arc::new(fixt!(KitsuneSpace));
    harness_a.op_store().write().push(
        TestHostOp::new(space.clone()).make_historical(std::time::Duration::from_secs(30 * 60)),
    );
    harness_a.op_store().write().push(
        TestHostOp::new(space.clone()).make_historical(std::time::Duration::from_secs(45 * 60)),
    );
    harness_a.op_store().write().push(
        TestHostOp::new(space.clone()).make_historical(std::time::Duration::from_secs(60 * 60)),
    );

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for the nodes to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let op_store_b = harness_b.op_store();
        async move {
            loop {
                if op_store_b.read().len() == 3 {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(3, harness_b.op_store().read().len());
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn publish_only_fetches_ops_once() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    let mut harness_c = KitsuneTestHarness::try_new("host_c")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_c = harness_c
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_c = harness_c.create_agent().await;
    sender_c
        .join(space.clone(), agent_c.clone(), None, None)
        .await
        .unwrap();

    // Wait for nodes A and B to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    // Wait for nodes B and C to discover each other
    wait_for_connected(sender_b.clone(), agent_c.clone(), space.clone()).await;
    wait_for_connected(sender_c.clone(), agent_b.clone(), space.clone()).await;

    // Wait for nodes A and C to discover each other
    wait_for_connected(sender_a.clone(), agent_c.clone(), space.clone()).await;
    wait_for_connected(sender_c.clone(), agent_a.clone(), space.clone()).await;

    // TODO This requires host code, does it make sense to construct valid values here?
    let basis = Arc::new(KitsuneBasis::new(vec![0; 32]));

    let test_data = TestHostOp::new(space.clone());
    harness_a.op_store().write().push(test_data.clone());

    sender_a
        .broadcast(
            space.clone(),
            basis.clone(),
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agent_a.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_data.clone().into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let op_store_b = harness_b.op_store();
        let op_store_c = harness_c.op_store();
        async move {
            loop {
                if !op_store_b.read().is_empty() && !op_store_c.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, harness_b.op_store().read().len());
    assert_eq!(1, harness_c.op_store().read().len());

    let events = harness_a.drain_legacy_host_events().await;
    let fetch_op_events = events
        .iter()
        .filter_map(|e| match e {
            e @ RecordedKitsuneP2pEvent::FetchOpData { .. } => Some(e),
            _ => None,
        })
        .collect::<Vec<_>>();

    // The op should be fetched once each by B and C
    assert_eq!(2, fetch_op_events.len());

    // Broadcast the op again, which will cause a delegate publish but B and C should not try to fetch it again
    sender_a
        .broadcast(
            space.clone(),
            basis,
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agent_a.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_data.into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    // Wait for the delegate publish to happen and give the remotes a chance to fetch the op if they were going to.
    tokio::time::timeout(std::time::Duration::from_millis(250), async {
        loop {
            let events = harness_a.drain_legacy_host_events().await;
            let fetch_op_events = events
                .iter()
                .filter_map(|e| match e {
                    e @ RecordedKitsuneP2pEvent::FetchOpData { .. } => Some(e),
                    _ => None,
                })
                .collect::<Vec<_>>();

            // There should be no new fetch events because everyone already has this op
            assert_eq!(0, fetch_op_events.len());

            tokio::time::sleep(std::time::Duration::from_millis(50)).await;
        }
    })
    .await
    // This should time out, if it doesn't then an event was received when it shouldn't have been.
    .unwrap_err();
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "flaky on CI"]
async fn delegate_publish() {
    holochain_trace::test_run();

    let (bootstrap_addr, mut bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    let mut harness_c = KitsuneTestHarness::try_new("host_c")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.disable_recent_gossip = true;
            c.disable_historical_gossip = true;
            c
        });

    let sender_c = harness_c
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_c = harness_c.create_agent().await;
    sender_c
        .join(space.clone(), agent_c.clone(), None, None)
        .await
        .unwrap();

    // Wait for nodes A and B to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    // Wait for nodes B and C to discover each other
    wait_for_connected(sender_b.clone(), agent_c.clone(), space.clone()).await;
    wait_for_connected(sender_c.clone(), agent_b.clone(), space.clone()).await;

    // Wait for nodes A and C to discover each other
    wait_for_connected(sender_a.clone(), agent_c.clone(), space.clone()).await;
    wait_for_connected(sender_c.clone(), agent_a.clone(), space.clone()).await;

    // Stop bootstrapping
    bootstrap_handle.abort();

    // Make A and C forget about each other. Because gossip is disabled, this should stick.
    harness_a
        .agent_store()
        .write()
        .retain(|a| a.agent() != agent_c);
    harness_c
        .agent_store()
        .write()
        .retain(|a| a.agent() != agent_a);

    // TODO This requires host code, does it make sense to construct valid values here?
    let basis = Arc::new(KitsuneBasis::new(vec![0; 32]));

    let test_data = TestHostOp::new(space.clone());
    harness_a.op_store().write().push(test_data.clone());

    // Now A should just publish to B. B should delegate publish to C.
    sender_a
        .broadcast(
            space.clone(),
            basis.clone(),
            KitsuneTimeout::from_millis(5_000),
            BroadcastData::Publish {
                source: agent_a.clone(),
                transfer_method: kitsune_p2p_fetch::TransferMethod::Publish,
                op_hash_list: vec![test_data.clone().into()],
                context: FetchContext::default(),
            },
        )
        .await
        .unwrap();

    tokio::time::timeout(std::time::Duration::from_secs(30), {
        let op_store_b = harness_b.op_store();
        let op_store_c = harness_c.op_store();
        async move {
            loop {
                if !op_store_b.read().is_empty() && !op_store_c.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    assert_eq!(1, harness_b.op_store().read().len());
    assert_eq!(1, harness_c.op_store().read().len());
}

// Note that even with the ignore reason, this test isn't in perfect shape. I wrote it with the expectation that the bandwidth limits apply to op data
// which they do not. That will need to be figured out then the test can be completed around that. For now I just want to keep what I've done so far.
#[tokio::test(flavor = "multi_thread")]
#[ignore = "This doesn't really work, the bandwidth limits are only applied to gossip directly and not the fetch mechanism so this test can't work as is"]
async fn single_large_op_exceeds_gossip_rate_limit() {
    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let space = Arc::new(fixt!(KitsuneSpace));

    let mut harness_a = KitsuneTestHarness::try_new("host_a")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.gossip_outbound_target_mbps = 1.0;
            c.gossip_inbound_target_mbps = 1.0;
            c
        });

    let sender_a = harness_a
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_a = harness_a.create_agent().await;
    sender_a
        .join(space.clone(), agent_a.clone(), None, None)
        .await
        .unwrap();

    let mut harness_b = KitsuneTestHarness::try_new("host_b")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr)
        .update_tuning_params(|mut c| {
            // 3 seconds between gossip rounds, to keep the test fast
            c.gossip_peer_on_success_next_gossip_delay_ms = 1000 * 3;
            c.gossip_outbound_target_mbps = 1.0;
            c.gossip_inbound_target_mbps = 1.0;
            c
        });

    let sender_b = harness_b
        .spawn()
        .await
        .expect("should be able to spawn node");

    let agent_b = harness_b.create_agent().await;
    sender_b
        .join(space.clone(), agent_b.clone(), None, None)
        .await
        .unwrap();

    // Wait for nodes A and B to discover each other
    wait_for_connected(sender_a.clone(), agent_b.clone(), space.clone()).await;
    wait_for_connected(sender_b.clone(), agent_a.clone(), space.clone()).await;

    harness_a
        .op_store()
        .write()
        .push(TestHostOp::new(space.clone()).sized_5mb());

    tokio::time::timeout(std::time::Duration::from_secs(60), {
        let op_store_b = harness_b.op_store();
        async move {
            loop {
                if !op_store_b.read().is_empty() {
                    break;
                }
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }
    })
    .await
    .unwrap();

    // TODO the op should get through because of the logic for handling messages larger than the limit. To complete this test we need to send more
    //      data and actually assert the rate or something like that.
    assert_eq!(1, harness_b.op_store().read().len());
}

#[tokio::test(flavor = "multi_thread")]
#[ignore = "This test deadlocks because the event receivers aren't consumed. This should not stall Kitsune"]
async fn test_two_nodes_on_same_host_deadlock() {
    use std::sync::Arc;

    use kitsune_p2p::actor::KitsuneP2pSender;

    holochain_trace::test_run();

    let (bootstrap_addr, _bootstrap_handle) = start_bootstrap().await;
    let (signal_url, _signal_srv_handle) = start_signal_srv().await;

    let mut harness_a = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr);

    let mut harness_b = KitsuneTestHarness::try_new("")
        .await
        .expect("Failed to setup test harness")
        .configure_tx5_network(signal_url)
        .use_bootstrap_server(bootstrap_addr);

    let (sender_a, _receiver_a) = harness_a
        .spawn_without_legacy_host("host_a".to_string())
        .await
        .expect("should be able to spawn node");
    let (_sender_b, _receiver_b) = harness_b
        .spawn_without_legacy_host("host_b".to_string())
        .await
        .expect("should be able to spawn node");

    let space = Arc::new(fixt!(KitsuneSpace));
    let agent_a = Arc::new(fixt!(KitsuneAgent));

    println!("Will join");

    sender_a.join(space, agent_a, None, None).await.unwrap();

    println!("Joined!");
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/data.rs
================================================
//! Mock host data for Kitsune to work with in tests. This is needed to create reasonably realistic tests that can exercise a range of Kitsune behaviour.
//!

use fixt::prelude::*;
use kitsune_p2p_bin_data::{KOp, KitsuneBinType, KitsuneOpData, KitsuneOpHash};
use kitsune_p2p_fetch::RoughSized;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::config::RECENT_THRESHOLD_DEFAULT;
use kitsune_p2p_types::dht::region::RegionBounds;
use kitsune_p2p_types::KSpace;
use kitsune_p2p_types::{dht_arc::DhtLocation, KOpHash};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TestHostOp {
    space: KSpace,
    hash: KitsuneOpHash,
    authored_at: Timestamp,
    sized_content: Vec<u8>,
}

impl TestHostOp {
    pub fn new(space: KSpace) -> Self {
        Self {
            space,
            hash: generated_hash(),
            authored_at: Timestamp::now(),
            sized_content: vec![0; (fixt!(u8) as u32 + 1).try_into().unwrap()], // +1 because we don't want this to be 0
        }
    }

    pub fn make_historical(mut self, offset: std::time::Duration) -> Self {
        assert!(offset > RECENT_THRESHOLD_DEFAULT);
        self.authored_at = (Timestamp::now() - offset).unwrap();
        self
    }

    pub fn sized_5mb(mut self) -> Self {
        self.sized_content = vec![0; 5_000_000];
        self
    }

    pub fn with_forced_location(mut self, loc: DhtLocation) -> Self {
        assert_eq!(36, self.hash.len(), "Should already have a hash");
        let x = loc.as_u32().to_le_bytes();
        self.hash.0[32..].copy_from_slice(&x);
        self
    }

    pub fn space(&self) -> KSpace {
        self.space.clone()
    }

    pub fn kitsune_hash(&self) -> KitsuneOpHash {
        self.hash.clone()
    }

    pub fn hash(&self) -> [u8; 32] {
        // Assumes 32 byte hash, followed by 4 byte location
        self.hash[..32].try_into().unwrap()
    }

    pub fn location(&self) -> DhtLocation {
        self.hash.get_loc()
    }

    pub fn authored_at(&self) -> Timestamp {
        self.authored_at
    }

    pub fn size(&self) -> u32 {
        self.sized_content.len() as u32
    }

    pub fn is_in_bounds(&self, bounds: &RegionBounds) -> bool {
        let loc = self.location();
        let time = self.authored_at();
        if bounds.x.0 <= bounds.x.1 {
            if loc < bounds.x.0 || loc > bounds.x.1 {
                return false;
            }
        } else if loc > bounds.x.0 && loc < bounds.x.1 {
            return false;
        }

        if time < bounds.t.0 || time > bounds.t.1 {
            return false;
        }

        true
    }
}

impl From<TestHostOp> for RoughSized<KOpHash> {
    fn from(val: TestHostOp) -> Self {
        RoughSized::new(val.kitsune_hash().into(), Some(36.into()))
    }
}

impl From<TestHostOp> for KOp {
    fn from(val: TestHostOp) -> Self {
        let str = serde_json::to_string(&val).unwrap();
        KitsuneOpData::new(str.into_bytes())
    }
}

impl From<KOp> for TestHostOp {
    fn from(op: KOp) -> Self {
        let str = String::from_utf8(op.0.clone()).unwrap();
        serde_json::from_str(&str).unwrap()
    }
}

fn generated_hash() -> KitsuneOpHash {
    let mut buf = vec![];
    buf.extend_from_slice(&fixt!(ThirtyTwoBytes)); // A random hash
    buf.extend(&dht_location(buf.as_slice()[..32].try_into().unwrap()));

    KitsuneOpHash::new(buf)
}

// Ideally this would match the implementation in `holo_dht_location_bytes`
pub fn dht_location(data: &[u8; 32]) -> [u8; 4] {
    let hash = blake2b_simd::Params::new()
        .hash_length(16)
        .hash(data)
        .as_bytes()
        .to_vec();

    let mut out = [hash[0], hash[1], hash[2], hash[3]];
    for i in (4..16).step_by(4) {
        out[0] ^= hash[i];
        out[1] ^= hash[i + 1];
        out[2] ^= hash[i + 2];
        out[3] ^= hash[i + 3];
    }
    out
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/harness.rs
================================================
use ghost_actor::{GhostControlSender, GhostSender};
use std::{net::SocketAddr, sync::Arc};

use super::{test_keystore, RecordedKitsuneP2pEvent, TestHost, TestHostOp, TestLegacyHost};
use kitsune_p2p::{
    actor::KitsuneP2p, event::KitsuneP2pEventReceiver, spawn_kitsune_p2p, HostApi,
    KitsuneP2pResult, PreflightUserData,
};
use kitsune_p2p_bootstrap::BootstrapShutdown;
use kitsune_p2p_types::{
    agent_info::AgentInfoSigned,
    config::{tuning_params_struct, KitsuneP2pConfig},
    tls::TlsConfig,
    KAgent,
};
use parking_lot::RwLock;
use tokio::task::AbortHandle;

pub struct KitsuneTestHarness {
    name: String,
    config: KitsuneP2pConfig,
    tls_config: kitsune_p2p_types::tls::TlsConfig,
    host_api: HostApi,
    legacy_host_api: TestLegacyHost,
    agent_store: Arc<RwLock<Vec<AgentInfoSigned>>>,
    op_store: Arc<RwLock<Vec<TestHostOp>>>,
}

impl KitsuneTestHarness {
    pub async fn try_new(name: &str) -> KitsuneP2pResult<Self> {
        let keystore = test_keystore();
        let agent_store = Arc::new(RwLock::new(Vec::new()));
        let op_store = Arc::new(RwLock::new(Vec::new()));

        // Unpack the keystore, since we need to pass it to the host_api
        let keystore = Arc::try_unwrap(keystore).unwrap().into_inner();

        let host_api =
            Arc::new(TestHost::new(keystore.clone(), agent_store.clone(), op_store.clone()).await);
        let legacy_host_api = TestLegacyHost::new(keystore);

        Ok(Self {
            config: KitsuneP2pConfig::mem(),
            name: name.to_string(),
            tls_config: TlsConfig::new_ephemeral().await?,
            host_api,
            legacy_host_api,
            agent_store,
            op_store,
        })
    }

    pub fn configure_tx5_network(mut self, signal_url: SocketAddr) -> Self {
        self.config
            .transport_pool
            .push(kitsune_p2p_types::config::TransportConfig::WebRTC {
                signal_url: format!("ws://{signal_url}"),
                webrtc_config: None,
            });
        self
    }

    pub fn use_bootstrap_server(mut self, bootstrap_addr: SocketAddr) -> Self {
        self.config.bootstrap_service = Some(url2::url2!("http://{:?}", bootstrap_addr));
        self
    }

    pub fn update_tuning_params(
        mut self,
        f: impl Fn(
            tuning_params_struct::KitsuneP2pTuningParams,
        ) -> tuning_params_struct::KitsuneP2pTuningParams,
    ) -> Self {
        let new_config: KitsuneP2pConfig = self.config.tune(f);
        self.config = new_config;
        self
    }

    pub async fn spawn(&mut self) -> KitsuneP2pResult<GhostSender<KitsuneP2p>> {
        let (sender, receiver) = self.spawn_without_legacy_host(self.name.clone()).await?;

        self.start_legacy_host(vec![receiver]).await;

        Ok(sender)
    }

    pub async fn spawn_without_legacy_host(
        &mut self,
        name: String,
    ) -> KitsuneP2pResult<(GhostSender<KitsuneP2p>, KitsuneP2pEventReceiver)> {
        let mut config = self.config.clone();
        config.tracing_scope = Some(name);

        let (sender, receiver) = spawn_kitsune_p2p(
            config,
            self.tls_config.clone(),
            self.host_api.clone(),
            PreflightUserData::default(),
        )
        .await?;

        Ok((sender, receiver))
    }

    pub async fn start_legacy_host(&mut self, receivers: Vec<KitsuneP2pEventReceiver>) {
        self.legacy_host_api
            .start(self.agent_store.clone(), self.op_store.clone(), receivers)
            .await;
    }

    /// Attempts to do a reasonably realistic restart of the Kitsune module.
    /// The host is restarted but not recreated so that in-memory state like the peer store and op data is retained.
    ///
    /// Provide the `sender` that you got from calling `spawn`.
    pub async fn simulated_restart(
        &mut self,
        sender: GhostSender<KitsuneP2p>,
    ) -> KitsuneP2pResult<GhostSender<KitsuneP2p>> {
        // Shutdown the Kitsune module
        sender.ghost_actor_shutdown_immediate().await?;

        // Shutdown the legacy host so that it can be started with a channel to the new Kitsune module
        self.legacy_host_api.shutdown();

        // Start up again
        self.spawn().await
    }

    pub async fn create_agent(&mut self) -> KAgent {
        self.legacy_host_api.create_agent().await
    }

    #[allow(dead_code)]
    pub fn agent_store(&self) -> Arc<parking_lot::RwLock<Vec<AgentInfoSigned>>> {
        self.agent_store.clone()
    }

    pub fn op_store(&self) -> Arc<parking_lot::RwLock<Vec<TestHostOp>>> {
        self.op_store.clone()
    }

    pub async fn drain_legacy_host_events(&mut self) -> Vec<RecordedKitsuneP2pEvent> {
        self.legacy_host_api.drain_events().await
    }

    #[allow(dead_code)]
    pub fn duplicate_ops_received_count(&self) -> u32 {
        self.legacy_host_api.duplicate_ops_received_count()
    }
}

pub struct TestBootstrapHandle {
    shutdown_cb: Option<BootstrapShutdown>,
    abort_handle: AbortHandle,
}

impl TestBootstrapHandle {
    fn new(shutdown_cb: BootstrapShutdown, abort_handle: AbortHandle) -> Self {
        Self {
            shutdown_cb: Some(shutdown_cb),
            abort_handle,
        }
    }

    pub fn abort(&mut self) {
        if let Some(shutdown_cb) = self.shutdown_cb.take() {
            shutdown_cb();
        }
        self.abort_handle.abort();
    }
}

impl Drop for TestBootstrapHandle {
    fn drop(&mut self) {
        self.abort();
    }
}

pub async fn start_bootstrap() -> (SocketAddr, TestBootstrapHandle) {
    let (bs_driver, bs_addr, shutdown) =
        kitsune_p2p_bootstrap::run("127.0.0.1:0".parse::<SocketAddr>().unwrap(), vec![])
            .await
            .expect("Could not start bootstrap server");

    let abort_handle = tokio::spawn(async move {
        bs_driver.await;
    })
    .abort_handle();

    (bs_addr, TestBootstrapHandle::new(shutdown, abort_handle))
}

pub async fn start_signal_srv() -> (SocketAddr, sbd_server::SbdServer) {
    let server = sbd_server::SbdServer::new(Arc::new(sbd_server::Config {
        bind: vec!["127.0.0.1:0".to_string(), "[::1]:0".to_string()],
        limit_clients: 100,
        ..Default::default()
    }))
    .await
    .unwrap();

    (*server.bind_addrs().first().unwrap(), server)
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/mod.rs
================================================
#![allow(dead_code)]

mod data;
mod harness;
mod test_host;
mod test_keystore;
mod test_legacy_host;
mod util;

pub use data::*;
pub use harness::*;
pub use test_host::*;
pub use test_keystore::*;
pub use test_legacy_host::*;
pub use util::*;



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/test_host.rs
================================================
use super::data::TestHostOp;
use futures::FutureExt;
use kitsune_p2p::{KitsuneHost, KitsuneP2pResult};
use kitsune_p2p_block::BlockTargetId;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::{
    agent_info::AgentInfoSigned,
    config::RECENT_THRESHOLD_DEFAULT,
    dependencies::lair_keystore_api::LairClient,
    dht::{
        hash::RegionHash,
        region::RegionData,
        region_set::{RegionCoordSetLtcs, RegionSetLtcs},
        spacetime::*,
    },
};
use std::sync::Arc;

#[derive(Clone)]
pub struct TestHost {
    tag: String,
    keystore: LairClient,
    agent_store: Arc<parking_lot::RwLock<Vec<AgentInfoSigned>>>,
    op_store: Arc<parking_lot::RwLock<Vec<TestHostOp>>>,
    blocks: Arc<parking_lot::RwLock<Vec<kitsune_p2p_block::Block>>>,
}

impl std::fmt::Debug for TestHost {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("TestHost")
            .field("agent_store", &self.agent_store.read())
            .field("op_store", &self.op_store.read())
            .finish()
    }
}

impl TestHost {
    pub async fn new(
        keystore: LairClient,
        agent_store: Arc<parking_lot::RwLock<Vec<AgentInfoSigned>>>,
        op_store: Arc<parking_lot::RwLock<Vec<TestHostOp>>>,
    ) -> Self {
        let tag = nanoid::nanoid!();
        keystore
            .new_seed(tag.clone().into(), None, false)
            .await
            .expect("Could not register lair seed");

        Self {
            tag,
            keystore,
            agent_store,
            op_store,
            blocks: Arc::new(parking_lot::RwLock::new(vec![])),
        }
    }
}

impl KitsuneHost for TestHost {
    fn block(&self, input: kitsune_p2p_block::Block) -> kitsune_p2p::KitsuneHostResult<()> {
        self.blocks.write().push(input);

        async move { Ok(()) }.boxed().into()
    }

    fn unblock(&self, input: kitsune_p2p_block::Block) -> kitsune_p2p::KitsuneHostResult<()> {
        self.blocks.write().retain(|b| b != &input);

        async move { Ok(()) }.boxed().into()
    }

    fn is_blocked(
        &self,
        input: kitsune_p2p_block::BlockTargetId,
        timestamp: kitsune_p2p_types::dht::prelude::Timestamp,
    ) -> kitsune_p2p::KitsuneHostResult<bool> {
        let blocked = self.blocks.read().iter().any(|b| {
            let target_id: BlockTargetId = b.target().clone().into();

            target_id == input && b.start() <= timestamp && b.end() >= timestamp
        });

        async move { Ok(blocked) }.boxed().into()
    }

    fn get_agent_info_signed(
        &self,
        input: kitsune_p2p::event::GetAgentInfoSignedEvt,
    ) -> kitsune_p2p::KitsuneHostResult<Option<AgentInfoSigned>> {
        let res = self
            .agent_store
            .read()
            .iter()
            .find(|p| p.agent == input.agent)
            .cloned();

        async move { Ok(res) }.boxed().into()
    }

    fn remove_agent_info_signed(
        &self,
        input: kitsune_p2p::event::GetAgentInfoSignedEvt,
    ) -> kitsune_p2p::KitsuneHostResult<bool> {
        self.agent_store.write().retain(|p| p.agent != input.agent);

        // TODO This boolean return doesn't seem to be documented, what does it mean?
        async move { Ok(true) }.boxed().into()
    }

    fn peer_extrapolated_coverage(
        &self,
        _space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        _dht_arc_set: kitsune_p2p_types::dht_arc::DhtArcSet,
    ) -> kitsune_p2p::KitsuneHostResult<Vec<f64>> {
        // This is only used for metrics, so just return a dummy value
        async move { Ok(vec![]) }.boxed().into()
    }

    fn query_region_set(
        &self,
        space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        arq_set: kitsune_p2p_types::dht::ArqSet,
    ) -> kitsune_p2p::KitsuneHostResult<kitsune_p2p_types::dht::prelude::RegionSetLtcs> {
        async move {
            let topology = self.get_topology(space.clone()).await?;

            let times = TelescopingTimes::historical(&topology);
            let coords = RegionCoordSetLtcs::new(times, arq_set);

            let region_set: RegionSetLtcs<RegionData> = coords
                .into_region_set(|(_, coords)| -> KitsuneP2pResult<RegionData> {
                    let bounds = coords.to_bounds(&topology);

                    Ok(self
                        .op_store
                        .read()
                        .iter()
                        .filter(|op| op.is_in_bounds(&bounds))
                        .fold(
                            RegionData {
                                hash: RegionHash::from_vec(vec![0; 32]).unwrap(),
                                size: 0,
                                count: 0,
                            },
                            |acc, op| {
                                let mut current_hash = acc.hash.to_vec();
                                let op_hash = op.hash();
                                for i in 0..32 {
                                    current_hash[i] ^= op_hash[i];
                                }
                                RegionData {
                                    hash: RegionHash::from_vec(current_hash.to_vec()).unwrap(),
                                    size: acc.size + op.size(),
                                    count: acc.count + 1,
                                }
                            },
                        ))
                })
                .unwrap();

            Ok(region_set)
        }
        .boxed()
        .into()
    }

    // TODO This is never called, can it be removed or is it for future use?
    fn query_size_limited_regions(
        &self,
        _space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        _size_limit: u32,
        _regions: Vec<kitsune_p2p_types::dht::prelude::Region>,
    ) -> kitsune_p2p::KitsuneHostResult<Vec<kitsune_p2p_types::dht::prelude::Region>> {
        todo!()
    }

    fn query_op_hashes_by_region(
        &self,
        space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        region: kitsune_p2p_types::dht::prelude::RegionCoords,
    ) -> kitsune_p2p::KitsuneHostResult<Vec<kitsune_p2p_fetch::OpHashSized>> {
        async move {
            let topology = self.get_topology(space).await?;
            let bounds = region.to_bounds(&topology);

            Ok(self
                .op_store
                .read()
                .iter()
                .filter_map(|op| {
                    if op.is_in_bounds(&bounds) {
                        Some(op.clone().into())
                    } else {
                        None
                    }
                })
                .collect::<Vec<_>>())
        }
        .boxed()
        .into()
    }

    fn record_metrics(
        &self,
        _space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        _records: Vec<kitsune_p2p_types::metrics::MetricRecord>,
    ) -> kitsune_p2p::KitsuneHostResult<()> {
        todo!()
    }

    fn get_topology(
        &self,
        _space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
    ) -> kitsune_p2p::KitsuneHostResult<kitsune_p2p_types::dht::prelude::Topology> {
        let cutoff = RECENT_THRESHOLD_DEFAULT;
        async move {
            Ok(Topology {
                space: SpaceDimension::standard(),
                time: TimeDimension::new(std::time::Duration::from_secs(60 * 5)),
                time_origin: Timestamp::ZERO,
                time_cutoff: cutoff,
            })
        }
        .boxed()
        .into()
    }

    fn op_hash(
        &self,
        op_data: kitsune_p2p_types::KOpData,
    ) -> kitsune_p2p::KitsuneHostResult<kitsune_p2p_types::KOpHash> {
        let op: TestHostOp = op_data.into();
        async move { Ok(Arc::new(op.kitsune_hash())) }
            .boxed()
            .into()
    }

    fn check_op_data(
        &self,
        space: Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        op_hash_list: Vec<kitsune_p2p_types::KOpHash>,
        _context: Option<kitsune_p2p_fetch::FetchContext>,
    ) -> kitsune_p2p::KitsuneHostResult<Vec<bool>> {
        let res = op_hash_list
            .iter()
            .map(|op_hash| {
                self.op_store
                    .read()
                    .iter()
                    .any(|op| op.space() == space && &Arc::new(op.kitsune_hash()) == op_hash)
            })
            .collect();

        async move { Ok(res) }.boxed().into()
    }

    fn lair_tag(&self) -> Option<Arc<str>> {
        Some(self.tag.clone().into())
    }

    fn lair_client(
        &self,
    ) -> Option<kitsune_p2p_types::dependencies::lair_keystore_api::LairClient> {
        Some(self.keystore.clone())
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/test_keystore.rs
================================================
use std::sync::Arc;

use futures::lock::Mutex;
use hc_seed_bundle::{dependencies::sodoken, PwHashLimits};
use kitsune_p2p_types::dependencies::lair_keystore_api::{self, LairClient, LairResult};
use lair_keystore_api::dependencies::hc_seed_bundle;
use tokio::runtime::Handle;

/// Construct a new TestKeystore with the new lair api.
pub async fn spawn_test_keystore() -> LairResult<Arc<Mutex<LairClient>>> {
    // in-memory secure random passphrase
    let passphrase = sodoken::BufWrite::new_mem_locked(32)?;
    sodoken::random::bytes_buf(passphrase.clone()).await?;

    // in-mem / in-proc config
    let config = Arc::new(
        PwHashLimits::Minimum
            .with_exec(|| {
                lair_keystore_api::config::LairServerConfigInner::new("/", passphrase.to_read())
            })
            .await?,
    );

    // the keystore
    let keystore = lair_keystore_api::in_proc_keystore::InProcKeystore::new(
        config,
        lair_keystore_api::mem_store::create_mem_store_factory(),
        passphrase.to_read(),
    )
    .await?;

    // return the client
    let client = keystore.new_client().await?;
    Ok(Arc::new(Mutex::new(client)))
}

pub fn test_keystore() -> Arc<Mutex<LairClient>> {
    tokio::task::block_in_place(move || {
        Handle::current().block_on(async { spawn_test_keystore().await.unwrap() })
    })
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/test_legacy_host.rs
================================================
use futures::{channel::mpsc::Receiver, FutureExt, StreamExt};
use itertools::Itertools;
use kitsune_p2p::event::{
    full_time_window, FetchOpDataEvt, FetchOpDataEvtQuery, KitsuneP2pEvent, PutAgentInfoSignedEvt,
    QueryAgentsEvt, QueryOpHashesEvt, SignNetworkDataEvt,
};
use kitsune_p2p_bin_data::{
    KitsuneAgent, KitsuneBinType, KitsuneOpData, KitsuneSignature, KitsuneSpace,
};
use kitsune_p2p_fetch::FetchContext;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::bootstrap::AgentInfoPut;
use kitsune_p2p_types::{
    agent_info::AgentInfoSigned,
    dependencies::lair_keystore_api::LairClient,
    dht::{spacetime::*, ArqStrat, PeerStrat},
    dht_arc::{DhtArc, DhtArcRange},
    KAgent,
};
use std::{
    collections::{HashMap, HashSet},
    sync::{
        atomic::{AtomicU32, Ordering},
        Arc,
    },
};

use super::{dht_location, TestHostOp};

pub struct TestLegacyHost {
    handle: Option<tokio::task::JoinHandle<()>>,
    keystore: LairClient,
    events: Arc<futures::lock::Mutex<Vec<RecordedKitsuneP2pEvent>>>,
    duplicate_ops_received_count: Arc<AtomicU32>,
}

impl TestLegacyHost {
    pub fn new(keystore: LairClient) -> Self {
        let events = Arc::new(futures::lock::Mutex::new(Vec::new()));
        let duplicate_ops_received_count = Arc::new(AtomicU32::new(0));

        Self {
            handle: None,
            keystore,
            events,
            duplicate_ops_received_count,
        }
    }

    pub async fn start(
        &mut self,
        agent_store: Arc<parking_lot::RwLock<Vec<AgentInfoSigned>>>,
        op_store: Arc<parking_lot::RwLock<Vec<TestHostOp>>>,
        receivers: Vec<Receiver<KitsuneP2pEvent>>,
    ) {
        if self.handle.is_some() {
            panic!("TestLegacyHost already started");
        }

        let handle = tokio::task::spawn({
            let keystore = self.keystore.clone();
            let events_record = self.events.clone();
            let duplicate_ops_received_count = self.duplicate_ops_received_count.clone();
            async move {
                let mut receiver = futures::stream::select_all(receivers).fuse();
                while let Some(evt) = receiver.next().await {
                    record_event(events_record.clone(), &evt).await;
                    match evt {
                        KitsuneP2pEvent::PutAgentInfoSigned { respond, input, .. } => {
                            let mut store = agent_store.write();
                            let incoming_agents = input
                                .peer_data
                                .iter()
                                .map(|p| (p.agent.clone(), p))
                                .collect::<HashMap<_, _>>();

                            let mut agent_info_puts = Vec::new();
                            store.retain(|p: &AgentInfoSigned| {
                                let keep = !incoming_agents.contains_key(&p.agent);

                                // If we're not keeping it, we're replacing it so check for a URL change
                                if !keep {
                                    let existing_urls =
                                        p.url_list.iter().cloned().collect::<HashSet<_>>();
                                    let new_urls = incoming_agents
                                        .get(&p.agent)
                                        .unwrap()
                                        .url_list
                                        .iter()
                                        .cloned()
                                        .collect::<HashSet<_>>();

                                    agent_info_puts.push(AgentInfoPut {
                                        removed_urls: existing_urls
                                            .difference(&new_urls)
                                            .cloned()
                                            .collect(),
                                    });
                                }

                                keep
                            });
                            store.extend(input.peer_data);
                            respond.respond(Ok(async move { Ok(agent_info_puts) }.boxed().into()))
                        }
                        KitsuneP2pEvent::QueryAgents { respond, input, .. } => {
                            let kitsune_p2p::event::QueryAgentsEvt {
                                space,
                                agents,
                                window,
                                arq_set: arc_set,
                                near_basis,
                                limit,
                            } = input;

                            let store = agent_store.read();

                            let agents = match (agents, window, arc_set, near_basis, limit) {
                                // Handle as a "near basis" query.
                                (None, None, None, Some(basis), Some(limit)) => {
                                    let mut out: Vec<(u32, &AgentInfoSigned)> = store
                                        .iter()
                                        .filter_map(|v| {
                                            if v.is_active() {
                                                Some((v.storage_arc().dist(basis.as_u32()), v))
                                            } else {
                                                None
                                            }
                                        })
                                        .collect();

                                    out.sort_by(|a, b| a.0.cmp(&b.0));

                                    out
                                        .into_iter()
                                        .filter(|(dist, _)| *dist != u32::MAX) // Filter out Zero arcs
                                        .take(limit as usize)
                                        .map(|(_, v)| v.clone())
                                        .collect()
                                }

                                // Handle as a "gossip agents" query.
                                (_agents, window, Some(arq_set), None, None) => {
                                    let window = window.unwrap_or_else(full_time_window);
                                    let since_ms = window.start.as_millis().max(0) as u64;
                                    let until_ms = window.end.as_millis().max(0) as u64;

                                    store.iter().filter_map(|info| {
                                        if !info.is_active() {
                                            return None;
                                        }

                                        if info.signed_at_ms < since_ms {
                                            return None;
                                        }

                                        if info.signed_at_ms > until_ms {
                                            return None;
                                        }

                                        let interval = DhtArcRange::from(info.storage_arc());
                                        if !arq_set.to_dht_arc_set_std().overlap(&interval.into()) {
                                            return None;
                                        }

                                        Some(info.clone())
                                    })
                                    .collect()
                                }

                                // Otherwise, do a simple agent query with optional agent filter
                                (agents, None, None, None, None) => {
                                    match agents {
                                        Some(agents) => store
                                            .iter()
                                            .filter(|p| {
                                                p.space == space
                                                    && agents.contains(&p.agent)
                                            })
                                            .cloned()
                                            .collect::<Vec<_>>(),
                                        None => store.iter().cloned().collect(),
                                    }
                                }

                                // If none of the above match, we have no implementation for such a query
                                // and must fail
                                tuple => unimplemented!(
                                    "Holochain cannot interpret the QueryAgentsEvt data as given: {:?}",
                                    tuple
                                ),
                            };

                            respond.respond(Ok(async move { Ok(agents) }.boxed().into()))
                        }
                        KitsuneP2pEvent::QueryPeerDensity {
                            respond,
                            space,
                            dht_arc: _,
                            ..
                        } => {
                            let cutoff = std::time::Duration::from_secs(60 * 15);
                            let topology = Topology {
                                space: SpaceDimension::standard(),
                                time: TimeDimension::new(std::time::Duration::from_secs(60 * 5)),
                                time_origin: Timestamp::now(),
                                time_cutoff: cutoff,
                            };
                            let now = Timestamp::now().as_millis() as u64;
                            let arcs = agent_store
                                .read()
                                .iter()
                                .filter_map(|agent: &AgentInfoSigned| {
                                    if agent.space == space && now < agent.expires_at_ms {
                                        Some(agent.storage_arq)
                                    } else {
                                        None
                                    }
                                })
                                .collect::<Vec<_>>();

                            let strat = PeerStrat::Quantized(ArqStrat::default());
                            let view = strat.view(topology, &arcs);

                            respond.respond(Ok(async move { Ok(view) }.boxed().into()))
                        }
                        KitsuneP2pEvent::Call {
                            respond, payload, ..
                        } => {
                            // Echo the request payload
                            respond.respond(Ok(async move { Ok(payload) }.boxed().into()))
                        }
                        KitsuneP2pEvent::ReceiveOps { respond, ops, .. } => {
                            let mut op_store = op_store.write();
                            for op in ops {
                                let incoming_op: TestHostOp = op.clone().into();
                                if op_store.iter().any(|existing_op| {
                                    existing_op.kitsune_hash() == incoming_op.kitsune_hash()
                                }) {
                                    duplicate_ops_received_count.fetch_add(1, Ordering::Acquire);
                                    continue;
                                }
                                op_store.push(op.into());
                            }
                            respond.respond(Ok(async move { Ok(()) }.boxed().into()))
                        }
                        KitsuneP2pEvent::QueryOpHashes { respond, input, .. } => {
                            tracing::debug!("QueryOpHashes: {:?}", input);
                            let op_store = op_store.read();
                            let selected_ops: Vec<TestHostOp> = op_store
                                .iter()
                                .filter(|op| {
                                    if op.space() != input.space {
                                        return false;
                                    }

                                    if op.authored_at() < input.window.start
                                        || op.authored_at() >= input.window.end
                                    {
                                        return false;
                                    }

                                    let intervals = input.arc_set.intervals();
                                    if let Some(DhtArcRange::Full) = intervals.first() {
                                        // Keep everything
                                    } else {
                                        let mut in_any = false;
                                        for interval in intervals {
                                            match interval {
                                                DhtArcRange::Bounded(lower, upper) => {
                                                    if lower <= upper {
                                                        if lower <= op.location()
                                                            && op.location() <= upper
                                                        {
                                                            in_any = true;
                                                            break;
                                                        }
                                                    } else if lower < op.location()
                                                        || op.location() < upper
                                                    {
                                                        in_any = true;
                                                        break;
                                                    }
                                                }
                                                _ => unreachable!(
                                                    "Invalid input to host query for op hashes"
                                                ),
                                            }
                                        }

                                        if !in_any {
                                            return false;
                                        }
                                    }

                                    true
                                })
                                .sorted_by_key(|op| op.authored_at())
                                .take(input.max_ops)
                                .cloned()
                                .collect();

                            if !selected_ops.is_empty() {
                                let low_time = selected_ops.first().unwrap().authored_at();
                                let high_time = selected_ops.last().unwrap().authored_at();

                                respond.respond(Ok(async move {
                                    Ok(Some((
                                        selected_ops
                                            .into_iter()
                                            .map(|op| Arc::new(op.kitsune_hash()))
                                            .collect(),
                                        low_time..=high_time,
                                    )))
                                }
                                .boxed()
                                .into()))
                            } else {
                                respond.respond(Ok(async move { Ok(None) }.boxed().into()))
                            }
                        }
                        KitsuneP2pEvent::FetchOpData { respond, input, .. } => {
                            let result = match input.query {
                                FetchOpDataEvtQuery::Hashes { op_hash_list, .. } => {
                                    let search_hashes =
                                        op_hash_list.into_iter().collect::<HashSet<_>>();
                                    let op_store = op_store.read();
                                    let matched_host_data = op_store.iter().filter(|op| {
                                        op.space() == input.space
                                            && search_hashes.contains(&op.kitsune_hash())
                                    });

                                    matched_host_data
                                        .map(|h| (Arc::new(h.kitsune_hash()), h.clone().into()))
                                        .collect()
                                }
                                _ => {
                                    unimplemented!("Only know how to handle Hashes variant");
                                }
                            };

                            respond.respond(Ok(async move { Ok(result) }.boxed().into()))
                        }
                        KitsuneP2pEvent::SignNetworkData { respond, input, .. } => {
                            let mut key = [0; 32];
                            key.copy_from_slice(&input.agent.0[0..32]);
                            let sig = keystore
                                .sign_by_pub_key(
                                    key.into(),
                                    None,
                                    input.data.as_slice().to_vec().into(),
                                )
                                .await
                                .unwrap();
                            respond.respond(Ok(async move { Ok(KitsuneSignature(sig.0.to_vec())) }
                                .boxed()
                                .into()))
                        }
                        _ => todo!("Unhandled event {:?}", evt),
                    }
                }
            }
        });

        self.handle = Some(handle);
    }

    pub fn shutdown(&mut self) {
        if let Some(handle) = self.handle.take() {
            handle.abort();
        }
    }

    pub async fn drain_events(&self) -> Vec<RecordedKitsuneP2pEvent> {
        let mut events = self.events.lock().await;
        std::mem::take(&mut *events)
    }

    #[allow(dead_code)]
    pub fn duplicate_ops_received_count(&self) -> u32 {
        self.duplicate_ops_received_count.load(Ordering::Acquire)
    }

    pub async fn create_agent(&self) -> KAgent {
        let tag = nanoid::nanoid!();
        let info = self
            .keystore
            .new_seed(tag.into(), None, false)
            .await
            .unwrap();
        let mut pub_key_bytes = info.ed25519_pub_key.0.to_vec();
        let loc = dht_location(&pub_key_bytes[0..32].try_into().unwrap());
        pub_key_bytes.extend(&loc);
        Arc::new(KitsuneAgent::new(pub_key_bytes))
    }
}

/// For recording events being received by the legacy host. This enum should match KitsuneP2pEvent with
/// the responders and tracing context removed. Just the payload should be available for test assertions.
pub enum RecordedKitsuneP2pEvent {
    PutAgentInfoSigned {
        input: PutAgentInfoSignedEvt,
    },
    QueryAgents {
        input: QueryAgentsEvt,
    },
    QueryPeerDensity {
        space: Arc<KitsuneSpace>,
        dht_arc: DhtArc,
    },
    Call {
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
    },
    Notify {
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
    },
    ReceiveOps {
        space: Arc<KitsuneSpace>,
        ops: Vec<Arc<KitsuneOpData>>,
        context: Option<FetchContext>,
    },
    QueryOpHashes {
        input: QueryOpHashesEvt,
    },
    FetchOpData {
        input: FetchOpDataEvt,
    },
    SignNetworkData {
        input: SignNetworkDataEvt,
    },
}

async fn record_event(
    events: Arc<futures::lock::Mutex<Vec<RecordedKitsuneP2pEvent>>>,
    evt: &KitsuneP2pEvent,
) {
    if events.lock().await.len() % 500 == 0 {
        dump_event_dist(events.clone()).await;
    }

    let mut events = events.lock().await;

    match evt {
        KitsuneP2pEvent::PutAgentInfoSigned { input, .. } => {
            events.push(RecordedKitsuneP2pEvent::PutAgentInfoSigned {
                input: input.clone(),
            });
        }
        KitsuneP2pEvent::QueryAgents { input, .. } => {
            events.push(RecordedKitsuneP2pEvent::QueryAgents {
                input: input.clone(),
            });
        }
        KitsuneP2pEvent::QueryPeerDensity { space, dht_arc, .. } => {
            events.push(RecordedKitsuneP2pEvent::QueryPeerDensity {
                space: space.clone(),
                dht_arc: *dht_arc,
            });
        }
        KitsuneP2pEvent::Call {
            space,
            to_agent,
            payload,
            ..
        } => {
            events.push(RecordedKitsuneP2pEvent::Call {
                space: space.clone(),
                to_agent: to_agent.clone(),
                payload: payload.clone(),
            });
        }
        KitsuneP2pEvent::Notify {
            space,
            to_agent,
            payload,
            ..
        } => {
            events.push(RecordedKitsuneP2pEvent::Notify {
                space: space.clone(),
                to_agent: to_agent.clone(),
                payload: payload.clone(),
            });
        }
        KitsuneP2pEvent::ReceiveOps {
            space,
            ops,
            context,
            ..
        } => {
            events.push(RecordedKitsuneP2pEvent::ReceiveOps {
                space: space.clone(),
                ops: ops.clone(),
                context: *context,
            });
        }
        KitsuneP2pEvent::QueryOpHashes { input, .. } => {
            events.push(RecordedKitsuneP2pEvent::QueryOpHashes {
                input: input.clone(),
            });
        }
        KitsuneP2pEvent::FetchOpData { input, .. } => {
            events.push(RecordedKitsuneP2pEvent::FetchOpData {
                input: input.clone(),
            });
        }
        KitsuneP2pEvent::SignNetworkData { input, .. } => {
            events.push(RecordedKitsuneP2pEvent::SignNetworkData {
                input: input.clone(),
            });
        }
    }
}

async fn dump_event_dist(events: Arc<futures::lock::Mutex<Vec<RecordedKitsuneP2pEvent>>>) {
    let events = events.lock().await;
    let mut counts = HashMap::new();
    for evt in events.iter() {
        let key = match evt {
            RecordedKitsuneP2pEvent::PutAgentInfoSigned { .. } => "PutAgentInfoSigned",
            RecordedKitsuneP2pEvent::QueryAgents { .. } => "QueryAgents",
            RecordedKitsuneP2pEvent::QueryPeerDensity { .. } => "QueryPeerDensity",
            RecordedKitsuneP2pEvent::Call { .. } => "Call",
            RecordedKitsuneP2pEvent::Notify { .. } => "Notify",
            RecordedKitsuneP2pEvent::ReceiveOps { .. } => "ReceiveOps",
            RecordedKitsuneP2pEvent::QueryOpHashes { .. } => "QueryOpHashes",
            RecordedKitsuneP2pEvent::FetchOpData { .. } => "FetchOpData",
            RecordedKitsuneP2pEvent::SignNetworkData { .. } => "SignNetworkData",
        };
        let count = counts.entry(key).or_insert(0);
        *count += 1;
    }
    tracing::info!("Events: {}, dist: {:?}", events.len(), counts);
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/tests/common/util.rs
================================================
use std::sync::Arc;

use ghost_actor::GhostSender;
use kitsune_p2p::actor::{KitsuneP2p, KitsuneP2pSender};
use kitsune_p2p_bin_data::{KitsuneAgent, KitsuneSpace};

pub async fn wait_for_connected(
    sender: GhostSender<KitsuneP2p>,
    to_agent: Arc<KitsuneAgent>,
    space: Arc<KitsuneSpace>,
) {
    tokio::time::timeout(std::time::Duration::from_secs(32), async move {
        loop {
            match sender
                .rpc_single(
                    space.clone(),
                    to_agent.clone(),
                    "connection test".as_bytes().to_vec(),
                    Some(std::time::Duration::from_secs(10).as_millis() as u64),
                )
                .await
            {
                Ok(resp) => {
                    return resp;
                }
                Err(_) => {
                    tokio::time::sleep(std::time::Duration::from_millis(100)).await;
                }
            }
        }
    })
    .await
    .expect("wait_for_connected timed out");
}



================================================
File: crates/kitsune_p2p/mdns/README.md
================================================
# kitsune_p2p_mdns

License: Apache-2.0



================================================
File: crates/kitsune_p2p/mdns/Cargo.toml
================================================
[package]
name = "kitsune_p2p_mdns"
version = "0.5.0-dev.2"
description = "p2p / mdns discovery framework"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_mdns"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "mdns", "networking"]
categories = ["network-programming"]
edition = "2021"

[[example]]
name = "broadcast"
path = "examples/broadcast.rs"

[[example]]
name = "discover"
path = "examples/discover.rs"

# reminder - do not use workspace deps
[dependencies]
libmdns = "=0.9.1"
mdns = "=3.0.0"
base64 = "0.22"
thiserror = "2"
tokio = { version = "1.27", features = ["full"] }
tokio-stream = { version = "0.1" }

[dev-dependencies]
futures = "0.3"

[lints]
workspace = true



================================================
File: crates/kitsune_p2p/mdns/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.2

## 0.5.0-dev.1

- Bump libmdns to 0.9.1 to avoid constantly logged warnings of Address already in use

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.0

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/kitsune_p2p/mdns/examples/broadcast.rs
================================================
use kitsune_p2p_mdns::*;

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    println!("Starting broadcast");
    // Create buffer
    // let buffer = [0, 1, 2];
    // let buffer: [u8; 190] = [42; 190];
    let mut buffer: Vec<u8> = Vec::new();
    for i in 0..12_u32 {
        buffer.push((i % 255) as u8);
    }
    // Launch thread
    let service_type = "bobby".to_owned();
    let service_name = (0..62).map(|_| "X").collect::<String>();
    let tx = mdns_create_broadcast_thread(service_type, service_name, &buffer);
    // Kill thread after a minute
    tokio::time::sleep(::std::time::Duration::from_secs(60)).await;
    mdns_kill_thread(tx);
}



================================================
File: crates/kitsune_p2p/mdns/examples/discover.rs
================================================
use futures::{pin_mut, StreamExt};
use kitsune_p2p_mdns::*;

#[tokio::main]
async fn main() {
    println!("Starting discovery");
    let service_name = "bobby".to_owned();
    // Start Stream
    let stream = mdns_listen(service_name);
    pin_mut!(stream);
    while let Some(maybe_response) = stream.next().await {
        match maybe_response {
            Ok(response) => {
                println!("Discovered: {:?}", response);
            }
            Err(e) => {
                println!("!!! Discovery Error: {:?}", e);
            }
        }
    }
}



================================================
File: crates/kitsune_p2p/mdns/src/lib.rs
================================================
//! Crate for discovering Holochain peers over MDNS
//! Works by broadcasting a service named `HC_SERVICE_NAME`
//! and adding base64 encoded data in a TXT record
//!
//! Uses libmdns crate for broadcasting
//! Uses mdns crate for discovery
use mdns::RecordKind;
use std::time::Duration;
use thiserror::Error;
use tokio_stream::{Stream, StreamExt};

use base64::Engine;
use std::sync::atomic::{AtomicBool, Ordering};

const HC_SERVICE_PROTOCOL: &str = "._udp";
const BROADCAST_INTERVAL_SEC: u64 = 8;
const QUERY_INTERVAL_SEC: u64 = 5;
const MAX_TXT_SIZE: usize = 192;

#[derive(Debug, Error)]
pub enum MdnsError {
    #[error("Regular Mdns error {0}")]
    Mdns(#[from] mdns::Error),
    #[error("Base64 decoding error {0}")]
    Base64(#[from] base64::DecodeError),
}

/// Stop thread created by `mdns_create_broadcast_thread()`
pub fn mdns_kill_thread(can_run: ::std::sync::Arc<AtomicBool>) {
    can_run.store(false, Ordering::Relaxed);
}

/// Create a thread that will broadcast a holochain service over mdns
/// Returns Sender for sending thread termination command
pub fn mdns_create_broadcast_thread(
    service_type: String,
    service_name: String,
    buffer: &[u8],
) -> ::std::sync::Arc<AtomicBool> {
    let svc_type = format!("_{}{}", service_type, HC_SERVICE_PROTOCOL);
    // Constraints in libmdns
    assert!(
        svc_type.len() < 63,
        "len = {} ({}) ; {}",
        svc_type.len(),
        service_type.len(),
        service_type
    );
    assert!(service_name.len() < 63);
    // Create Termination command variable
    let can_run = ::std::sync::Arc::new(AtomicBool::new(true));
    let can_run_clone = can_run.clone();
    // Change buffer to base64 string
    let mut b64 = format!(
        "u{}",
        base64::prelude::BASE64_URL_SAFE_NO_PAD.encode(buffer)
    );
    //println!(
    //    "Broadcasting service type '{}', named '{}' over mdns ({})",
    //    svc_type,
    //    service_name,
    //    b64.len()
    //);
    // Create thread
    let _handle = tokio::task::spawn(async move {
        // Split buffer to fix TXT max size
        let mut substrs = Vec::new();
        while b64.len() > MAX_TXT_SIZE {
            let start: String = b64.drain(..MAX_TXT_SIZE).collect();
            substrs.push(start);
        }
        substrs.push(b64);
        let txts: Vec<_> = substrs.iter().map(AsRef::as_ref).collect();
        //println!("Entering mdns broadcasting thread...");
        // Create mdns responder

        let responder = libmdns::Responder::new().unwrap();
        let _svc = responder.register(svc_type, service_name, 0, &txts);
        // Loop forever unless termination command received
        loop {
            tokio::time::sleep(::std::time::Duration::from_secs(BROADCAST_INTERVAL_SEC)).await;
            if !can_run_clone.load(Ordering::Relaxed) {
                //println!("Terminating.");
                break;
            }
        }
    });
    // Done
    can_run
}

#[derive(Debug, Clone)]
pub struct MdnsResponse {
    /// Service type used
    pub service_type: String,
    /// Service name used
    pub service_name: String,
    /// IP address that responded to the mdns query
    pub addr: std::net::IpAddr,
    /// Data contained in the TXT record
    pub buffer: Vec<u8>,
}

/// Queries the network for the holochain service.
/// Returns an iterator over all responses received.
#[allow(clippy::let_and_return)]
pub fn mdns_listen(service_type: String) -> impl Stream<Item = Result<MdnsResponse, MdnsError>> {
    //let service_name = format!("{}.local", HC_SERVICE_TYPE);
    let svc_type = format!("_{}{}.local", service_type, HC_SERVICE_PROTOCOL);
    //println!("MDNS query for service type '{}'", svc_type);
    let query = mdns::discover::all(svc_type, Duration::from_secs(QUERY_INTERVAL_SEC))
        .expect("mdns Discover failed");
    // Get Mdns Response stream
    let response_stream = query.listen();
    // Change it into a MdnsResponse stream
    let mdns_stream = response_stream
        // Filtering out Empty responses
        .filter(move |res| {
            match res {
                Ok(response) => !response.is_empty() && response.ip_addr().is_some(),
                Err(_) => true, // Keep errors
            }
        })
        .map(|maybe_response| {
            if let Err(e) = maybe_response {
                return Err(MdnsError::Mdns(e));
            }
            let response = maybe_response.unwrap();
            // NOTE: if response.ip_addr() is not te right address,
            // we should give all A/AAA records found in the answers instead
            let addr = response.ip_addr().unwrap(); // should have already been filtered out
            let mut buffer = Vec::new();
            let mut service_name = String::new();
            let mut service_type = String::new();
            //println!("Response Answer count = {}", response.answers.len());
            for answer in response.answers {
                match answer.kind {
                    RecordKind::TXT(txts) => {
                        //println!("TXT count = {}", txts.len());
                        let mut b64 = String::new();
                        for txt in txts {
                            //println!("Response TXT = {:?}", txt);
                            b64.push_str(&txt);
                        }
                        buffer = match base64::prelude::BASE64_URL_SAFE_NO_PAD.decode(&b64[1..]) {
                            Err(e) => return Err(MdnsError::Base64(e)),
                            Ok(s) => s,
                        };
                    }
                    // Retrieve service name stored in PTR record
                    RecordKind::PTR(ptr) => {
                        //println!("PTR = {}", ptr);
                        service_name = ptr
                            .split('.')
                            .next()
                            .expect("Found service without a name")
                            .to_string();
                        let names: Vec<&str> = answer.name.split("._").collect();
                        //println!("answer.name = {}", answer.name);
                        service_type = names[0][1..].to_string();
                    }
                    _ => {}
                }
            }
            Ok(MdnsResponse {
                service_type,
                service_name,
                addr,
                buffer,
            })
        });
    // Done
    mdns_stream
}



================================================
File: crates/kitsune_p2p/timestamp/README.md
================================================
# kitsune_p2p_types

A microsecond-precision Timestamp datatype for general use in kitsune-p2p and Holochain.

License: Apache-2.0



================================================
File: crates/kitsune_p2p/timestamp/Cargo.toml
================================================
[package]
name = "kitsune_p2p_timestamp"
version = "0.5.0-dev.1"
description = "Microsecond-precision timestamp datatype for kitsune_p2p"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_timestamp"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
serde = { version = "1.0", features = ["derive"] }

# Dependencies not needed for integrity.
chrono = { version = "0.4.22", default-features = false, features = [
  "clock",
  "std",
  "oldtime",
  "serde",
], optional = true }

# Dependencies only needed for full.
rusqlite = { version = "0.32.1", optional = true }

# Dependencies only needed for testing by downstream crates.
proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }

once_cell = { version = "1.4.1", optional = true }
rand = { version = "0.8", optional = true }


[dev-dependencies]
holochain_serialized_bytes = "=0.0.55"
serde_yaml = "0.9"

[lints]
workspace = true

[features]
default = ["chrono"]
now = ["chrono"]

full = ["now"]

fuzzing = ["proptest", "proptest-derive", "once_cell", "rand"]

sqlite-encrypted = ["rusqlite", "rusqlite/bundled-sqlcipher-vendored-openssl"]
sqlite = ["rusqlite", "rusqlite/bundled"]



================================================
File: crates/kitsune_p2p/timestamp/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

- **BREAKING**: All chrono logic is behind the `chrono` feature flag which is on by default. If you are using this crate with `no-default-features` you will no longer have access to any chrono related functionality.

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2



================================================
File: crates/kitsune_p2p/timestamp/src/chrono_ext.rs
================================================
use super::*;
use std::{convert::TryFrom, fmt, ops::Sub, str::FromStr};

pub(crate) type DateTime = chrono::DateTime<chrono::Utc>;

/// Display as RFC3339 Date+Time for sane value ranges (0000-9999AD).  Beyond that, format
/// as (seconds, nanoseconds) tuple (output and parsing of large +/- years is unreliable).
impl fmt::Display for Timestamp {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let ce = -(62167219200 * MM)..=(253402214400 * MM);
        if ce.contains(&self.0) {
            if let Ok(ts) = chrono::DateTime::<chrono::Utc>::try_from(self) {
                return write!(
                    f,
                    "{}",
                    ts.to_rfc3339_opts(chrono::SecondsFormat::AutoSi, true)
                );
            }
        }
        // Outside 0000-01-01 to 9999-12-31; Display raw value tuple, or not a valid DateTime<Utc>;
        // Display raw value tuple
        write!(f, "({}s)", self.0)
    }
}

impl fmt::Debug for Timestamp {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "Timestamp({})", self)
    }
}

impl TryFrom<String> for Timestamp {
    type Error = TimestampError;

    fn try_from(t: String) -> Result<Self, Self::Error> {
        Timestamp::from_str(t.as_ref())
    }
}

impl TryFrom<&String> for Timestamp {
    type Error = TimestampError;

    fn try_from(t: &String) -> Result<Self, Self::Error> {
        Timestamp::from_str(t.as_ref())
    }
}

impl TryFrom<&str> for Timestamp {
    type Error = TimestampError;

    fn try_from(t: &str) -> Result<Self, Self::Error> {
        Timestamp::from_str(t)
    }
}

impl From<DateTime> for Timestamp {
    fn from(t: DateTime) -> Self {
        std::convert::From::from(&t)
    }
}

impl From<&DateTime> for Timestamp {
    fn from(t: &DateTime) -> Self {
        let t = t.naive_utc().and_utc();
        Timestamp(t.timestamp() * MM + t.timestamp_subsec_nanos() as i64 / 1000)
    }
}

// Implementation note: There are *no* infallible conversions from a Timestamp to a DateTime.  These
// may panic in from_timestamp due to out-of-range secs or nsecs, making all code using/displaying a
// Timestamp this way dangerously fragile!  Use try_from, and handle any failures.

impl TryFrom<Timestamp> for DateTime {
    type Error = TimestampError;

    fn try_from(t: Timestamp) -> Result<Self, Self::Error> {
        std::convert::TryFrom::try_from(&t)
    }
}

impl TryFrom<&Timestamp> for DateTime {
    type Error = TimestampError;

    fn try_from(t: &Timestamp) -> Result<Self, Self::Error> {
        let (secs, nsecs) = t.as_seconds_and_nanos();
        chrono::DateTime::from_timestamp(secs, nsecs).ok_or(TimestampError::Overflow)
    }
}

impl FromStr for Timestamp {
    type Err = TimestampError;

    fn from_str(t: &str) -> Result<Self, Self::Err> {
        let t = chrono::DateTime::parse_from_rfc3339(t)?;
        let t = chrono::DateTime::from_naive_utc_and_offset(t.naive_utc(), chrono::Utc);
        Ok(t.into())
    }
}

impl Timestamp {
    /// Returns the current system time as a Timestamp.
    ///
    /// This is behind a feature because we need Timestamp to be WASM compatible, and
    /// chrono doesn't have a now() implementation for WASM.
    #[cfg(feature = "now")]
    pub fn now() -> Timestamp {
        Timestamp::from(chrono::offset::Utc::now())
    }
    /// Compute signed difference between two Timestamp, returning `None` if overflow occurred, or
    /// Some(chrono::Duration).  Produces Duration for differences of up to +/- i64::MIN/MAX
    /// microseconds.
    pub fn checked_difference_signed(&self, rhs: &Timestamp) -> Option<chrono::Duration> {
        Some(chrono::Duration::microseconds(self.0.checked_sub(rhs.0)?))
    }

    /// Add a signed chrono::Duration{ secs: i64, nanos: i32 } to a Timestamp.
    pub fn checked_add_signed(&self, rhs: &chrono::Duration) -> Option<Timestamp> {
        Some(Self(self.0.checked_add(rhs.num_microseconds()?)?))
    }

    /// Subtracts a chrono::Duration from a Timestamp
    pub fn checked_sub_signed(&self, rhs: &chrono::Duration) -> Option<Timestamp> {
        self.checked_add_signed(&-*rhs)
    }
}
/// Distance between two Timestamps as a chrono::Duration (subject to overflow).  A Timestamp
/// represents a *signed* distance from the UNIX Epoch (1970-01-01T00:00:00Z).  A chrono::Duration
/// is limited to +/- i64::MIN/MAX microseconds.
impl Sub<Timestamp> for Timestamp {
    type Output = TimestampResult<chrono::Duration>;

    fn sub(self, rhs: Timestamp) -> Self::Output {
        self.checked_difference_signed(&rhs)
            .ok_or(TimestampError::Overflow)
    }
}



================================================
File: crates/kitsune_p2p/timestamp/src/error.rs
================================================
#[cfg(feature = "chrono")]
use chrono::ParseError;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TimestampError {
    Overflow,
    #[cfg(feature = "chrono")]
    ParseError(ParseError),
    OutOfOrder,
}

pub type TimestampResult<T> = Result<T, TimestampError>;

impl std::error::Error for TimestampError {
    #[cfg(feature = "chrono")]
    fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {
        match self {
            TimestampError::Overflow => None,
            TimestampError::ParseError(e) => e.source(),
            TimestampError::OutOfOrder => None,
        }
    }
}

#[cfg(feature = "chrono")]
impl From<ParseError> for TimestampError {
    fn from(e: ParseError) -> Self {
        Self::ParseError(e)
    }
}

impl core::fmt::Display for TimestampError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            TimestampError::Overflow => write!(
                f,
                "Overflow in adding, subtracting or creating from a Duration."
            ),
            #[cfg(feature = "chrono")]
            TimestampError::ParseError(s) => s.fmt(f),
            TimestampError::OutOfOrder => {
                write!(f, "Start was after the end of a Timestamp bounded range.")
            }
        }
    }
}



================================================
File: crates/kitsune_p2p/timestamp/src/human.rs
================================================
use crate::{DateTime, Timestamp};
use serde::{Deserialize, Serialize};
use std::convert::TryFrom;

/// A human-readable timestamp which is represented/serialized as an RFC3339
/// when possible, and a microsecond integer count otherwise.
/// Both representations can be deserialized to this type.
#[derive(Clone, Copy, Debug, Deserialize, Serialize)]
#[serde(untagged)]
pub enum HumanTimestamp {
    /// A microsecond resolution [`Timestamp`].
    Micros(Timestamp),
    /// A RFC3339 [`DateTime`](chrono::DateTime).
    RFC3339(DateTime),
}

impl From<Timestamp> for HumanTimestamp {
    fn from(t: Timestamp) -> Self {
        DateTime::try_from(t)
            .map(Self::RFC3339)
            .unwrap_or_else(|_| Self::Micros(t))
    }
}

impl From<DateTime> for HumanTimestamp {
    fn from(t: DateTime) -> Self {
        Self::RFC3339(t)
    }
}

impl From<HumanTimestamp> for Timestamp {
    fn from(h: HumanTimestamp) -> Self {
        match h {
            HumanTimestamp::Micros(t) => t,
            HumanTimestamp::RFC3339(d) => d.into(),
        }
    }
}

impl From<&HumanTimestamp> for Timestamp {
    fn from(h: &HumanTimestamp) -> Self {
        match h {
            HumanTimestamp::Micros(t) => *t,
            HumanTimestamp::RFC3339(d) => d.into(),
        }
    }
}

impl PartialEq for HumanTimestamp {
    fn eq(&self, other: &Self) -> bool {
        Timestamp::from(self) == Timestamp::from(other)
    }
}

impl Eq for HumanTimestamp {}

#[cfg(test)]
mod tests {
    use std::str::FromStr;

    use super::*;
    use holochain_serialized_bytes::{holochain_serial, SerializedBytes};

    holochain_serial!(HumanTimestamp);

    #[test]
    fn human_timestamp_conversions() {
        let show = |v| format!("{:?}", v);
        let s = "2022-02-11T23:05:19.470323Z";
        let t = Timestamp::from_str(s).unwrap();
        let h = HumanTimestamp::from(t);
        let sb = SerializedBytes::try_from(h).unwrap();
        let ser = show(&sb);
        assert_eq!(ser, format!("\"{}\"", s));
        let h2 = HumanTimestamp::try_from(sb).unwrap();
        let t2 = Timestamp::from(h2);
        assert_eq!(t, t2);
    }
}



================================================
File: crates/kitsune_p2p/timestamp/src/lib.rs
================================================
//! A microsecond-precision UTC timestamp for use in Holochain's actions.
#![deny(missing_docs)]

#[allow(missing_docs)]
mod error;
#[cfg(feature = "chrono")]
mod human;

#[cfg(feature = "chrono")]
pub use human::*;

use core::ops::{Add, Sub};
use serde::{Deserialize, Serialize};
use std::convert::{TryFrom, TryInto};

pub use crate::error::{TimestampError, TimestampResult};

#[cfg(feature = "chrono")]
pub(crate) use chrono_ext::*;

#[cfg(feature = "chrono")]
mod chrono_ext;

#[cfg(feature = "fuzzing")]
pub mod noise;

/// One million
pub(crate) const MM: i64 = 1_000_000;

/// This represents general compatibility between kitsune versions.
/// Instances are compatible if and only if they share the same protocol version.
///
/// If there is a breaking change at any level of the wire protocol, including
/// changes to serialization, this version should be incremented.
pub const KITSUNE_PROTOCOL_VERSION: u16 = 0;

/// A microsecond-precision UTC timestamp for use in Holochain's actions.
///
/// It is assumed to be untrustworthy:
/// it may contain times offset from the UNIX epoch with the full +/- i64 range.
/// Most of these times are *not* representable by a `chrono::DateTime<Utc>`
/// (which limits itself to a +/- i32 offset in days from Jan 1, 0AD and from 1970AD).
///
/// Also, most differences between two Timestamps are *not*
/// representable by either a `chrono::Duration` (which limits itself to +/- i64 microseconds), *nor*
/// by `core::time::Duration` (which limits itself to +'ve u64 seconds).  Many constructions of these
/// chrono and core::time types will panic!, so painful measures must be taken to avoid this outcome
/// -- it is not acceptable for our core Holochain algorithms to panic when accessing DHT Action
/// information committed by other random Holochain nodes!
///
/// Timestamp implements `Serialize` and `Display` as rfc3339 time strings (if possible).
///
/// Supports +/- `chrono::Duration` directly.  There is no `Timestamp::now()` method, since this is not
/// supported by WASM; however, `holochain_types` provides a `Timestamp::now()` method.
#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Deserialize, Serialize)]
#[cfg_attr(not(feature = "chrono"), derive(Debug))]
pub struct Timestamp(
    /// Microseconds from UNIX Epoch, positive or negative
    pub i64,
);

/// Timestamp +/- Into<core::time::Duration>: Anything that can be converted into a
/// core::time::Duration can be used as an overflow-checked offset (unsigned) for a Timestamp.  A
/// core::time::Duration allows only +'ve offsets
impl<D: Into<core::time::Duration>> Add<D> for Timestamp {
    type Output = TimestampResult<Timestamp>;

    fn add(self, rhs: D) -> Self::Output {
        self.checked_add(&rhs.into())
            .ok_or(TimestampError::Overflow)
    }
}

impl<D: Into<core::time::Duration>> Add<D> for &Timestamp {
    type Output = TimestampResult<Timestamp>;

    fn add(self, rhs: D) -> Self::Output {
        self.to_owned() + rhs
    }
}

/// Timestamp - core::time::Duration.
impl<D: Into<core::time::Duration>> Sub<D> for Timestamp {
    type Output = TimestampResult<Timestamp>;

    fn sub(self, rhs: D) -> Self::Output {
        self.checked_sub(&rhs.into())
            .ok_or(TimestampError::Overflow)
    }
}

impl<D: Into<core::time::Duration>> Sub<D> for &Timestamp {
    type Output = TimestampResult<Timestamp>;

    fn sub(self, rhs: D) -> Self::Output {
        self.to_owned() - rhs
    }
}

impl Timestamp {
    /// The Timestamp corresponding to the UNIX epoch
    pub const ZERO: Timestamp = Timestamp(0);
    /// The smallest possible Timestamp
    pub const MIN: Timestamp = Timestamp(i64::MIN);
    /// The largest possible Timestamp
    pub const MAX: Timestamp = Timestamp(i64::MAX);
    /// Jan 1, 2022, 12:00:00 AM UTC
    pub const HOLOCHAIN_EPOCH: Timestamp = Timestamp(1640995200000000);

    /// Largest possible Timestamp.
    pub fn max() -> Timestamp {
        Timestamp(i64::MAX)
    }

    /// Construct from microseconds
    pub fn from_micros(micros: i64) -> Self {
        Self(micros)
    }

    /// Access time as microseconds since UNIX epoch
    pub fn as_micros(&self) -> i64 {
        self.0
    }

    /// Access time as milliseconds since UNIX epoch
    pub fn as_millis(&self) -> i64 {
        self.0 / 1000
    }

    /// Access seconds since UNIX epoch plus nanosecond offset
    pub fn as_seconds_and_nanos(&self) -> (i64, u32) {
        let secs = self.0 / MM;
        let nsecs = (self.0 % 1_000_000) * 1000;
        (secs, nsecs as u32)
    }

    /// Add unsigned core::time::Duration{ secs: u64, nanos: u32 } to a Timestamp.  See:
    /// <https://doc.rust-lang.org/src/core/time.rs.html#53-56>
    pub fn checked_add(&self, rhs: &core::time::Duration) -> Option<Timestamp> {
        let micros = rhs.as_micros();
        if micros <= i64::MAX as u128 {
            Some(Self(self.0.checked_add(micros as i64)?))
        } else {
            None
        }
    }

    /// Sub unsigned core::time::Duration{ secs: u64, nanos: u32 } from a Timestamp.
    pub fn checked_sub(&self, rhs: &core::time::Duration) -> Option<Timestamp> {
        let micros = rhs.as_micros();
        if micros <= i64::MAX as u128 {
            Some(Self(self.0.checked_sub(micros as i64)?))
        } else {
            None
        }
    }

    /// Add a duration, clamping to MAX if overflow
    pub fn saturating_add(&self, rhs: &core::time::Duration) -> Timestamp {
        self.checked_add(rhs).unwrap_or(Self::MAX)
    }

    /// Subtract a duration, clamping to MIN if overflow
    pub fn saturating_sub(&self, rhs: &core::time::Duration) -> Timestamp {
        self.checked_sub(rhs).unwrap_or(Self::MIN)
    }

    /// Create a [`Timestamp`] from a [`core::time::Duration`] saturating at i64::MAX.
    pub fn saturating_from_dur(duration: &core::time::Duration) -> Self {
        Timestamp(std::cmp::min(duration.as_micros(), i64::MAX as u128) as i64)
    }
}

impl TryFrom<core::time::Duration> for Timestamp {
    type Error = error::TimestampError;

    fn try_from(value: core::time::Duration) -> Result<Self, Self::Error> {
        Ok(Timestamp(
            value
                .as_micros()
                .try_into()
                .map_err(|_| error::TimestampError::Overflow)?,
        ))
    }
}

#[cfg(feature = "rusqlite")]
impl rusqlite::ToSql for Timestamp {
    fn to_sql(&self) -> rusqlite::Result<rusqlite::types::ToSqlOutput> {
        Ok(rusqlite::types::ToSqlOutput::Owned(self.0.into()))
    }
}

#[cfg(feature = "rusqlite")]
impl rusqlite::types::FromSql for Timestamp {
    fn column_result(value: rusqlite::types::ValueRef<'_>) -> rusqlite::types::FromSqlResult<Self> {
        match value {
            // NB: if you have a NULLable Timestamp field in a DB, use `Option<Timestamp>`.
            //     otherwise, you'll get an InvalidType error, because we don't handle null
            //     values here.
            rusqlite::types::ValueRef::Integer(i) => Ok(Self::from_micros(i)),
            _ => Err(rusqlite::types::FromSqlError::InvalidType),
        }
    }
}

/// It's an interval bounded by timestamps that are not infinite.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, Eq, PartialEq, Hash)]
pub struct InclusiveTimestampInterval {
    start: Timestamp,
    end: Timestamp,
}

impl InclusiveTimestampInterval {
    /// Try to make the interval but fail if it ends before it starts.
    pub fn try_new(start: Timestamp, end: Timestamp) -> TimestampResult<Self> {
        if start > end {
            Err(TimestampError::OutOfOrder)
        } else {
            Ok(Self { start, end })
        }
    }

    /// Accessor for start timestamp.
    pub fn start(&self) -> Timestamp {
        self.start
    }

    /// Accessor for end timestamp.
    pub fn end(&self) -> Timestamp {
        self.end
    }
}

#[cfg(test)]
mod tests {
    use std::convert::TryInto;

    use super::*;

    const TEST_TS: &str = "2020-05-05T19:16:04.266431Z";

    #[test]
    fn timestamp_distance() {
        // Obtaining an ordering of timestamps and their difference / distance is subtle and error
        // prone.  It is easy to get panics when converting Timestamp to chrono::Datetime<Utc> and
        // chrono::Duration, both of which have strict range limits.  Since we cannot generally
        // trust code that produces Timestamps, it has no intrinsic range limits.
        let t1 = Timestamp(i64::MAX); // invalid secs for DateTime
        let d1: TimestampResult<chrono::DateTime<chrono::Utc>> = t1.try_into();
        assert_eq!(d1, Err(TimestampError::Overflow));

        let t2 = Timestamp(0) + core::time::Duration::new(0, 1000);
        assert_eq!(t2, Ok(Timestamp(1)));
    }

    #[test]
    fn micros_roundtrip() {
        for t in [Timestamp(1234567890), Timestamp(987654321)] {
            let micros = t.clone().as_micros();
            let r = Timestamp::from_micros(micros);
            assert_eq!(t.0, r.0);
            assert_eq!(t, r);
        }
    }

    #[test]
    fn test_timestamp_serialization() {
        use holochain_serialized_bytes::prelude::*;
        let t: Timestamp = TEST_TS.try_into().unwrap();
        let (secs, nsecs) = t.as_seconds_and_nanos();
        assert_eq!(secs, 1588706164);
        assert_eq!(nsecs, 266431000);
        assert_eq!(TEST_TS, &t.to_string());

        #[derive(Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
        struct S(Timestamp);
        let s = S(t);
        let sb = SerializedBytes::try_from(s).unwrap();
        let s: S = sb.try_into().unwrap();
        let t = s.0;
        assert_eq!(TEST_TS, &t.to_string());
    }

    #[test]
    fn test_timestamp_alternate_forms() {
        use holochain_serialized_bytes::prelude::*;

        decode::<_, Timestamp>(&encode(&(0u64)).unwrap()).unwrap();
        decode::<_, Timestamp>(&encode(&(i64::MAX as u64)).unwrap()).unwrap();
        assert!(decode::<_, Timestamp>(&encode(&(i64::MAX as u64 + 1)).unwrap()).is_err());
    }

    #[test]
    fn inclusive_timestamp_interval_test_new() {
        // valids.
        for (start, end) in [(0, 0), (-1, 0), (0, 1), (i64::MIN, i64::MAX)] {
            InclusiveTimestampInterval::try_new(Timestamp(start), Timestamp(end)).unwrap();
        }

        // invalids.
        for (start, end) in [(0, -1), (1, 0), (i64::MAX, i64::MIN)] {
            assert!(
                super::InclusiveTimestampInterval::try_new(Timestamp(start), Timestamp(end))
                    .is_err()
            );
        }
    }
}



================================================
File: crates/kitsune_p2p/timestamp/src/noise.rs
================================================
//! This doesn't really belong here, but it's the most upstream place to put
//! it without making a new crate

/// 10MB of entropy free for the taking.
/// Useful for initializing arbitrary::Unstructured data
#[cfg(feature = "fuzzing")]
pub static NOISE: once_cell::sync::Lazy<Vec<u8>> = once_cell::sync::Lazy::new(|| {
    use rand::Rng;

    let mut rng = rand::thread_rng();

    // use rand::SeedableRng;
    // let mut rng = rand::rngs::StdRng::seed_from_u64(0);

    std::iter::repeat_with(|| rng.gen())
        .take(10_000_000)
        .collect()
});



================================================
File: crates/kitsune_p2p/types/README.md
================================================
# kitsune_p2p_types

Types subcrate for kitsune-p2p.

License: Apache-2.0



================================================
File: crates/kitsune_p2p/types/Cargo.toml
================================================
[package]
name = "kitsune_p2p_types"
version = "0.5.0-dev.9"
description = "types subcrate for kitsune-p2p"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_types"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
lair_keystore_api = "=0.5.3"
base64 = "0.22"
derive_more = "0.99"
futures = "0.3"
ghost_actor = "=0.3.0-alpha.6"
kitsune_p2p_dht = { version = "^0.5.0-dev.3", path = "../dht" }
kitsune_p2p_dht_arc = { version = "^0.5.0-dev.2", path = "../dht_arc" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../bin_data" }
kitsune_p2p_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
mockall = { version = "0.11.3", optional = true }
holochain_trace = { version = "^0.5.0-dev.1", path = "../../holochain_trace" }
holochain_util = { version = "^0.5.0-dev.1", path = "../../holochain_util", default-features = false, features = [
  "jsonschema",
] }
once_cell = "1.4"
parking_lot = "0.12.1"
paste = "1.0.12"
rmp-serde = "=1.3.0"
rustls = { version = "0.21", features = ["dangerous_configuration"] }
serde = { version = "1", features = ["derive", "rc"] }
serde_bytes = "0.11"
serde_json = { version = "1", features = ["preserve_order"] }
sysinfo = "0.32"
thiserror = "1.0.22"
tokio = { version = "1.27", features = ["full"] }
url = "2"
url2 = "0.0.6"
schemars = "0.8.21"

fixt = { version = "^0.5.0-dev.1", path = "../../fixt", optional = true }

# proptest
proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }

[dev-dependencies]
kitsune_p2p_types = { path = ".", features = ["test_utils", "sqlite"] }
criterion = "0.5.1"

[[bench]]
name = "api_thru"
harness = false

[lints]
workspace = true

[features]

fuzzing = [
  "proptest",
  "proptest-derive",
  "kitsune_p2p_bin_data/fuzzing",
  "kitsune_p2p_dht_arc/fuzzing",
  "kitsune_p2p_dht/fuzzing",
]

test_utils = [
  "kitsune_p2p_bin_data/test_utils",
  "kitsune_p2p_dht_arc/test_utils",
  "kitsune_p2p_dht/test_utils",
  "ghost_actor/test_utils",
  "mockall",
  "fuzzing",
  "fixt",
]

fixt = ["dep:fixt", "kitsune_p2p_bin_data/fixt"]

unstable-sharding = []

sqlite-encrypted = [
  "kitsune_p2p_dht/sqlite-encrypted",
  "kitsune_p2p_dht_arc/sqlite-encrypted",
  "kitsune_p2p_bin_data/sqlite-encrypted",
]

sqlite = [
  "kitsune_p2p_dht/sqlite",
  "kitsune_p2p_dht_arc/sqlite",
  "kitsune_p2p_bin_data/sqlite",
]



================================================
File: crates/kitsune_p2p/types/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.9

- Prevent TODO comments from being rendered in cargo docs.

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.5

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.39

## 0.0.38

## 0.0.37

## 0.0.36

## 0.0.35

## 0.0.34

## 0.0.33

## 0.0.32

## 0.0.31

## 0.0.30

## 0.0.29

## 0.0.28

## 0.0.27

## 0.0.26

## 0.0.25

## 0.0.24

## 0.0.23

## 0.0.22

## 0.0.21

## 0.0.20

## 0.0.19

## 0.0.18

- Sharded DHT arcs is on by default. This means that once the network reaches a certain size, it will split into multiple shards.

## 0.0.17

## 0.0.16

## 0.0.15

## 0.0.14

## 0.0.13

## 0.0.12

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

- Adds a prototype protocol for checking consistency in a sharded network.

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/kitsune_p2p/types/benches/api_thru.rs
================================================
fn main() {
    todo!("rewrite for tx5")
}



================================================
File: crates/kitsune_p2p/types/examples/codec.rs
================================================
use kitsune_p2p_types::codec::*;
use kitsune_p2p_types::*;

write_codec_enum! {
    /// My codec is awesome.
    codec MyCodec {
        /// My codec has only one variant.
        MyVariant(0x00) {
            /// My variant has only one type
            my_type.0: String,
        },
    }
}

fn main() {
    let item1 = MyCodec::my_variant("test".to_string());
    let data = item1.encode_vec().unwrap();
    println!("Encoded: {:?}", &data);
    let (_, item2) = MyCodec::decode_ref(&data).unwrap();
    println!("Decoded: {:?}", item2);
    assert_eq!(item1, item2);
}



================================================
File: crates/kitsune_p2p/types/src/agent_info.rs
================================================
//! Data structures to be stored in the agent/peer database.

use crate::bin_types::*;
use crate::dht_arc::DhtArc;
use crate::tx_utils::TxUrl;
use crate::*;
use agent_info_helper::*;
use dht::Arq;

/// A list of Urls.
pub type UrlList = Vec<TxUrl>;

/// An agent paired with its storage arc in interval form
pub type AgentArc = (Arc<KitsuneAgent>, DhtArc);

/// agent_info helper types
pub mod agent_info_helper {
    use dht::arq::ArqSize;

    use super::*;

    #[allow(missing_docs)]
    #[derive(Debug, serde::Serialize, serde::Deserialize, derive_more::From, derive_more::Into)]
    pub struct AgentMetaInfoEncode {
        pub arq_size: ArqSize,
    }

    impl From<Arq> for AgentMetaInfoEncode {
        fn from(arq: Arq) -> Self {
            ArqSize::from(arq).into()
        }
    }

    #[allow(missing_docs)]
    #[derive(Debug, serde::Serialize, serde::Deserialize)]
    pub struct AgentInfoEncode {
        pub space: Arc<KitsuneSpace>,
        pub agent: Arc<KitsuneAgent>,
        pub urls: UrlList,
        pub signed_at_ms: u64,

        /// WARNING-this is a weird offset from the signed_at_ms time!!!!
        pub expires_after_ms: u64,

        #[serde(with = "serde_bytes")]
        pub meta_info: Box<[u8]>,
    }

    #[allow(missing_docs)]
    #[derive(Debug, serde::Deserialize)]
    pub struct AgentInfoSignedEncode {
        pub agent: Arc<KitsuneAgent>,
        pub signature: Arc<KitsuneSignature>,
        #[serde(with = "serde_bytes")]
        pub agent_info: Box<[u8]>,
    }

    #[allow(missing_docs)]
    #[derive(Debug, serde::Serialize)]
    pub struct AgentInfoSignedEncodeRef<'lt> {
        pub agent: &'lt Arc<KitsuneAgent>,
        pub signature: &'lt Arc<KitsuneSignature>,
        #[serde(with = "serde_bytes")]
        pub agent_info: &'lt [u8],
    }
}

/// The inner constructable AgentInfo struct
pub struct AgentInfoInner {
    /// The space this agent info is relevant to.
    pub space: Arc<KitsuneSpace>,

    /// The pub key of the agent id this info is relevant to.
    pub agent: Arc<KitsuneAgent>,

    /// The storage arc currently being published by this agent.
    pub storage_arq: Arq,

    /// List of urls the agent can be reached at, in the agent's own preference order.
    pub url_list: UrlList,

    /// The absolute unix ms timestamp that the agent info was signed at,
    /// according to the agent's own clock.
    pub signed_at_ms: u64,

    /// The absolute unix ms timestamp this info will expire at,
    /// according to the agent's own clock.
    /// Note--the encoded bootstrap version of this struct uses a weird
    /// offset from the signed time... but this value here is the more
    /// intuitive absolute value.
    pub expires_at_ms: u64,

    /// Raw bytes of agent info signature as kitsune signature.
    pub signature: Arc<KitsuneSignature>,

    /// the raw encoded bytes sent to bootstrap server to use for sig verify.
    pub encoded_bytes: Box<[u8]>,
}

impl AgentInfoInner {
    /// If this agent is considered active.
    pub fn is_active(&self) -> bool {
        !self.url_list.is_empty()
    }
}

impl std::fmt::Debug for AgentInfoInner {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("AgentInfoSigned")
            .field("space", &self.space)
            .field("agent", &self.agent)
            .field("storage_arq", &self.storage_arq)
            .field("url_list", &self.url_list)
            .field("signed_at_ms", &self.signed_at_ms)
            .field("expires_at_ms", &self.expires_at_ms)
            .finish()
    }
}

impl PartialEq for AgentInfoInner {
    fn eq(&self, oth: &Self) -> bool {
        self.encoded_bytes.eq(&oth.encoded_bytes)
    }
}

impl Eq for AgentInfoInner {}

impl std::hash::Hash for AgentInfoInner {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.encoded_bytes.hash(state);
