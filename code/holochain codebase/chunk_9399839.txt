A careful reading of the “The Byzantine Generals Problem” papers[^byzantine] reveals one source of this frame: a mandate to build control systems for making critical automated decisions in contexts such as in nuclear power plant control and interpreting radar data about possible nuclear strikes.  The Generals in the problem are understood to be sources of input (either from sensors or human agents) that are sent to Lieutenants who are understood to be the computers carrying out the orders indicated by those inputs.  The critical insights of these papers are 1: proofs that simple majority voting on data from redundant systems is insufficient to overcome the fault cases, and 2: provision of algorithms they prove to solve the problem under various restrictions of how many nodes must not be faulty for correct decisions to be reached.

The core axiom (though not explicitly stated as such) of the Byzantine Generals Problem is that coordination starts AFTER “consensus on state”, i.e. that the Lieutenants can’t execute their plan until they have followed the consensus algorithm and arrived at single data reality[^reality].  This axiom at first blush seems sensible, and it is carried over into Blockchain-based distributed ledger systems which are often explicitly described as solutions to the Byzantine Generals problem.[^bzp_blockchain]  This axiom gives rise to the fundamental data and process architecture of a single globally advancing chain of blocks that miners or stakers select into existence.  The chain of blocks is the single data reality with miners and stakers acting as selectors of that expanding reality using the various proof-of-work/stake algorithms.  Unfortunately this axiom does not actually correctly represent the realities of large scale distributed coordination, furthermore its use as a starting point results in some significant consequences that emerge from the resulting architecture:

1. Negative or zero scaling: Adding new miners or stakers to the network does not increase the capacity of the network as a whole to do coordinative work.  In the case of proof-of-work reality selection, new miners only increase the amount of wasted energy used while achieving the same throughput in transactions-per-second.  In the case of proof-of-stake reality selection (either chain-based or BFT leader-selection based)  though there is less redundant burned energy work, there is still no overall throughput increase as the number of stakers increases.

2. Inherent centralizing dynamics: Blockchain reality selection (via mining or staking) produces an inherently centralizing network effect through “rich get richer” power concentration. The more you mine or stake, the more rewards you get, which you can spend on mining rigs or further staking to continually increase your rewards. These dynamics increase the influence of large scale participants. Fundamentally, you cannot successfully operate a decentralized system using a consensus algorithm which centralizes the power and wealth within that system.

Thus, the aspiration of such systems for decentralizing planetary scale coordination appears to us as extremely unlikely to be realized without a fundamental ontological shift.

In the spirit of the Byzantine Generals Problems, we offer a story to aid in discovering a new starting point from which to design systems for distributed coordination.

## The Players of Ludos

Imagine a civilization, which for flavor, perhaps lived somewhere close to Byzantium, but nomadic, more in harmony with nature and great lovers of playing board games (like chess, checkers or go), whom we will call the Players of Ludos.  Imagine that these highly independent and egalitarian nomadic bands with their long tradition of playing would gather for tournaments played in a large arena with many simultaneous boards and players.

Now at some point, because these Players of Ludos so loved their games, they decided they would like to keep their inter-band playing going year round, even as bands would be on the move, living in harmony with the land, as they did.  And thus they devised to send written correspondence by messengers out to other bands with game moves.  As you might expect, they soon realized that their game playing broke down, because messages between bands were not always reliably delivered. Sometimes just because the messengers were just lazy or distracted and would fail to deliver messages, sometimes the messengers would fail to protect the message from the rain which would cause the ink to run and garble the messages, and sometimes nefarious and overly serious game players wanting to affect the outcome of these games would purposefully make changes to messages\!  These failures resulted in players in different bands not seeing the same game reality and making incorrect moves.

At first the Players of Ludos believed that to solve this problem, they had to replicate their experience of the tournaments where players could simultaneously look at all the board’s states before making moves.  To this end they created a drawing, one for each band, of the “virtual arena” that they would be synchronized across all the bands.  However, they knew that to do this would be complicated by the fact that messages of lists of moves sent between bands would never arrive at the same time or order.  Because of their deep egalitarian ethics, they couldn’t just elect one band as the authoritative sender of update-messages, rather they wanted a way of choosing different band’s board-state-update messages over time in a way that was random and fair.  They did indeed find several ingenious methods of choosing which band’s proposal would be the “real” one for the next round of moves.[^proof_of_wait]  But they soon realized that starting with the assumption of having a single agreed upon arena drawing was actually an unnecessary starting assumption, and that an entirely different approach would make it possible to coordinate the games much more efficiently.  We won’t go into just how deeply inefficient their original “ingenious” solutions were (unless you care to read the footnote above) and just how much these solutions created the very inequality between bands that they were trying to avoid in the first place\!

The heart of the new approach was just to do a few things:

1. Require individual players to keep track of their own moves
2. Require individual players to validate and keep track of a portion of other players moves
3. Require players to respond to requests of the moves of other players they are keeping track of..

But it all hinged on a few special abilities that the Players of Ludos used:

1. They developed a way to unforgeably sign any document.
2. They had a ingenious method to create a “fingerprint” of each move that looked exactly as if it were a human fingerprint, and just like a human fingerprint was different from all other fingerprints of other moves, AS WELL as being different from all other human fingerprints
3. They also had an even more ingenious method of very easily being able to tell if one fingerprint was similar to another, i.e. they could group themselves into “fingerprint neighborhoods” according to those similarities.
4. Finally they created a way to locate players by ensuring that all the fingerprint neighborhoods overlap in such a way that it was guaranteed that you could always either send a message to someone in a neighborhood that’s closer to the destination than your own, or know that you are in the neighborhood that’s responsible for holding data of a given fingerprint.

Given these abilities this is how the Players coordinate:

1. When making a move in a game they take a fingerprint of the move, and then write a small document which contains the fingerprint of the move, the date, and the fingerprint of the previous document.  They called these documents “Actions”.  Note how storing the fingerprint of the previous document creates an unbreakable Action chain.
2. They also “publish” the move by sending out a few messages to other players as follows:
   1. Send the full move and action documents to players whose human fingerprint is in the neighborhood of the move document, and of the action document.  These players will be able to respond to requests for the actions and moves.
   2. Send the action document to the players whose human fingerprint is in the neighborhood of the publisher's human fingerprint.  These players will be able to respond to requests of player move history.  This is important because sometimes validation of a given move requires the ability to check the history of a player's previous moves.
3. Any player receiving a published document checks to see if all of the data in the document they receive conforms to the rules of the game (this may involve making requests of other players, for example to retrieve the history of previous moves).  They then sign and send a receipt confirming the validity or invalidity of the published document back to the player that sent the document as well as to the other players in the same neighborhood who also should have received the original document.
4. Players periodically gossip with other players in their neighborhood about what they’ve heard about, validating and updating their records accordingly. Thus, players who cheat, including by changing their history’s and reporting different moves to different players, will be found out because all moves must be signed, and the history of the moves is baked into the actions. Remember that each action contains the fingerprint of the previous action. Players who receive actions by gossip (i.e not as a result of publishing as in step 2.b) will eventually be able to detect any actions that show contradictory histories as soon as they see two different actions that use the same fingerprint of a previous action.
5. Finally, players who receive notices of cheating players, or who observe the cheating directly, may, depending on the particular rules of the game, simply drop communication with offending players, or give them warnings, as there are some circumstances where players may have sent conflicting messages accidentally, in which case they can send corrections.

With these simple steps all players could confidently re-create the state of the boards.  Every move is signed and validated, and players receive confirmation from other players in similar neighborhoods about the correctness of all the moves.  Players can request copies of moves they are interested in (both the specific moves themselves, and the actions that provide a history of the moves, by requesting them from players in the correct neighborhood of the moves.  Of course it’s not guaranteed that at any given moment in time if you ask a neighborhood for any given move that it will have reached the nodes who receive your request, but eventually they always will, and the answers returned will converge on the same reality.

Thus the Players of Ludos greatly improved the speed of their play.  Adding a new band or new players did not slow down the play, in fact it actually increased the overall resiliency of the play.  Furthermore they realized that they could increase the complexity of play from turn based board games, to arbitrarily complex games.  As long as every player of a game started with the same rule-set, and they could deterministically validate any move, and they could get notification of cheaters, the system worked perfectly well for coordination.

There's a second phase to our story that relates to scaling up distributed coordination which we offer here in the broadest of strokes. It goes like this: a clever player created general rules for a “tournament” game, which could reference any other type of game that had winners and losers, thus allowing games to compose with other games.  Because tournament winners gained social status in their society, they soon realized that many other of their social interactions could be encoded similarly as games.  They even realized they could replace their monetary system with an accounting “game” where players simply recorded the granting and receiving credit with each other.   All this further allowed them to live into their commitment to both independence and egalitarian ethics.

## New Axioms for Distributed Coordination

This story is meant to elucidate what happens if we start from a different ground when thinking about distributed coordination.  Our story only focuses on the first part of coordination, the low level mechanisms of distributed coordination but points to what’s possible when one treats that capacity as something to assemble larger capacities. And so, from this fanciful account, we would like to offer somewhat less fancifully, two axioms[^axioms] from which to start when considering building systems for large scale distributed coordination:

1. Coordination arises from agents starting from the same ground-rules and acting as soon as parties can confirm that actions or interactions conform to those ground rules. (Thus, in our frame, coordination looks like a swarm of agents that converge on a direction, rather than all agents proceeding in lockstep agreement.)
2. Coordination is grammatic[^grammatic]. It comes from two forms of embodiment[^embodiment] by groups of independent agents: 1\) embodying a collective understanding of the shape of interaction (the “ground-rules”) or what you could think of as a geometry of the space of play, which removes enough uncertainty that it’s worth risking to play.  2\) embodying an ability to compose different coordinative subsystems that have different ground-rules.

Axiom 1 arises from the insight of not fighting against what’s true about physical reality.  Namely that different nodes in a network of interaction will experience different realities.  In the real world there is no such thing as simultaneity, which means there is no such thing as global temporal ordering of events out of which to build a global state of a system.  Instead, any coordination system must align its ontology with the truth that **global state actually does not exist**.  Thus, we start with what does actually exist: local temporal state.  This local state can be shared with, and validated by, others in conformity with pre-defined ground-rules.  In so doing we can still achieve difficult and complex coordination safely (including in the context of problems on the scale of global monetary transactions) without the costs and bottlenecks that arise from starting from the ontology of a single shared global state. Holochain is an implementation of a system using this alternate frame.

Axiom 2 arises from the insight that systems for successful large scale coordination demand the property of anti-fragility, that is, they must perform better under perturbation[^antifragile].  Coordination happens in the context of fundamentally dynamic environments in which the coordinating elements are changed by the fact of their coordination.  Coordination is a co-evolutionary context.  We claim by this axiom that what meets the challenge of anti-fragility in such contexts is composable sub-systems, in which the composition comes out of a grammatics that embodies the actual dimensionality of the problem subdomains (i.e. their geometry), and by which agents in that context can react powerfully to perturbations because the available composability is dimensionally aligned.

The old axiom that coordination starts after consensus on state, leads system designers to figure out how to implement machinery for **Global Consensus**. Our new axioms lead us, instead, to implement tooling for **Scaling Consent**.

Holochain is built starting from the above two axioms and thus delivers on coordination at scale with out global consensus.

[^experience]:  These differences can arise from the fundamental physical properties of the medium carrying messages between nodes and the way these media may introduce noise in messages or delay message transmission which results in nodes receiving messages in different orders. They may also arise because of intentional malicious behavior of nodes sending false information or simply not sending messages.

[^leader_selection]:  The appropriateness of these leader-selection algorithms as solutions to distributed coordination is argued differently according to the particular selection algorithm.  In the case of blockchain proof-of-work the argument is that the pure computational probabilistic nature of finding the cryptographic result and the energy cost of doing so ensures that collusion can only happen above 50% malicious nodes and then the cost of that collusion is higher than any value that can be gained from it.  In the case of proof-of-stake algorithms, the loss of the “stake” is argued to be the incentive for non-malicious behavior when validating and presenting a given block for inclusion.  These arguments often ignore the ways in which these selection algorithms defeat the very premise of distributing collaboration as they re-introduce centralizing dynamics.

[^byzantine]: See: *The Byzantine Generals Problem*, Leslie Lamport, Robert Shostak, and Marshall Pease <https://lamport.azurewebsites.net/pubs/byz.pdf> and *Reaching Agreement in the Presence of Faults* Marshall Pease, Robert Shostak, and Leslie Lamport <https://dl.acm.org/doi/pdf/10.1145/322186.322188>

[^reality]: In *Reaching Agreement…* this single data reality  is called “interactive consistency” as it is about the vector of “Private Values” sent by each node.

[^bzp_blockchain]: E.g. <https://cointelegraph.com/blockchain-for-beginners/how-does-blockchain-solve-the-byzantine-generals-problem> and <https://medium.com/swlh/bitcoins-proof-of-work-the-problem-of-the-byzantine-generals-33dc4540442>

[^proof_of_wait]: One of these methods they called Proof-of-Wait whereby they would feed a small but very hard-shelled gourd to their pack animals.  Now the seeds of these gourds had a very interesting property when passing through the animal’s digestive tract.  It so happened that most often these gourds would not be digestible.  But occasionally and very randomly but also very predictably (on average once every 10 days) a gourd would be digested, and fascinatingly the seeds of the gourd would be transformed by the digestive juices of the pack animal into a unmistakable and otherwise unreproducible color which faded away in just a couple days.  It also so happened, that this gourd had exactly as many seeds as there were nomad bands.  Thus, the ingenious Players knew that they could send one gourd seed along with a proposal to update the board drawings, and that would unequivocally and randomly select one band’s proposal, and they would be able to do so, on average every 10 days, and you couldn't cheat because the seed color faded before the next update\! Of course it could happen randomly that two bands would get a gourd at the same time, but they just agreed to just use the proposal that had the longest list of moves. This worked, but it kept the pace of games quite slow, and moreover it involved sorting through a lot of pack-animal dung, which sadly some players actually came to believe was valuable work\!

[^axioms]: The term "axiom" is often understood as being a statement that is self-evident and not proven but rather assumed.  As in the cases of the parallel axiom of Euclidean geometry it seemed natural to be able assume that such lines never meet.  Similarly it might seem natural to simply assume that coordinated action begins after consensus on state.  We hope here that it has been sufficiently demonstrated that taking on perhaps surprising axioms (as was done the "parallel lines allways meet" of spherical non-Euclidean geometry), provides a similar expansion of understanding and possibility.

[^antifragile]: *Antifragile: Things that Gain from Disorder*, Nassim Nicholas Taleb, 2012.

[^grammatic]: Think of the term "grammatic" as a way to generalize from the usual understanding of grammar which is linguistic.  Where grammar is often understood to be limited to language, grammatics points to the pattern of creating templates with classes of items that can fill slots in those templates. This pattern can be used for creating "grammars" of social interaction, "grammars" of physical structures (we would call Christopher Alexander's "A Pattern Language" for architecture an example of grammatics) and so on.

[^embodiment]: Insofar as our compute-powered platforms are meant to solve problems in particular domains, it follows that the ways those problems show up in the platform actually meet the dimensionality of the problem space. By this I mean that the independent variables or ontological entities that are part of the problem space are reflected in the compute system. That reflection I call **embodiment** in the system. A generalized platform for creating applications that solve problems must therefore embody this higher-level dimensionality of the problem space of "generalized application creation" itself, and it must do so in an evolvable manner. The use of the term "geometry" here is similarly intended to help elucidate the notion of dimensionality, in that geometries distinguish independent directions of motion and the relations between them.




================================================
File: docs/specs/src/holochain-white-paper-alpha.tex
================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% writeLaTeX Example: Academic Paper Template
%
% Source: http://www.writelatex.com
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[twocolumn,showpacs,%
  nofootinbib,aps,superscriptaddress,%
  eqsecnum,prd,notitlepage,showkeys,10pt]{revtex4-1}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{setspace} % for \onehalfspacing and \singlespacing macros
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespace\vspace{-\topsep}\small}
\AtEndEnvironment{quote}{\vspace{-\topsep}\endsinglespace}
\usepackage{url}


\newtheorem{itemlet}{}

\begin{document}

% macros
\newcommand\term[1]{\textbf{\textit{#1}}}
\newcommand{\sbtc}{$\Omega_{\text{bitcoin}}$}
\newcommand{\sgit}{$\Omega_{\text{git}}$}
\newcommand{\shc}{$\Omega_{\text{hc}}$}
\newcommand{\hcdna}{$\text{DNA}$}
\newcommand{\hcid}{\iota}
\newcommand{\dhtget}{\texttt{get}}
\newcommand{\dhtput}{\texttt{put}}
\newcommand{\dhtstate}{\Delta}
\newcommand{\dhtfns}{F_\mathrm{DHT}}
\newcommand{\sysfns}{F_\mathrm{sys}}
\newcommand{\appfns}{F_\mathrm{app}}
\newcommand{\hcdht}{DHT_\text{hc}}
\newcommand{\chain}{\mathcal{X}}
\newcommand{\eqbang}{\stackrel{!}{=}}
\title{Holochain \\
\small Scalable agent-centric distributed computing\\ALPHA -- 2/15/2018}
\author{Eric Harris-Braun}
\affiliation{Ceptr, LLC}
\author{Nicolas Luck}
\affiliation{Ceptr, LLC}
\author{Arthur Brock}
\affiliation{Ceptr, LLC}
\begin{abstract}
ABSTRACT : We present a scalable, agent-centric distributed computing platform.  We use a  formalism to characterize distributed systems, show how it applies to some existing distributed systems, and demonstrate the benefits of shifting from a data-centric to an agent-centric model. We present a detailed formal specification of the Holochain system, along with an analysis of its systemic integrity, capacity for evolution, total system computational complexity, implications for use-cases, and current implementation status.
                                                                            
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

Distributed computing platforms have achieved a new level of viability with the advent of two foundational cryptographic tools: secure hashing algorithms, and public-key encryption.  These have provided solutions to key problems in distributed computing: verifiable, tamper-proof data for sharing state across nodes in the distributed system and confirmation of data provenance via digital signature algorithms.  The former is achieved by hash-chains, where monotonic data-stores are rendered intrinsically tamper-proof (and thus confidently sharable across nodes) by including hashes of previous entries in subsequent entries.  The latter is achieved by combining cryptographic encryption of hashes of data and using the public keys themselves as the addresses of agents, thus allowing other agents in the system to mathematically verify the data's source.

Though hash-chains help solve the problem of independently acting agents reliably sharing state, we see two very different approaches in their use which have deep systemic consequences.  These approaches are demonstrated by two of today's canonical distributed systems: \begin{enumerate}
\item git\footnote{https://git-scm.com/about}:  In git, all nodes can update their hash-chains as they see fit.  The degree of overlapping shared state of chain entries (known as commit objects) across all nodes is not managed by git but rather explicitly by action of the agent making pull requests and doing merges.  We call this approach \term{agent-centric} because of its focus on allowing nodes to share independently evolving data realities.
\item Bitcoin\footnote{https://bitcoin.org/bitcoin.pdf}: In Bitcoin (and blockchain in general), the ``problem" is understood to be that of figuring out how to choose one block of transactions among the many variants being experienced by the mining nodes (as they collect transactions from clients in different orders), and committing that single variant to the single globally shared chain. We call this approach \term{data-centric} because of its focus on creating a single shared data reality among all nodes.
\end{enumerate}

We claim that this fundamental original stance results directly in the most significant limitation of the blockchain: scalability.  This limitation is widely known and many solutions have been offered.  Holochain offers a way forward by directly addressing the root data-centric assumptions of the blockchain approach.

\section{Prior Work}
This paper builds largely on recent work in cryptographic distributed systems and distributed hash tables and multi-agent systems.

Ethereum: Wood \cite{yellowpaper}, DHT: \cite{kademlia}  Benet \cite{ipfs}

\section{Distributed Systems}
\label{sec:data-centric-systems}

\subsection{Formalism}
\label{sec:formalism}

We define a simple generalized model of a distributed system $\Omega$ using hash-chains as follows:
\begin{enumerate}
\item Let $N$ be the set of elements $\{n_1,n_2,\dots n_n\}$ participating in the system. Call the elements of $N$ \term{nodes} or \term{agents}.
\item Let each node $n$ consist of a set $S_n$ with elements $\{\sigma_1,\sigma_2,\dots\}$. Call the elements of $S_n$ the \term{state} of node $n$. For the purposes of this paper we assume $\forall \sigma_i \in S_n : \sigma_i = \{ \chain_i, D_i\}$ with $\chain_i$ being a \term{hash-chain} and $D$  a set of non-hash chain \term{data elements}.
\item Let $H$ be a cryptographically secure hash function.
\item Let there be a \term{state transition function}:
\begin{equation}
\tau(\sigma_i, t) = (\tau_\chain(\chain_i, t), \tau_D(D_i,t))
\end{equation}
where:
\begin{enumerate}
\item
$\tau_\chain(\chain_i,t) = \chain_{i+1}$ where
\begin{equation}
\begin{split}
 \chain_{i+1} &= \chain_{i} \cup \{x_{i+1}\} \\
  &= \{x_1, \dots, x_i, x_{i+1}\}
\end{split}
\end{equation}
with
\begin{equation}
\begin{split}
x_{i+1} &= \{h,t\} \\
h &= \{ H(t),y\} \\
y &= \{H(x_j) | j<i\}
\end{split}
\end{equation}

Call $h$ a \term{header} and note how the sequence of headers creates a chain (tree, in the general case) by linking each header to the previous header(s) and the transaction.
\item $D_\text{i+1}=\tau_\mathrm{D}(\sigma_i,t)$

\end{enumerate}
\item Let $V(t,v)$ be a function that takes $t$, along with extra validation data $v$, verifies the validity of $t$ and only if valid calls a transition function for $t$. Call $V$ a \term{validation} function.
\label{formalism:validation}
\item Let $I(t)$ be a function that takes a transaction $t$, evaluates it using a function $V$, and if valid, uses $\tau$ to transform $S$. Call $I$ the \term{input} or \term{stimulus} function.
\item Let $P(x)$ be a function that can create transactions $t$ and trigger functions $V$ and $\tau$, and $P$ itself is triggered by state changes or the passage of time.  Call $P$ the \term{processing} function.
\item Let $C$ be a channel that allows all nodes in $N$ to communicate and over which each node has a unique address $A_n$. Call $C$ and the nodes that communicate on it the \term{network}.
\item Let $E(i)$ be a function that changes functions $V,I,P$.  Call $E$ the \term{evolution} function.
\end{enumerate}

Explanation: this formalism allows us to model separately key aspects of agents.

First we separate the agent's state into a cryptographically secured hash-chain part $\chain$ and another part that holds arbitrary data $D$. Then we split the process of updating the state into two steps: 1) the validation of new transactions $t$ through the validation function $V(t,v)$, and 2) the actual change of internal state $S$ (as either $\chain$ or $D$) through the state transition functions $\tau_\chain$ and $\tau_D$. Finally, we distinguish between 1) state transitions triggered by external events, stimuli, received through $I(t)$, and 2) a node's internal processing $P(x)$ that also results in calling $V$ and $\tau$ with an internally created transaction.

We define some key properties of distributed systems:
\begin{enumerate}
\item Call a set of nodes in $N$ for which any of the functions $T,V,P$ and $E$ have the properties of being both reliably known and also known to be identical for that set of nodes: \term{trusted} nodes with respect to the functions so known.
\item Call a channel $C$ with the property that messages in transit can be trusted to arrive exactly as sent: \term{secure}.
\item Call a channel $C$ on which the address $A_n$ of a node $n$ is $A_n=H(pk_n)$, where $pk_n$ is the public key of the node $n$, and on which all messages include a digital signature of the message signed by sender: \term{authenticated}.

\item Call a data element that is accessible by its hash \term{content addressable}.
\end{enumerate}
For the purposes of this paper we assume untrusted nodes, i.e., independently acting agents solely under their own control, and an insecure channel.  We do this because the very \textit{raison d'\^etre} of the cryptographic tools mentioned above is to allow individual nodes to trust the whole system under this assumption.  The cryptography immediately makes visible in the state data when any other node in the system uses a version of the functions different from itself.  This property is often referred to as a \term{trustless} system.  However, because it simply means that the locus of trust has been shifted to the state data, rather than other nodes, we refer to it as systemic reliance on \term{intrinsic data integrity}. See \ref{sec:integrity} for a detailed discussion on trust in distributed systems.

\subsection{Data-Centric and Agent-Centric Systems}
\label{sec:data-agent}

Using this definition, Bitcoin can be understood as that system \sbtc where:
\begin{enumerate}
\item $\forall n,m \in N: \chain_n\eqbang\chain_m$ where $\eqbang$ means \textit{is enforced}.
\item $V(e,v)$ $e$ is a block and $v$ is the output from the ``proof-of-work" hash-crack algorithm, and $V$ confirms the validity of $v$, the structure and validity of $e$ according to the double-spend rules.
\item $I(t,n)$ accepts transactions from clients and adds them to $D$ (the \textit{mempool}) to build a block for later use in triggering $V()$.
\item $P(i)$ is the \textit{mining} process including the ``proof-of-work" algorithm and composes with $V()$ and $\tau_\chain$ when the hash is cracked.
\item $E(i)$ is not formally defined but can be mapped informally to a decision by humans operating the nodes to install new versions of the Bitcoin software.
\end{enumerate}

The first point establishes the central aspect of Bitcoin's (and Blockchain applications' in general) strategy for solving or avoiding problems otherwise encountered in decentralized systems, and that is by trying to maintain a network state in which all nodes \textbf{should} have the same (local) chain.

By contrast, for \sgit there is no such constraint on any $\chain_n$, $\chain_m$ in nodes $n$ and $m$ matching, as git's core intent is to allow different agents act autonomously and divergently on a shared code-base, which would be impossible if the states always had to match.

Through the lens of the formalism some other aspects of \sgit can be understood as follows:
\begin{enumerate}
\item the validation function $V(e,v)$ by default only checks the structural validity of $e$ as a commit object not it's content (though note that git does also support signing of commits which is also part of the validation)
\item the stimulus function $I(t)$ for \sgit\ consists of the set of git commands available to the user
\item the state transition function $\tau_\chain$ is the internal git function that adds a commit object and $\tau_\textrm{D}$ is the git function that adds code to the \texttt{index} triggered by \texttt{add}
\item $E$ is, similarly to \sbtc, not formally defined for \sgit.
\end{enumerate}

We leave a more in depth application of the formalism to \sgit\ as an exercise for the reader, however we underscore that the core difference between  \sbtc\ and \sgit\ lies in the former's constraint of $\forall n,m \in N: \chain_n\eqbang\chain_m$.  One direct consequence of this for \sbtc\ is that as the size of $\mathcal{X}_n$ grows, necessarily all nodes of \sbtc\ must grow in size, whereas this is not necessarily the case for \sgit\, and in it lies the core of Bitcoin's scalability issues.

It's not surprising that a data-centric approach was used for Bitcoin.  This comes from the fact that its stated intent was to create digitally transferable ``coins," i.e., to model in a distributed digital system that property of matter known as location. On centralized computer systems this doesn't even appear as a problem because centralized systems have been designed to allow us to think from a data-centric perspective.  They allow us to believe in a kind of data objectivity, as if data exists, like a physical object sitting someplace having a location. They allow us to think in terms of an absolute frame - as if there \textit{is} a correct truth about data and/or time sequence, and suggests that ``consensus" should converge on this truth. In fact, this is not a property of information.  Data exists always from the vantage point of an observer.  It is this fact that makes digitally transferable ``coins" a \textit{hard problem} in distributed systems which consist entirely of multiple vantage points by definition.

In the distributed world, events don't happen in the same sequence for all observers.  For Blockchain specifically, this is the heart of the matter: choosing which block, from all the nodes receiving transactions in different orders, to use for the ``consensus," i.e., what single vantage point to enforce on all nodes.  Blockchains don't record a universal ordering of events -- they manufacture a single authoritative ordering of events -- by stringing together a tiny fragment of local vantage points into one global record that has passed validation rules.

The use of the word consensus seems at best dubious as a description of a systemic requirement that all nodes carry identical values of $\chain_n$.  Especially when the algorithm for ensuring that sameness is essentially a digital lottery powered by expensive computation of which the primary design feature is to randomize which node gets to run $V_n$ such that no node has preference to which $e$ gets added to $\chain_n$.

The term consensus, as normally used, implies deliberation with regard to differences and work on crafting a perspective that holds for all parties, rather than simply selecting one party's dataset at random.  In contrast, as a more agent-centric distributed system, git's \textit{merge} command provides for a processes more recognizable as consensus, however it's not automated.

Perhaps a more accurate term for the hash-crack algorithm applied in \sbtc\ would be ``proof-of-luck" and for the process itself simply sameness, not consensus.  If you start from a data-centric viewpoint, which naturally throws out the ``experience" of all agents in favor of just one, it's much harder to design them to engage in processes that actually have the real-world properties of consensus. If the constraint of keeping all nodes' states the same were adopted consciously as a fit for a specific purpose, this would not be particularly problematic.  Unfortunately the legacy of this data-centric viewpoint has been held mostly unconsciously and is adopted by more generalized distributed computing systems, for which the intent doesn't specifically include the need to model ``digital matter" with universally absolute location.  While having the advantages of conceptual simplicity, it also immediately creates scalability issues, but worse, it makes it hard to take advantages inherent in the agent-centric approach.

\section{Generalized Distributed Computation}
\label{sec:dist-comp}

The previous section described a general formalism for distributed systems and compared git to Bitcoin as an example of an agent-centric vs. a data-centric distributed system.  Neither of these systems, however, provides generalized computation in the sense of being a framework for writing computer programs or creating applications. So, lets add the following constraints to formalism~\ref{sec:formalism} as follows:

\begin{enumerate}
\item With respect to a machine $M$, some values of $S_n$ can be interpreted as: executable code and the results of code execution, and they may be accessible to $M$ and the code.  Call such values the \term{machine state}.
\item $\exists t$ and nodes $n$ such that $I_n(t)$ will trigger execution of that code. Call such transaction values \term{calls}.
\end{enumerate}

\subsection{Ethereum}
Ethereum\footnote{https://github.com/ethereum/wiki/wiki/White-Paper} provides the current premier example of generalized distributed computing using the Blockchain model. The Ethereum approach comes from an ontology of replicating the data certainty of single physical computer, on top of the stratum of a bunch of distributed nodes using the blockchain strategy of creating a single data reality in a cryptographic chain, but committing computations, instead of just monetary transactions as in bitcoin, into the blocks.

This approach does live up to the constraints listed above as described by Wood \cite{yellowpaper} where the bulk of that paper can be understood as a specification of a validation function $V_n()$ and the described state transition function $\sigma_\text{t+1} \equiv \Upsilon(\sigma,T)$ as a specification of how constraints above are met.

Unfortunately the data-centric legacy inherited by Ethereum from the blockchain model, is immediately observable in its high compute cost and difficulty in scaling.


\subsection{Holochain}
\label{holochain}
We now proceed to describe an agent-centric distributed generalized computing system, where nodes can still confidently participate in the system as whole even though they are not constrained to maintaining the same chain state as all other nodes.

In broad strokes: a Holochain application consists of a network of agents maintaining a unique source chain of their transactions, paired with a shared space implemented as a validating, monotonic, sharded, distributed hash table (DHT) where every node enforces validation rules on that data in the DHT as well as providing provenance of data from the source chains where it originated.

Using our formalism, a Holochain based application \shc is defined as:

\begin{enumerate}

\item Call $\chain_n$ the \term{source chain} of $n$.

\item Let $M$ be a virtual machine used to execute code.

\item Let the initial entry of all $\chain_n$ in $N$ be identical and consist in the set \hcdna $\{e_1,e_2,\dots,f_1,f_2,\dots,p_1,p_2,\dots\}$ where $e_x$ are definitions of entry types that can be added to the chain, $f_x$ are functions defined as executable on $M$ (which we also refer to as the set $\appfns = \{app_1,app_2,\dots\}$), and $p_x$ are  system properties which among other things declare the expected operating parameters of the application being specificed.  For example the resilience factor as defined below is set as one such property.

\item Let $\hcid_n$ be the second entry of all $\chain_n$ and be a set of the form $\{p,i\}$ where $p$ is the public key and $i$ is identifying information appropriate to the use of this particular \shc. Note that though this entry is of the same format for all $\chain_n$ it's content is not the same. Call this entry the \term{agent identity} entry.

\item $\forall e_x \in DNA$ let there be an $app_x \in \appfns$ which can be used to validate transactions that involve entries of type $e_x$.  Call this set $F_\mathrm{v}$ or the \term{application validation functions}.

\item Let there be a function $V_\mathrm{sys}(ex,e,v)$ which checks that $e$ is of the form specified by the entry definition for $e_x \in$ \hcdna.  Call this function the \term{system entry validation function}.

\item Let the overall validation function $V(e,v) \equiv \bigvee_x  F_\mathrm{v}(e_x)(v) \wedge V_\mathrm{sys}(e_x,e,v)$.

\item Let $F_\mathrm{I}$ be a subset of $\appfns$ distinct from $F_\mathrm{v}$ such that $\forall f_x(t) \in F_\mathrm{I}$ there exists a $t$ to $I(t)$ that will trigger $f_x(t)$. Call the functions in $F_\mathrm{I}$ the \term{exposed functions}.

\item Call any functions in $\appfns$ not in $F_\mathrm{v}$ or $F_\mathrm{I}$ \term{internal functions} and allow them to be called by other functions.
\item Let the channel $C$ be \term{authenticated}.

\item Let $DHT$ define a distributed hash table on an authenticated channel as follows:
\begin{enumerate}

\item Let $\dhtstate$ be a set $\{\delta_1,\delta_2,\dots\}$ where $\delta_x$ is a set $\{key,value\}$ where $key$ is always the hash $H(value)$ of $value$.  Call $\dhtstate$ the \term{DHT state}.
\item Let $\dhtfns$ be the set of functions $\{dht_\text{put},dht_\text{get}\}$ where:
\begin{enumerate}
\item $dht_\text{put}(\delta_\text{key,value})$ adds $\delta_\text{key,value}$ to $\dhtstate$
\item $dht_\text{get}(key) = value$ of $\delta_\text{key,value}$ in $\dhtstate$
\end{enumerate}
\item \label{routable} Assume $x,y \in N$ and $\delta_i \in \dhtstate_x$ but $\delta_i \notin \dhtstate_y$. Allow that when $y$ calls $dht_\text{get}(key)$, $\delta_i$ will be retrieved from $x$ over channel $X$ and added to $\dhtstate_y$.
\end{enumerate}
DHT are sufficiently mature that there are a number of ways to ensure property \ref{routable}.  For our current alpha version we use a modified version of \cite{kademlia} as implemented in \cite{libp2p}.

\item Let $\hcdht$ augment $DHT$ as follows:
\begin{enumerate}

\item $\forall \delta_\text{key,value} \in \dhtstate$ constrain $value$ to be of an entry type as defined in \hcdna.  Furthmore, enforce that any function call $dht_x(y)$ which modifies $\dhtstate$ also uses $F_\mathrm{v}(y)$ to validate $y$ and records whether it is valid.  Note that this validation phase may include contacting the source nodes involved in generating $y$ to gather more information about the context of the transaction, see \ref{sec:membandprov}.

\item Enforce that all elements of $\dhtstate$ only be changed monotonically, that is, elements $\delta$ can only be added to $\dhtstate$ not removed.

\item Include in $\dhtfns$ functions to add and retreive meta-data to elements that point to other elements with tags, thus creating a graph relations between the elments.

\item Let $d(x,y)$ be a \textit{symmetric} and \textit{unidirectional} distance metric within the hash space defined by $H$, as for example the XOR metric defined in \cite{kademlia}. Note that this metric can be applied between entries and nodes alike since the addresses of both are values of the same hash function $H$ (i.e. $\delta_{key}=H(\delta_{value})$ and $A_n=H(pk_n)$).

\item Let $r$ be a parameter of $\hcdht$ to be set dependent on the characteristics deemed beneficial for maintaining multiple copies of entries in the $DHT$ for the given application.
Call $r$ the \term{resilience factor}.

\label{dht:metrics}
\item Allow that each node can maintain a set $M = \{m_n, \dots \}$ of metrics $m_n$ about other nodes, where each $m_n$ contains both a node's direct experience of $n$ with respect to that metric, as well as the experience of other nodes of $n$.  Enforce that one such metric kept is \textit{•}{uptime} which keeps track of the percentage of time a node is experienced to be available.   Call the process of nodes sharing these metrics \term{gossip} and refer to \ref{sec:gossip} for details.

\item  Enforce that $\forall \delta \in \dhtstate_n$ each node $n$ maintains a set $V_\delta = \{n_1,\dots,n_q\}$ of $q$ closest nodes to $\delta$ as seen from $n$, which are \textit{expected by n} to also hold $\delta$. Resiliency is maintained by taking into account node uptimes and choosing the value of $q$ so that:
\begin{equation}
\sum_{i=0}^q uptime(n_i)\geq r
\end{equation}
whith $uptime(n) \in [0,1]$.

Call the union of such sets $V_\delta$, from a given node's perspective, the \term{overlap list} and also note that $q\geq r$.

\item \label{hc:shards} Allow every node $n$ to discard every $\delta_x \in \dhtstate_n$ if the number of closer (with regards to $d(x,y)$) nodes is greater than $q$
(i.e. if other nodes are able to construct their $V_\delta$ sets without including $n$, which in turn means there are enough other nodes responsible for holding $\delta$ in their $\Delta_m$ to have the system meet the resilience set by $r$ even without $n$ participating in storing $\delta$).
Note that this results in the network adapting to changes in topology and DHT state migrations by regulating the number of network-wide redundant copies of all $\delta_i\in\dhtstate$ to match $r$ according to node uptime.

\end{enumerate}

Call $\hcdht$ a \term{validating}, \term{monotonic}, \term{sharded} DHT.

\item $\forall n \in N$ assume $n$ implements $\hcdht$, that is: $\dhtstate$ is a subset of $D$ (the non hash-chain state data), and $\dhtfns$ are available to $n$, though note that these functions are NOT directly available to the functions $\appfns$ defined in \hcdna.

\item Let $\sysfns$ be the set of functions $\{sys_\text{commit},sys_\text{get}, \dots\}$ where:
\begin{enumerate}
\item $sys_\text{commit}(e)$ uses the system validation function $V(e,v)$ to add $e$ to $\chain$, and if successful calls $dht_\text{put}(H(e),e)$.
\item $sys_\text{get}(k) = dht_\text{get}(k)$.
\item Note: there may be additional system functions for a number of other purpose, such as content encryption, decryption, signature validation, etc that $\appfns$ may pragmatically want to rely on.
\end{enumerate}

\item Allow the functions in $\appfns$ defined in the \hcdna\ to call the functions in $\sysfns$.
\item Let $m$ be an arbitrary message. Include in $\sysfns$ the function $sys_\text{send}(A_\text{to},m)$ which when called on $n_\text{from}$ will trigger the function $app_\text{receive}(A_\text{from},m)$ in the \hcdna\ on the node $n_\text{to}$. Call this mechanism \term{node-to-node messaging}.
\item \label{private} Allow that the definition of entries in \hcdna\ can mark entry types as \term{private}. Enforce that if an entry $\sigma_x$ is of such a type then $\sigma_x \notin \dhtstate$. Note however that entries of such type can be sent as node-to-node messages.
\item Let the system processing function $P(i)$ be a set of functions in $\appfns$ to be registered in the system as callbacks based on various criteria, e.g. notification of rejected puts to the DHT, passage of time, etc.
\end{enumerate}

\subsection{Systemic Integrity Through Validation}
\label{sec:integrity}

The appeal of the data-centric approach to distributed computing comes from the fact that if you can prove that all nodes reliably have the same data then that provides strong general basis from which to prove the integrity of the system as a whole.  In the case of Bitcoin, the $\chain$ holds the transactions and the unspent transaction outputs, which allows nodes to verify future transactions against double-spend. In the case of Ethereum, $\chain$ holds what ammounts to pointers to machine state. Proving the consistency across all nodes of those data sets is fundamental to the integrity of those systems.

However, because we have started with the assumption (see \ref{sec:formalism}) of distributed systems of independently acting agents, any \textit{proof} of  $\forall n,m \in N: \chain_n\eqbang\chain_m$ in a blockchain based system is better understood as a \textit{choice} (hence our use of the $\eqbang$),  in that nodes use their agency to decide when to stop interacting with other nodes based on detecting that the $\chain$ state no longer matches.  This might also be called  ``proof by enforcement," and is also appropriately known as a \term{fork} because essentially it results in partitioning of the network.

The heart of the matter has to do with the trust any single agent has is in the system.  In \cite{yellowpaper} Section 1.1 (Driving Factors) we read:
\begin{quote}
Overall, I wish to provide a system such that users can be guaranteed that no matter with which other individuals, systems or organizations they interact, they can do so with absolute confidence in the possible outcomes and how those outcomes might come about.
\end{quote}

The idea of ``absolute confidence" here seems important, and we attempt to understand it more formally and generally for distributed systems.

\begin{enumerate}
\item Let $\Psi_\alpha$ be a measure of the confidence an agent has in various aspects of the system it participates in, where $0 \leq \Psi \leq 1$, 0 represents no confidence, and 1 represents absolute confidence.
\item Let $R_n = \{\alpha_1,\alpha_2,...\dots\}$ define a set of aspects about the system with which an agent $n \in N$ measures confidence.  Call $R_n$ the \term{requirements} of $n$ with respect to $\Omega$.
\item Let $\varepsilon_n(\alpha)$ be a thresholding function for node $n \in N$ with respect to $\alpha$ such that when $\Psi_\alpha < \varepsilon(\alpha)$ then $n$ will either stop participating in the system, or reject the participation of others (resulting in a fork).

\item  Let $R_\mathrm{A}$ and Let $R_\mathrm{C}$ be partitions of $R$ where
\begin{equation}
\begin{split}
\forall \alpha \in R_A:\varepsilon(\alpha)=1\\
\forall \alpha \in R_C:\varepsilon(\alpha)<1
\end{split}
\end{equation}
so any value of $\Psi \neq 1$ is rejected in $R_\mathrm{A}$ and any value $\Psi < \varepsilon(\alpha)$ is rejected in $R_\mathrm{C}$. Call $R_\mathrm{A}$ the \term{absolute requirements} and $R_\mathrm{C}$ the \term{considered requirements}.
\end{enumerate}

So we have formally separated system characteristics that we have absolute confidence in ($R_A$) from those we only have considered confidence in ($R_C$). Still unclear is how to measure a concrete confidence level $\Psi_\alpha$. In real-world contexts and for real-world decisions, confidence is mainly dependent on an (human) agent's vantage point, set of data at hand, and maybe even intuition. Thus we find it more adequate to call it a soft criteria. In order to comprehend this concept objectively and relate it to the notion conveyed by Woods in the quote above, we proceed by defining the measure of confidence of an aspect $\alpha$ as the conditional probability of it being the case in a given context:
\begin{equation}
\Psi_\alpha \equiv \mathcal{P}(\alpha | \mathcal{C})
\end{equation}
where the context $\mathcal{C}$ models all other information available to the agent, including basic and intuitive assumptions.

Consider the fundamental example of cryptographically signed messages with asymetric keys as applied throughout the field of cryptographic systems (basically what coins the term crypto-currency). The central aspect in this context
we call $\alpha_{signature}$ which provides us with the ability to \textit{know with certainty} that a given message's real author $Author_{real}$ is the same agent indicated solely via locally available data in the message's meta information through the cryptographic signature $Author_{local}$. We gain this confidence because we deem it \textit{very hard} for any agent not in possession of the private key to create a valid signature for a given message.
\begin{equation}
\alpha_{signature} \equiv Author_{real} = Author_{local}
\end{equation}

The appeal of this aspect is that we can check authorship locally, i.e., without the need of a 3rd party or direct trusted communication channel to the real author.
But, the confidence in this aspect of a certain cryptographic system depends on the context $\mathcal{C}$:
\begin{equation}
\Psi_{signature} = \mathcal{P}(Author_{real} = Author_{local} | \mathcal{C})
\end{equation}

If we constrain the context to remove the possibility of an adversary gaining access to an agent's private key and also exclude the possible (future) existence of computing devices or algorithms that could easily calculate or brute force the key, we might then assign a (constructed) confidence level of 1, i.e., ``absolute confidence". Without such constraints on $\mathcal{C}$, we must admit that $\Psi_{signature}<1$, which real world events, for instance the Mt.Gox hack from 2014\footnote{"Most or all of the missing bitcoins were stolen straight out of the Mt. Gox hot wallet over time, beginning in late 2011" \cite{mt-gox}}, make clear.

We aim to describe these relationships in such detail in order to point out that any set $R_A$ of \textit{absolute requirements} can't reach beyond trivial statements - statements about the content and integrity of the local state of the agent itself. Following Descarte's way of questioning the confidence in every thought, we project his famous statement \textit{cogito ergo sum} into the reference frame of multi-agent systems by stating: \textbf{Agents can only have honest confidence in the fact that they perceive a certain stimulus to be present and whether any particular abstract a priori model matches that stimulus without contradiction,} i.e., that an agent sees a certain piece of data and that it \textit{is possible to interpret it in a certain way}. Every conclusion being drawn a posteriori through the application of sophisticated models of the context is dependent on assumptions about the context that are inherent to the model. This is the heart of the agent-centric outlook, and what we claim must always be taken into account in the design of decentralized multi-agent systems, as it shows that any aspect of the system as a whole that includes assumptions about other agents and non-local events must be in $R_C$, i.e., have an a priori confidence of $\Psi<1$. Facing this truth about multi-agent systems, we find little value in trying to force an absolute truth $\forall n,m \in N: \chain_n\eqbang\chain_m$ and we instead frame the problem as:
\\
\begin{quote}
We wish to provide generalized means by which decentralized multi-agent systems can be built so that:
\begin{enumerate}
\item fit-for-purpose solutions can be applied in order to optimize for application contextualized confidences $\Psi_\alpha$,
\item violation of any threshold $\varepsilon(\alpha)$ through the actions of other agents can be detected and managed by any agent, such that
\item the system integrity is maintained at any point in time or, when not, there is a path to regain it.
\end{enumerate}
\end{quote}

We perceive the agent-centric solution to these requirements to be the holographic management of system-integrity within every agent/node of the system through application specific validation routines. These sets of validation rules lie at the heart of every decentralized application, and they vary across applications according to context. Every agent carefully keeps track of their representation of that portion of reality that is of importance to them - within the context of a given application that has to manage the trade-off between having high confidence thresholds $\varepsilon(\alpha)$ and a low need for resources and complexity.

For example, consider two different use cases of transactions:
\begin{enumerate}
\item receipt of an email message where we are trying to validate it as spam or not and
\item commit of monetary transaction where we are trying to validate it against double-spend.
\end{enumerate}
These contexts have different consequences that an agent may wish to evaluate differently and may be willing to expend differing levels of resources to validate. We designed Holochain to allow such validation functions to be set contextually per application and expose these contexts explicitly. Thus, one could conceivably build a Holochain application that deliberately makes choices in its validation functions to implement either all or partial characteristics of Blockchains. Holochain, therefore, can be understood as a framework that opens up a spectrum of decentralized application architectures in which Blockchain happens to be one specific instance at one end of this spectrum.

In the following sections we will show what categories of validation algorithms exist and how these can be stacked on top of each other in order to build decentralized systems that are able to maintain integrity without introducing an absolute truth every agent would be forced to accept or consider.

\subsubsection{Intrinsic Data Integrity}
\label{sec:intrinsic}
Every application but the most low-level routines utilize non-trivial, structured data types.
Structured implies the existence of a model describing how to interpret raw bits as an instance of a type and how pieces of the structure relate to each other.
Often, this includes certain assumptions about the set of possible values.
Certain value combinations might not be meaningful or violate the intrinsic integrity of this data type.

Consider the example of a cryptographically signed message $m=\{body, signature, author\}$,
where $author$ is given in the form of their public key.
This data type conveys the assumption that the three elements $body$, $signature$ and $author$ correspond to each other
as constrained by the cryptographic algorithm that is assumed to be determined through the definition of this type.
The intrinsic data integrity of a given instance can be validated just by looking at the data itself and checking the signature by
applying the cryptographic algorithm that constitutes the central part of the type's a priori model.
The validation yields a result $\in \{true,false\}$ which means that the confidence in the intrinsic data integrity is absolute, i.e. $\Psi_{intrinsic}=1$.

Generally, \textbf{we define the intrinsic data integrity} of a transaction type $\phi$ as an aspect
$\alpha_{\phi,intrinsic}\in R_A$, expressed through the existence of a deterministic and local
validation function $V_\alpha(t)$ for transactions $t\in\phi$ that does not depend on any other inputs
but $t$ itself.

Note how the intrinsic data integrity of the message example above does not make any assumptions about any message's real author, as the aspect $\alpha_{signature}$ from the previous section does.
With this definition, we focus on aspects that don't make any claims about system properties non-local to the agent under consideration, which roots the sequence of inferences that constitutes the validity and therefore confidence of a system's high-level aspects and integrity in consistent environmental inputs.

\subsubsection{Membranes \& Provenance}
\label{sec:membandprov}

Distributed systems must rely on mechanisms to restrict participation by nodes in processes that without such restriction would compromise systemic integrity.
Systems where the restrictions are based on the nodes' identity, whether that be as declared by type or  authority, or collected from the history of the nodes' behaviors, are know as \term{permissioned} \cite{CaaS}.
Systems where these restrictions are not based on properties of the nodes themselves are known as \term{permissionless}.
In permissionless multi-agent systems, a principle threat to systemic integrity comes from \textit{Sybil-Attacks} \cite{sybil}, where an adversary tries to
overcome the system's validation rules by spawning a large number of compromised
nodes.\\

However, for both permissioned and permissionless systems, mechanisms exists to gate
participation.
\label{mebandprov:membfn}
Formally: \\

Let $M(n,\phi,z)$ be a binary function that evaluates whether transactions of type $\phi$ submitted by $n\in N$ are to be accepted, and where $z$ is any arbitrary extra information needed to make that evaluation.  Call $M$ the \term{membrane} function, and note that it will be a component of the validation function $V(t,v)$ from the initial formalism\ref{formalism:validation}.\\

In the case of \sbtc\ and $\Omega_{ethereum}$, $M$ ignores the value of $n$ and makes its determination solely on whether $z$ demonstrates the ``proof" in proof-of-\textit{X} be it \textit{work} or \textit{stake} which is a sufficient gating to protect against Sybil-Attacks.

Giving up the data-centric fallacy of forcing one absolute truth $\forall n,m \in N: \chain_n\eqbang\chain_m$ reveals that we can't discard transaction provenance.
Agent-centric distributed systems instead must rely on two central facts about data:
\begin{enumerate}
\item it originates from a source and
\item its historical sequence is local to that source.
\end{enumerate}
For this reason, \shc\ splits the system state data into two parts:
\begin{enumerate}
\item each node is responsible to maintain its own entire $\chain_n$ or \term{source chain} and be ready to confirm that state to other nodes when asked and
\item all nodes are responsible to share portions of other nodes' transactions and those transactions' meta data in their \textbf{DHT shard} - meta data includes validity status, source, and optionally the source's chain headers which provide historical sequence.
\end{enumerate}

Thus, the DHT provides distributed access to others' transactions and their evaluations of the validity of those transactions.
This resembles how knowledge gets constructed within social fields and through interaction with others, as described by the sociological theory of \textit{social constructivism}.

The properties of the DHT in conjunction with the hash function provide us with a
deterministically defined set of nodes, i.e., a neighborhood for every transaction.
One cannot easily construct a transaction such that it lands in a given neighborhood.
Formally:
\begin{equation}
\begin{split}
\forall t\in\dhtstate: \exists \eta: \mathcal{H}\rightarrow N^r\\
\eta(H(t))=(n_1, n_2, \dots, n_r)
\end{split}
\end{equation}
where the function $\eta$ maps from the range $\mathcal{H}$ of the hash function $H$
to the $r$ nodes that keep the $r$ redundant shards of the given transaction $t$ (see \ref{hc:shards}).

Having the list of nodes $\eta(H(t))$ allows an agent to compare third-party viewpoints regarding $t$, with its own and that of the transaction's source(s).
The randomization of the hash function $H$ ensures that those viewpoints represent
an unbiased sample.
$r$ can be adjusted depending on the application's constraints and the chosen trade-off between costs and system integrity.
These properties provide sufficient infrastructure to create system integrity
by detecting nodes that don't play by the rules - like changing the history or
content of their source chain.
Additionally tooling appropriate for different contexts, including ones where detailed analysis of source chain history may be required, including:
\begin{enumerate}
    \item Countersigning
    \item Notaries (random agents on the network can be selected as such)
    \item Published header examination
    \item Source-chain examination. 
    \item Blocked-lists
    \item etc..
\end{enumerate}

Depending on the application's domain, neighborhoods could become vulnerable to Sybil-Attacks because a sufficiently large percentage of compromised nodes could introduce bias into the sample used by an agent to evaluate a given transaction.
Holochain allows applications to handle Sybil-Attacks through domain specific
membrane functions.
Because we chose to inherently model agency within the system,
permission can be granted or declined in a programmatic and decentralized manner
thus allowing applications to appropriately land on the spectrum between permissioned and permissionless.

In appendix \ref{apdx:membranes}, we provide some membrane schemes that can be
chosen either for the outer
membrane of that application that nodes have to cross in order to talk to
any other node within the application or for any secondary membrane inside
the application.
That latter means that nodes could join permissionless and participate in aspects
of the application that are not integrity critical without further condition
but need to provide certain criteria in order to pass the membrane into application crucial
validation.

Thus, Holochain applications maintain systemic integrity without introducing
consensus and therefore (computationally expensive) absolute truth because 1) any single node uses provenance to independently verify any single transaction with the sources involved in that transaction and 2) because each Holochain application runs independently of all others, they are inherently permissioned by application specific rules for joining and continuing participation in that application's network.
These both provide the benefit that any given Holochain application can tune the expense of that validation to a contextually appropriate level.


\subsubsection{Gossip \& World Model}
\label{sec:gossip}

So far, we have focused on those parts of the validation function $V$ used to verify elements of $\chain$.  However, maintaining system integrity in distributed systems also requires that nodes have mechanisms sharing information about nodes that have broken the validation rules so that they can be excluded from participation. There exist, additionally, forms of bad-acting that do not live in the content of a transaction but in the patterns of transacting that are detrimental to the system, for example, denial of service attacks.

Holochain uses gossip for nodes to share information about their own experience of the behavior of other nodes.  Informally we call this information the node's \term{world model}. In this section we describe the nature of Holochain's gossip protocols and how they build and maintain a node's world model.

In \ref{dht:metrics} we described one such part of the world model, the \textit{uptime} metric and how it is used for maintaining redundant copies of entries.  In \ref{mebandprov:membfn} we defined a membrane function that determines if a node shall accept a transaction and allowed that function to take arbitrary data $z$.  The main source of that data comes from this world model.


More formally:

\begin{enumerate}
\item Recall that each node maintains a set $M$ of metrics $m$ about other nodes it knows about. Note that in terms of our formalism, this world model is part of each node's non-chain state data $D$.
\item Let $m$ be a tuple of tuples: $((\mu,c)_\text{self},(\mu,c)_\text{others})$ which record an experience $\mu$ of a node with respect to a given metric and a confidence $c$ of that experience, both as directly experienced or as "hearsay" received from other nodes.
\item Allow a class of entries stored in $\chain_n$ be used also as a metric $m_w$ which act as a signed declaration of the experience of $n$ regarding some other node.  Call such entries \term{warrants}.  These warrants allow us to use the standard tooling of Holochain to make provenance based, verifiable claims about other nodes in the network, which propagate orthogonally from the usual DHT methods, via gossip to nodes that need to "hear" about these claims so as to make decisions about interacting with nodes.
\item $\forall m \in M$ let the function $G_\text{with}(m)$ return a set of nodes important for a node to gossip \textbf{with} defined by a probabilistic weighting that information received from those nodes will result in changing $m_\text{other}$.
\item $\forall m \in M$ let the function $G_\text{about}(m)$ return a set of nodes important for a node to gossip \textbf{about} defined by the properties of $m$.
\item Define subsets of $G_\text{with}(m)$ according to a correlation with what it means to have low vs. high confidence value $c$:
\begin{enumerate}
\item \textbf{Pull}: consisting of nodes about which a low confidence means a need for more frequent gossip to raise a node's confidence.  Such nodes would include those for which, with respect to the given node, hold its published entries, hold entries it is also responsible for holding, are close the then node (i.e. in its lowest k-bucket), and which it relies on for routing (i.e. a subset of each k-bucket)
\item \textbf{Push}: consisting of nodes about which a high confidence implies a need for more frequent gossip to spread the information about that node.  Such nodes would include ones for which a given node has high confidence is a bad actor, i.e. it has directly experienced bad acting, or has received bad actor gossip from nodes that it has high confidence in being able to make that bad actor evaluation.
\end{enumerate}
\end{enumerate}

The computational costs of gossip depend on the set of metrics that a particular application needs to keep track of to maintain system integrity.  For an application with a very strong membership membrane perhaps only $uptime$ metrics are necessary to gossip about to balance resilience.  But this too may depend on a priori knowledge of the nodes involved in the application.  Applications with very loose membership membranes may have a substantial number of metrics and complex membrane functions using those metrics which may require substantial compute effort.  The Holochain design intentionally leaves these parameters only loosely specified so that applications can be built fit for purpose.

\section{Complexity In Distributed Systems}
\label{sec:complexity}

In this section we discuss the complexity of our proposed architecture for decentralized systems and compare it to the increasingly adopted Blockchain pattern.

Formally describing the complexity of decentralized multi-agent systems is a non-trivial task for which more complex approaches have been suggested (\cite{multi-agent-complex}).
This might be the reason why there happens to be unclarity and misunderstandings within communities discussing complexity and scalability of Bitcoin for example {\cite{bitcoin-complex}}.

In order to be able to have a ball-park comparison between our approach and the current status quo in decentralized application architecture, we proceed by modeling the worst-case time complexity both for a single node $\Omega_{SystemNode}$ as well as for the whole system $\Omega_{System}$ and both as functions of the number of state transitions (i.e., transactions) $n$ and the number of nodes in the system $m$.

\subsection{Bitcoin}
Let $\Omega_{Bitcoin}$ be the Bitcoin network, $n$ be the number of transactions and $m$ be the number full validating nodes (i.e., \textit{miners}\footnote{For the sake of simplicity and focusing on a lower bound of the system's complexity, we are neglecting all nodes that are not crucial for the operation of the network, such as light-clients and clients not involved in the process of validation}) within $\Omega_{Bitcoin}$.

For every new transaction being issued, any given node will have to check the transaction's signature (among other checks, see. \cite{bitcoin-protocol}) and especially check if this transaction's output is not used in any other transaction to reject double-spendings, resulting in a time complexity of
\begin{equation}
c+n
\end{equation}
per transaction. The time complexity in big-O notation per node as a function of the number of transactions is therefore:
\begin{equation}
\Omega_{BitcoinNode}\in O(n^2)
\end{equation}
The complexity handled by one Bitcoin node does not \footnote{not inherently - that is more participants will result in more transactions but we model both values as separate parameters} depend on $m$ the number of total nodes of the system. But since every node has to validate exactly the same set of transactions, the system's time complexity as a function of number of transactions and number of nodes results as
\begin{equation}
\Omega_{Bitcoin}\in O(n^2m)
\end{equation}

Note that this quadratic time complexity of Bitcoin's transaction validation process is what creates its main bottleneck as this reduces the network's gossip bandwidth since every node has to validate every transaction before passing it along. In order to still have an average transaction at least flood through $90\%$ of the network, block size and time can't be pushed beyond 4MB and 12s respectively, according to \cite{scaling}.

\subsection{Ethereum}
Let $\Omega_{Ethereum}$ be the Ethereum main network, $n$ be the number of transactions and $m$ the number of full-clients within in the network.

The time complexity of processing a single transaction on a single node is a function of the code that has its execution being triggered by the given transaction plus a constant:
\begin{equation}
c+f_{tx_i}(n,m)
\end{equation}
Similarly to Bitcoin and as a result of the Blockchain design decision to maintain one single state ($\forall n,m \in N: \chain_n\eqbang\chain_m$, \textit{``This is to be avoided at all costs as the uncertainty that would ensue would likely kill all confidence in the entire system."} \cite{yellowpaper}), every node has to process every transaction being sent resulting in a time complexity per node as
\begin{equation}
c+\sum_{i=0}^n f_{tx_i}(n,m)
\end{equation}
that is
\begin{equation}
\Omega_{EthereumNode} \in O(n \cdot f_{avg}(n,m))
\end{equation}
whereas users are incentivized to hold the average complexity $f_{avg}(n,m)$
of the code being run by Ethereum small
since execution has to be payed for in gas and which is due to restrictions such as the \textit{block gas limit}.
In other words, because of the complexity $\sum_{i=0}^n f_{tx_i}(n,m)$ being burdened upon all nodes of the system, other systemic properties have to keep users from running complex code on Ethereum so as to not bump into the network's limits.

Again, since every node has to process the same set of all transactions, the time complexity of the whole system then is that of one node multiplied by $m$:
\begin{equation}
\Omega_{Ethereum} \in O(nm\cdot f_{tx_i}(n,m))
\end{equation}

\subsection{Blockchain}
\label{sec:complex:blockchain}
Both examples of Blockchain systems above do need a non-trivial computational overhead in order to work at all: the proof-of-work, hash-crack process also called \textit{mining}. Since this overhead is not a function of either the number of transactions nor directly of the number of nodes, it is often omitted in complexity analysis. With the total energy consumption of all Bitcoin miners today being greater than the country of Iceland \cite{mining-consumption}, neglecting the complexity of Blockchain's consensus algorithm seems like a silly mistake.

Blockchains set the block time, the average time between two blocks, as a fixed parameter that the system keeps in homeostasis by adjusting the hash-crack's difficulty according to the network's total hash-rate. For a given network with a given set of mining nodes and a given total hash-rate, the complexity of the hash-crack is constant. But as the system grows and more miners come on-line, which increases the networks total hash-rate, the difficulty needs to increase in order to keep the average block time constant.

With this approach, the benefit of a higher total hash-rate $x_{HR}$ is an increased difficulty of an adversary to influence the system by creating biased blocks (which would render this party able to do double-spend attacks). That is why Blockchains have to subsidize mining, depending on a high $x_{HR}$ as to make it economically impossible for an attacker to overpower the trusted miners.

So, there is a direct relationship between the network's total trusted hash-rate and its level of security against mining power attacks.
This means that the confidence $\Psi_{Blockchain}$ any agent can have in the integrity of the system is a function of the system's hash-rate $x_{HR}$, and more precisely, the cost/work $cost(x_{HR})$ needed to provide it.
Looking only at a certain transaction $t$ and given any hacker acts economically rationally only, the confidence in $t$ being added to all $\chain_n$ has an upper bound in
\begin{equation}
\Psi_{Blockchain}(t) < min\left(1, \frac{cost(x_{HR})}{value(t)}\right)
\end{equation}

In order to keep this confidence unconstrained by the mining process and therefore the architecture of Blockchain itself, $cost(x_{HR})$ (which includes the setup of mining hardware as well as the energy consumption) has to grow linearly with the value exchanged within the system.

\subsection{Holochain}
Let $\Omega_{HC}$ be a given Holochain system, let $n$ be the sum of all public\footnote{private (see:\ref{private}) state transitions, i.e., that are confined to a local $\chain_n$, are completely within the scope of a node's agency and don't affect other parts of the system directly and can therefore be omitted for the complexity analysis of $\Omega_{HC}$ as a distributed system} (i.e., \textit{put} to the DHT) state transitions (\textit{transactions}), let all agents in $\Omega_{HC}$ trigger in total, and let $m$ be the number of agents (= nodes) in the system.

Putting a new entry to the DHT involves finding a node that is responsible for holding that specific entry, which in our case according to \cite{kademlia} has a time complexity of \begin{equation}
c+\lceil{log(m)}\rceil.
\end{equation}
After receiving the state transition data, this node will gossip with its $q$ neighbors which will result in $r$ copies of this state transition entry being stored throughout the system - on $r$ different nodes. Each of these nodes has to validate this entry which is an application specific logic of which the complexity we shall call $v(n, m)$.

Combined, this results in a system-wide complexity per state transition as given with
\begin{equation}
\underbrace{c+\lceil{log(m)}\rceil}_{DHT lookup}
+ q + r \cdot
\underbrace{v(n,m)}_{validation}
\end{equation}
which implies the following whole system complexity in $O$-notation
\begin{equation}
\Omega_{Holochain} \in O(n\cdot(log(m) + v(n,m))
\end{equation}

Now, this is the overall system complexity. In order to enable comparison, we reason that in the case of Holochain without loss of generality (i.e., dependent on the specific Holochain application), the load of the whole system is shared equally by all nodes. Without further assumptions, for any given state transition, the probability of it originating at a certain node is $\frac{1}{m}$, so the term for the lookup complexity needs to be divided by $m$ to  describe the average lookup complexity per node. Other than in Blockchain systems where every node has to see every transaction, for the vast majority of state transitions one particular node is not involved at all. The stochastic closeness of the node's public key's hash with the entry's hash is what triggers the node's involvement. We assume the hash function $H$ to show a uniform distribution of hash values which results in the probability of a certain node being one of the $r$ nodes that cannot discard this entry to be $\frac{1}{m}$ times $r$.  The average time complexity being handled by an average node then is
\begin{equation}
\Omega_{HolochainNode} \in
O\left(\frac{n}{m}\cdot\left(log(m) + v(n,m)\right)\right)
\end{equation}
Note that the factor $\frac{n}{m}$ represents the average number of state transactions per node (i.e., the load per node) and that though this is a highly application specific value, it is an \textit{a priori }expected lower bound since nodes have to process at least the state transitions they produce themselves.

The only overhead that is added by the architecture of this decentralized system is the node look-up with its complexity of $log(m)$.

The unknown and also application specific complexity $v(n,m)$ of the validation routines is what could drive up the whole system's complexity still. And indeed it is conceivable to think of Holochain applications with a lot of complexity within their validation routines. It is basically possible to mimic Blockchain's consensus validation requirement by enforcing that a validating node communicates with all other nodes before adding an entry to the DHT. It could as well only be half of all nodes. And there surely is a host of applications with only little complexity - or specific state transitions within an application that involve only little complexity. \textit{In a Holochain app one can put the complexity where it is needed and keep the rest of the system fast and scalable.}

\section{Implementation}
\label{sec:implementation}

At the time of this writing we have a fully operational implementation of system as described in this paper, that includes two separate virtual machines for writing \hcdna\ functions in JavaScript, or Lisp, along with proof-of-concept implementations of a number of applications including a twitter clone, a slack-like chat system, DPKI, and a set mix-in libraries useful for building applications.

\begin{enumerate}
\item 30k+ lines of Go code.
\item DHT: customized version of libp2p/IPFS's Kademlia implementation.
\item Network Transport: libp2p including end-to-end encryption.
\item Javascript Virtual Machine: Otto \\\url{https://github.com/robertkrimen/otto}.
\item Lisp Virtual Machines: Zygomys \\\url{https://github.com/glycerine/zygomys}.
\end{enumerate}

Additionally we have created a benchmarking suite to examine the processing, bandwidth and storage used in various scenarios, and compared these with Ethereum applications in similar scenarios.  These can be seen here: \\\url{https://github.com/holochain/benchmarks}

We have yet to implement scalability tests for large scale applications, but it is in our roadmap.

\appendix

\section{Membranes}
\label{apdx:membranes}

\begin{itemize}
  \item \textit{Invitation}\\
  One of the most natural approaches for membrane crossing in a space in which
  agents provide identity is to rely on invitation by agents that are already
  in the membrane. This could be invitation:
  \begin{itemize}
    \item by anyone
    \item by an admin (that could either be set in the application's DNA or a
    variable shared within the DHT - both could be mutable or constant)
    \item by multiple users (applying social triangulation)
  \end{itemize}
  \item \textit{Proof-of-Identity / Reputation}\\
  Given the presence of other applications/chains, these can be used to attach the
  identity and its reputation in that chain to the agent that wants to join.
  Since this seems to be a crucial pillar of the ecosystem of Holochain
  applications, we plan to deliver a system-level application called DPKI
  (distributed public key infrastructure) that will function as the main
  identity and reputation platform.
  A prototype of this app was already developed prior to the writing of
  this paper.
  \item \textit{Proof-of-Presence}\\
  Use of notarized national documents/passports/identity cards within the agent
  entry (second entry in $\chain$).
  \item \textit{Proof-of-Service}\\
  Cryptographic proof of delivery of a service / hosting of an application.
  We intend to leverage this technique with our distributed cloud hosting
  application \textbf{Holo}, which we will build on top of Holochain.
  See our Holo Hosting white paper for more detail \cite{hosting-wp}.
  \item \textit{Proof-of-Work}\\
  If the application's requirement is not anonymity, other than the
  cryptographic hash-cracking work applied in most of the Blockchains,
  this could also be useful work that new members are asked to contribute
  to the community
  or a puzzle to proof domain knowledge. Examples are:
  \begin{itemize}
    \item Test for knowledge about local maps to proof citizenship
    \item DNA sequencing
    \item Protein folding
    \item SETI
    \item Publication of scientific article
  \end{itemize}
  \item \textit{Proof-of-Stake / Payment}\\
  Deposit or payment to have agent certified.
  \item \textit{Immune System}\\
  Blacklisting of nodes that don't play by the application rules.

\end{itemize}

\begin{acknowledgments}

We thank Jonathan Paprocki for his care in editing and supporting the mathematical formalizations, and Steve Sawin for his review of this paper, \LaTeXe  support, and moral support.

\end{acknowledgments}
\bibliographystyle{alpha}
\begin{thebibliography}{9}

\bibitem[DUPONT]{dupont}
Quinn DuPont.
\textit{Experiments in Algorithmic Governance: A history and ethnography of “The DAO,” a failed Decentralized Autonomous Organization}
\\\url{http://www.iqdupont.com/assets/documents/DUPONT-2017-Preprint-Algorithmic-Governance.pdf}

\bibitem[EIP-150]{yellowpaper}
Gavin Wood.
\textit{Ethereum: A Secure Decentralised Generalised Transaction Ledger}.
\\\url{http://yellowpaper.io/}

\bibitem[Kademlia]{kademlia}
Petar Maymounkov and David Mazieres
\textit{Kademlia: A Peer-to-peer Information System Base on the XOR Metric}
\\\url{https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf}

\bibitem[Zhang13]{dht}
Zhang, H., Wen, Y., Xie, H., Yu, N.
\textit{
Distributed Hash Table
Theory, Platforms and Applications}

\bibitem[Croman et al 16]{scaling}
Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe Gencer, Ari Juels, Ahmed Kosba, Andrew Miller, Prateek Saxena, Elaine Shi, Emin Gün Sirer, Dawn Song, Roger Wattenhofer,
\textit{On Scaling Blockchains},
Financial Cryptography and Data Security,
Springer Verlag 2016

\bibitem[Bitcoin Reddit]{bitcoin-complex}
/u/mike\_hearn, /u/awemany, /u/nullc et al.
\\\url{https://www.reddit.com/r/Bitcoin/comments/3a5f1v/mike_hearn_on_those_who_want_all_scaling_to_be/csa7exw/?context=3&st=j8jfak3q&sh=6e445294}
Reddit discussion
2015

\bibitem[Marir2014]{multi-agent-complex}
Marir, Toufik and Mokhati, Farid and Bouchelaghem-Seridi, Hassina and Tamrabet, Zouheyr",
\textit{Complexity Measurement of Multi-Agent Systems"},
Multiagent System Technologies: 12th German Conference, MATES 2014, Stuttgart, Germany, September 23-25, 2014. Proceedings,
Springer International Publishing
2014
\\\url{https://doi.org/10.1007/978-3-319-11584-9_13}

\bibitem[Coppock17]{mining-consumption}
Mark Coppock
\textit{THE WORLD’S CRYPTOCURRENCY MINING USES MORE ELECTRICITY THAN ICELAND}
\\\url{https://www.digitaltrends.com/computing/bitcoin-ethereum-mining-use-significant-electrical-power/}

a\bibitem[BitcoinWiki]{bitcoin-protocol}
\textit{Bitcoin Protocol}
\\\url{https://en.bitcoin.it/wiki/Protocol_rules#.22tx.22_messages}
Bitcoin Wiki

\bibitem[IPFS]{ipfs}
Juan Benet
\textit{IPFS - Content Addressed, Versioned, P2P File System (DRAFT 3)}
\\\url{https://ipfs.io/ipfs/QmR7GSQM93Cx5eAg6a6yRzNde1FQv7uL6X1o4k7zrJa3LX/ipfs.draft3.pdf}

\bibitem[LibP2P]{libp2p}
Juan Benet, David Dias
\textit{libp2p Specification}
\\\url{https://github.com/libp2p/specs}

\bibitem[Oxford]{provenance}
Oxford
Online dictionary
\\\url{https://en.oxforddictionaries.com/definition/provenance}

\bibitem[Douceur02]{sybil}
Douceur, John R. (2002).
"The Sybil Attack"
\\\url{https://www.microsoft.com/en-us/research/publication/the-sybil-attack/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F74220%2Fiptps2002.pdf}
International workshop on Peer-To-Peer Systems. Retrieved 23 April 2016.


\bibitem[HoloCurrency]{currency-wp}
Arthur Brock and Eric Harris-Braun 2017
\textit{Holo: Cryptocurrency Infrastructure
for Global Scale and Stable Value}
\\\url{https://holo.host/holo-currency-wp/}

\bibitem[Nilsson15]{mt-gox}
 Nilsson, Kim (19 April 2015).
 \textit{The missing MtGox bitcoins". Retrieved 10 December 2015.}
\\\url{http://blog.wizsec.jp/2015/04/the-missing-mtgox-bitcoins.html}

\bibitem[Swanson15]{CaaS}
Tim Swanson
\textit{Consensus-as-a-service: a brief report on the emergence of permissioned, distributed ledger systems}
April 6, 2015
\\\url{https://pdfs.semanticscholar.org/f3a2/2daa64fc82fcda47e86ac50d555ffc24b8c7.pdf}

\bibitem[HoloHosting]{hosting-wp}
Arthur Brock et al, 2017
\textit{Holo Green Paper}
\\\url{https://files.holo.host/2018/03/Holo-Green-Paper.pdf}


\end{thebibliography}


\end{document}



================================================
File: docs/specs/src/hwp_1_intro.md
================================================
---
title: 'Holochain'
subtitle: 'Distributed Coordination by Scaled Consent, not Global Consensus'
author:
 - Eric Harris-Braun
 - Arthur Brock
 - Paul d'Aoust
date: '2024-11-08 — v2.0'
abstract: |
 We present a generalized system for large-scale distributed coordination that does not rely
 on global consensus, explicating the problem instead through the lens of scaling consent. After
 establishing initial axioms, we illustrate their application for coordination in games, then
 provide a formal outline of the necessary integrity guarantees and functional components needed
 to deliver those guarantees. Finally, we provide a fairly detailed, high-level, technical
 specification for our operating implementation for scaling consent.
documentclass: 'revtex4-1'
---

# Introduction

## Preamble -- A Focus on Practice, Not Just Theory

The original/alpha version of the Holochain white paper[^hwp-alpha] took a formal approach to modeling generalized distributed computation schemes and contrasted Holochain's approach with blockchain-based systems. It also provided formal reasoning for the scaling and performance benefits of Holochain's agent-centric approach.

[^hwp-alpha]: <https://holochain.org/documents/holochain-white-paper-alpha.pdf>

When dealing with distributed systems, however, the application of logical models and formal proofs are often deceiving. This stems from how easy it is to define sets and conditions which are logically solid in theory but fundamentally impossible and unintelligible in practice. Since our primary intent with Holochain is to provide a highly functional and scalable framework for sophisticated decentralized coordination, our focus must be on what is practicable, and resist the pull of the purely conceptual which frequently steers builders into unwieldy architectures.

Note how easy it is to reference a theoretical set like "all living persons" or "all nodes in the network." But in reality, it is impossible to identify that set in the physical world. Even if one could eliminate the fuzzy boundaries in the meaning of "persons" or "living," there is no way to discover and record the information quickly enough about who is dying, being resuscitated, and being born to construct the actual data set. Likewise, no single agent on a network can determine with certainty which nodes have come online, gone offline, or have declared themselves as new nodes. Also, since network partitions are a real, at any moment, one must question which partition is considered "the network," and how to enable a single node or group of nodes to continue operating appropriately even when no longer connected to the main network.

The initial example should be a comparatively easy data set to work with, since it changes relatively slowly. Typically each person undergoes a state change only twice in their life (when they become a living person, and when they cease to be one). However, the real-world use-cases that modern distributed tooling needs to handle involve data sets with much more rapid and complex changes. A more apt logical construct might be "all people with just one foot on the ground". Membership in this set changes quite rapidly -- around 1/2 billion times per second[^steps].

[^steps]: *Globally, The Average Person Walks About 5,000 Steps Per Day*, American Institute of Cancer Research, 2017 <https://www.aicr.org/news/globally-the-average-person-walks-about-5000-steps-per-day/>.

It should be obvious there is no practical way to work with that data set without requiring actions that either break reality (like freezing time) or asserting a god-like, omniscient being, who not only has instantaneous access to all knowledge (requiring information propagation faster than the speed of light), but also has infinite attention to all states (likely requiring infinite energy). However, since current computing architectures are bound by laws of physics, we should avoid the temptation of injecting such impossible constructs into our theoretical models. A proof that involves simple logical concepts which cannot be reliably worked with in practice is not much of a proof at all.

"Global state" and strategies for consensus about it are exactly one of these dodgy constructs which are easy concepts, but involve a drastic reduction of complexity, agency, and degrees of freedom to reflect a small subset of events happening in physical reality. Yet most current distributed systems undertake the expensive task of having each node construct and maintain this unwieldy global fiction. So, for example, although many blockchains run on tens of thousands of processors, they advance in lockstep as if a single-threaded process, and they are only reliable for very simple world models, like moving tokens (subtracting from a number in one address and adding it to the number in another address).

"State" within the local computing context is likely rooted in the concept of the Turing Tape[^turing] or Von Neumann linear memory address architecture[^memory] which assume a single tape head or system bus for making changes to the single local memory space where changes are stored. With the introduction of multi-core processors, programmers encountered the myriad problems of having multiple agents (CPUs) operating on just one shared local state. They developed various strategies to enforce memory safety for concurrent local operations. So, in distributed computing, people extrapolated these local strategies and starting inventing some new ones, **still in the attempt to manage one single state across many physical machines.** The assumption of the need to sustain this simple logical concept of managing one global state persisted, even when that concept was mapped onto a physical topology which made it fundamentally unknowable in practice.

[^turing]: See <https://en.wikipedia.org/wiki/Turing_machine>.

[^memory]: See <https://en.wikipedia.org/wiki/Von_Neumann_architecture>.

Early influential works in decentralized computing (such as the Byzantine Generals Problem[^byzantine]) may have also set such expectations. Those papers were written in the context of reaching consensus in finite control systems where there was a known number of sensors and states, and the goal was to reach a unified decision (like nine generals deciding a time for all to attack). Therefore, to be Byzantine Fault Tolerant, seems simply that a system is tolerant of the kinds of faults introduced by the generals problem (messages that are corrupted, misordered, or lost and generals/nodes that are malicious), but most distributed systems have assumed that global consensus must be constructed and maintained in order to reach a unified outcome. In this paper, we will detail some more efficient paths to enable agents to act without a construct of global consensus at all, yet still have strong guarantees that even when nodes act in partitioned groups or individually, they will reach a unified outcome.

[^byzantine]: Some readers may come to the problems of distributed coordination from the framework laid out by the literature on Byzantine Fault Tolerance such as *The Byzantine Generals Problem*, Leslie Lamport, Robert Shostak, and Marshall Pease <https://lamport.azurewebsites.net/pubs/byz.pdf>, and *Reaching Agreement in the Presence of Faults*, Marshall Pease, Robert Shostak, and Leslie Lamport <https://dl.acm.org/doi/pdf/10.1145/322186.322188>. For a story elucidating our frame that isn't about generals coordinating an attack, please see our paper *The Players of Ludos: Beyond Byzantium* <https://holochain.org/documents/holochain-players-of-ludos.pdf>.

So, while the formalizations from the original Holochain white paper are still valid in theory, this white paper is more concerned with addressing what works in practice. We will start by stating our underlying assumptions as axioms -- each of which correlates to architectural properties of Holochain. And we will take special care not to make grand, categorical statements which cannot be implemented inside the constraints of computational systems bounded by the laws of physics.

## Axioms -- Our Underlying Basis for Coordination

Here we spell out the assumptions upon which we have built our approach to address the challenges of decentralized coordination.

First, let us clarify what we mean by coordination. Our goals for coordination are:

* To enable a group to establish ground rules which form the context needed for coordination,
* To enable agents in the group to take effective or correct action inside that context,
* To protect agents and the group from incorrect actions any agent may take.

### Axioms for Multi-agent Coordination Through Scaled Consent

1. Agency is defined by the ability to take individual action.
2. "State" is persisted data that an agent can access through introspection.
3. It is easy to agree on a starting state; therefore it is easy to establish ground rules for coordination up front.
4. It is hard to maintain a unified, dynamic, shared state across a network, because of the constraints of physics.
5. Since only local time is knowable, non-local ordering is constructed by explicit reference.
6. Agents always act based only on their state; that is, data they can access locally.
7. Incorrect actions taken by an agent must harm only themself.
8. Long-term coordination must include the ability to orchestrate changes to ground rules.

### Detailed Axioms and Architectural Consequences

The aforementioned axioms have affected the design of Holochain in the following ways.

**Agency is defined by the ability to take individual action:** Each agent is the sole authority for changing their state; the corollary of this is that an agent _cannot_ change other agents' states. Since Holochain uses cryptography to eliminate many types of faults, this primarily means constructing a public/private key pair and using it to sign state changes recorded on an append-only log of the agent's actions. The log contains only actions of this agent, and writing to it (changing their own state), then sharing their state changes, is essentially the only authority (in terms of authorship) an agent has.

**"State" is persisted data, local to an agent, that the agent can access through introspection:** Because each agent is the sole author of their state[^ephemeral-state], agents interact with their world by sharing their own state changes and discovering, querying, and integrating state changes from other agents. Agents must be able to safely do so regardless of whether the peer delivering such data is its original author. Once an agent holds the data (whether because they authored it or received via networked communication) it is now part of their introspectable state. To act on such data, the agent still must verify whether it is true/false, complete/incomplete, authentic/forged, isolated/connected, reliable/suspect, etc.

[^ephemeral-state]: The "state" of a Holochain app does not generally include ephemeral, non-persisted data such as what network sessions with other agents one may currently have open, although Holochain itself uses that kind of data to drive coordination.

**It is easy to agree on a starting state; therefore it is easy to establish ground rules for coordination up front:** The very first entry in an agent's state log for an app is the hashed reference to the code which establishes the grammar of coordination for that app. This code defines data structures, linking patterns, validation rules, business logic, and roles which are used and mutually enforced by all participating agents. The hash of this first entry defines the space and methods of coordination -- agents only need to coordinate with other agents who "speak the same language". This establishes intentional partitions between networks in support of scalability, because an agent does not need to interact with all agents running Holochain apps, only the agents operating under the same ground rules. This simplifies and focuses overhead for validation and coordination.

**It is hard to maintain a unified, dynamic, shared state across a network, because of the constraints of physics:** In a distributed and open system, which enables autonomous agents to make valid changes to their state and freely associate with other agents in order to communicate those state changes to them, one cannot expect any one agent to know the state of all other agents across whatever partitions they may be in and whatever realtime changes they may be making. Such an assumption requires either centralization or omniscience. However, it is feasible to ensure strong eventual consistency[^strong-eventual-consistency], so that when any agents interact with and integrate the same data, all will converge to matching conclusions about the validity of any state change it represents.

[^strong-eventual-consistency]: *Conflict-free Replicated Data Types*, Marc Shapiro, Nuno Preguiça, Carlos Baquero, Marek Zawirski <https://pages.lip6.fr/Marc.Shapiro/papers/RR-7687.pdf>.

**Since only local time is knowable, non-local ordering is constructed by explicit reference:** In the physical universe, entities experience time only as a local phenomenon; any awareness of other entities' experience of time comes from direct interaction with them. Thus, "global time" as a total ordering of causal events does not exist and entities' interactions form only partial orderings.

In Holochain, "direct interaction" comes in the form of explicit hash references to other data. The first simple structure for constructing order by reference is that each agent's action log is structured as an unbroken hash chain, with every new action containing the hash of the prior action. (Timestamps are also included, but are understood to be an expression of local time only.) When agents make state changes that rely on their prior state, the chain automatically provides explicit sequence. When an agent's action has logical dependencies on another agent's data, they must reference the hash of those action(s) to provide explicit ordering. In almost every application, there is no need to construct absolute time or sequencing to guarantee correct action; the only applications that absolutely require this are ones that deal with rivalrous state data, that is, exclusive ownership of a resource. If the problem cannot be restructured to eliminate all rivalrous state data, Holochain provides tools to implement conflict resolution or micro-consensus for that small subset of data for which it remains useful.

**Agents always act based on their state; that is, data they can access locally:** Since agents must act on what they know, they should be able to act *as soon* as they have have whatever local knowledge they need to take an action, with the assurance that any other nodes validating their action will reach the same conclusion. There is no reason to wait for other agents to reach a state *unless that is the confidence threshold required* to take that particular action. It is possible to architect agent-centric solutions to most decentralized problems which are many orders of magnitude more efficient than managing global, data-centric consensus. For example, this even includes building cryptocurrencies structured as peer-to-peer accounting instead of global tracking of token ownership, enabling the transacting agents (the only ones who are changing their states) to validate each other's actions and countersign a transaction independent from the rest of the network, who will validate it when they receive it after it is done.

**Incorrect actions taken by an agent must harm only themself:** We mentioned in the goals of collaboration that incorrect actions must not harm other agents or the integrity of the group. This is accomplished via the validation that occurs when data is persisted in the network. When a node receives a data element which it is supposed to store and serve as part of the architecture of global visibility into local state changes, they must validate it before integrating and serving it to others. For the previous example of a cryptocurrency, if the sender did not have enough credits in their chain state for the amount they are sending, the transaction would fail validation. The validating agents mark this action invalid, add both parties who signed the transaction to their blocked peers list, and publish a "warrant" letting others know about the corrupt nodes so other agents can block them. These warrants function as an immune system which protects individuals and the group from malicious or corrupt actors. The agents did not need to be prevented from taking the bad action, because they only changed their own states, and the bad action becomes the proof needed for the warrant to protect others from it.

**Long-term coordination must include the ability to orchestrate changes to ground rules:** Coordination cannot be effective without comprehension of the real-world context within which it is happening. However, agents cannot fully comprehend their context at first; understanding comes with interaction over time. And as the agents interact with their context, they may need to evolve their understanding, as they encounter new new situations which were not comprehended when the current ground rules were established. Finally, the act of interacting with a context changes the context itself. Any "grammar" in which ground rules are written must be expressive enough to write rules that address the needs of the problem domain as well as a capacity to evolve rules in response to changing context. In Holochain, the ground rules for a group are expressed in executable code. Its tools include an affordance for agents to migrate to a new group with a new set of rules, as well as the ability for an agent to "bridge" their presence in two different groups via cross-space function calls.

Building a distributed coordination framework starting from these axioms results in a system that empowers agents to take independent and autonomous action with partial information as soon as they have whatever they need to ensure it is a correct action. This constitutes a significant departure from the frame of thinking that Byzantine Fault Tolerance traditionally assumes: that complete consensus must be constructed *before* an agent can act.

## From Global Consensus to Scaled Consent

We start from the understanding that networks, social spaces, and decentralized activities are inherently uncertain. Thus, coordination is never about absolute certainty; rather, it is about the capacity to remove sufficient uncertainty to provide confidence for action, which is always contextual. In distributed systems, it is absolutely fundamental to understand that every action taken by an agent happens because that agent has crossed a confidence threshold of some sort -- from its own point of view, the action is appropriate to take.

### Fault Tolerance and Reducing Uncertainty

Like blockchain and other cryptographic systems, Holochain starts the path of establishing confidence by leveraging cryptography to reduce uncertainties which remove most of the sources of Byzantine faults:

* **Corrupt Messages:** Data is retrieved by cryptographic hash which makes records self-validating, ensuring they are as requested and remain uncorrupted by checking data received against requested hash. Network messages are also cryptographically signed.
* **Misordered Messages:** Each agent writes their actions to a local append-only cryptographic hash chain, and must make explicit references to the hashes of any other agent's actions which one of their actions logically relies on, thus establishing indisputable ordering of data.
* **Lost Messages:** If the validity of any action relies on prior data, there will be either missing hash references or chain links which can be explicitly retrieved or have validation paused until available.
* **Forged Messages or Actions:** Each action is signed in sequence to its author's local hash chain. The public signing key is the same as the agent's address on the network. Hence all actions or messages are self-validating with respect to identity of author.
* **Malicious Actors/Actions:** Actions are validated based on local state established by the sequence of actions in an agent's hash chain, plus any actions included by reference. This enables every peer who is responsible for performing such validation to reach the same deterministic conclusion regarding validity.

These strategies help reduce sources of uncertainty; however, when it comes to concerns related to "consensus," still admit actions which individually pass validation but conflict with each other in content, substance, or perspective.

### Increasing Gradients of Certainty

Given the above, we propose a very simple approach to creating tooling capacity for building increasing certainty: **enable validated global visibility, on demand, of local state**. In this approach, we distinguish between *authorship*, which is about local state changes initiated by agents, and *responsibility*, which is about distributing the workload of validating and serving records of local state changes across the participants in the network. This approach requires that we:

1. Ensure that all agents can *reliably* see what's going on; i.e., offer a framework for adding to and querying a collectively held database in which there is a minimum or "floor" of certainty regarding the contents and authorship of data even in the presence of an unbounded number of adversaries.

2. Ensure that all agents know the "ground-rules"; i.e., offer a framework for composing many small units of social agreement in which players can add elements of deterministic certainty into their interactions, yielding an appropriate level of certainty ranging from arbitrarily low to arbitrarily high.

The first point we deliver through various types of **Intrinsic Data Integrity**. We use a number of cryptographic methods to create self-proving data of various types:

* **Provenance**: An agent's network address is their public key. Thus, when interacting with agents it's possible to have deterministic confidence in whom one is interacting with because there is no identity layer between network locations subject to attack surface. I.e., unlike a web address, you don't need a domain name certificate associated with the domain name to become confident of "whom" you are talking to.

* **Signatures**: Because provenance is a public key, it's also easy to create self-proving authenticity. All messages sent, and all data committed to chains, is signed by agents using their public key. Thus any agent can immediately, and with high confidence, verify the authenticity of messages and data.

* **Hashes**: All shared data in a Holochain application is addressed by its hash. Thus, when retrieving data it's possible to have deterministic confidence that it hasn't been tampered with by whoever was storing or relaying it.

* **Monotonicity**: The system is both structurally and logically monotonic. Structurally, local state is append-only and shared state can only grow. Data can be marked as deleted, but it is never actually removed from the history of the agent who authored it. Logically, once any peer has identified that a state change is invalid, no peers should identify it as valid.

* **Common genesis**: The validation rules and joining criteria of an application are the first entries in every agent's chain. This provides a mechanism for self-proving, shared ground rules. Any agent can examine the chain of any other agent all the way back to the source and thus have high confidence that they have actually committed to play by the same rules.

Building upon this fundament, we deliver the second point through the ability to compose various types of **Validation Rules**. Validation Rules create certainty in the following dimensions, with some examples:

* **Content**: a string does not exceed a maximum length
* **Structure**: an entry consists of a certain set of types of data[^content-structure]
* **Sequence**: someone cannot spend credits they have not already received
* **Process**: a transaction must be approved and signed by a notary
* **Behavior**: one does not take an action more frequently than a certain rate
* **Dependency**: an editor can only make changes if another agent has given them prior authorization

[^content-structure]: While Per Martin-Löf demonstrated (see <https://en.wikipedia.org/wiki/Intuitionistic_type_theory>) that values can be unified with classical types into a single dependent type theory (see <https://en.wikipedia.org/wiki/Dependent_type>), thus showing that content and structure can be equivalent and share a single calculus, here we distinguish the two in order to speak a language that is more familiar to programmers.

The two domains of Intrinsic Data Integrity and Validation Rules, and their component dimensionality, amounts to what we might call a "Geometry of Certainty". Inside the clarity of such a geometry, the gradients of certainty become both much more visible, and much easier to build for appropriately. Thus it provides a context of agents being able to scale up their consent to play together in ways that meet their safety requirements. This is why we call our approach "Scaling Consent." It is what enables coherent collaborative action without first achieving global consensus.

### Scaling Coherence across Consenting Agents

The concept of **social coherence** may be the single most important design goal for Holochain applications -- to provide a simple and consistent means of mutually enforcing shared ground rules appropriate to a social context. Some applications may require stricter validation because they contain high-value data with weak trust relationships between peers. Other apps which hold informal data or have higher relational trust between agents may be significantly less strict. Part of Holochain's scalability comes from the ability to implement appropriate coherence for each application's context. To illustrate appropriate social coherence, the existence of and resolution of conflicts in rivalrous data provides some clear examples.

Consider a social microblogging application built on Holochain. Since the precise global ordering of most actions is not vital, there is no reason to undertake the coordination overhead of global consensus for each post, like, follow, unfollow, reply, etc. Instead, simple causal ordering, in which data explicitly refers to its logical predecessors, will suffice for almost all actions.

If this application, unaware of total global ordering, ran at the scale of X (formerly Twitter) with hundreds of millions of daily users, a serious network partition[^latency-as-partition] such as an extended loss in intercontinental connectivity would not cause a change in functionality for users, beyond being unable to see new posts from the far side of the partition. Old data would be accessible in the near side of the partition, and everything would keep functioning for both hemispheres. If this continued for a week, and the partition was resolved, all the data from both hemispheres would merge gracefully except for one possible source of conflict -- new username registrations -- because they are the only rivalrous data in such an application.

[^latency-as-partition]: It should be noted that communication latency induces conditions equivalent to a network partition, differing only in scope; therefore, there is still a risk of conflicting username registrations even in an unpartitioned network.

Now, a given group's rules for social coherence may not require username registrations to be unique across all participants. Systems that refer to participants by a random unique key, allowing participants to identify themselves and others by assigning non-unique "petnames"[^petnames] (personally meaningful identifiers) to those keys, are proven to be usable in cases such as Signal Messenger and Secure Scuttlebutt[^petnames-usecases].

[^petnames]: *An Introduction to Petname Systems*, Marc Stiegler, 2005 <http://www.skyhunter.com/marcs/petnames/IntroPetNames.html>.

[^petnames-usecases]: See <https://signal.org> and <https://scuttlebutt.nz>.

But let us assume that users of this application demand unique usernames in the manner of X. It could employ one of a number of strategies for resolving or preventing conflicts:

1. Users could timestamp username registrations using a neutral, trusted timestamping strategy such as Roughtime[^roughtime], and the application would automatically resolve a conflict by favoring the earliest registration time.
2. Conflicts could be permitted, but upon detection, a social resolution procedure could be engaged, possibly processing through multiple stages if less costly strategies fail. Assisted by logical or cryptographic algorithms, this procedure could take forms such as:
    * A relationship-building approach, in which contestants are encouraged to sort it out amongst themselves, ending in one or both parties releasing their registration,
    * Awarding the username to the participant with the highest reputation or social capital, or
    * An auction.
3. Similar to client/server or blockchain approaches, a set of one or more witnesses can be elected to approve all name registrations and ensure there are no conflicts. While this approach prevents conflicts from happening, it requires a majority of a known set of witnesses, and any partition which contains a minority will not be able to process registrations.

[^roughtime]: Roughtime IETF draft, W Ladd, Akamai Technologies, M Dansarie, Netnod, 2024 <https://datatracker.ietf.org/doc/draft-ietf-ntp-roughtime/>.

Each one of these strategies achieves the same outcome while embodying very different patterns of social coherence; and in each case, there was no resolution overhead expended on all the non-rivalrous forms of data.

Another commonly used example of the rivalrous data problem is a "double-spend" attempt in a cryptocurrency. This involves fooling two separate parties into receiving units from the same pool of currency, such that the same units are sent twice, thus artificially expanding one's outflow of currency beyond what should be possible. Each transaction is valid in isolation, but conflicts with the other.

Cryptocurrencies in Holochain are typically implemented as accounting records stored in the histories of individual agents' hash chains --  If Alice sends Bob 1 million credits of a currency, only Alice and Bob are changing their states, so they can move forward with the transaction as soon as they are confident the other party has committed to do so, and that all parties are in a position to do so validly.

Accordingly, the double-spend problem presents differently. It involves Alice "forking" her own hash chain: if Alice tries to send her 1 million credits to Bob and Carol simultaneously, then each of those transaction records in her chain will point to the same parent record hash. Each looks valid on its own, but taken together they demonstrate an invalid chain fork.

The protections an application implements against this kind of forking may be very different based on the social context and purpose of the application. At its most basic, the application's shared database (described in requirement 1 under [Increasing Gradients of Certainty]) acts as an "ambient witness" to all transactions. This allows agents to see each other's past behavior, including whether they have forked their chains.

An application which assumes high network uptime, low latency, and low risk of partitions might prevent this by requiring a time delay between Alice's promise of funds and Bob or Carol's acceptance, giving them time to check the shared database for proof that Alice has published a promise to them, and only them. Upon detection of the fork neither will accept the funds.

However, if this currency were designed to work in regions with unreliable network connectivity but strong, long-term trust relationships between members, it may not require such protections. This would increase the risk of Alice forking her chain, but it could provide a way for double-spends to be remedied after the fact. If both Bob and Carol know Alice, where her business is, or where she lives, there are social repercussions to cheating, and Alice will have an incentive to fix her public record of trying to cheat people, for instance cancelling one branch of the fork and eventually delivering a new valid payment to the recipient who had received payment via the cancelled branch.

The above examples illustrate how the demand for appropriate social coherence drives an application's approach to selecting from affordances that Holochain provides to resolve conflicts and reach unified outcomes. They also demonstrate how coordination overhead becomes unnecessarily high if all non-rivalrous data is treated as rivalrous, and how forcing conflict resolution into a single costly pattern should not apply to all data nor in all social contexts. Agentic assessment of the social context, and mutual enforcement of only the necessary rules for coherence, enables agents to act as soon as their certainty threshold is reached. This is always true, whether it is reached through centralized coordination, a Byzantine Generals' Problem approach, or blockchain consensus algorithms.

## Conclusion

While our axioms may seem obvious to those familiar with distributed and agent-based systems, they yield surprising and often-overlooked consequences when taken to their logical conclusions in the design of a practical distributed system. As we have seen, such a system is likely to be more efficient than a consensus-based system of equivalent functionality in terms of computation, communication, and storage. It is also likely to be more respectful of the agency of individuals and the group than either consensus or centralized systems: there is an underlying theme in these axioms, that of full agency constrained by the obligation to respect the agency of others (and indeed the inability to override their agency).

As we have also argued, and as other authors formally prove[^byzantine-eventual-consistency], such freedom need not compromise the technical or social integrity needed to take confident action. There is a broad space of design possibilities that allow groups to embody non-coercive, highly coherent, contextually appropriate patterns of coordination even in the presence of malicious actors. In the remainder of this paper, we will explore how Holochain's design realizes the expressivity necessary to build these patterns.

[^byzantine-eventual-consistency]: *Byzantine Eventual Consistency and the Fundamental Limits of Peer-to-Peer Databases*, Martin Kleppmann and Heidi Howard, 2020 <https://arxiv.org/pdf/2012.00472>.


================================================
File: docs/specs/src/hwp_2_overview.md
================================================
# Holochain Design Overview: A Game Play Metaphor

It may help to understand the design of Holochain through a well-known pattern of agentic collaboration: playing games.

## Playing Games

People define the rules of a *Game* they want to play together. As *Players* join the *Game* and start playing, they append new *Records* of their play to their own *Action* history. Then they update the *Game Board* by sharing the *Records* of their *Actions*[^chess] with other players.

[^chess]: You can think of this somewhat like correspondence chess, but with substantially more formality.

The first requirement to create social coherence is ensuring that people are playing the same game; therefore, the very first record in every Agent's history is the rules of the game by which they agree to play. Obviously, Players are not in the same game or able to use the same Game Board if they don't start with the same rule set. These rules are the actual computer code that is executed in running the Game, making moves and validating the Actions of all Players.

## System Description

We can describe the system as Agents, who play Games together by taking Actions, the Records of which are held in a distributed Ledger that is built by sharing these Records over a Network with other Agents. We capitalize terms that comprise the ontological units of the system, and which are formally described in the later sections.

### Agents

Agents have these properties:

1. Agents are the only source of Actions in the system, thus Agents are the source of agency. All such Actions are uniquely identifiable as to which Agent took them; i.e., all Actions are signed by public-key cryptography (see Actions below).

2. Agents are uniquely addressable by other Agents.

3. An Agent's address is its public key.

4. Agents share Records of the Actions they take with other Agents through distributed storage so that those Records can be retrieved by other Agents reliably.

5. Agents validate received Actions before storing or sharing them.

6. Agents respond to requests for stored information.

7. Agents can send messages with arbitrary content directly to other Agents.

### Games

Games have these properties:

1. A Game consists of an Integrity specification with these parts:

    1. A deterministic description of the types of data that are used to record some "play" in the game. Such data is called an Entry, where the act of generating such data is called an Action, which is also recorded. Note: both types of data, the content of the play (Entry) and the meta-data about the play (Action), when taken together, are called a Record.

    2. A deterministic description of of the types of relations between Entries or Actions. Such a relation is called a Link.

    3. A deterministic description of a properly formed Membrane Proof, a credential that grants an Agent permission to join a Game. This description may not be able to fully validate a Membrane Proof if its validity depends on data elsewhere on the Game Board, as the Agent's Membrane Proof is checked against this description before they join.

    4. A deterministic description of the validity of an Action and any Entry, Link, or Membrane Proof data it contains, in the context in which it is produced. This may include rules about the contents of data fields, the author's current state (for instance, whether a game move is allowed given their history), presence of dependencies (such as moves by their opponent or certificates permitting them to play a certain role), and any other rules that may be expressed deterministically given the context available to the Action.

2. Along with the Integrity specification, a Game also consists of a Coordination specification. This specification contains instruction sets that wrap the reading, writing, and modification of Actions into function call units and thus serve as an API to the Game. For example, for a blogging "Game" one such function call might be `create_post` which takes a number of Actions that atomically write a number of Records to the Agent's Source Chain, which include a Create-Entry Action for the post as well as Create-Link Actions relating the post to other Entries such as a "category" Entry (see below for definitions of Actions and Source Chain).

3. Each instance of the Game is played on its own Game Board which exists as a unique and independent network of Agents playing that Game. The consequence of this is that Games cannot interact with each other directly, as all action in the system is only taken by Agents. Note that Games can be composed together, but only by groups of Agents all playing across multiple games. This at first may seem like a weakness, but it's part of a key design decision that contributes to the system's overall design goals of evolvability. Essentially this creates the pattern of game-within-a-game. For example a chess tournament is really two games: the game of "chess", and the game of "tournament". Composition thus happens at the edges of games, through the agency of the Agents who are playing both Games, which allows each game to remain coherent on its own terms.

In keeping with the metaphor of Game, we also refer to the Integrity specification as the Validation Rules of the Game.

We also refer to Integrity specification of a Game as its DNA because this evokes the pattern of all the "cells" in the social "body" as being built out of the same instruction set, thus being the ground of self for that social body. We also call an Integrity or Coordination specification generically by the name Zomes (short for chromosomes) as they also function as composable units for building larger bodies of "genetic code" for said body.

### Actions (and Entries and Records)

Actions have these properties:

1. An Action has cryptographic provenance in that it is signed by the Agent that took the Action.

2. Actions are Recorded in a monotonically temporally increasing hash-chain by the Agent that takes the Action. We refer to this as a hash-chain because each Action includes in it the hash of the previous Action, thus creating a tamper-proof Action history.

3. Actions are addressed by the hash of the Action.

4. There are a number of Action types:

    1. `Dna`: An action which contains the hash of the DNA's code, demonstrating that the Agent possesses a copy of the rules of the Game and agrees to abide by them.

    2. `AgentValidationPkg`: An action which presents an Agent's Membrane Proof, the data that proves that they have permission to join a Game.

    3. `Create`: An Action for adding new Game-specific content. We call such content an Entry. The Entry may be declared as public, and will thus be published by the Agent to the network, or declared as private, in which case publishing is limited to just the Action data and not the content. Entries are addressed by their hash, and thus for Create Actions, this Entry hash is included in the Action. Thus sometimes the Action may be understood as "meta-data" where the Entry is understood as "data"[^headers]. Note that Actions and Entries are thus independently addressable and retrievable. This is a valuable property of the system. Note also that many actions (for example ones taken by different agents) may create the exact same Entry; e.g., a hash-tag may be its own entry but be created by different people.

    4. `Update`: An Action which adds new Game-specific content onto the chain that is intended to update an existing entry creation Action (either a Create or an Update).

    5. `Delete`: An Action which indicates a previous entry creation Action should be considered deleted.

    6. `CreateLink`: An Action that unidirectionally links one hash to another.

    7. `DeleteLink`: An Action that indicates a previous link Action should be considered deleted.

    8. `InitZomesComplete`: An action which announces that the Agent has completed any necessary preparations and is ready to play the Game.

    9. `OpenChain`: An action which indicates that an Agent is continuing their participation in this Game from another Source Chain or an entirely different Game.

    10. `CloseChain`: An action which indicates that an Agent is ending their participation in this Game on this Source Chain, and may be continuing their participation in another Source Chain or an entirely different Game.

5. A Record is just a name for both an Action and, when applicable, its Entry, taken together. As an implementation detail, note that for actions other than Create and Update we don't need to address the content of the Action separately, in which case the Record contains no Entry and we simply retrieve all the data we need from the recorded Action.

6. Subsets of Agents can mutually consent to a single Action by atomically recording the Action in their history through Countersigning. Countersigning can also be seen as an affordance in the system for "micro-consensus" when that is necessary.

[^headers]: In many cryptographic systems hash-chains are thought of as having "headers" and "entries". Historically in Holochain development we also used that nomenclature, but realized that the name "header" comes from an implementation detail in building hash chains. Ontologically what's actually happening is that in building such intrinsic integrity data structures, not only must we record the "content" of what is to be written, but we must also record data about the act of writing itself; i.e., who is doing the writing, when they did so, and what they previously wrote. Thus, in keeping with the core ontology of agent-centricity, we switched to using the term "Action" instead of "header", but we retain the name Entry for that which is written.

### The Distributed Ledger

The Ledger, when seen systemically as a whole, consists of the collection of all Records of Actions and their Entries in a Game that have been taken by all the Agents together. The Ledger is stored in two distinct forms:

1. As self-recorded Source Chains of each of the Agent's Actions.

2. As a Graphing Distributed Hash Table that results from the sharing and validation of these Actions across Agents, collectively taking responsibility for validating and storing portions of the data.

The first form ensures the integrity of all data stored in the network because it creates the coherence of provenance and ordering of local state. The second form ensures the validity and visibility of that data.

Note, there is never a point or place where a canonical copy of the entire state of the ledger exists. It is always distributed, either as the Source Chain of Actions taken by a single agent, or broken into parts and stored after validation by other participating Agents in the system. An Agent may elect to take responsibility for validating and storing the entire contents of the Ledger, but as Holochain is an eventually consistent system, their copy can never be said to be canonical.

#### The Ledger as Local State: Source Chain

An Agent's Source Chain for a Game consists of a hash chain of Records of Actions taken by the Agent in accordance with the validation rules of that Game.

A Record consists of an Action which holds context and points to an Entry which is the content of the Action. The context information held by the Action includes:

1. The Action type (e.g. create/update/delete/link, etc)

2. A time-stamp

3. The hash of the previous Action (to create the chain)

4. The sequence index of the Action in the chain

5. The hash of the Entry

The first few Records of every Source Chain - called Genesis Records - create a "common ground" for all the agents "playing" a Game to be able to verify the Game and its "players" as follows:

1. The first Record always contains the full Validation Rules of the Game, and is hence referred to as the DNA. It's what makes each Game unique, and, as part of validation, always allows Agents to check that other Agents are playing the same Game.

2. The second Record is a Game-specific Membrane Proof, which allows Games to create Validation rules for permissioned access to a Game.

3. The third Record is the Agent's address, i.e. its public key.

4. The final Genesis Records are any Game-specific Records added during Genesis, followed by an `InitZomesComplete` Record indicating the end of the Genesis Records.

All subsequent Records in the Source Chain are simply the Actions taken by that Agent. Note that Source Chains may end with a Closing Record which points to an opening record in a new Game.

#### The Ledger as Validated Shared State: Graphing DHT

After Agents record the Actions they take to their Source Chains, they Publish these Actions to other Agents on the Network. Agents receiving published data validate it and make it available to other agents to query, thus creating a distributed database. Because all retrieval requests are keyed on the hashes of Actions or Entries, we describe this database as a Distributed Hash Table (DHT). Because such content-addressable stores create sparse spaces in which discovery is prohibitively expensive, we have extended the usual Put/Get operators of a DHT to include linking hashes to other hashes, thus creating a Graphing DHT.

As a distributed database, the DHT may be understood as a topological transform of many Agents' Source Chain states into a form that makes that data retrievable by all the other Agents for different purposes. These purposes include:

1. Retrieval of Agent's Actions and created Entries

2. Confirmation of "good behavior" by retrieving an Agent's activity history which is used to verify that agents haven't forked their chains

3. Retrieval of link information

4. Retrieval of validation receipts

To achieve this end, we take advantage of the fact that an Agent's public key (which serves as its address) is in the same numeric space as the hashes of the data that we want to store and retrieve. Using this property, we can create a mapping between Agents and the portions of the overall data they are responsible for holding, by using a nearness algorithm between the Agent's public key and the address of the data to be stored. Agents that are "close" to a given piece of data are responsible to store it and are said to comprise a Neighborhood for that data. Hashing creates an essentially random distribution of which data will be stored with which Agents. The degree of redundancy of how many Agents should store copies of data is a per-Game parameter.

Agents periodically gossip with other Agents in their Neighborhood about the published data they've received, validating and updating their Records accordingly. This gossip ensures that eventually all Agents querying a Neighborhood for information will receive the same information. Furthermore it creates a social space for detecting bad actors. Because all gossiped data can intrinsically be validated, any Agents who cheat, including by changing their (or other's) histories, will be found out, and because all data includes Provenance, any bad actors can be definitively identified and ejected from the system.

See the [Formal Design Elements](#holochain-formal-design-elements) section for more information on how Agents convert Source Chain data to operations that are published into the collectively stored data on the DHT, and how this works to provide eventual consistency, and the sections in [System Correctness](#system-correctness-confidence) for details on detection of malicious actors.



================================================
File: docs/specs/src/hwp_3_correctness.md
================================================
# System Correctness: Confidence

In the frame of the Byzantine Generals Problem, the correctness of a distributed coordination system is analyzed through the lens of "fault tolerance". In our frame we take on a broader scope and address the question of the many kinds of confidence necessary for a system's adoption and continued use. We identify and address the following dimensions of confidence:

1.  **Fault Tolerance:** the system's resilience to external perturbations, both malicious and natural. Intrinsic integrity.

2.  **Completeness/Fit:** the system's *a priori* design elements that demonstrate fitness for purpose. We demonstrate this by describing how Holochain addresses multi-agent reality binding, scalability, and shared-state finality.

3.  **Security:** the system's ability to cope with intentional disruption by malicious action, beyond mere detection of faults.

4.  **Evolvability:** the system's inherent architectural affordances for increasing confidence over time, especially based on data from failures of confidence in the above dimensions.

Our claim is that if all of these dimensions are sufficiently addressed, then the system takes on the properties of anti-fragility; that is, it becomes more resilient and coherent in the presence of perturbations rather than less.

## Fault Tolerance

In distributed systems much has been written about Fault Tolerance especially to those faults known as "Byzantine" faults. These faults might be caused by either random chance or by malicious action. For aspects of failures in system confidence that arise purely from malicious action, see the [Security] section.

1. **Faults from unknown data provenance:** Because all data transmitted in the system is generated and cryptographically signed by Agents, and those signatures are also included in the hash-chains, it is always possible to verify any datum's provenance. Thus, faults from intentional or accidental impostors is not possible. The system cannot prevent malicious or incautious actors from stealing or revealing private keys, however, although it does include affordances to deal with these eventualities. These are discussed under [Human Error].

2. **Faults from data corruptibility in transmission and storage:** Because all state data is stored along with a cryptographic hash of that data, and because all data is addressed and retrieved by that hash and can be compared against the hash, the only possible fault is that the corruption resulted in data that has the same hash. For Blake2b-256 hashing (which is what we use), this is known to be a vanishingly small possibility for both intentional and unintentional data corruption.[^corruption] Furthermore, because all data is stored as hash-chains, it is not possible for portions of data to be retroactively changed. Agents' Source Chains thus become immutable append-only event logs.

    One possible malicious act that an Agent can take is to roll back their chain to some point and start publishing different data from that point forward. But because the publishing protocol requires Agents to also publish all of their Actions to the neighborhood of their own public key, any Actions that lead to a forked chain will be easily and immediately detected by simply detecting more than one action linked to the same previous action.

    It is also possible to unintentionally rollback one's chain. Imagine a setting where a hard-drive corruption leads to a restore from an outdated backup. If a user starts adding to their chain from that state, it will appear as a rollback and fork to validators.

    Holochain adds an affordance for such situations in which a good-faith actor can add a Record repudiating such an unintentional chain fork.

3. **Faults from temporal indeterminacy:** In general these faults do not apply to the system described here because it only relies on temporality where it is known that one can rely on it; i.e., when recording Actions that take place locally as experienced by an Agent. As these temporally recorded Actions are shared into the space in which nodes may receive messages in an unpredictable order, the system still guarantees eventual consistency (though not uniform global state) because of the intrinsic integrity of recorded hash-chains and deterministic validation. Additionally, see the [Multi-agent reality binding (Countersigning)] section for more details on how some of the use cases addressed by consensus systems are handled in this system.

## Completeness/Fit

### Multi-agent reality binding (Countersigning)

The addition of the single feature of Countersigning to Holochain enables our eventually consistent framework to provide most of the consensus assurances people seek from decentralized systems. Countersigning provides the capacity for specific groups of agents to mutually sign a single state-change on all their respective source-chains. It makes the deterministic validity of a single Entry require the cryptographic signatures of multiple agents instead of just one. Furthermore any slow-downs necessary to add coordinated countersigned entries are not just localized to the DNA involved, they are also localized to just the parties involved. The same parties can
continue to interact in other DNAs.

The following are common use cases for countersigning:

* **Multi-Agent State Changes:** Some applications require changes that affect multiple agents simultaneously. Consider the transfer of a deed or tracking a chain of custody, where Alice transfers ownership or custody of something to Bob and they want to produce an **atomic change across both of their source chains**. We must be able to prevent indeterminate states like Alice committing a change releasing an item without Bob having taken possession yet, or Bob committing an entry acknowledging possession while Alice's release fails to commit. Holochain provides a countersigning process for multiple agents to momentarily lock their chains while they negotiate one matching entry that each one commits to their chain. An entry which has roles for multiple signers requires signed chain Actions from each counterparty to enter the validation process. This ensures no party's state changes unless every party's state changes.

* **Cryptocurrencies Based on P2P Accounting:** Extending the previous example, if Alice wants to transfer 100 units of a currency to Bob, they can both sign a single entry where Alice is in the spender role, and Bob the receiver. This provides similar guarantees as familiar double-entry accounting, ensuring changes happen to both accounts simultaneously. Someone's balance can be easily computed by replaying the transactions on their source chain, and both signing parties can be held accountable for any fraudulent transfers that break the data integrity rules of the currency application. There's no need for global time of transactions when each is clearly ordered by its sequence in the chains of the only accounts affected by the change.

* **Witnessed Authoritative Sequence:** Some applications may require an authoritative sequence of changes to a specific data type. Consider changes to membership of a group of administrators, where Carol and David are both members of the group, and Carol commits a change which removes David from the group, and David commits a change which removes Carol. With no global time clock to trust, whose change wins? An application can set up a small pool of N witnesses and configure any change to be the result of a countersigning session that requires M optional witnesses (where M > 50% of N). Whichever action the witnesses sign first would prevent the other action from being signed, because either Carol or David would have been successfully removed and would no longer be authorized participate in a countersigning session to remove the other.

* **Exclusive Control of Rivalrous Data:** Another common need for an authoritative time sequence involves determining control of rivalrous data such as name registrations. Using M of N signing from a witness pool makes it easy to require witnessing for only rivalrous data types, and forgo the overhead of witnessing for all other data. For example, a Twitter-like app would not need witnessing for tweets, follows, unfollows, likes, replies, etc, only for registration of new usernames and for name changes. This preserves the freedom for low-overhead and easy scaling by not forcing consensus to be managed on non-rivalrous data (which typically comprises the majority of the data in web apps).

* **Generalized Micro-Consensus: Entwined multi-agent state change:** Even though Holochain is agent-centric and designed to make only local state changes, the countersigning process may be seen as an implementation of Byzantine consensus applied to specific data elements or situations. Contextual countersigning is exactly what circumvents the need for global consensus in Holochain applications.

### Scaling

Holochain's architecture is specifically designed to maintain resilience and performance as both the number of users and interactions increase. Key factors contributing to its scaling capabilities include:

1. **Agent-centric approach:** Unlike traditional blockchain systems, which require global consensus before progressing, Holochain adopts an agent-centric approach where changes made to an agent's state become authoritative once stored on their chain, signed, and communicated to others via the DHT. As a result, agents are able to initiate actions without delay and in parallel to other agents initiating their own actions.

2. **Bottleneck-Free Sharded DHT:** Holochain's DHT is sharded, meaning that each node only stores a fraction of the total data, reducing the storage and computational requirements for each participant. At the same time, the storage of content with agents whose public key is "near" the hash of each Action or Entry, in combination with the use of Linking metadata attached to such hashes, transforms the DHT into a graphing DHT in which data discovery is simple in spite of the sparseness of the address space. When the agents responsible for validating a particular state change receive an authoring agent's proposed state change, they are able to:

    1. Request information from others in the DHT regarding the prior state of the authoring agent (where relevant), and

    2. Make use of their own copy of the app's validation rules to deterministically validate the change.

    While that agent and its validating peers are engaged with the creation and validation of a particular change to the state of the authors chain, in parallel, other agents are able to author state changes to their own chain and have these validated by the validating peers for each of those changes.  This bottle-neck free architecture allows users to continue interacting with the system without waiting for global agreement.

    With singular actions by any particular agent (and the validation of those actions by a small number of other agents) able to occur simultaneously with singular actions by other agents as well as countersigned actions by particular groups of agents, he network is not updating state globally (as blockchains typically do) but is instead creating, validating, storing, and serving changes of the state of particular agents in parallel.

3. **Multiple networks**: In Holochain, each application (DNA) operates on its own independent network, effectively isolating the performance of individual apps. This prevents a high-traffic, data-heavy, or processing-heavy app from affecting the performance of other lighter apps within the ecosystem. Participants are able to decide for themselves which applications they want to participate in.

4. **Order of Complexity**: "Big O" notation is usually only applied to local computation based on handling `n` number of inputs. However, we may consider a new type of O-notation for decentralized systems which includes two inputs, `n` as the number transactions/inputs/actions, and `m` as the number of nodes/peers/agents/users, as a way of expressing the time complexity for both an individual node and for the aggregate power of the entire network of nodes. Most blockchains are some variant of $\mathcal{O}(n^2*m)$ in their order of complexity. Every node must gossip and validate all state changes. However, Holochain retains a constant $\mathcal{O}(\frac{log(n)}{m})$ complexity for any network larger than a given size $R$, where $R$ is the sharding threshold. As the number of nodes in the network grows, each node performs a static workload irrespective of network size; or expressed inversely, a smaller portion of the total network workload.

### Shared-state Finality

Many blockchains approximate chain finality by assuming that the "longest chain wins." That strategy does not translate well to agent-centric chains, which are simply histories of an agent's actions. While there is no concern about forking global state because a Holochain app doesn't have one, we can imagine a situation where Alice and Bob have countersigned a transaction, then Alice forks her source chain by later committing an Action to an earlier sequence position in her chain. If the timestamp of this new, conflicting Action precedes the timestamp of the transaction with Bob, it could appear that Bob had knowingly participated in a transaction with a malicious actor, putting his own integrity in question. This can even happen non-maliciously when someone suffers data loss and restores from a backup after having made changes that were not included in the backup. While the initial beta version of Holochain does not offer fork finality protections for source chains, later versions will incorporate "meta-data hardening" which enables gossiping peers to tentatively solidify a state of affairs when they see that gossip for a time window has calmed and neighbors have converged on the same state. After this settling period (which might be set to something between 5 to 15 minutes) any later changes which would produce a conflict (such as forking a chain) can be rejected, preserving the legitimacy of state changes that were made in good faith.

## Security

The system's resilience to intentional gaming and disruption by malicious actors will be covered in depth in future papers, but here we provide an overview.

Many factors contribute to a system's ability to live up to the varying safety and security requirements of its users. In general, the approach taken in Holochain is to provide affordances that take into account the many types of real-world costs that result from adding security and safety to systems such that application developers can match the trade-offs of those costs to their application context. The integrity guarantees listed in the formal system description detail the fundamental data safety that Holochain applications provide. Some other important facets of system security and safety come from:

1. Gating access to functions that change local state, for which Holochain provides a unified and flexible Object Capabilities model

2. Detecting and blocking participation of bad actors, including attempts to flood a DHT with otherwise valid data, for which Holochain provides the affordances of validation and warranting.

3. Protection from attack categories

4. Resilience to human error

### Gating Access via Cryptographic Object Capabilities

To use a Holochain application, end-users must trigger Zome Calls that effect local state changes on their Source Chains. Additionally, Zome Functions can make calls to other Zome Functions on remote nodes in the same app, or to other DNAs running on the same Conductor. All of these calls must happen in the context of some kind of permissioning system. Holochain's security model for calls is based on the Object-capability[^object_capability] security model, but augmented for a distributed cryptographic context in which we use cryptographic signatures to prove the necessary agency for taking action.

[^object_capability]: See <https://en.wikipedia.org/wiki/Object-capability\_model>

Access is thus mediated by Capability Grants of four types:

* **Author**: only the agent owning the source change can make the zome call

* **Assigned**: only the specified public key holders can make the zome call, as verified by a signature on the function call payload

* **Transferrable**: anybody with the given secret can make the zome call

* **Unrestricted**: anybody can make the zome call (no secret nor proof of authorized key needed to use this capability)

All zome calls must be signed and also take a required capability claim parameter that MUST be checked by the system for making the call. Agents record capability grants on their source chains and distribute their corresponding secrets as applicable according to the application's needs. Receivers of secrets can record them as private capability claim entries on their chains for later lookup and use. The "agent" type grant is just the agent's public key.

### Validation & Warranting

We have already covered how Holochain's agent-centric validation and intrinsic data integrity provides security from malicious actors trying to introduce invalid or incorrect information into an Application's network, as every agent can deterministically verify data and thus secure itself. It is also important, however, to be able to eject malicious actors from network participation who generate or propagate invalid data, so as to proactively secure the network against the resource drain that future such actions from those actors may incur.

As agents publish their actions to the DHT, other agents serve as validators. When validation passes, they send a validation receipt back to the authoring agent, so they know the network has seen and stored their data. When validation fails, they send a negative validation receipt, known as a warrant, back to the author and their neighbors so the system can propagate these provably invalid attempted actions. This also flags the offending agent as corrupted or malicious so that other nodes can block them and stop interacting with the offending agent. Every node can confirm the warrant for themselves, as it is justified by the shared deterministic validation rules, of which all agents have a copy.

This enables a dynamic whereby any single honest agent can detect and report any invalid actions. So instead of needing a majority consensus to establish reliability of data (an "N/2 of N" trust model), Holochain enables "one good apple to heal the bunch" with a "1 of N" trust model for any data you acquire from agents on the network.

For even stricter situations, apps can achieve a "0 of N" trust model, where no external agents need to be trusted, because nodes can always validate data for themselves, independent of what any other nodes say.

### Security from Attack Categories

#### Consensus​ ​Attacks

This whole category of attack starts from the assumption that consensus is required for distributed systems. Because Holochain doesn't start from that assumption, the attack category really doesn't apply, but it's worth mentioning because there​ ​are​ ​a​ ​number​ ​of​ ​attacks​ ​on​ ​blockchain​ ​which​ threaten confidence in the reliability of the chain data through collusion between some majority of nodes. ​The​ ​usual thinking​ ​is​ ​that​ ​it​ ​takes​ ​a​ ​large​ ​number​ ​of​ ​nodes ​and​ ​massive​ ​amounts​ ​of​ ​computing​ ​power or financial incentives​ ​to prevent​ ​undue​ ​hijacking​ ​of​ ​consensus.​ ​However,​ ​since​ ​Holochain's data coherence doesn't derive from all nodes awaiting consensus,​ ​but​ rather ​on​ deterministic validation, nobody​ ​ever​ ​needs​ ​to​ ​trust​ ​a​ ​consensus​ ​lottery.​

#### Sybil Attacks

Since Holochain does not rely on any kind of majority consensus, it is already less vulnerable to Sybil Attacks, the creation of many fake colluding accounts which are typically used to overwhelm consensus of honest agents. And since Holochain enables "1 of N" and even "0 of N" trust models, Sybils cannot entirely overwhelm honest agents' ability to determine the validity of data.

Additionally, since Holochain is a heterogeneous environment in which every app operates on its own isolated network, a Sybil Attack can only be attempted on a single app's network at a time. For each app, an appropriate membrane can be defined on a spectrum from very open and permissive to closed and strict by defining validation rules on a Membrane Proof.

A membrane proof is passed in during the installation process of an agent's instance of the app, so that the proof can be committed to the agent's chain just ahead of their public key. An agent's public key acts as their address in that application's DHT network, and is created during the genesis process in order to join the network. Other agents can confirm whether an agent may join by validating the membership proof.

A large variety of membrane proofs is possible, ranging from none at all, loose social triangulation, or an invitation from any current user, to stricter invitation lists, proof-of-work requirements, or a kind of proof-of-stake showing the agent possesses and has staked some value which they lose if their account gets warranted.

We generally suggest that applications may want to enforce some kind of membrane against Sybils, not because consensus or data integrity are at risk but because carrying a lot of Sybils makes unnecessary work for honest agents running an application. We cover more about this in the next section.

<!-- [WP-TODO-v2: talk about spamming attacks and weighing] -->

#### Denial-of-Service Attacks

Holochain is not systemically subject to denial-of-service attacks because there is no central point to attack. Because each application is its own network, attackers would have to flood every agent of every application to carry out a systemic denial-of-service attack; to do that would require knowing who all those agents are, which is also not recorded in one single place. One point of vulnerability is the bootstrap servers for an application. But this is not a systemic vulnerability, as each application can designate its own bootstrap server, and they can also be arbitrarily hardened against denial-of-service to suit the needs of the application.

#### Eclipse Attacks

An Eclipse Attack happens when a newly joining node is prevented from ever joining the main/honest network because it initially connects to a dishonest node which only ever shares information about other colluding dishonest nodes. This attack is specific to gossip-based peer-to-peer networks such as Bitcoin, Holochain, and DHTs like IPFS.

Holochain apps bypass the risk of an Eclipse Attacks by providing an address for a bootstrap service which ensures your first connection is to a trusted or honest peer. If an application fails to provide a bootstrap service, nodes will try connecting via https://bootstrap.holochain.org which provides initial trusted peers, if those have been specified. If not specified, the default bootstrap service provides random access to any and all peers using the app, which at least ensures nodes cannot be partitioned from honest peers.

Application developers can take steps to further protect their users by providing in-app methods of exchanging signed pings with known nodes (such as a progenitor, migrator, notary, witness, or initial admin node) so a node can ensure it is not partitioned from the real network.

#### Human Error

There are some aspects of security, especially those of human error, that all systems are subject to. People​ ​will​ ​still​ ​lose​ ​their​ ​keys,​ ​use​ ​weak​ ​passwords,​ ​get​ ​computer​ ​viruses, etc. But, crucially, in the realm of "System Correctness" and "confidence,"​ the question that needs addressing is how the system interfaces with mechanisms to mitigate against human error. Holochain provides significant tooling to support key management in the form of its ​core​ ​Distributed​ ​Public Key​ ​Infrastructure (DPKI) and DeepKey app built on that infrastructure. Among other things, this tooling ​provides​ ​assistance​ ​in​ ​managing​ ​keys,​ ​managing​ ​revocation​ ​methods,​ ​and reclaiming​ ​control​ ​of​ ​applications​ ​when​ ​keys​ ​or​ ​devices​ ​have​ ​become​ ​compromised.

A definition and specification of a DPKI system is outside of the scope of this paper; see the DeepKey design specification[^deepkey-spec] for a more thorough exploration.

[^deepkey-spec]: See <https://github.com/holochain/deepkey/blob/main/docs/2023/README.md>.

## Evolvability

For large-scale systems to work well over time, we contend that specific architectural elements and affordances make a significant difference in their capacity to evolve while maintaining overall coherence as they do so:

1. **Subsidiarity:** From the Wikipedia definition:

    > Subsidiarity is a principle of social organization that holds that social and political issues should be dealt with at the most immediate (or local) level that is consistent with their resolution.

    Subsidiarity enhances evolvability because it insulates the whole system from too much change, while simultaneously allowing change where it is needed. Architecturally, however, subsidiarity is not easy to implement because it is rarely immediately obvious what level of any system is consistent with an issue's resolution.

    In Holochain, the principle of subsidiarity is embodied in many ways, but crucially in the architecture of app instances having fully separate DNAs running on their own separate networks, each also having clear and differentiable Integrity and Coordination specifications. This creates very clear loci of change, both at the level of when the integrity rules of a DNA need to change, and at the level of how one interacts with a DNA. This allows applications to evolve exactly in the necessary area by updating only the DNA and DNA portion necessary for changing the specific functionality that needs evolving.

2. **Grammatic[^grammatic] composability:** Highly evolvable systems are built of grammatic elements that compose well with each other both "horizontally", which is the building of a vocabulary that fills out a given grammar, and "vertically" which is the creation of new grammars out of expressions of a lower level grammar. There is much more that can be said about grammatics and evolvability, but that is out of scope for this paper. However, we contend that the system as described above lives up to these criteria of having powerful grammatical elements that compose well as described. DNAs are essentially API definitions that can be used to create a large array of micro-services that can be assembled into small applications. Applications themselves can be assembled at the User Interface level. A number of frameworks in the Holochain ecosystem are already building off of this deep capacity for evolvability that is built into the system's architecture[^evolvability].

[^grammatic]: We use the term "grammatic" as a way to generalize from the usual understanding of grammar which is linguistic. Where grammar is often understood to be limited to language, grammatics points to the pattern of creating templates with classes of items that can fill slots in those templates. This pattern can be used for creating "grammars" of social interaction, "grammars" of physical structures (we would call Christopher Alexander's "A Pattern Language" for architecture an example of grammatics), and so on.

[^evolvability]: A number of projects in the Holochain ecosystem are already exhibiting this characteristic of evolvability, such as The Weave / Moss (see <https://theweave.social>), Ad4m (<https://ad4m.dev/>), Memetic Activation Platform (see <https://github.com/evomimic/we-all-map/wiki/MAP-Overview>).



================================================
File: docs/specs/src/hwp_4_formal.md
================================================
# Holochain Formal Design Elements

Now we turn to a more formal and detailed presentation of the Holochain
system, including assumptions, architecture, integrity guarantees, and
formal state model.

**Purpose of this Section:** To provide an understanding of the functional requirements of Holochain and specify a technical implementation of the cryptographic state transitions and application processes that enforce Holochain's integrity guarantees.

## Definition of Foundational Principles

* **Cryptography:** Holochain's integrity guarantees are largely enabled by cryptography. It is used in three main ways.
    * **Hashes:** Data is uniquely identified by its hash, which is the key used to retrieve the data from a Content Addressable Store.
    * **Signing:** Origination of data (for all storage and network communications) is verified by signing a hash with a private key.
    * **Encryption:** Data is encrypted at rest and on the wire throughout the system.
* **Agency:** Holochain is agent-centric. Each and every state change is a result of:

    1. A record of an agent's action,
    2. signed by the authoring agent,
    3. linearly sequenced and timestamped
    4. to their local source chain.

    Each agent is the sole authority for managing its local state (by virtue of controlling their private key required for signing new actions to their source chain).
* **Accountability:** Holochain is also socio-centric. Each Holochain application defines its set of mutually enforced data integrity rules. Every local state change gets validated by other agents to ensure that it adheres to the rules of that application. Peers also enforce limits on publishing rates, protect against network integrity threats, and can ban rule-breakers by a process we call *warranting*.
* **Data:** Unlike some other decentralized approaches, in Holochain, data does not have first-order, independent, ontological existence. Every record in the shared DHT network space MUST CARRY its provenance from a local source chain as described below.
* **Provenance:** Each record created in a Holochain application starts as an action pair on someone's local source chain. As such, even when published to the shared DHT, records must carry the associated public key and signature of the agent who created it. This means every piece of data carries meta-information about where that data came from (who created it, and in what sequence on the their chain). Note: In other hash-chain based systems Holochain's "actions" are often called "headers," which link to the previous headers to create the chain. In Holochain, while the action does establish temporal order, its core function is to record an act of agency, that of "speaking" data into existence.
* **State:** State changes in Holochain are local (signed to a local *Source Chain*) and then information about having created a local state change is shared publicly on the DHT. This allows global visibility of local state changes, without a need to manage consensus about a global state, because there is truly no such thing as global state in a system that allows massive, simultaneous, decentralized change.
* **Time:** There is no global time nor global absolute sequence of events in Holochain either. No global time is needed for local state changes, and since each local change is stored in a hash chain, we get a clear, immutable, sequence of actions tagged with local timestamps. (Note: For apps that need some kind of time proof to interface with the outside world (e.g. token or certificate expiration timestamps) we plan to provide a time proof service that replaces the need for centrally trusted timeservers.)

## System Architecture Overview

In Holochain every app defines a distinct, peer-to-peer, encrypted network where one set of rules is mutually enforced by all users. This network consists of the peers running the app, who participate in routing messages to each other and validating and storing redundant copies of the application's database.

Holochain operates different subsystems, each of which functions on separate workflows and change models. Even though Holochain functions as a common underlying database on the back-end, the workflows in each subsystem each have different input channels which trigger different transformational processes. Each workflow has distinct structural bottlenecks and security constraints, which necessitates that execution of workflows is parallelized across subsystems, and sometimes within a subsystem.

1. **Local Agent State:** Represented as changes to an agent's state by signing new records with their private key, and committing them to a local hash chain of their action history called a Source Chain. Initial chain genesis happens upon installation/activation, and all following changes result from "zome calls" into the app code.
2. **Global Visibility of Local State Changes:** After data has been signed to a Source Chain it gets published to a Graphing DHT (Distributed Hash Table) where it is validated by the peers who will store and serve it. The DHT is continually balanced and healed by gossip among the peers.
3. **Network Protocols:** Holochain instantiates the execution of app DNA on each node under the agency identified by the public key, transforming code into a collective networked organism. An agent's public key ***is*** their network address, and is used as the to/from target for remote zome calls, signals, publishing, and gossip. Holochain is transport-agnostic, and can operate on any network transport protocol which a node has installed for routing, bootstrapping, or proxying connections through NAT and firewalls.
4. **Distributed Application:** Apps are compiled and distributed into WebAssembly (WASM) code bundles which we call a DNA. Data integrity is enforced by the validation defined in an app's DNA, which is composed of data structures, functions, and callbacks packaged in Zomes (short for chromosome) which function as reusable modules. DNAs are coupled with an Agent's public key and activated or instantiated into a Cell. Installation and activation status of these bundles is managed by a runtime container.

### Some notes on terminology

#### Biological Language

We have chosen biological language to ground us in the pattern of collective distributed coherence that we observe in biological organisms. This is a pattern in which the agents that compose an organism (cells) all start with the same ground rules (DNA). Every agent has a copy of the rules that *all* the other agents are playing by, clearly identifying membership in the collective self based on matching DNA.

This is true of all Holochain DNAs, which can also be combined together to create a multi-DNA application (with each DNA functioning like a distinct micro-service in a more complex application). In a hApp bundle, each DNA file is the complete set of integrity zomes (WASM) and settings whose hash also becomes the first genesis entry in the agent's source chain. Therefore, if the DNA hash in your first chain record does not match mine, we are not cells of the same network organism. A "zome" is a code module, which functions as the basic compositional unit for assembling the complete set of an application's DNA.

When a DNA is instantiated along with a public/private key pair, it becomes a "cell" which is identified by the combination of the DNA hash and the public key.

Students of biology may recognize ways that our language doesn't fully mesh with their expectations. Please forgive any imprecision with understanding of our intent to build better language for the nature of distributed computing that more closely matches biology than typical mechanistic models.

#### The Conductor

Much of the discussion below is from the perspective of a single DNA, which is the core unit in Holochain that provides a set of integrity guarantees for binding agents together into a single social context. However, Holochain can also be seen as micro-service provider, with each DNA providing one micro-service. From this perspective, a Holochain node is a running process that manages many connections to many DNAs simultaneously, from user interfaces initiating actions, from other nodes sharing a subset of identical DNAs, and from cells within the same node sharing the same agent ID but bound to different DNAs. Thus, we call a Holochain node the **Conductor** as it manages the information flows from "outside" (UI calls and calls from other local cells) and from "inside" (network interactions) as they flow into and out of the may DNA instances running code. This term was chosen as it suggests the feel of musical coordination of a group, as well as the conduit of an electrical flow. Please see the [Implementation Spec (Appendix A)](hwp_A_implementation_spec.md) for a more detailed on how a complete Holochain Conductor must be built.

## Integrity Guarantees

Within the context of the Basic Assumptions and the System Architecture both described above, the Holochain system makes the following specific integrity guarantees for a given Holochain DNA and network:

1. **State:** Agents' actions are unambiguously ordered from any given action back to genesis, unforgeable, non-repudiable, and immutable (accomplished via local hash chains called a Source Chain, because all data within the network is sourced from these chains.)
2. **Self-Validating Data:** Because all DHT data is stored at the hash of its content, if the data returned from a request does not hash to the address you requested, you know you've received altered data.
3. **Self-Validating Keys:** Agents declare their address on the network as their public key, and key rotation is subject to rules defined by the agent and enforced by their peers. Peers can confirm any published data or remote call is valid by checking the signature using the from address as the public key.
4. **Termination of Execution:** No node can be coerced into infinite loops by non-terminating application code in either remote zome call or validation callbacks. Holochain uses WASM metering to guarantee a maximum execution budget to address the Halting Problem.
5. **Deterministic Validation:** Ensure that only deterministic behaviors (ones that will always get the same result no matter who calls them on what computer) are available in validation functions. An interim result of "missing dependency" is also acceptable, but final evaluation of valid/invalid status for each datum must be consistent across all nodes and all time spans.
6. **Strong Eventual Consistency:** Despite network partitions, all nodes who are authorities for a given DHT address (or become one at any point) will eventually converge to the same state for data at that address. This is ensured by the DHT functioning as a conflict-free replicated data type (CRDT).
8. **"0 of N" Trust Model:** Holochain is immune to "majority attacks" because any node can always validate data for themselves independent of what any other nodes say.[^zero-of-n]
9. **Data Model Scalability:** Because of the overlapping sharding scheme of DHT storage and validation, the total computing power and overall throughput for an application scales linearly as more users join the app.
10. **Atomic Zome Calls:** Multiple writes in a single zome call will all be committed in a single SQL transaction or all fail together. If they fail the zome call, they will report an error to the caller and the writes will be rolled back.

[^zero-of-n]:  See this Levels of Trust Diagram <https://miro.medium.com/max/1248/0*k3o00pQovnOWRwtA>.

## Source Chain: Formal State Model

Data in a Holochain application is created by agents changing their local state. This state is stored as an append-only hash chain. Only state changes originated by that agent (or state changes that they are party to in a multi-agent action) are stored to their chain. Source Chains are NOT a representation of global state or changes that others are originating, but only a sequential history of local state changes authored by one agent.

The structure of a Source Chain is that of a hash chain which uses headers (called "actions" in Holochain terms) to connect a series of entries. Each record in the chain is a two-element tuple, containing the action and the entry (if applicable for the action type).

Since the action contains the prior action hash and current entry hash (if applicable), each record is a tamper-proof atomic data element. Additionally, in practice a record is always transmitted along with a signature on the action's hash, signed by the private complement of the public key in the action. This means that anyone can hash the entry content to make sure it hasn't been tampered with, and they can hash the action data and compare the accompanying signature on that hash to ensure it matches the author's public key. The action's chain sequence and monotonic timestamp properties provide further immutable reinforcement of logical chain ordering.

Data in Holochain is kept in Content Addressable Stores which are key-value stores where the key is the hash of the content. This makes all content self-validating, whether served locally or remotely over the DHT. Data can be retrieved by the action hash (synonymous with record hash) or the entry hash.

The code that comprises a Holochain application is categorized into two different types of zomes:

1. **Integrity Zomes** which provide the immutable portion of the app's code that:
    * identifies the types of entries and links that may be committed in the app,
    * defines the structure of data entries, and
    * defines the validation code each node runs for each type of operation that intends to add to state at a given DHT address.
2. **Coordinator Zomes**, the set of which can be removed from or added to while an app is live, and which contain various create, read, update, and delete (CRUD) operations for entries and links, functions related to following graph links and querying collections of data on the DHT, and any auxillary functionality someone wants to bundle in their application.

Each application running on Holochain is uniquely identified by a DNA hash of the integrity zome code, after being compiled to Web Assembly (WASM) and bundled with additional settings and properties required for that app.

*Application Note: Multiple DNA-level apps can be bundled together like interoperating micro-services in a larger Holochain Application (hApp), but the locus of data integrity and enforcement remains at the single DNA level, so we will stay focused on that within this document.*

There are three main types of Zome functions:

1. ( $z _f$ ) zome functions which do not alter state.
2. ( $Z _f$ ) that can be called to produce state changes, as well as the
3. Validation Rules ( $V _R$ ) for enforcing data integrity of any such state changes (additions, modifications, or deletions of data).

$$
\begin{aligned}
 z _{f_1} \dots z _{f_x} &\in \text{Coordinator Zomes} \\
 Z _{f_1} \dots Z _{f_x} &\in \text{Coordinator Zomes} \\
 V _{R_1} \dots V _{R_x} &\in \text{Integrity Zomes}
\end{aligned}
$$

*Note about Functions: Most functionality does not need to be in the immutable, mutually enforced rules included in the DNA hash (Integrity Zomes); only the functionality which validates data ( ( $V _R$ ) ) does. In practice, including code that does not contribute to data validation ( ( $z _f$ ), ( $Z _f$ ) ) in the integrity zome creates a brittle DNA that is difficult to update when bugs are repaired or functionality needs to be introduced or retired.*

The first record in each agent's source chain contains the DNA hash. This initial record is what demonstrates that each agent possessed, at installation time, identical and complete copies of the rules by which they intend to manage and mutually enforce all state changes. If a source chain begins with a different DNA hash, then its agent is in a different network playing by a different set of rules.

**Genesis:** The genesis process for each agent creates three initial entries.

1. The hash of the DNA is stored in the first chain record with action $C _0$ like this:

$$
C _0 = WASM
\begin{Bmatrix}
a_{DNA} \\
e_{DNA}
\end{Bmatrix}
$$

2. Followed by a "Membrane Proof" which other nodes can use to validate whether the agent is allowed to join the application network. It can be left empty if the application membrane is completely open and it doesn't check or use proofs of membership.

$$
C _1 =
\begin{Bmatrix}
    a _{mp} \\
    e _{mp}
\end{Bmatrix}
$$

3. And finally the agent's Public Key that they have generated, which also becomes their address on the network and DHT. Keys are the only entry type for which the hash algorithm is equality (meaning the hash of a key is the key itself, so it cannot contain any content other than the public key).

$$
C _2 =
\begin{Bmatrix}
    a_{K}  \\
    e_{K}
\end{Bmatrix}
$$

**Initialization:** After genesis, DNAs may have also provided initialization functions which are all executed the first time an inbound zome call is received and run. This delay in initialization is to allow time for the application to have joined and been validated into the network, just in case initialization functions may need to retrieve some data from the network.

Initialization functions may write entries to the chain, send messages, or perform any variety of actions, but after all coordinator zomes' initialization functions (according to the order they were bundled together) have successfully completed their initializations, an InitZomesComplete action is written to the source chain, so that it will not re-attempt initialization, thus preventing any redundant side-effects.

**Ongoing Operation via Calls to Zome Functions:** All changes following genesis and initialization occur by Zome call to a function contained in a Coordinator Zome in the following form:

$$ Z_c = \{Z_f, Params, CapTokenSecret \} $$

Where $Z_f$ is the Zome function being called, $Params$ are the parameters passed to that Zome function, and $CapTokenSecret$ references the capability token which explicitly grants the calling agent the permission to call that function.

Based on the interface connection and state when the Zome call is received we construct a context which provides additional necessary parameters to validate state transformation:

$$ Context(Z_c) = \{Provenance, C_n \} $$

$Provenance$ contains the public key of the caller along with their cryptographic signature of the call as proof that it originated from the agent controlling the associated private key.

$C_n$ is the Source Chain's latest action at the time we begin processing the zome call. The Zome call sees (and potentially builds upon) a snapshot of this state through its lifetime, and validation functions will all be called "as at" this state. Since multiple simultaneous zome calls might be made, tracking the "as at" enables detection of another call having successfully changed the state of the chain before this call completed its execution, at which point any actions built upon the now-obsolete state may need to be reapplied to and validated on the new state.

#### Zome Calls & Changing Local State

First, Holochain's "subconscious" security system confirms the $CapTokenSecret$ permits the agent identified by the $Provenance$ to call the targeted function. It returns a failure if not. Otherwise it proceeds to further check if the function was explicitly permitted by the referenced capability token.

*Note on Permissions: Capability tokens function similarly to API keys. Cap token grants are explicitly saved as private entries on the granting agent's source chain and contain a secret used to call them. Cap token claims containing the secret are saved on the calling agent's chain so they can be used later to make calls that execute the capabilities that have been granted.*

If the Zome call is one which alters local state (distinct from a call that just reads from the chain or DHT state), we must construct a bundle of state changes that will attempt to be appended to the source chain in an atomic commit:

$$ \Delta _C ( C _n, Z _c ) = \begin{Bmatrix}
    a_\prime & a_{\prime\prime} & \dots & {a_x}  \\
    e_\prime & e_{\prime\prime} & \dots & {e_x}
\end{Bmatrix} $$

where a Chain is composed of paired actions, $a_x$, and entries, $e_x$.

The next chain state is obtained by appending the changes produced by a zome call to the state of the chain at that point.

$$
 C _{n} = C _n + \Delta _C ( C _n, Z _c )
$$

*If the validation rules pass* for these state changes **and the current top of chain is still in state $C_n$** then the transaction is committed to the persistent store, and the chain is updated as follows:

$$
C _n =
\begin{Bmatrix}
    a _{DNA} & \dots & a _n  \\
    e _{DNA} & \dots & e _n
\end{Bmatrix}
$$

*If the validation rules fail,* the deltas will be rejected with an error. Also, if the chain state has changed from $C_n$, we can:

1. return an error (e.g. "Chain head has moved"),
2. commit anyway, restarting the validation process at a new "as at" $C_n^\prime$ if the commit is identified as "stateless" in terms of validation dependencies (e.g., a tweet generally isn't valid or invalid because of prior tweets/state changes). We refer to any application entry types that can be committed this way as allowing "relaxed chain ordering".

*Note about Action/Entry Pairs: This paired structure of the source chain holds true for all application data. However, certain action types defined by the system, whose entry payloads are small or require metadata that is additional to primary entry content, integrate what would be entry content as **additional fields inside the action** instead of creating a separate entry which would add unnecessary gossip on the DHT. These types are identified and described in Appendix A, Implementation.*

### Countersigning

So far we have discussed individual agents taking Actions and recording them on their Source Chains. It is also desireable for subsets of agents to mutually consent to a single Action by atomically recording the Action to their chains. We achieve this through a process of Countersigning, whereby a session is initiated during which the subset of agents builds an Action that all participating agents sign, and during which all agents promise one another that they will not take some other action in the meantime.

There are two ways of managing the countersigning process:

1. Assigned completion: where one preselected agent (whom we call the Enzyme) acts as a coordinator for ensuring completion of a signing session.
2. Randomized completion: where any agent in the neighborhood of the Entry address (which is cryptographically pseudorandom and is computed on data contributed by each counterparty) can report completion.

Additionally there are two contexts for making these atomic changes across multiple chains:

1. When the change is about parties who are accountable to the change, i.e., their role is structurally part of the state change, as in spender/receiver of a mutual credit transaction
2. When the change simply requires witnessing by M of N parties, i.e., all that's needed is a "majority" of a group to agree on the atomicity. This allows a kind of "micro-consensus" to be implemented in parts of an application. It's an affordance for applications to implement a set of "super-nodes" that manage a small bit of consensus. Note that in our current implementation, M of N countersigning always uses an Enzyme to manage the session completion.

#### Countersigning Constraints

1. All actions must be signed together; one action is not enough to validate an atomic state change across multiple chains.
    * All parties must be able to confirm the validity of each others's participation in that state change (meaning each chain is in a valid state to participate in the role/capacity which they are engaging -- e.g., a spender has the credits they're spending at that point in their chain).
2. The moment the enzyme or random session completer agent holds and broadcasts all the signed and valid actions, then everyone is committed.
3. It should not be possible for a participant to withhold and/or corrupt data and damage/fork/freeze another participant's source chain.
4. It should not require many network fetches to calculate state changes based on countersignatures (i.e., it should be possible to `get` a unified logical unit -- that is, multiple actions on a single entry hash address on the DHT).
5. Participants can NOT move their chain forward without a provable completion of the process, and there IS a completion of the process in a reasonable time frame
    * The countersigning process should work as closely as possible to the standard single-agent "agent-centric network rejection of unwanted realities": anyone who moves forward before the process has timed out or completed, or anyone who tries to submit completion outside of timeouts, will be detected as a bad fork.

#### Countersigning Flow

Here is a high-level summary of how a countersigning session flows:

0. Alice sends a **preflight request** to Bob, Carol, etc, via a remote call.
    * The preflight request includes all information required to negotiate the session with the entry itself, for example:
        * Entry hash: What data are we agreeing to countersign? (The contents of the entry are often negotiated beforehand and communicated to all parties separately, although the app data field described below can also be used for this purpose.)
        * Action base: What type is the entry we'll be countersigning, and will it be a Create or an Update?
        * Update/delete references: what are we agreeing to modify?
        * Session times: Will I be able to accept the session start time, or will it cause my chain to be invalid? Am I willing to freeze my chain for this long?
        * The agents and roles: Are these the parties I expected to be signing with?
        * App data: can point to necessary dependencies or, if the contents of the entry to be countersigned are small, the entry itself.
1. If the other parties accept, they freeze their chains and each return a **preflight response** to Alice. It contains:
    * The original request.
    * The state of the party's source chain "as at" the time they froze it.
    * Their signature on the above two fields.
2. Alice builds a session data package that contains the preflight request along with the source chain states and signatures of all consenting parties, and sends it to them.
3. Each party builds and commits an action that writes the countersigned entry (including the contents of the session data package and the entry data itself) to their source chains. At this point, unsigned actions are created for themselves and every other party and full record validation is run against each action, as though they were authoring as that agent.
4. After everything validates, each agent signs and sends their action to the session completer -- either the enzyme (if one was elected) or the entry's DHT neighborhood.
5. The session completer reveals all the signed actions as a complete set, sending it back to all parties.
6. Each signer can check for themselves that the set is valid simply by comparing against the session entry and preflight info. They do not have to rerun validation; they only need to check signatures, integrity, and completeness of the action set data.
7. All counterparties now proceed to write the completed action to their source chain and publish its data to the DHT.
8. The DHT authorities validate and store the action and entry data as normal.

## Graph DHT: Formal State Model

Holochain performs a topological transform on the set of the various agents' source chains into a content-addressable graph database (graph DHT or GDHT) sharded across many nodes who each function as authoritative sources for retrieving certain data.

**Fundamental Write Constraint:** The DHT can never be "written" to directly. All data enters the DHT **only** by having been committed to an agent's source chain and then being **transformed** from validated local chain state into the elements (DHT operations) required for GDHT representation and lookup.

**Structure of GDHT data:** The DHT is a content-addressable space where each piece of content is found at the address which is the hash of its content. In addition, any address in the DHT can have metadata attached to it. This metadata is not part of the content being hashed.

*Note about hashing: Holochain uses 256-bit Blake2b hashes with the exception of one entry type, AgentPubKey, which is a 256-bit Ed25519 public key and its hash function is simply the identity function. In other words, **the content of the AgentPubKey is identical to its hash**. This preserves content-addressability but also enables agent keys to function as self-proving identifiers for network transport and cryptographic functions like signing or encryption.*

**DHT Addresses:** Both Actions and Entries from source chains can be retrieved from the DHT by either the ActionHash or EntryHash. The DHT `get()` function call returns a Record, a tuple containing the most relevant action/entry pair. Structurally, Actions "contain" their referenced entries so that pairing is obvious when a Record is retrieved by ActionHash. However, Actions are also attached as metadata at an EntryHash, and there could be many Actions which have created the same Entry content. A `get()` function called by EntryHash returns the oldest undeleted Action in the pair, while a `get_details()` function call on an EntryHash returns all of the Actions.

**Agent Addresses & Agent Activity:** Technically an AgentPubKey functions as both a content address (which is never really used because performing a `get()` on the key just returns the key itself) and a network address to send communications to that agents. But in addition to the content of the key stored on the DHT is metadata about that agent's chain activity. In other words, a `get_agent_activity()` request retrieves metadata about their chain records and chain status.

Formally, the entire GDHT is represented as a set of 'basis hashes' $b_{c_x}$, or addresses where both content $c$ and metadata $m$ may be stored:

$$
GDHT = \{d_1, \dots, d_n \}
$$

The data at a basis hash can consist of content and/or metadata:

$$
d_{b_{c_x}} = (c_x, M )
$$

A basis hash is the hash of the content stored at the address:

$$
b_{c_x} = hash(c_x)
$$

The total set of content represented by the GDHT consists of entries $E$, actions $A$, and external content $T$ (where the addresses can still store metadata and be used as references, but the content is not stored in the DHT):

$$
\begin{aligned}
E &= \{e_1, \dots, e_n\} \\
A &= \{a_1, \dots, a_n\} \\
T &= \{t_1, \dots, t_n\} \\
C &= E \bigsqcup A \bigsqcup T \\
\end{aligned}
$$

An address can hold a set of metadata:

$$
\begin{aligned}
M &= \{m_1, \dots, m_n \} \\
m_x &= \text{metadata}
\end{aligned}
$$

There may be arbitrary types of metadata. For instance, every instance of entry content $e$ has a set of creation actions $A_e$ associated with it:

$$
\forall e \: M_{context} = \{{a_e}_1, \dots, {a_e}_n \}
$$

And any address may have a set of links pointing to other addresses, each of which is a tuple of its type, an arbitrary tag, and a reference to the target address $b_{c_T}$:

$$
\begin{aligned}
M_{link} &= \{ link_1, \dots, link_n \} \\
\exists c_T \: link &= ( type, tag, b_{c_T} ) \\
