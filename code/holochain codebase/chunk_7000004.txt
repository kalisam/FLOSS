            author,
            timestamp,
        }
    }

    /// Constructor with timestamp set to now()
    #[cfg(feature = "full")]
    pub fn new_now(proof: WarrantProof, author: AgentPubKey) -> Self {
        Self {
            proof,
            author,
            timestamp: Timestamp::now(),
        }
    }
}

impl HashableContent for Warrant {
    type HashType = holo_hash::hash_type::Warrant;

    fn hash_type(&self) -> Self::HashType {
        Self::HashType::new()
    }

    fn hashable_content(&self) -> HashableContentBytes {
        HashableContentBytes::Content(self.try_into().expect("Could not serialize Warrant"))
    }
}

/// The self-proving part of a Warrant which demonstrates bad behavior by another agent
#[derive(
    Clone, Debug, Serialize, Deserialize, SerializedBytes, Eq, PartialEq, Hash, derive_more::From,
)]
pub enum WarrantProof {
    /// Signifies evidence of a breach of chain integrity
    ChainIntegrity(ChainIntegrityWarrant),
}

/// Just the type of the warrant
#[derive(
    Clone,
    Copy,
    Debug,
    Serialize,
    Deserialize,
    SerializedBytes,
    Eq,
    PartialEq,
    Hash,
    derive_more::From,
)]
pub enum WarrantType {
    // NOTE: the values here cannot overlap with ActionType,
    // because they occupy the same field in the Action table.
    //
    /// Signifies evidence of a breach of chain integrity
    ChainIntegrityWarrant,
}

impl From<Warrant> for WarrantType {
    fn from(warrant: Warrant) -> Self {
        warrant.get_type()
    }
}

#[cfg(any(feature = "sqlite", feature = "sqlite-encrypted"))]
impl rusqlite::ToSql for WarrantType {
    fn to_sql(&self) -> rusqlite::Result<rusqlite::types::ToSqlOutput> {
        Ok(rusqlite::types::ToSqlOutput::Owned(
            format!("{:?}", self).into(),
        ))
    }
}

/// A warrant which is sent to AgentActivity authorities
#[derive(Clone, Debug, Serialize, Deserialize, SerializedBytes, Eq, PartialEq, Hash)]
pub enum ChainIntegrityWarrant {
    /// Something invalid was authored on a chain.
    /// When we receive this warrant, we fetch the Action and validate it
    /// under every applicable DhtOpType.
    // TODO: include ChainOpType, which allows the receipient to only run
    //       validation for that op type. At the time of writing, this was
    //       non-trivial because ChainOpType is in a downstream crate.
    InvalidChainOp {
        /// The author of the action
        action_author: AgentPubKey,
        /// The hash of the action to fetch by
        action: ActionHashAndSig,
        /// Whether to run app or sys validation
        validation_type: ValidationType,
    },

    /// Proof of chain fork.
    ChainFork {
        /// Author of the chain which is forked
        chain_author: AgentPubKey,
        /// Two actions of the same seq number which prove the fork
        action_pair: (ActionHashAndSig, ActionHashAndSig),
    },
}

/// Action hash with the signature of the action at that hash
pub type ActionHashAndSig = (ActionHash, Signature);

impl WarrantProof {
    /// Basis hash where this warrant should be delivered.
    /// Warrants always have the authoring agent as a basis, so that warrants
    /// can be accumulated by the agent activity authorities.
    pub fn dht_basis(&self) -> OpBasis {
        self.action_author().clone().into()
    }

    /// The author of the action which led to this warrant, i.e. the target of the warrant
    pub fn action_author(&self) -> &AgentPubKey {
        match self {
            Self::ChainIntegrity(w) => match w {
                ChainIntegrityWarrant::InvalidChainOp { action_author, .. } => action_author,
                ChainIntegrityWarrant::ChainFork { chain_author, .. } => chain_author,
            },
        }
    }

    /// Get the warrant type
    pub fn get_type(&self) -> WarrantType {
        match self {
            Self::ChainIntegrity(_) => WarrantType::ChainIntegrityWarrant,
        }
    }
}

/// Not necessary but nice to have
#[derive(
    Clone, Debug, Serialize, Deserialize, SerializedBytes, Eq, PartialEq, Hash, derive_more::Display,
)]
pub enum ValidationType {
    /// Sys validation
    Sys,
    /// App validation
    App,
}

/// A signed warrant with timestamp
pub type SignedWarrant = Signed<Warrant>;



================================================
File: crates/holochain_zome_types/src/x_salsa20_poly1305.rs
================================================
use crate::prelude::*;

pub use holochain_integrity_types::x_salsa20_poly1305::*;

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct XSalsa20Poly1305SharedSecretExport {
    pub sender: X25519PubKey,
    pub recipient: X25519PubKey,
    pub key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
}

impl XSalsa20Poly1305SharedSecretExport {
    pub fn new(
        sender: X25519PubKey,
        recipient: X25519PubKey,
        key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
    ) -> Self {
        Self {
            sender,
            recipient,
            key_ref,
        }
    }

    pub fn as_sender_ref(&self) -> &X25519PubKey {
        &self.sender
    }

    pub fn as_recipient_ref(&self) -> &X25519PubKey {
        &self.recipient
    }

    pub fn as_key_ref_ref(&self) -> &crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef {
        &self.key_ref
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct XSalsa20Poly1305SharedSecretIngest {
    pub recipient: X25519PubKey,
    pub sender: X25519PubKey,
    pub encrypted_data: crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData,
    pub key_ref: Option<crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef>,
}

impl XSalsa20Poly1305SharedSecretIngest {
    pub fn new(
        recipient: X25519PubKey,
        sender: X25519PubKey,
        encrypted_data: crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData,
        key_ref: Option<crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef>,
    ) -> Self {
        Self {
            recipient,
            sender,
            encrypted_data,
            key_ref,
        }
    }

    pub fn as_recipient_ref(&self) -> &X25519PubKey {
        &self.recipient
    }

    pub fn as_sender_ref(&self) -> &X25519PubKey {
        &self.sender
    }

    pub fn as_encrypted_data_ref(
        &self,
    ) -> &crate::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData {
        &self.encrypted_data
    }

    pub fn as_key_ref_ref(
        &self,
    ) -> &Option<crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef> {
        &self.key_ref
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct XSalsa20Poly1305Encrypt {
    pub key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
    pub data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
}

impl XSalsa20Poly1305Encrypt {
    pub fn new(
        key_ref: crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef,
        data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
    ) -> Self {
        Self { key_ref, data }
    }

    pub fn as_key_ref_ref(&self) -> &crate::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef {
        &self.key_ref
    }

    pub fn as_data_ref(&self) -> &crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data {
        &self.data
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct X25519XSalsa20Poly1305Encrypt {
    pub sender: X25519PubKey,
    pub recipient: X25519PubKey,
    pub data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
}

impl X25519XSalsa20Poly1305Encrypt {
    pub fn new(
        sender: X25519PubKey,
        recipient: X25519PubKey,
        data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
    ) -> Self {
        Self {
            sender,
            recipient,
            data,
        }
    }

    pub fn as_sender_ref(&self) -> &X25519PubKey {
        &self.sender
    }

    pub fn as_recipient_ref(&self) -> &X25519PubKey {
        &self.recipient
    }

    pub fn as_data_ref(&self) -> &XSalsa20Poly1305Data {
        &self.data
    }
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes)]
pub struct Ed25519XSalsa20Poly1305Encrypt {
    pub sender: AgentPubKey,
    pub recipient: AgentPubKey,
    pub data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
}

impl Ed25519XSalsa20Poly1305Encrypt {
    pub fn new(
        sender: AgentPubKey,
        recipient: AgentPubKey,
        data: crate::x_salsa20_poly1305::data::XSalsa20Poly1305Data,
    ) -> Self {
        Self {
            sender,
            recipient,
            data,
        }
    }

    pub fn as_sender_ref(&self) -> &AgentPubKey {
        &self.sender
    }

    pub fn as_recipient_ref(&self) -> &AgentPubKey {
        &self.recipient
    }

    pub fn as_data_ref(&self) -> &XSalsa20Poly1305Data {
        &self.data
    }
}



================================================
File: crates/holochain_zome_types/src/zome.rs
================================================
//! A `Zome` is a module of app-defined code which can be run by Holochain.
//! A group of Zomes are composed to form a `DnaDef`.
//!
//! Real-world Holochain Zomes are written in Wasm.
//! This module also provides for an "inline" zome definition, which is written
//! using Rust closures, and is useful for quickly defining zomes on-the-fly
//! for tests.

use std::path::PathBuf;

pub use holochain_integrity_types::zome::*;

use holochain_serialized_bytes::prelude::*;

mod error;
pub use error::*;

#[cfg(feature = "full-dna-def")]
pub mod inline_zome;

#[cfg(feature = "full-dna-def")]
use inline_zome::InlineIntegrityZome;
#[cfg(feature = "full-dna-def")]
use std::sync::Arc;

/// A Holochain Zome. Includes the ZomeDef as well as the name of the Zome.
#[derive(Serialize, Deserialize, Hash, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
#[cfg_attr(feature = "full-dna-def", derive(shrinkwraprs::Shrinkwrap))]
pub struct Zome<T: Send + Sync = ZomeDef> {
    pub name: ZomeName,
    #[cfg_attr(feature = "full-dna-def", shrinkwrap(main_field))]
    pub def: T,
}

pub type IntegrityZome = Zome<IntegrityZomeDef>;

pub type CoordinatorZome = Zome<CoordinatorZomeDef>;

// Use an integrity zome as a coordinator zome,
// for cases where integrity zomes define zome functions
impl From<IntegrityZome> for CoordinatorZome {
    fn from(zome: IntegrityZome) -> Self {
        Zome {
            name: zome.name,
            def: CoordinatorZomeDef(zome.def.0),
        }
    }
}

impl<T: Send + Sync> Zome<T> {
    /// Constructor
    pub fn new(name: ZomeName, def: T) -> Self {
        Self { name, def }
    }

    /// Accessor
    pub fn zome_name(&self) -> &ZomeName {
        &self.name
    }

    pub fn zome_name_mut(&mut self) -> &mut ZomeName {
        &mut self.name
    }

    /// Accessor
    pub fn zome_def(&self) -> &T {
        &self.def
    }

    /// Split into components
    pub fn into_inner(self) -> (ZomeName, T) {
        (self.name, self.def)
    }
}

impl IntegrityZome {
    /// Erase the type of [`Zome`] because you no longer
    /// need to know if this is an integrity or coordinator def.
    pub fn erase_type(self) -> Zome {
        Zome {
            name: self.name,
            def: self.def.erase_type(),
        }
    }
}

impl CoordinatorZome {
    /// Erase the type of [`Zome`] because you no longer
    /// need to know if this is an integrity or coordinator def.
    pub fn erase_type(self) -> Zome {
        Zome {
            name: self.name,
            def: self.def.erase_type(),
        }
    }

    /// Add a dependency to this zome.
    pub fn set_dependency(&mut self, zome_name: impl Into<ZomeName>) {
        self.def.set_dependency(zome_name);
    }
}

impl From<(ZomeName, ZomeDef)> for Zome {
    fn from(pair: (ZomeName, ZomeDef)) -> Self {
        Self::new(pair.0, pair.1)
    }
}

impl From<(ZomeName, IntegrityZomeDef)> for IntegrityZome {
    fn from(pair: (ZomeName, IntegrityZomeDef)) -> Self {
        Self::new(pair.0, pair.1)
    }
}

impl From<(ZomeName, CoordinatorZomeDef)> for CoordinatorZome {
    fn from(pair: (ZomeName, CoordinatorZomeDef)) -> Self {
        Self::new(pair.0, pair.1)
    }
}

impl<T: Send + Sync> From<Zome<T>> for (ZomeName, T) {
    fn from(zome: Zome<T>) -> Self {
        zome.into_inner()
    }
}

impl<T: Send + Sync> From<Zome<T>> for ZomeName {
    fn from(zome: Zome<T>) -> Self {
        zome.name
    }
}

impl From<IntegrityZome> for IntegrityZomeDef {
    fn from(zome: IntegrityZome) -> Self {
        zome.def
    }
}

impl From<CoordinatorZome> for CoordinatorZomeDef {
    fn from(zome: CoordinatorZome) -> Self {
        zome.def
    }
}

/// A zome defined by Wasm bytecode
// TODO: move to `holochain_types`

#[derive(Serialize, Deserialize, Hash, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
pub struct WasmZome {
    /// The WasmHash representing the WASM byte code for this zome.
    pub wasm_hash: holo_hash::WasmHash,

    /// The zome dependencies
    pub dependencies: Vec<ZomeName>,

    /// DEPRECATED: Bundling precompiled and preserialized wasm for iOS is deprecated. Please use the wasm interpreter instead.
    ///
    /// The path to a preserialized wasmer module used as a "dynamic library" (dylib).
    /// Useful for iOS and other targets.
    #[serde(default)]
    pub preserialized_path: Option<PathBuf>,
}

/// Just the definition of a Zome, without the name included. This exists
/// mainly for use in HashMaps where ZomeDefs are keyed by ZomeName.
///
/// NB: Only Wasm Zomes are valid to pass through round-trip serialization,
/// because Rust functions are not serializable. Hence, this enum serializes
/// as if it were a bare WasmZome, and when deserializing, only Wasm zomes
/// can be produced. InlineZomes are serialized as their network seed, so that a
/// hash can be computed, but it is invalid to attempt to deserialize them
/// again.
///
/// In particular, a real-world DnaFile should only ever contain Wasm zomes!
// TODO: move to `holochain_types`
#[derive(Serialize, Deserialize, Hash, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
// This can be untagged, since the only valid serialization target is WasmZome
#[serde(untagged, into = "ZomeDefSerialized")]
pub enum ZomeDef {
    /// A zome defined by Wasm bytecode
    Wasm(WasmZome),

    /// A zome defined by Rust closures. Cannot be deserialized.
    #[serde(skip_deserializing)]
    #[cfg(feature = "full-dna-def")]
    Inline {
        inline_zome: self::inline_zome::DynInlineZome,
        dependencies: Vec<ZomeName>,
    },
}

#[derive(Serialize, Deserialize, Hash, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
pub struct IntegrityZomeDef(ZomeDef);

#[derive(Serialize, Deserialize, Hash, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
pub struct CoordinatorZomeDef(ZomeDef);

/// The serialized form of a ZomeDef, which is identical for Wasm zomes, but
/// unwraps InlineZomes to just a bare network seed.
#[derive(Serialize)]
#[serde(untagged)]
enum ZomeDefSerialized {
    Wasm(WasmZome),

    #[cfg(feature = "full-dna-def")]
    InlineUid(String),
}

impl From<ZomeDef> for ZomeDefSerialized {
    fn from(d: ZomeDef) -> Self {
        match d {
            ZomeDef::Wasm(zome) => Self::Wasm(zome),

            #[cfg(feature = "full-dna-def")]
            ZomeDef::Inline { inline_zome, .. } => Self::InlineUid(inline_zome.0.uuid()),
        }
    }
}

impl IntegrityZomeDef {
    pub fn as_any_zome_def(&self) -> &ZomeDef {
        &self.0
    }
}

impl CoordinatorZomeDef {
    /// Use this as any [`ZomeDef`].
    pub fn as_any_zome_def(&self) -> &ZomeDef {
        &self.0
    }

    /// Add a dependency to this zome.
    pub fn set_dependency(&mut self, zome_name: impl Into<ZomeName>) {
        match &mut self.0 {
            ZomeDef::Wasm(WasmZome { dependencies, .. }) => dependencies.push(zome_name.into()),

            #[cfg(feature = "full-dna-def")]
            ZomeDef::Inline { dependencies, .. } => dependencies.push(zome_name.into()),
        }
    }
}

#[cfg(feature = "full-dna-def")]
impl From<InlineIntegrityZome> for ZomeDef {
    fn from(iz: InlineIntegrityZome) -> Self {
        Self::Inline {
            inline_zome: inline_zome::DynInlineZome(Arc::new(iz)),
            dependencies: Default::default(),
        }
    }
}

#[cfg(feature = "full-dna-def")]
impl From<InlineIntegrityZome> for IntegrityZomeDef {
    fn from(iz: InlineIntegrityZome) -> Self {
        Self(ZomeDef::Inline {
            inline_zome: inline_zome::DynInlineZome(Arc::new(iz)),
            dependencies: Default::default(),
        })
    }
}

#[cfg(feature = "full-dna-def")]
impl From<crate::prelude::InlineCoordinatorZome> for ZomeDef {
    fn from(iz: crate::prelude::InlineCoordinatorZome) -> Self {
        Self::Inline {
            inline_zome: inline_zome::DynInlineZome(Arc::new(iz)),
            dependencies: Default::default(),
        }
    }
}

#[cfg(feature = "full-dna-def")]
impl From<crate::prelude::InlineCoordinatorZome> for CoordinatorZomeDef {
    fn from(iz: crate::prelude::InlineCoordinatorZome) -> Self {
        Self(ZomeDef::Inline {
            inline_zome: inline_zome::DynInlineZome(Arc::new(iz)),
            dependencies: Default::default(),
        })
    }
}

impl ZomeDef {
    /// If this is a Wasm zome, return the WasmHash.
    /// If not, return an error with the provided zome name
    //
    // NB: argument uses underscore here because without full-dna-def feature,
    //     the arg is unused.
    pub fn wasm_hash(&self, _zome_name: &ZomeName) -> ZomeResult<holo_hash::WasmHash> {
        match self {
            ZomeDef::Wasm(WasmZome { wasm_hash, .. }) => Ok(wasm_hash.clone()),

            #[cfg(feature = "full-dna-def")]
            ZomeDef::Inline { .. } => Err(ZomeError::NonWasmZome(_zome_name.clone())),
        }
    }

    /// Get the dependencies of this zome.
    pub fn dependencies(&self) -> &[ZomeName] {
        match self {
            ZomeDef::Wasm(WasmZome { dependencies, .. }) => &dependencies[..],

            #[cfg(feature = "full-dna-def")]
            ZomeDef::Inline { dependencies, .. } => &dependencies[..],
        }
    }
}

impl IntegrityZomeDef {
    pub fn wasm_hash(&self, zome_name: &ZomeName) -> ZomeResult<holo_hash::WasmHash> {
        self.0.wasm_hash(zome_name)
    }
}

impl CoordinatorZomeDef {
    pub fn wasm_hash(&self, zome_name: &ZomeName) -> ZomeResult<holo_hash::WasmHash> {
        self.0.wasm_hash(zome_name)
    }
}

impl From<ZomeDef> for IntegrityZomeDef {
    fn from(z: ZomeDef) -> Self {
        Self(z)
    }
}

impl From<ZomeDef> for CoordinatorZomeDef {
    fn from(z: ZomeDef) -> Self {
        Self(z)
    }
}

impl WasmZome {
    /// Constructor
    pub fn new(wasm_hash: holo_hash::WasmHash) -> Self {
        Self {
            wasm_hash,
            dependencies: Default::default(),
            preserialized_path: None,
        }
    }
}

impl ZomeDef {
    /// create a Zome from a holo_hash WasmHash instead of a holo_hash one
    pub fn from_hash(wasm_hash: holo_hash::WasmHash) -> Self {
        Self::Wasm(WasmZome {
            wasm_hash,
            dependencies: Default::default(),
            preserialized_path: None,
        })
    }
}

impl IntegrityZomeDef {
    pub fn from_hash(wasm_hash: holo_hash::WasmHash) -> Self {
        Self(ZomeDef::from_hash(wasm_hash))
    }

    /// Erase the type of [`ZomeDef`] because you no longer
    /// need to know if this is an integrity or coordinator def.
    pub fn erase_type(self) -> ZomeDef {
        self.0
    }
}

impl CoordinatorZomeDef {
    pub fn from_hash(wasm_hash: holo_hash::WasmHash) -> Self {
        Self(ZomeDef::from_hash(wasm_hash))
    }

    /// Erase the type of [`ZomeDef`] because you no longer
    /// need to know if this is an integrity or coordinator def.
    pub fn erase_type(self) -> ZomeDef {
        self.0
    }
}



================================================
File: crates/holochain_zome_types/src/zome_io.rs
================================================
use crate as zt;
use crate::prelude::*;
use holo_hash::AgentPubKey;
pub use holochain_integrity_types::zome_io::*;
use holochain_nonce::Nonce256Bits;

/// All wasm shared I/O types need to share the same basic behaviours to cross the host/guest
/// boundary in a predictable way.
macro_rules! wasm_io_types {
    ( $( $(#[cfg(feature = $feat:literal)])? fn $f:ident ( $in_arg:ty ) -> $out_arg:ty; )* ) => {
        pub trait HostFnApiT {
            $(
                $(#[cfg(feature = $feat)])?
                fn $f(&self, _: $in_arg) -> Result<$out_arg, HostFnApiError>;
            )*
        }
    }
}

// Every externed function that the zome developer exposes to holochain returns `ExternIO`.
// The zome developer can expose callbacks in a "sparse" way based on names and the functions
// can take different input (e.g. validation vs. hooks like init, etc.).
// All we can say is that some SerializedBytes are being received and returned.
// In the case of ZomeExtern functions exposed to a client, the data input/output is entirely
// arbitrary so we can't say anything at all. In this case the happ developer must BYO
// deserialization context to match the client, either directly or via. the HDK.
// Note though, that _unlike_ zome externs, the host _does_ know exactly the guest should be
// returning for callbacks, it's just that the unpacking of the return happens in two steps:
// - first the sparse callback is triggered with SB input/output
// - then the guest inflates the expected input or the host the expected output based on the
//   callback flavour

wasm_io_types! {

    // ------------------------------------------------------------------
    // These definitions can be copy-pasted into the ribosome's HostFnApi
    // when updated

    // Attempt to accept a preflight request.
    #[cfg(feature = "unstable-countersigning")]
    fn accept_countersigning_preflight_request(zt::countersigning::PreflightRequest) -> zt::countersigning::PreflightRequestAcceptance;

    // Info about the calling agent.
    fn agent_info (()) -> zt::info::AgentInfo;

    // Block some agent on the same DNA.
    #[cfg(feature = "unstable-functions")]
    fn block_agent (zt::block::BlockAgentInput) -> ();

    // Info about the current DNA.
    fn dna_info_1 (()) -> zt::info::DnaInfoV1;
    fn dna_info_2 (()) -> zt::info::DnaInfoV2;

    // @todo
    fn call_info (()) -> zt::info::CallInfo;

    fn call (Vec<zt::call::Call>) -> Vec<zt::prelude::ZomeCallResponse>;

    // @todo List all the local capability claims.
    fn capability_claims (()) -> ();

    // @todo List all the local capability grants.
    fn capability_grants (()) -> ();

    // @todo Get the capability for the current zome call.
    fn capability_info (()) -> ();

    // Returns ActionHash of the newly created record.
    fn create (zt::entry::CreateInput) -> holo_hash::ActionHash;

    // Create a link between two entries.
    fn create_link (zt::link::CreateLinkInput) -> holo_hash::ActionHash;

    fn create_x25519_keypair(()) -> zt::x_salsa20_poly1305::x25519::X25519PubKey;

    // The debug host import takes a TraceMsg to output wherever the host wants to display it.
    // TraceMsg includes line numbers. so the wasm tells the host about it's own code structure.
    fn trace (zt::trace::TraceMsg) -> ();

    // Action hash of the CreateLink record.
    fn delete_link (zt::link::DeleteLinkInput) -> holo_hash::ActionHash;

    // Delete a record.
    fn delete (zt::entry::DeleteInput) -> holo_hash::ActionHash;

    // Action hash of the newly committed record.
    // Emit a Signal::App to subscribers on the interface
    fn emit_signal (zt::signal::AppSignal) -> ();

    fn get_agent_activity (zt::agent_activity::GetAgentActivityInput) -> zt::query::AgentActivity;

    // DPKI
    #[cfg(feature = "unstable-functions")]
    fn get_agent_key_lineage (AgentPubKey) -> Vec<AgentPubKey>;

    fn get_details (Vec<zt::entry::GetInput>) -> Vec<Option<zt::metadata::Details>>;

    fn get_link_details (Vec<zt::link::GetLinksInput>) -> Vec<zt::link::LinkDetails>;

    // Get links by entry hash from the cascade.
    fn get_links (Vec<zt::link::GetLinksInput>) -> Vec<Vec<zt::link::Link>>;

    fn count_links(zt::query::LinkQuery) -> usize;

    // Attempt to get a live entry from the cascade.
    fn get (Vec<zt::entry::GetInput>) -> Vec<Option<zt::record::Record>>;

    // Hash data on the host.
    fn hash (zt::hash::HashInput) -> zt::hash::HashOutput;

    // Check if agent key 2 is of the same lineage as agent key 2.
    // TODO: This HDI function can't be easily removed, even though it's considered an
    // unstable function.
    fn is_same_agent ((AgentPubKey, AgentPubKey)) -> bool;

    // Retreive a record from the DHT or short circuit.
    fn must_get_valid_record (zt::entry::MustGetValidRecordInput) -> zt::record::Record;

    // Retreive a entry from the DHT or short circuit.
    fn must_get_entry (zt::entry::MustGetEntryInput) -> zt::entry::EntryHashed;

    // Retrieve an action from the DHT or short circuit.
    fn must_get_action (zt::entry::MustGetActionInput) -> zt::prelude::SignedActionHashed;

    fn must_get_agent_activity (zt::chain::MustGetAgentActivityInput) -> Vec<zt::op::RegisterAgentActivity>;

    // Query the source chain for data.
    fn query (zt::query::ChainQueryFilter) -> Vec<crate::prelude::Record>;

    // the length of random bytes to create
    fn random_bytes (u32) -> zt::bytes::Bytes;

    // Remotely signal many agents without waiting for responses
    fn send_remote_signal (zt::signal::RemoteSignal) -> ();

    // Schedule a schedulable function if it is not already.
    #[cfg(feature = "unstable-functions")]
    fn schedule (String) -> ();

    // TODO deprecated, remove me
    #[cfg(feature = "unstable-functions")]
    fn sleep (core::time::Duration) -> ();

    // @todo
    fn version (()) -> zt::version::ZomeApiVersion;

    // Attempt to have the keystore sign some data
    // The pubkey in the input needs to be found in the keystore for this to work
    fn sign (zt::signature::Sign) -> zt::signature::Signature;

    fn sign_ephemeral (zt::signature::SignEphemeral) -> zt::signature::EphemeralSignatures;

    // Current system time, in the opinion of the host, as a `Timestamp`.
    fn sys_time (()) -> zt::timestamp::Timestamp;

    // Same as  but also takes the ActionHash of the updated record.
    fn update (zt::entry::UpdateInput) -> holo_hash::ActionHash;

    // Unblock some previously blocked agent.
    #[cfg(feature = "unstable-functions")]
    fn unblock_agent(zt::block::BlockAgentInput) -> ();

    fn verify_signature (zt::signature::VerifySignature) -> bool;

    fn x_salsa20_poly1305_shared_secret_create_random(
        Option<zt::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef>
    ) -> zt::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef;

    fn x_salsa20_poly1305_shared_secret_export(
        zt::x_salsa20_poly1305::XSalsa20Poly1305SharedSecretExport
    ) -> zt::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData;

    fn x_salsa20_poly1305_shared_secret_ingest(
        zt::x_salsa20_poly1305::XSalsa20Poly1305SharedSecretIngest
    ) -> zt::x_salsa20_poly1305::key_ref::XSalsa20Poly1305KeyRef;

    fn x_salsa20_poly1305_encrypt(
        zt::x_salsa20_poly1305::XSalsa20Poly1305Encrypt
    ) -> zt::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData;

    fn x_salsa20_poly1305_decrypt(
        zt::x_salsa20_poly1305::XSalsa20Poly1305Decrypt
    ) -> Option<zt::x_salsa20_poly1305::data::XSalsa20Poly1305Data>;

    // Sender, Recipient, Data.
    fn x_25519_x_salsa20_poly1305_encrypt(zt::x_salsa20_poly1305::X25519XSalsa20Poly1305Encrypt) -> zt::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData;

    // Recipient, Sender, Encrypted data.
    fn x_25519_x_salsa20_poly1305_decrypt(zt::x_salsa20_poly1305::X25519XSalsa20Poly1305Decrypt) -> Option<zt::x_salsa20_poly1305::data::XSalsa20Poly1305Data>;

    // Sender, Recipient, Data.
    fn ed_25519_x_salsa20_poly1305_encrypt(zt::x_salsa20_poly1305::Ed25519XSalsa20Poly1305Encrypt) -> zt::x_salsa20_poly1305::encrypted_data::XSalsa20Poly1305EncryptedData;

    // Recipient, Sender, Encrypted data.
    fn ed_25519_x_salsa20_poly1305_decrypt(zt::x_salsa20_poly1305::Ed25519XSalsa20Poly1305Decrypt) -> zt::x_salsa20_poly1305::data::XSalsa20Poly1305Data;

    // The zome and agent info are constants specific to the current zome and chain.
    // All the information is provided by core so there is no input value.
    // These are constant for the lifetime of a zome call.
    fn zome_info (()) -> zt::info::ZomeInfo;

    // Create a clone of an existing cell.
    fn create_clone_cell(zt::clone::CreateCloneCellInput) -> zt::clone::ClonedCell;

    // Disable a clone cell.
    fn disable_clone_cell(zt::clone::DisableCloneCellInput) -> ();

    // Enable a clone cell.
    fn enable_clone_cell(zt::clone::EnableCloneCellInput) -> zt::clone::ClonedCell;

    // Delete a clone cell.
    fn delete_clone_cell(zt::clone::DeleteCloneCellInput) -> ();

    // Close your source chain, indicating that you are migrating to a new DNA
    fn close_chain(zt::chain::CloseChainInput) -> holo_hash::ActionHash;

    // Open your chain, pointing to the previous DNA
    fn open_chain(zt::chain::OpenChainInput) -> holo_hash::ActionHash;

    // Get validation receipts for an action
    fn get_validation_receipts(zt::validate::GetValidationReceiptsInput) -> Vec<zt::validate::ValidationReceiptSet>;
}

/// Anything that can go wrong while calling a HostFnApi method
#[derive(thiserror::Error, Debug)]
pub enum HostFnApiError {
    #[error("Error from within host function implementation: {0}")]
    RibosomeError(Box<dyn std::error::Error + Send + Sync>),
}

#[derive(PartialEq, Clone, Debug, serde::Serialize, serde::Deserialize)]
pub enum ZomeCallAuthorization {
    Authorized,
    BadCapGrant,
    BadNonce(String),
    BlockedProvenance,
}

impl std::fmt::Display for ZomeCallAuthorization {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{:?}", self)
    }
}

impl ZomeCallAuthorization {
    pub fn is_authorized(&self) -> bool {
        matches!(self, ZomeCallAuthorization::Authorized)
    }
}

/// Response to a zome call.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize, SerializedBytes, PartialEq)]
pub enum ZomeCallResponse {
    /// Arbitrary response from zome fns to the outside world.
    /// Something like a 200 http response.
    Ok(ExternIO),
    /// Authentication failure - signature could not be verified by the provenance.
    AuthenticationFailed(Signature, AgentPubKey),
    /// Cap grant failure.
    /// Something like a 401 http response.
    Unauthorized(
        ZomeCallAuthorization,
        Option<CapSecret>,
        ZomeName,
        FunctionName,
    ),
    /// This was a zome call made remotely but
    /// something has failed on the network
    NetworkError(String),
    /// A countersigning session has failed to start.
    CountersigningSession(String),
}

impl std::fmt::Display for ZomeCallResponse {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{:?}", self)
    }
}

#[cfg(feature = "test_utils")]
impl ZomeCallResponse {
    pub fn unwrap(self) -> ExternIO {
        match self {
            ZomeCallResponse::Ok(output) => output,
            _ => panic!("Attempted to unwrap a non-Ok ZomeCallResponse"),
        }
    }
}

/// Zome calls need to be signed regardless of how they are called.
/// This defines exactly what needs to be signed.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ZomeCallParams {
    /// Provenance to sign.
    pub provenance: AgentPubKey,
    /// Cell ID to sign.
    pub cell_id: CellId,
    /// Zome name to sign.
    pub zome_name: ZomeName,
    /// Function name to sign.
    pub fn_name: FunctionName,
    /// Cap secret to sign.
    pub cap_secret: Option<CapSecret>,
    /// Payload to sign.
    pub payload: ExternIO,
    /// Nonce to sign.
    pub nonce: Nonce256Bits,
    /// Time after which this zome call MUST NOT be accepted.
    pub expires_at: Timestamp,
}

impl ZomeCallParams {
    /// Prepare the canonical bytes for zome call parameters so that they are
    /// always signed and verified in the same way.
    /// Signature is generated for the hash of the bytes.
    pub fn serialize_and_hash(&self) -> Result<(Vec<u8>, Vec<u8>), SerializedBytesError> {
        let bytes = holochain_serialized_bytes::encode(&self)?;
        let bytes_hash = sha2_512(&bytes);
        Ok((bytes, bytes_hash))
    }
}



================================================
File: crates/holochain_zome_types/src/capability/grant.rs
================================================
use super::*;
use holochain_serialized_bytes::SerializedBytes;
use std::collections::BTreeMap;

/// Map of zome functions to the payloads to curry into to them
// @todo Ability to forcibly curry payloads into functions that are called with a claim.
#[derive(Default, PartialEq, Eq, Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CurryPayloads(pub BTreeMap<GrantedFunction, SerializedBytes>);



================================================
File: crates/holochain_zome_types/src/entry/app_entry_bytes.rs
================================================
pub use holochain_integrity_types::entry::AppEntryBytes;



================================================
File: crates/holochain_zome_types/src/zome/error.rs
================================================
use crate::prelude::*;

/// Anything that can go wrong while calling a HostFnApi method
#[derive(thiserror::Error, Debug)]
pub enum ZomeError {
    /// ZomeNotFound
    #[error("Zome not found: {0}")]
    ZomeNotFound(String),

    /// NonWasmZome
    #[error("Accessed a zome expecting to find a WasmZome, but found other type. Zome name: {0}")]
    NonWasmZome(ZomeName),

    /// SerializedBytesError (can occur during DnaDef::update_modifiers)
    #[error(transparent)]
    SerializedBytesError(#[from] holochain_serialized_bytes::SerializedBytesError),
}

pub type ZomeResult<T> = Result<T, ZomeError>;



================================================
File: crates/holochain_zome_types/src/zome/inline_zome.rs
================================================
//! A variant of Zome which is defined entirely by native, inline Rust code
//!
//! This type of Zome is only meant to be used for testing. It's designed to
//! make it easy to write a zome on-the-fly or programmatically, rather than
//! having to go through the heavy machinery of wasm compilation

// TODO: move entire module to `holochain_types`

use crate::prelude::*;
pub use error::*;
use serde::de::DeserializeOwned;
use std::collections::HashMap;
use std::marker::PhantomData;
use std::sync::Arc;

mod error;

pub type BoxApi = Box<dyn HostFnApiT>;

#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
/// A type marker for an integrity [`InlineZome`].
pub struct IntegrityZomeMarker;
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
/// A type marker for a coordinator [`InlineZome`].
pub struct CoordinatorZomeMarker;

pub type InlineIntegrityZome = InlineZome<IntegrityZomeMarker>;
pub type InlineCoordinatorZome = InlineZome<CoordinatorZomeMarker>;

/// An "inline" zome definition in pure Rust, as opposed to a zome defined in Wasm.
#[derive(Clone)]
pub struct InlineZome<T = IntegrityZomeMarker> {
    /// Inline zome type marker.
    _t: PhantomData<T>,

    /// Since closures cannot be serialized, we include a UUID which
    /// is the only part of an InlineZome that gets serialized.
    /// This UUID becomes part of the determination of the DnaHash
    /// that it is a part of.
    /// Think of it as a stand-in for the WasmHash of a WasmZome.
    pub(super) uuid: String,

    /// The collection of closures which define this zome.
    /// These functions are directly called by the Ribosome.
    pub(super) functions: HashMap<FunctionName, InlineZomeFn>,

    /// Global values for this zome.
    pub(super) globals: HashMap<String, u8>,
}

impl<T> InlineZome<T> {
    /// Inner constructor.
    fn new_inner<S: Into<String>>(uuid: S) -> Self {
        Self {
            _t: PhantomData,
            uuid: uuid.into(),
            functions: HashMap::new(),
            globals: HashMap::new(),
        }
    }

    pub fn functions(&self) -> Vec<FunctionName> {
        let mut keys: Vec<FunctionName> = self.functions.keys().cloned().collect();
        keys.sort();
        keys
    }

    /// Define a new zome function or callback with the given name
    pub fn function<F, I, O>(mut self, name: &str, f: F) -> Self
    where
        F: Fn(BoxApi, I) -> InlineZomeResult<O> + 'static + Send + Sync,
        I: DeserializeOwned + std::fmt::Debug,
        O: Serialize + std::fmt::Debug,
    {
        let z = move |api: BoxApi, input: ExternIO| -> InlineZomeResult<ExternIO> {
            Ok(ExternIO::encode(f(api, input.decode()?)?)?)
        };
        if self.functions.insert(name.into(), Arc::new(z)).is_some() {
            tracing::warn!("Replacing existing InlineZome callback '{}'", name);
        };
        self
    }

    /// Make a call to an inline zome callback.
    /// If the callback doesn't exist, return None.
    pub fn maybe_call(
        &self,
        api: BoxApi,
        name: &FunctionName,
        input: ExternIO,
    ) -> InlineZomeResult<Option<ExternIO>> {
        if let Some(f) = self.functions.get(name) {
            Ok(Some(f(api, input)?))
        } else {
            Ok(None)
        }
    }

    /// Accessor
    pub fn uuid(&self) -> String {
        self.uuid.clone()
    }

    /// Set a global value for this zome.
    pub fn set_global(mut self, name: impl Into<String>, val: u8) -> Self {
        self.globals.insert(name.into(), val);
        self
    }
}

impl InlineIntegrityZome {
    /// Create a new integrity zome with the given network seed
    pub fn new<S: Into<String>, E: IntoIterator<Item = EntryDef>>(
        uuid: S,
        entry_defs: E,
        num_link_types: u8,
    ) -> Self {
        let entry_defs = entry_defs.into_iter().collect::<Vec<_>>();
        let num_entry_types = entry_defs.len();
        let entry_defs_callback =
            move |_, _: ()| Ok(EntryDefsCallbackResult::Defs(entry_defs.clone().into()));
        Self::new_inner(uuid)
            .function("entry_defs", Box::new(entry_defs_callback))
            .set_global("__num_entry_types", num_entry_types.try_into().unwrap())
            .set_global("__num_link_types", num_link_types)
    }
    /// Create a new integrity zome with a unique random network seed
    pub fn new_unique<E: IntoIterator<Item = EntryDef>>(entry_defs: E, num_link_types: u8) -> Self {
        Self::new(nanoid::nanoid!(), entry_defs, num_link_types)
    }
}

impl InlineCoordinatorZome {
    /// Create a new coordinator zome with the given network seed
    pub fn new<S: Into<String>>(uuid: S) -> Self {
        Self::new_inner(uuid)
    }
    /// Create a new coordinator zome with a unique random network seed
    pub fn new_unique() -> Self {
        Self::new(nanoid::nanoid!())
    }
}

#[derive(Debug, Clone)]
/// An inline zome clonable type object.
pub struct DynInlineZome(pub Arc<dyn InlineZomeT + Send + Sync>);

pub trait InlineZomeT: std::fmt::Debug {
    /// Get the functions for this [`InlineZome`].
    fn functions(&self) -> Vec<FunctionName>;

    /// Make a call to an inline zome function.
    /// If the function doesn't exist, return None.
    fn maybe_call(
        &self,
        api: BoxApi,
        name: &FunctionName,
        input: ExternIO,
    ) -> InlineZomeResult<Option<ExternIO>>;

    /// Accessor
    fn uuid(&self) -> String;

    /// Get a global value for this zome.
    fn get_global(&self, name: &str) -> Option<u8>;
}

/// An inline zome function takes a Host API and an input, and produces an output.
pub type InlineZomeFn =
    Arc<dyn Fn(BoxApi, ExternIO) -> InlineZomeResult<ExternIO> + 'static + Send + Sync>;

impl<T: std::fmt::Debug> InlineZomeT for InlineZome<T> {
    fn functions(&self) -> Vec<FunctionName> {
        self.functions()
    }

    fn maybe_call(
        &self,
        api: BoxApi,
        name: &FunctionName,
        input: ExternIO,
    ) -> InlineZomeResult<Option<ExternIO>> {
        self.maybe_call(api, name, input)
    }

    fn uuid(&self) -> String {
        self.uuid()
    }

    fn get_global(&self, name: &str) -> Option<u8> {
        self.globals.get(name).copied()
    }
}

impl<T: std::fmt::Debug> std::fmt::Debug for InlineZome<T> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!("<InlineZome {}>", self.uuid))
    }
}

impl<T: PartialEq> PartialEq for InlineZome<T> {
    fn eq(&self, other: &InlineZome<T>) -> bool {
        self.uuid == other.uuid
    }
}

impl PartialEq for DynInlineZome {
    fn eq(&self, other: &DynInlineZome) -> bool {
        self.0.uuid() == other.0.uuid()
    }
}

impl<T: PartialOrd> PartialOrd for InlineZome<T> {
    fn partial_cmp(&self, other: &InlineZome<T>) -> Option<std::cmp::Ordering> {
        Some(self.uuid.cmp(&other.uuid))
    }
}

impl PartialOrd for DynInlineZome {
    fn partial_cmp(&self, other: &DynInlineZome) -> Option<std::cmp::Ordering> {
        Some(self.0.uuid().cmp(&other.0.uuid()))
    }
}

impl<T: Eq> Eq for InlineZome<T> {}

impl Eq for DynInlineZome {}

impl<T: Ord> Ord for InlineZome<T> {
    fn cmp(&self, other: &InlineZome<T>) -> std::cmp::Ordering {
        self.uuid.cmp(&other.uuid)
    }
}

impl Ord for DynInlineZome {
    fn cmp(&self, other: &DynInlineZome) -> std::cmp::Ordering {
        self.0.uuid().cmp(&other.0.uuid())
    }
}

impl<T: std::hash::Hash> std::hash::Hash for InlineZome<T> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.uuid.hash(state);
    }
}

impl std::hash::Hash for DynInlineZome {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.0.uuid().hash(state);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::prelude::GetOptions;
    use ::fixt::fixt;
    use holo_hash::fixt::ActionHashFixturator;
    use holo_hash::AnyDhtHash;

    #[test]
    #[allow(unused_variables, unreachable_code)]
    fn can_create_inline_dna() {
        let zome = InlineIntegrityZome::new("", vec![], 0).function("zome_fn_1", |api, a: ()| {
            let hash: AnyDhtHash = fixt!(ActionHash).into();
            Ok(api
                .get(vec![GetInput::new(hash, GetOptions::default())])
                .expect("TODO after crate re-org"))
        });
        // let dna = InlineDna::new(hashmap! {
        //     "zome".into() => zome
        // });
    }
}



================================================
File: crates/holochain_zome_types/src/zome/inline_zome/error.rs
================================================
#![allow(missing_docs)]

use crate::prelude::*;
use holochain_serialized_bytes::SerializedBytesError;
use thiserror::Error;

pub type InlineZomeResult<T> = Result<T, InlineZomeError>;

#[derive(Error, Debug)]
pub enum InlineZomeError {
    #[error("No such InlineZome callback: {0}")]
    NoSuchCallback(FunctionName),

    #[error("Error during host fn call: {0}")]
    HostFnApiError(#[from] HostFnApiError),

    #[error(transparent)]
    SerializationError(#[from] SerializedBytesError),

    #[cfg(any(test, feature = "test_utils"))]
    #[error("test error: {0}")]
    TestError(String),
}



================================================
File: crates/kitsune_p2p/bin_data/README.md
================================================
# Kitsune p2p bin data

Binary data types. Was `bin_types.rs` in `kitsune_p2p_types`, now a crate.


================================================
File: crates/kitsune_p2p/bin_data/Cargo.toml
================================================
[package]
name = "kitsune_p2p_bin_data"
version = "0.5.0-dev.5"
description = "Binary data types for kitsune_p2p"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_bin_data"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
holochain_util = { version = "^0.5.0-dev.1", path = "../../holochain_util", default-features = false }
kitsune_p2p_dht_arc = { version = "^0.5.0-dev.2", path = "../dht_arc" }
shrinkwraprs = "0.3.0"
derive_more = "0.99"
serde = { version = "1", features = ["derive", "rc"] }
base64 = "0.22"
serde_bytes = "0.11"

proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }
fixt = { version = "^0.5.0-dev.1", path = "../../fixt", optional = true }

[lints]
workspace = true

[features]

fuzzing = [
  "proptest",
  "proptest-derive",
  "kitsune_p2p_dht_arc/fuzzing",
]

test_utils = []

fixt = ["dep:fixt"]

sqlite-encrypted = ["kitsune_p2p_dht_arc/sqlite-encrypted"]
sqlite = ["kitsune_p2p_dht_arc/sqlite"]



================================================
File: crates/kitsune_p2p/bin_data/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

- Crate now exists



================================================
File: crates/kitsune_p2p/bin_data/src/fixt.rs
================================================
use crate::KitsuneAgent;
use crate::KitsuneBinType;
use crate::KitsuneOpHash;
use crate::KitsuneSignature;
use crate::KitsuneSpace;
use ::fixt::prelude::*;

fixturator!(
    KitsuneAgent;
    constructor fn new(ThirtySixBytes);
);

fixturator!(
    KitsuneSpace;
    constructor fn new(ThirtySixBytes);
);

fixturator!(
    KitsuneOpHash;
    constructor fn new(ThirtySixBytes);
);

fixturator!(
    KitsuneSignature;
    from SixtyFourBytesVec;
);



================================================
File: crates/kitsune_p2p/bin_data/src/lib.rs
================================================
//! Binary types, hashes, signatures, etc used by kitsune.

use base64::engine::general_purpose::URL_SAFE_NO_PAD;
use base64::Engine;
use kitsune_p2p_dht_arc::DhtLocation;

#[cfg(feature = "fixt")]
pub mod fixt;

pub mod dependencies {
    #[cfg(feature = "fuzzing")]
    pub use proptest;
    #[cfg(feature = "fuzzing")]
    pub use proptest_derive;
}

/// Kitsune hashes are expected to be 36 bytes.
/// The first 32 bytes are the proper hash.
/// The final 4 bytes are a hash-of-the-hash that can be treated like a u32 "location".
pub trait KitsuneBinType:
    'static
    + Send
    + Sync
    + std::fmt::Debug
    + Clone
    + std::hash::Hash
    + PartialEq
    + Eq
    + PartialOrd
    + Ord
    + std::convert::Into<Vec<u8>>
{
    /// Create an instance, ensuring the proper number of bytes were provided.
    fn new(bytes: Vec<u8>) -> Self;

    /// Fetch just the core 32 bytes (without the 4 location bytes).
    fn get_bytes(&self) -> &[u8];

    /// Fetch the dht "loc" / location for this hash.
    fn get_loc(&self) -> DhtLocation;
}

/// internal convert 4 location bytes into a u32 location
fn bytes_to_loc(bytes: &[u8]) -> u32 {
    (bytes[0] as u32)
        + ((bytes[1] as u32) << 8)
        + ((bytes[2] as u32) << 16)
        + ((bytes[3] as u32) << 24)
}

macro_rules! make_kitsune_bin_type {
    ($($doc:expr, $name:ident),*,) => {
        $(
            #[doc = $doc]
            #[derive(
                Clone,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                shrinkwraprs::Shrinkwrap,
                derive_more::Into,
                serde::Serialize,
                serde::Deserialize,
            )]
            #[shrinkwrap(mutable)]
            pub struct $name(
                #[serde(with = "serde_bytes")]
                pub Vec<u8>);

            impl KitsuneBinType for $name {

                // TODO This actually allows mixups, for example a KitsuneAgent can be constructed with a 36 byte vector
                fn new(mut bytes: Vec<u8>) -> Self {
                    if bytes.len() != 36 {
                        // If location bytes are not included, append them now.
                        debug_assert_eq!(bytes.len(), 32);
                        // FIXME: no way to compute location bytes at this time,
                        // so simply pad with 0's for now
                        bytes.append(&mut [0; 4].to_vec());

                        // todo!("actually calculate location bytes");
                        // bytes.append(&mut kitsune_location_bytes(&bytes));
                    }
                    debug_assert_eq!(bytes.len(), 36);
                    Self(bytes)
                }

                fn get_bytes(&self) -> &[u8] {
                    &self.0[..self.0.len() - 4]
                }

                fn get_loc(&self) -> DhtLocation {
                    DhtLocation::new(bytes_to_loc(&self.0[self.0.len() - 4..]))
                }
            }

            impl std::fmt::Debug for $name {
                fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                    f.write_fmt(format_args!("{}(0x{})", stringify!($name), &holochain_util::hex::bytes_to_hex(&self.0, false)))
                }
            }

            impl std::fmt::Display for $name {
                fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                    URL_SAFE_NO_PAD.encode(&self.0).fmt(f)
                }
            }

            impl AsRef<[u8]> for $name {
                fn as_ref(&self) -> &[u8] {
                    self.0.as_slice()
                }
            }
        )*
    };
}

// #[derive(Debug)]
// pub struct BT(Vec<u8>);

make_kitsune_bin_type! {
    "Distinguish multiple categories of communication within the same network module.",
    KitsuneSpace,

    "Distinguish multiple agents within the same network module.",
    KitsuneAgent,

    "The basis hash/coordinate when identifying a neighborhood.",
    KitsuneBasis,

    r#"Top-level "KitsuneDataHash" items are buckets of related meta-data.
These metadata "Operations" each also have unique OpHashes."#,
    KitsuneOpHash,
}

/// The op data with its location
#[derive(PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[repr(transparent)]
#[serde(transparent)]
pub struct KitsuneOpData(
    /// The op bytes
    #[serde(with = "serde_bytes")]
    pub Vec<u8>,
);

impl KitsuneOpData {
    /// Constructor
    pub fn new(op: Vec<u8>) -> KOp {
        KOp::new(Self(op))
    }

    /// Size in bytes of this Op
    pub fn size(&self) -> usize {
        self.0.len()
    }
}

impl From<Vec<u8>> for KitsuneOpData {
    fn from(d: Vec<u8>) -> Self {
        Self(d)
    }
}

impl std::fmt::Debug for KitsuneOpData {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!(
            "KitsuneOpData({})",
            &holochain_util::hex::many_bytes_string(self.0.as_slice())
        ))
    }
}

/// Convenience type
pub type KOp = std::sync::Arc<KitsuneOpData>;

/// A cryptographic signature.
#[derive(
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    shrinkwraprs::Shrinkwrap,
    derive_more::From,
    derive_more::Into,
    serde::Deserialize,
    serde::Serialize,
)]
#[shrinkwrap(mutable)]
pub struct KitsuneSignature(#[serde(with = "serde_bytes")] pub Vec<u8>);

impl std::fmt::Debug for KitsuneSignature {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!("Signature(0x"))?;
        for byte in &self.0 {
            f.write_fmt(format_args!("{:02x}", byte))?;
        }
        f.write_fmt(format_args!(")"))?;
        Ok(())
    }
}

/// A 32 byte cert identifying a peer
#[derive(
    Clone,
    PartialEq,
    Eq,
    Hash,
    derive_more::Deref,
    derive_more::From,
    serde::Serialize,
    serde::Deserialize,
)]
pub struct NodeCert(std::sync::Arc<[u8; 32]>);

impl std::fmt::Debug for NodeCert {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_tuple("NodeCert")
            .field(&holochain_util::hex::many_bytes_string(self.0.as_slice()))
            .finish()
    }
}



================================================
File: crates/kitsune_p2p/block/README.md
================================================
# Kitsune p2p block

Types for (un)block functionality that maps to the conductor.


================================================
File: crates/kitsune_p2p/block/Cargo.toml
================================================
[package]
name = "kitsune_p2p_block"
version = "0.5.0-dev.5"
description = "(un)Block datatype for kitsune_p2p"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_block"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
kitsune_p2p_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../bin_data" }
serde = { version = "1.0", features = ["derive"] }

[lints]
workspace = true

[features]
sqlite-encrypted = [
  "kitsune_p2p_timestamp/sqlite-encrypted",
  "kitsune_p2p_bin_data/sqlite-encrypted",
]
sqlite = ["kitsune_p2p_timestamp/sqlite", "kitsune_p2p_bin_data/sqlite"]



================================================
File: crates/kitsune_p2p/block/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.4

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

- Crate now exists



================================================
File: crates/kitsune_p2p/block/src/lib.rs
================================================
use kitsune_p2p_timestamp::InclusiveTimestampInterval;
use std::sync::Arc;

pub use kitsune_p2p_timestamp::Timestamp;

#[derive(Clone)]
pub enum AgentSpaceBlockReason {
    BadCrypto,
}

#[derive(Clone, serde::Serialize, Debug, Eq, PartialEq, Hash)]
pub enum NodeBlockReason {
    /// The node did some bad cryptography.
    BadCrypto,
    /// DOS attack.
    DOS,
}

#[derive(Clone, serde::Serialize, Debug, Eq, PartialEq, Hash)]
pub enum NodeSpaceBlockReason {
    BadWire,
}

#[derive(Clone, serde::Serialize, Debug, Eq, PartialEq, Hash)]
pub enum IpBlockReason {
    /// Classic DOS.
    DOS,
}

pub type NodeId = kitsune_p2p_bin_data::NodeCert;

#[derive(Clone, Eq, PartialEq, Hash)]
pub enum BlockTarget {
    Node(NodeId, NodeBlockReason),
    NodeSpace(
        NodeId,
        Arc<kitsune_p2p_bin_data::KitsuneSpace>,
        NodeSpaceBlockReason,
    ),
    Ip(std::net::Ipv4Addr, IpBlockReason),
}

#[derive(Eq, PartialEq)]
pub enum BlockTargetId {
    Node(NodeId),
    NodeSpace(NodeId, Arc<kitsune_p2p_bin_data::KitsuneSpace>),
    Ip(std::net::Ipv4Addr),
}

impl From<BlockTarget> for BlockTargetId {
    fn from(block_target: BlockTarget) -> Self {
        match block_target {
            BlockTarget::NodeSpace(node_id, space, _) => Self::NodeSpace(node_id, space),
            BlockTarget::Node(node_id, _) => Self::Node(node_id),
            BlockTarget::Ip(ip_addr, _) => Self::Ip(ip_addr),
        }
    }
}

#[derive(Clone, Eq, PartialEq, Hash)]
pub struct Block {
    target: BlockTarget,
    interval: InclusiveTimestampInterval,
}

impl Block {
    pub fn new(target: BlockTarget, interval: InclusiveTimestampInterval) -> Self {
        Self { target, interval }
    }

    pub fn target(&self) -> &BlockTarget {
        &self.target
    }

    pub fn into_target(self) -> BlockTarget {
        self.target
    }

    pub fn into_interval(self) -> InclusiveTimestampInterval {
        self.interval
    }

    pub fn start(&self) -> Timestamp {
        self.interval.start()
    }

    pub fn end(&self) -> Timestamp {
        self.interval.end()
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/README.md
================================================
# kitsune_p2p_bootstrap

Bootstrap server subcrate for kitsune-p2p.

License: Apache-2.0



================================================
File: crates/kitsune_p2p/bootstrap/Cargo.toml
================================================
[package]
name = "kitsune_p2p_bootstrap"
version = "0.4.0-dev.11"
description = "Bootstrap server written in rust for kitsune nodes to find each other"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_bootstrap"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
clap = { version = "4.3.21", features = ["derive"] }
futures = "0.3"
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../types" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../bin_data" }
parking_lot = "0.12.1"
rand = "0.8.5"
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls"] }
serde = { version = "1", features = ["derive", "rc"] }
serde_bytes = "0.11"
thiserror = "1"
tokio = { version = "1", features = ["full"] }
warp = "0.3"

[dev-dependencies]
kitsune_p2p_bootstrap = { path = ".", features = ["test_utils"] }
kitsune_p2p_types = { path = "../types", features = ["test_utils"] }
kitsune_p2p = { path = "../kitsune_p2p", features = ["sqlite", "test_utils"] }
fixt = { path = "../../fixt", version = "^0.5.0-dev.1"}
criterion = "0.5.1"
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls"] }

[[bench]]
name = "bench"
harness = false

[lib]
name = "kitsune_p2p_bootstrap"
path = "src/lib.rs"

[[bin]]
name = "kitsune-bootstrap"
path = "src/main.rs"

[lints]
workspace = true

[features]
sqlite-encrypted = ["kitsune_p2p_types/sqlite-encrypted"]
sqlite = ["kitsune_p2p_types/sqlite"]
test_utils = ["kitsune_p2p_types/fixt", "kitsune_p2p_bin_data/fixt"]



================================================
File: crates/kitsune_p2p/bootstrap/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-dev.16

## 0.3.0-dev.15

## 0.3.0-dev.14

## 0.3.0-dev.13

## 0.3.0-dev.12

## 0.3.0-dev.11

## 0.3.0-dev.10

## 0.3.0-dev.9

## 0.3.0-dev.8

## 0.3.0-dev.7

## 0.3.0-dev.6

## 0.3.0-dev.5

## 0.3.0-dev.4

## 0.3.0-dev.3

## 0.3.0-dev.2

## 0.3.0-dev.1

## 0.3.0-dev.0

## 0.2.0

## 0.2.0-beta-dev.28

## 0.2.0-beta-dev.27

## 0.2.0-beta-dev.26

## 0.2.0-beta-dev.25

## 0.2.0-beta-dev.24

## 0.2.0-beta-dev.23

## 0.2.0-beta-dev.22

## 0.2.0-beta-dev.21

## 0.2.0-beta-dev.20

## 0.2.0-beta-dev.19

## 0.2.0-beta-dev.18

## 0.2.0-beta-dev.17

## 0.2.0-beta-dev.16

## 0.2.0-beta-dev.15

## 0.2.0-beta-dev.14

## 0.2.0-beta-dev.13

## 0.2.0-beta-dev.12

## 0.2.0-beta-dev.11

## 0.2.0-beta-dev.10

## 0.2.0-beta-dev.9

## 0.2.0-beta-dev.8

## 0.2.0-beta-dev.7

## 0.2.0-beta-dev.6

## 0.2.0-beta-dev.5

## 0.2.0-beta-dev.4

## 0.2.0-beta-dev.3

## 0.2.0-beta-dev.2

## 0.2.0-beta-dev.1

## 0.2.0-beta-dev.0

## 0.1.0

## 0.1.0-beta-rc.4

## 0.1.0-beta-rc.3

## 0.1.0-beta-rc.2

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.12-dev.0

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/kitsune_p2p/bootstrap/benches/bench.rs
================================================
use std::sync::Arc;

use criterion::criterion_group;
use criterion::criterion_main;
use criterion::BenchmarkId;
use criterion::Criterion;

use ::fixt::prelude::*;
use kitsune_p2p::agent_store::AgentInfoSigned;
use kitsune_p2p::dependencies::url2::url2;
use kitsune_p2p::dht::arq::ArqSize;
use kitsune_p2p::fixt::*;
use kitsune_p2p::KitsuneSpace;
use kitsune_p2p_bootstrap::error::BootstrapClientError;
use kitsune_p2p_bootstrap::error::BootstrapClientResult;
use kitsune_p2p_types::bootstrap::RandomLimit;
use kitsune_p2p_types::bootstrap::RandomQuery;
use kitsune_p2p_types::fixt::UrlListFixturator;
use tokio::runtime::Builder;
use tokio::runtime::Runtime;

criterion_group!(benches, bootstrap);

criterion_main!(benches);

fn bootstrap(bench: &mut Criterion) {
    let mut group = bench.benchmark_group("bootstrap");
    group.sample_size(
        std::env::var_os("BENCH_SAMPLE_SIZE")
            .and_then(|s| s.to_string_lossy().parse::<usize>().ok())
            .unwrap_or(100),
    );
    let runtime = rt();
    let client = reqwest::Client::builder().use_rustls_tls().build().unwrap();

    let mut url = url2!("http://127.0.0.1:0");
    let (driver, addr, _shutdown) = runtime.block_on(async {
        kitsune_p2p_bootstrap::run(([127, 0, 0, 1], 0), vec![])
            .await
            .unwrap()
    });
    runtime.spawn(async move {
        driver.await;
        println!("BOOTSTRAP CLOSED");
    });
    url.set_port(Some(addr.port())).unwrap();
    group.bench_function(BenchmarkId::new("test", "now".to_string()), |b| {
        b.iter(|| {
            runtime.block_on(async {
                let time: u64 = do_api(url.clone(), "now", (), &client)
                    .await
                    .unwrap()
                    .unwrap();
                assert!(time > 0);
            });
        });
    });
    let space: Arc<KitsuneSpace> = runtime.block_on(async { Arc::new(fixt!(KitsuneSpace)) });
    group.bench_function(BenchmarkId::new("test", "put".to_string()), |b| {
        b.iter(|| {
            runtime.block_on(async {
                let info = AgentInfoSigned::sign(
                    space.clone(),
                    Arc::new(fixt!(KitsuneAgent, Unpredictable)),
                    ArqSize::from_half_len(u32::MAX / 4),
                    fixt!(UrlList, Empty),
                    0,
                    std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64 + 60_000_000,
                    |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable))) },
                )
                .await
                .unwrap();
                let _: Option<()> = do_api(url.clone(), "put", info, &client)
                    .await
                    .unwrap()
                    .unwrap();
            });
        });
    });
    let query = RandomQuery {
        space,
        limit: RandomLimit(10),
    };
    group.bench_function(BenchmarkId::new("test", "random".to_string()), |b| {
        b.iter(|| {
            runtime.block_on(async {
                let peers: Vec<serde_bytes::ByteBuf> =
                    do_api(url.clone(), "random", query.clone(), &client)
                        .await
                        .unwrap()
                        .unwrap();
                assert_eq!(peers.len(), 10);
            });
        });
    });
    runtime.shutdown_background();
}

async fn do_api<I: serde::Serialize, O: serde::de::DeserializeOwned>(
    url: kitsune_p2p::dependencies::url2::Url2,
    op: &str,
    input: I,
    client: &reqwest::Client,
) -> BootstrapClientResult<Option<O>> {
    let mut body_data = Vec::new();
    kitsune_p2p_types::codec::rmp_encode(&mut body_data, &input)?;
    let res = client
        .post(url.as_str())
        .body(body_data)
        .header("X-Op", op)
        .header(reqwest::header::CONTENT_TYPE, "application/octet")
        .send()
        .await?;
    if res.status().is_success() {
        Ok(Some(kitsune_p2p_types::codec::rmp_decode(
            &mut res.bytes().await?.as_ref(),
        )?))
    } else {
        Err(BootstrapClientError::Bootstrap(
            res.text().await?.into_boxed_str(),
        ))
    }
}

pub fn rt() -> Runtime {
    Builder::new_multi_thread().enable_all().build().unwrap()
}



================================================
File: crates/kitsune_p2p/bootstrap/src/clear.rs
================================================
use crate::store::Store;

use super::*;
use warp::Filter;

pub(crate) fn clear(
    store: Store,
) -> impl Filter<Extract = impl warp::Reply + Sized, Error = warp::Rejection> + Clone {
    warp::post()
        .and(warp::header::exact("X-Op", "clear"))
        .and(with_store(store))
        .and_then(clear_info)
}

async fn clear_info(store: Store) -> Result<impl warp::Reply, warp::Rejection> {
    store.clear();
    Ok(warp::reply())
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use super::*;
    use ::fixt::prelude::*;
    use kitsune_p2p::{agent_store::AgentInfoSigned, KitsuneSpace};
    use kitsune_p2p_bin_data::fixt::*;
    use kitsune_p2p_types::{dht::arq::ArqSize, fixt::*};

    #[tokio::test(flavor = "multi_thread")]
    async fn test_clear() {
        let store = Store::new(vec![]);

        let filter = super::clear(store.clone());
        let space: Arc<KitsuneSpace> = Arc::new(fixt!(KitsuneSpace));

        for _ in 0..20 {
            let info = AgentInfoSigned::sign(
                space.clone(),
                Arc::new(fixt!(KitsuneAgent, Unpredictable)),
                ArqSize::from_half_len(u32::MAX / 4),
                fixt!(UrlList, Empty),
                0,
                std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64 + 60_000_000,
                |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable))) },
            )
            .await
            .unwrap();
            let mut enc = Vec::new();
            kitsune_p2p_types::codec::rmp_encode(&mut enc, &info).unwrap();
            store.put(crate::store::StoreEntry::parse(enc).unwrap());
        }

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "clear")
            .reply(&filter)
            .await;
        assert_eq!(res.status(), 200);
        assert!(store.all().is_empty());
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/error.rs
================================================
#[derive(Debug, thiserror::Error)]
pub enum BootstrapClientError {
    /// std::io::Error
    #[error(transparent)]
    StdIoError(#[from] std::io::Error),

    /// Reqwest crate.
    #[error(transparent)]
    Reqwest(#[from] reqwest::Error),

    /// Bootstrap call failed.
    #[error("Bootstrap Error: {0}")]
    Bootstrap(Box<str>),

    /// Integer casting failed.
    #[error(transparent)]
    TryFromInt(#[from] std::num::TryFromIntError),

    /// SystemTime call failed.
    #[error(transparent)]
    SystemTime(#[from] std::time::SystemTimeError),
}

pub type BootstrapClientResult<T> = Result<T, BootstrapClientError>;



================================================
File: crates/kitsune_p2p/bootstrap/src/lib.rs
================================================
// Fixes some warnings introduced by `warp`
#![allow(opaque_hidden_inferred_bound)]

use std::net::SocketAddr;
use std::sync::atomic::AtomicUsize;

use kitsune_p2p_types::codec::rmp_decode;
use kitsune_p2p_types::codec::rmp_encode;
use store::Store;
use warp::{hyper::body::Bytes, Filter};

static NOW: AtomicUsize = AtomicUsize::new(0);
static RANDOM: AtomicUsize = AtomicUsize::new(0);
static PUT: AtomicUsize = AtomicUsize::new(0);

mod clear;
mod now;
mod proxy_list;
mod put;
mod random;
mod store;

pub mod error;

/// No reason to accept a peer data bigger then 1KB.
// TODO: Maybe even that's too high?
const SIZE_LIMIT: u64 = 1024;

/// how often should we prune the expired entries?
pub const PRUNE_EXPIRED_FREQ: std::time::Duration = std::time::Duration::from_secs(5);

pub type BootstrapDriver = futures::future::BoxFuture<'static, ()>;

pub type BootstrapShutdown = Box<dyn FnOnce() + 'static + Send + Sync>;

/// Run a bootstrap with the default prune frequency [`PRUNE_EXPIRED_FREQ`].
pub async fn run(
    addr: impl Into<SocketAddr> + 'static,
    proxy_list: Vec<String>,
) -> Result<(BootstrapDriver, SocketAddr, BootstrapShutdown), String> {
    run_with_prune_freq(addr, proxy_list, PRUNE_EXPIRED_FREQ).await
}

/// Run a bootstrap server with a set prune frequency.
pub async fn run_with_prune_freq(
    addr: impl Into<SocketAddr> + 'static,
    proxy_list: Vec<String>,
    prune_frequency: std::time::Duration,
) -> Result<(BootstrapDriver, SocketAddr, BootstrapShutdown), String> {
    let store = Store::new(proxy_list);

    {
        let store = store.clone();
        tokio::task::spawn(async move {
            loop {
                tokio::time::sleep(prune_frequency).await;
                store.prune();
            }
        });
    }

    let boot = now::now()
        .or(put::put(store.clone()))
        .or(random::random(store.clone()))
        .or(proxy_list::proxy_list(store.clone()))
        .or(clear::clear(store));

    let (s, r) = tokio::sync::oneshot::channel();
    let shutdown = Box::new(move || {
        let _ = s.send(());
    });

    let bind_result = warp::serve(boot).try_bind_with_graceful_shutdown(addr, async move {
        let _ = r.await;
    });
    match bind_result {
        Ok((addr, server)) => {
            let driver = futures::future::FutureExt::boxed(server);
            Ok((driver, addr, shutdown))
        }
        Err(e) => Err(format!("Failed to bind socket: {:?}", e)),
    }
}

fn with_store(
    store: Store,
) -> impl Filter<Extract = (Store,), Error = std::convert::Infallible> + Clone {
    warp::any().map(move || store.clone())
}



================================================
File: crates/kitsune_p2p/bootstrap/src/main.rs
================================================
use clap::Parser;

#[derive(Parser, Debug)]
#[clap(version, about, long_about = None)]
struct Args {
    /// bind to this interface
    #[clap(short, long, default_value = "0.0.0.0:0")]
    interface: String,

    /// include this proxy server address in
    /// `proxy_list` call, can be specified
    /// multiple times
    #[clap(short, long, verbatim_doc_comment)]
    proxy: Vec<String>,
}

#[tokio::main(flavor = "multi_thread")]
async fn main() {
    let args = Args::parse();

    use std::net::ToSocketAddrs;
    let addr = args
        .interface
        .as_str()
        .to_socket_addrs()
        .unwrap()
        .next()
        .unwrap();

    match kitsune_p2p_bootstrap::run(addr, args.proxy).await {
        Ok((driver, addr, _shutdown)) => {
            println!("http://{}", addr);
            driver.await;
        }
        Err(err) => eprintln!("{}", err),
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/now.rs
================================================
use super::*;
use warp::Filter;

pub(crate) fn now(
) -> impl Filter<Extract = impl warp::Reply + Sized, Error = warp::Rejection> + Clone {
    warp::post()
        .and(warp::header::exact("X-Op", "now"))
        .and_then(time)
}
async fn time() -> Result<impl warp::Reply, warp::Rejection> {
    let mut buf = Vec::new();
    let ms = std::time::UNIX_EPOCH
        .elapsed()
        .map(|e| e.as_millis() as u64)
        .unwrap_or(0);
    match rmp_encode(&mut buf, ms) {
        Ok(()) => {
            NOW.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
            Ok(buf)
        }
        Err(_) => Err(warp::reject()),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_now() {
        let filter = now();

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "now")
            .reply(&filter)
            .await;
        assert_eq!(res.status(), 200);
        let time: u64 = rmp_decode(&mut res.body().as_ref()).unwrap();
        assert!(time > 0);
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/proxy_list.rs
================================================
use crate::store::Store;

use super::*;
use warp::Filter;

pub(crate) fn proxy_list(
    store: Store,
) -> impl Filter<Extract = impl warp::Reply + Sized, Error = warp::Rejection> + Clone {
    warp::post()
        .and(warp::header::exact("X-Op", "proxy_list"))
        .and(with_store(store))
        .and_then(get_proxy_list)
}

async fn get_proxy_list(store: Store) -> Result<impl warp::Reply, warp::Rejection> {
    let proxy_list = store.proxy_list();
    let mut buf = Vec::new();
    rmp_encode(&mut buf, proxy_list).map_err(|_| warp::reject())?;
    Ok(buf)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_proxy_list() {
        let store = Store::new(vec![
            "https://test1.test".into(),
            "https://test2.test".into(),
        ]);
        let filter = super::proxy_list(store.clone());

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "proxy_list")
            .body(vec![])
            .reply(&filter)
            .await;
        assert_eq!(res.status(), 200);
        let mut result: Vec<String> = rmp_decode(&mut res.body().as_ref()).unwrap();
        assert_eq!(2, result.len());
        assert_eq!("https://test1.test", result.remove(0));
        assert_eq!("https://test2.test", result.remove(0));
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/put.rs
================================================
use crate::store::{Store, StoreEntry};

use super::*;
use warp::Filter;

pub(crate) fn put(
    store: Store,
) -> impl Filter<Extract = impl warp::Reply + Sized, Error = warp::Rejection> + Clone {
    warp::post()
        .and(warp::header::exact("X-Op", "put"))
        .and(warp::body::content_length_limit(SIZE_LIMIT))
        .and(warp::body::bytes())
        .and(with_store(store))
        .and_then(put_info)
}

async fn put_info(peer: Bytes, store: Store) -> Result<impl warp::Reply, warp::Rejection> {
    #[derive(Debug)]
    struct BadDecode(#[allow(dead_code)] String);
    impl warp::reject::Reject for BadDecode {}
    let peer = StoreEntry::parse(peer.to_vec()).map_err(|e| BadDecode(format!("{e:?}")))?;
    if !valid(&peer) {
        #[derive(Debug)]
        struct Invalid;
        impl warp::reject::Reject for Invalid {}
        return Err(Invalid.into());
    }
    store.put(peer);
    PUT.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    let mut buf = Vec::with_capacity(1);
    rmp_encode(&mut buf, ()).map_err(|_| warp::reject())?;
    Ok(buf)
}

fn valid(peer: &StoreEntry) -> bool {
    // TODO: actually verify signature... just checking size for now
    if peer.signature.len() != 64 {
        return false;
    }
    // Verify time
    peer.expires_at_ms as u128
        > std::time::UNIX_EPOCH
            .elapsed()
            .expect("Bootstrap system clock is set before the epoch")
            .as_millis()
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use super::*;
    use ::fixt::prelude::*;
    use kitsune_p2p_bin_data::fixt::*;
    use kitsune_p2p_types::{dht::arq::ArqSize, fixt::*};

    #[tokio::test(flavor = "multi_thread")]
    async fn test_put() {
        let store = Store::new(vec![]);
        let filter = put(store.clone());

        let info = kitsune_p2p_types::agent_info::AgentInfoSigned::sign(
            Arc::new(fixt!(KitsuneSpace, Unpredictable)),
            Arc::new(fixt!(KitsuneAgent, Unpredictable)),
            ArqSize::from_half_len(u32::MAX / 4),
            fixt!(UrlList, Empty),
            0,
            std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64 + 60_000_000,
            |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable))) },
        )
        .await
        .unwrap();
        let mut buf = Vec::new();
        rmp_encode(&mut buf, info.clone()).unwrap();

        let mut enc = Vec::new();
        kitsune_p2p_types::codec::rmp_encode(&mut enc, &info).unwrap();
        let info_as_entry = StoreEntry::parse(enc).unwrap();

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "put")
            .body(buf)
            .reply(&filter)
            .await;
        assert_eq!(res.status(), 200);
        assert_eq!(
            *store
                .all()
                .get(info.space.as_ref())
                .unwrap()
                .get(info.agent.as_ref())
                .unwrap(),
            info_as_entry,
        );
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/random.rs
================================================
use crate::store::Store;

use super::*;
use kitsune_p2p_types::bootstrap::RandomQuery;
use warp::Filter;

pub(crate) fn random(
    store: Store,
) -> impl Filter<Extract = impl warp::Reply + Sized, Error = warp::Rejection> + Clone {
    warp::post()
        .and(warp::header::exact("X-Op", "random"))
        .and(warp::body::content_length_limit(SIZE_LIMIT))
        .and(warp::body::bytes())
        .and(with_store(store))
        .and_then(random_info)
}

async fn random_info(query: Bytes, store: Store) -> Result<impl warp::Reply, warp::Rejection> {
    let query: RandomQuery =
        rmp_decode(&mut AsRef::<[u8]>::as_ref(&query)).map_err(|_| warp::reject())?;
    #[derive(serde::Serialize)]
    struct Bin(#[serde(with = "serde_bytes")] Vec<u8>);
    let result = store.random(query).into_iter().map(Bin).collect::<Vec<_>>();
    let mut buf = Vec::with_capacity(result.len());
    rmp_encode(&mut buf, result).map_err(|_| warp::reject())?;
    RANDOM.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    Ok(buf)
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use super::*;
    use ::fixt::prelude::*;
    use kitsune_p2p::{agent_store::AgentInfoSigned, KitsuneSpace};
    use kitsune_p2p_bin_data::fixt::*;
    use kitsune_p2p_types::{bootstrap::RandomLimit, dht::arq::ArqSize, tx_utils::TxUrl};

    async fn put(store: Store, peers: Vec<AgentInfoSigned>) {
        let filter = crate::put::put(store);

        for peer in peers {
            let mut buf = Vec::new();
            rmp_encode(&mut buf, peer).unwrap();

            let res = warp::test::request()
                .method("POST")
                .header("Content-type", "application/octet")
                .header("X-Op", "put")
                .body(buf)
                .reply(&filter)
                .await;
            assert_eq!(res.status(), 200);
        }
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_random_returns_offline_nodes() {
        let store = Store::new(vec![]);
        let filter = super::random(store.clone());
        let space: Arc<KitsuneSpace> = Arc::new(fixt!(KitsuneSpace));
        let offline_peer = AgentInfoSigned::sign(
            space.clone(),
            Arc::new(fixt!(KitsuneAgent, Unpredictable)),
            ArqSize::from_half_len(u32::MAX / 4),
            vec![], // no url means offline
            0,
            std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64 + 60_000_000,
            |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable))) },
        )
        .await
        .unwrap();
        put(store.clone(), vec![offline_peer.clone()]).await;

        let query = RandomQuery {
            space,
            limit: RandomLimit(10),
        };
        let mut buf = Vec::new();
        rmp_encode(&mut buf, query).unwrap();

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "random")
            .body(buf)
            .reply(&filter)
            .await;

        assert_eq!(res.status(), 200);
        #[derive(Debug, serde::Deserialize)]
        struct Bytes(#[serde(with = "serde_bytes")] Vec<u8>);
        let result: Vec<Bytes> = rmp_decode(&mut res.body().as_ref()).unwrap();

        let mut result: Vec<AgentInfoSigned> = result
            .into_iter()
            .map(|bytes| rmp_decode(&mut AsRef::<[u8]>::as_ref(&bytes.0)).unwrap())
            .collect();

        assert_eq!(result.len(), 1);

        let result = result.remove(0);
        assert_eq!(result, offline_peer);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_random() {
        let store = Store::new(vec![]);
        let filter = super::random(store.clone());
        let space: Arc<KitsuneSpace> = Arc::new(fixt!(KitsuneSpace));
        let mut peers = Vec::new();
        for _ in 0..20 {
            let info = AgentInfoSigned::sign(
                space.clone(),
                Arc::new(fixt!(KitsuneAgent, Unpredictable)),
                ArqSize::from_half_len(u32::MAX / 4),
                vec![TxUrl::from_str_panicking("fake:")],
                0,
                std::time::UNIX_EPOCH.elapsed().unwrap().as_millis() as u64 + 60_000_000,
                |_| async move { Ok(Arc::new(fixt!(KitsuneSignature, Unpredictable))) },
            )
            .await
            .unwrap();
            peers.push(info);
        }
        put(store.clone(), peers.clone()).await;

        let query = RandomQuery {
            space,
            limit: RandomLimit(10),
        };
        let mut buf = Vec::new();
        rmp_encode(&mut buf, query).unwrap();

        let res = warp::test::request()
            .method("POST")
            .header("Content-type", "application/octet")
            .header("X-Op", "random")
            .body(buf)
            .reply(&filter)
            .await;
        assert_eq!(res.status(), 200);
        #[derive(Debug, serde::Deserialize)]
        struct Bytes(#[serde(with = "serde_bytes")] Vec<u8>);
        let result: Vec<Bytes> = rmp_decode(&mut res.body().as_ref()).unwrap();
        let result: Vec<AgentInfoSigned> = result
            .into_iter()
            .map(|bytes| rmp_decode(&mut AsRef::<[u8]>::as_ref(&bytes.0)).unwrap())
            .collect();
        for peer in &result {
            assert!(peers.iter().any(|p| p == peer));
        }
        assert_eq!(result.len(), 10);

        // Test different space
        // Test expired
    }
}



================================================
File: crates/kitsune_p2p/bootstrap/src/store.rs
================================================
use std::{collections::HashMap, sync::Arc};

use kitsune_p2p_types::{
    bin_types::{KitsuneAgent, KitsuneSignature, KitsuneSpace},
    bootstrap::RandomQuery,
};
use parking_lot::RwLock;
use rand::seq::IteratorRandom;

#[derive(Debug)]
#[cfg_attr(test, derive(PartialEq, Clone))]
pub(crate) struct StoreEntry {
    pub encoded: Vec<u8>,
    pub signature: Arc<KitsuneSignature>,
    pub space: Arc<KitsuneSpace>,
    pub agent: Arc<KitsuneAgent>,
    pub signed_at_ms: u64,
    pub expires_at_ms: u64,
}

impl StoreEntry {
    pub fn parse(encoded: Vec<u8>) -> Result<Self, std::io::Error> {
        let mut bytes: &[u8] = &encoded;
        let kitsune_p2p_types::agent_info::agent_info_helper::AgentInfoSignedEncode {
            agent,
            signature,
            agent_info,
        } = kitsune_p2p_types::codec::rmp_decode(&mut bytes)?;

        let mut bytes: &[u8] = &agent_info;
        let info: kitsune_p2p_types::agent_info::agent_info_helper::AgentInfoEncode =
            kitsune_p2p_types::codec::rmp_decode(&mut bytes)?;

        if agent != info.agent {
            return Err(std::io::Error::other(
                "signed inner agent does not match unsigned outer agent",
            ));
        }

        Ok(StoreEntry {
            encoded,
            signature,
            space: info.space,
            agent,
            signed_at_ms: info.signed_at_ms,
            expires_at_ms: info.signed_at_ms + info.expires_after_ms,
        })
    }
}

type AgentMap = HashMap<Arc<KitsuneAgent>, StoreEntry>;
type SpaceMap = HashMap<Arc<KitsuneSpace>, AgentMap>;

#[derive(Clone, Debug)]
pub(crate) struct Store(Arc<RwLock<SpaceMap>>, Arc<Vec<String>>);

impl Store {
    pub fn new(proxy_list: Vec<String>) -> Self {
        Self(Arc::new(RwLock::new(HashMap::new())), Arc::new(proxy_list))
    }

    pub fn proxy_list(&self) -> Arc<Vec<String>> {
        self.1.clone()
    }

    pub fn prune(&self) {
        let now = std::time::UNIX_EPOCH
            .elapsed()
            .expect("Bootstrap server time set before epoch")
            .as_millis() as u64;

        self.0.write().retain(|_, map| {
            map.retain(|_, info| info.expires_at_ms >= now);
            !map.is_empty()
        });
    }

    pub fn put(&self, entry: StoreEntry) {
        let mut lock = self.0.write();
        let space_map = lock.entry(entry.space.clone()).or_default();
        match space_map.entry(entry.agent.clone()) {
            std::collections::hash_map::Entry::Occupied(mut e) => {
                if entry.signed_at_ms > e.get().signed_at_ms {
                    e.insert(entry);
                }
            }
            std::collections::hash_map::Entry::Vacant(e) => {
                e.insert(entry);
            }
        }
    }

    pub fn random(&self, query: RandomQuery) -> Vec<Vec<u8>> {
        // TODO: Max this limit
        let limit = query.limit.0 as usize;
        let mut rng = rand::thread_rng();
        let now = std::time::UNIX_EPOCH
            .elapsed()
            .expect("Bootstrap server time set before epoch")
            .as_millis() as u64;
        self.0
            .read()
            .get(query.space.as_ref())
            .map(|space| {
                space
                    .values()
                    .filter_map(|i| {
                        if i.expires_at_ms <= now {
                            return None;
                        }
                        Some(i.encoded.to_vec())
                    })
                    .choose_multiple(&mut rng, limit)
            })
            .unwrap_or_default()
    }

    pub fn clear(&self) {
        self.0.write().clear()
    }

    #[cfg(test)]
    pub fn all(&self) -> HashMap<Arc<KitsuneSpace>, HashMap<Arc<KitsuneAgent>, StoreEntry>> {
        self.0.read().clone()
    }
}



================================================
File: crates/kitsune_p2p/bootstrap_client/README.md
================================================
# kitsune_p2p_bootstrap_client

A client for the Holochain bootstrap service. 



================================================
File: crates/kitsune_p2p/bootstrap_client/Cargo.toml
================================================
[package]
name = "kitsune_p2p_bootstrap_client"
version = "0.5.0-dev.11"
description = "a client library for the bootstrap service used by Kitsune P2P"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_bootstrap_client"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
kitsune_p2p_bootstrap = { version = "^0.4.0-dev.11", path = "../bootstrap", features = [
  "sqlite",
] }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../types" }
kitsune_p2p_bin_data = { version = "^0.5.0-dev.5", path = "../bin_data" }
serde_bytes = "0.11"
serde = "1"
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls"] }
url2 = "0.0.6"

[dev-dependencies]
kitsune_p2p_bootstrap_client = { path = ".", features = ["test_utils"] }
kitsune_p2p_types = { version = "^0.5.0-dev.9", path = "../types", features = [
  "test_utils",
] }
fixt = { version = "^0.5.0-dev.1", path = "../../fixt" }
tokio = "1"
ed25519-dalek = { version = "2.1", features = ["rand_core"] }
rand = "0.8.5"

[lints]
workspace = true

[features]

test_utils = ["kitsune_p2p_bin_data/fixt", "kitsune_p2p_types/fixt"]



================================================
File: crates/kitsune_p2p/bootstrap_client/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.11

## 0.5.0-dev.10

## 0.5.0-dev.9

## 0.5.0-dev.8

## 0.5.0-dev.7

## 0.5.0-dev.6

## 0.5.0-dev.5

## 0.5.0-dev.4

## 0.5.0-dev.3

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.16

## 0.4.0-dev.15

## 0.4.0-dev.14

## 0.4.0-dev.13

## 0.4.0-dev.12

## 0.4.0-dev.11

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.34

## 0.3.0-beta-dev.33

## 0.3.0-beta-dev.32

## 0.3.0-beta-dev.31

## 0.3.0-beta-dev.30

## 0.3.0-beta-dev.29

## 0.3.0-beta-dev.28

## 0.3.0-beta-dev.27

## 0.3.0-beta-dev.26

## 0.3.0-beta-dev.25

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

- Extracted bootstrap client crate from `kitsune_p2p` to allow re-use.

## 0.2.0-beta-dev.17



================================================
File: crates/kitsune_p2p/bootstrap_client/src/lib.rs
================================================
use kitsune_p2p_bootstrap::error::BootstrapClientError;
use kitsune_p2p_bootstrap::error::BootstrapClientResult;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::bootstrap::RandomQuery;
use std::convert::TryFrom;
use std::convert::TryInto;
use std::sync::OnceLock;
use url2::Url2;

pub mod prelude {
    pub use kitsune_p2p_bootstrap::error::*;

    pub use super::{now, now_once, proxy_list, put, random, BootstrapNet};
}

/// The "net" flag / bucket to use when talking to the bootstrap server.
#[derive(Debug, Copy, Clone, PartialEq, Eq)]
pub enum BootstrapNet {
    Tx5,
}

impl BootstrapNet {
    fn value(&self) -> &'static str {
        match self {
            BootstrapNet::Tx5 => "tx5",
        }
    }
}

/// Reuse a single reqwest Client for efficiency as we likely need several connections.
static CLIENT: OnceLock<reqwest::Client> = OnceLock::new();

/// A cell to hold our local offset for calculating a 'now' that is compatible with the remote
/// service. This is much less precise and comprehensive than NTP style calculations.
/// We simply need to ensure that we don't sign things 'in the future' from the perspective of the
/// remote service, and any inaccuracy caused by network latency or similar problems is negligible
/// relative to the expiry times.
pub static NOW_OFFSET_MILLIS: OnceLock<i64> = OnceLock::new();

/// The HTTP header name for setting the op on POST requests.
const OP_HEADER: &str = "X-Op";
/// The header op to tell the service to put a signed agent info.
const OP_PUT: &str = "put";
/// The header op to tell the service to return its opinion of 'now' in milliseconds.
const OP_NOW: &str = "now";
/// The header op to tell the service to return a random set of agents in a specific space.
const OP_RANDOM: &str = "random";
/// The header op to fetch the proxy_list from the bootstrap service
const OP_PROXY_LIST: &str = "proxy_list";

/// Standard interface to the remote bootstrap service.
///
/// - url: the url of the bootstrap service or None to short circuit and not send a request
/// - op: the header op for the remote service
/// - input: op-specific struct that will be messagepack encoded and sent as binary data in the
///          body of the POST
///
/// Output type O is op specific and needs to be messagepack decodeable.
async fn do_api<I: serde::Serialize, O: serde::de::DeserializeOwned>(
    url: Option<Url2>,
    op: &str,
    input: I,
    net: BootstrapNet,
) -> BootstrapClientResult<Option<O>> {
    let mut body_data = Vec::new();
    kitsune_p2p_types::codec::rmp_encode(&mut body_data, &input)?;
    match url {
        Some(url) => {
            let url = format!("{}?net={}", url.as_str(), net.value());

            let res = CLIENT
                .get_or_init(|| reqwest::Client::builder().use_rustls_tls().build().unwrap())
                .post(url.as_str())
                .body(body_data)
                .header(OP_HEADER, op)
                .header(reqwest::header::CONTENT_TYPE, "application/octet")
                .send()
                .await?;
            if res.status().is_success() {
                Ok(Some(kitsune_p2p_types::codec::rmp_decode(
                    &mut res.bytes().await?.as_ref(),
                )?))
            } else {
                Err(BootstrapClientError::Bootstrap(
                    res.text().await?.into_boxed_str(),
                ))
            }
        }
        None => Ok(None),
    }
}

/// `do_api` wrapper for the `put` op.
///
/// Input must be an AgentInfoSigned with a valid signature otherwise the remote service will not
/// accept the data.
pub async fn put(
    url: Option<Url2>,
    agent_info_signed: AgentInfoSigned,
    net: BootstrapNet,
) -> BootstrapClientResult<()> {
    match do_api(url, OP_PUT, agent_info_signed, net).await {
        Ok(Some(())) => Ok(()),
        Ok(None) => Ok(()),
        Err(e) => Err(e),
    }
}

/// Simple wrapper to get the local time as milliseconds, to be compared against the remote time.
fn local_now() -> BootstrapClientResult<u64> {
    Ok(std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)?
        .as_millis()
        .try_into()?)
}

/// Thin `do_api` wrapper for the `now` op.
///
/// There is no input to the `now` endpoint, just `()` to be encoded as nil in messagepack.
#[allow(dead_code)]
pub async fn now(url: Option<Url2>, net: BootstrapNet) -> BootstrapClientResult<u64> {
    match do_api(url, OP_NOW, (), net).await {
        // If the server gives us something useful we use it.
        Ok(Some(v)) => Ok(v),
        // If we don't have a server url we should trust ourselves.
        Ok(None) => Ok(local_now()?),
        // Any error from the server should be handled by the caller.
        // The caller will probably fallback to the local_now.
        Err(e) => Err(e),
    }
}

/// Thick wrapper around `do_api` for the `now` op.
///
/// Calculates the offset on the first call and caches it in the cell above.
/// Only calls `now` once then keeps the offset for the static lifetime.
pub async fn now_once(url: Option<Url2>, net: BootstrapNet) -> BootstrapClientResult<u64> {
    match NOW_OFFSET_MILLIS.get() {
        Some(offset) => Ok(u64::try_from(i64::try_from(local_now()?)? + offset)?),
        None => {
            let offset: i64 = match now(url.clone(), net).await {
                Ok(v) => {
                    let offset = v as i64 - local_now()? as i64;
                    match NOW_OFFSET_MILLIS.set(offset) {
                        Ok(_) => offset,
                        Err(v) => v,
                    }
                }
                // @todo Do something more sophisticated here with errors.
                // Currently just falls back to a zero offset if the server is not happy.
                Err(_) => {
                    let offset = 0;
                    match NOW_OFFSET_MILLIS.set(offset) {
                        Ok(_) => offset,
                        Err(v) => v,
                    }
                }
            };

            Ok(u64::try_from(i64::try_from(local_now()?)? + offset)?)
        }
    }
}

/// `do_api` wrapper around the `random` op.
///
/// Fetches up to `limit` agent infos randomly from the `space`.
///
/// If there are fewer than `limit` agents listing themselves in the space then `limit` agents will
/// be returned in a random order.
///
/// The ordering is random, the return is not sorted.
/// Randomness is determined by the bootstrap service, it is one of the important roles of the
/// service to mitigate eclipse attacks by having a strong randomness implementation.
#[allow(dead_code)]
pub async fn random(
    url: Option<Url2>,
    query: RandomQuery,
    net: BootstrapNet,
) -> BootstrapClientResult<Vec<AgentInfoSigned>> {
    let outer_vec: Vec<serde_bytes::ByteBuf> = match do_api(url, OP_RANDOM, query, net).await {
        Ok(Some(v)) => v,
        Ok(None) => Vec::new(),
        Err(e) => return Err(e),
    };
    let ret: Result<Vec<AgentInfoSigned>, _> = outer_vec
        .into_iter()
        .map(|bytes| kitsune_p2p_types::codec::rmp_decode(&mut AsRef::<[u8]>::as_ref(&bytes)))
        .collect();
    Ok(ret?)
}

/// `do_api` wrapper around the `proxy_list` op.
///
/// Fetches the list of proxy servers currently stored in the bootstrap service.
#[allow(dead_code)]
pub async fn proxy_list(url: Url2, net: BootstrapNet) -> BootstrapClientResult<Vec<Url2>> {
    Ok(do_api::<_, Vec<String>>(Some(url), OP_PROXY_LIST, (), net)
        .await?
        .unwrap_or_default()
        .into_iter()
        .flat_map(|s| Url2::try_parse(s).ok())
        .collect())
}

#[cfg(test)]
mod tests {
    use super::*;
    use ::fixt::prelude::*;
    use ed25519_dalek::{Signer, SigningKey};
    use kitsune_p2p_bin_data::fixt::*;
    use kitsune_p2p_bin_data::KitsuneAgent;
    use kitsune_p2p_bin_data::KitsuneBinType;
    use kitsune_p2p_bin_data::KitsuneSignature;
    use kitsune_p2p_types::dht::prelude::SpaceOffset;
    use kitsune_p2p_types::dht::Arq;
    use kitsune_p2p_types::dht_arc::DhtLocation;
    use kitsune_p2p_types::fixt::*;
    use rand::rngs::OsRng;
    use std::convert::TryInto;
    use std::net::SocketAddr;
    use std::sync::Arc;
    use tokio::task::AbortHandle;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_bootstrap() {
        let (addr, abort_handle) = start_bootstrap().await;

        let keypair = create_test_keypair();
        let space = fixt!(KitsuneSpace);
        let agent = KitsuneAgent::new(keypair.verifying_key().as_bytes().to_vec());
        let urls = fixt!(UrlList);
        let now = std::time::SystemTime::now();
        let millis = now
            .duration_since(std::time::UNIX_EPOCH)
            .expect("Time went backwards")
            .as_millis();
        let signed_at_ms = (millis - 100).try_into().unwrap();
        let expires_at_ms = signed_at_ms + 1000 * 60 * 20;
        let agent_info_signed = AgentInfoSigned::sign(
            Arc::new(space),
            Arc::new(agent),
            Arq::new(8, DhtLocation::new(0), SpaceOffset(0)),
            urls,
            signed_at_ms,
            expires_at_ms,
            |d| {
                let d = Arc::new(d.to_vec());
                async move {
                    Ok(Arc::new(KitsuneSignature(
                        keypair.sign(d.clone().as_slice()).to_vec(),
                    )))
                }
            },
        )
        .await
        .unwrap();

        // Simply hitting the endpoint should be OK.
        put(
            Some(url2::url2!("http://{:?}", addr)),
            agent_info_signed,
            BootstrapNet::Tx5,
        )
        .await
        .unwrap();

        // We should get back an error if we don't have a good signature.
        let bad = fixt!(AgentInfoSigned);
        let mut bad = Arc::try_unwrap(bad.0).unwrap();
        bad.signature = Arc::new(vec![].into());
        let bad = AgentInfoSigned(Arc::new(bad));
        let res = put(
            Some(url2::url2!("http://{:?}", addr)),
            bad,
            BootstrapNet::Tx5,
        )
        .await;
        assert!(res.is_err());

        abort_handle.abort();
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn test_now() {
        let (addr, abort_handle) = start_bootstrap().await;

        let local_now = std::time::SystemTime::now();
        let local_millis: u64 = local_now
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis()
            .try_into()
            .unwrap();

        // We should be able to get a milliseconds timestamp back.
        let remote_now: u64 = now(Some(url2::url2!("http://{:?}", addr)), BootstrapNet::Tx5)
            .await
            .unwrap();
        let threshold = 5000;

        assert!((remote_now - local_millis) < threshold);

        // Now once should return some number and the remote server offset should be set in the
        // NOW_OFFSET_MILLIS once cell.
        let _: u64 = now_once(Some(url2::url2!("http://{:?}", addr)), BootstrapNet::Tx5)
            .await
            .unwrap();
        assert!(NOW_OFFSET_MILLIS.get().is_some());

        abort_handle.abort();
    }

    #[tokio::test(flavor = "multi_thread")]
    // Fixturator seed: 17591570467001263546
    // thread 'spawn::actor::bootstrap::tests::test_random' panicked at 'dispatch dropped without returning error', /rustc/d3fb005a39e62501b8b0b356166e515ae24e2e54/src/libstd/macros.rs:13:23
    async fn test_random() {
        let (addr, abort_handle) = start_bootstrap().await;

        let space = fixt!(KitsuneSpace, Unpredictable);
        let now = now(Some(url2::url2!("http://{:?}", addr)), BootstrapNet::Tx5)
            .await
            .unwrap();

        let alice = create_test_keypair();
        let bob = create_test_keypair();

        let mut expected: Vec<AgentInfoSigned> = Vec::new();
        for agent in vec![alice, bob] {
            let kitsune_agent = KitsuneAgent::new(agent.verifying_key().as_bytes().to_vec());
            let signed_at_ms = now;
            let expires_at_ms = now + 1000 * 60 * 20;
            let agent_info_signed = AgentInfoSigned::sign(
                Arc::new(space.clone()),
                Arc::new(kitsune_agent.clone()),
                Arq::new(0, DhtLocation::new(0), SpaceOffset(0)),
                fixt!(UrlList),
                signed_at_ms,
                expires_at_ms,
                |d| {
                    let d = Arc::new(d.to_vec());
                    async move {
                        Ok(Arc::new(KitsuneSignature(
                            agent.sign(d.clone().as_slice()).to_vec(),
                        )))
                    }
                },
            )
            .await
            .unwrap();

            put(
                Some(url2::url2!("http://{:?}", addr)),
                agent_info_signed.clone(),
                BootstrapNet::Tx5,
            )
            .await
            .unwrap();

            expected.push(agent_info_signed);
        }

        let mut random = super::random(
            Some(url2::url2!("http://{:?}", addr)),
            RandomQuery {
                space: Arc::new(space.clone()),
                ..Default::default()
            },
            BootstrapNet::Tx5,
        )
        .await
        .unwrap();

        expected.sort_by(|a, b| a.agent.partial_cmp(&b.agent).unwrap());
        random.sort_by(|a, b| a.agent.partial_cmp(&b.agent).unwrap());

        assert!(random.len() == 2);
        assert!(random == expected);

        let random_single = super::random(
            Some(url2::url2!("http://{:?}", addr)),
            RandomQuery {
                space: Arc::new(space.clone()),
                limit: 1.into(),
            },
            BootstrapNet::Tx5,
        )
        .await
        .unwrap();

        assert!(random_single.len() == 1);
        assert!(expected[0] == random_single[0] || expected[1] == random_single[0]);

        abort_handle.abort();
    }

    async fn start_bootstrap() -> (SocketAddr, AbortHandle) {
        let (bs_driver, bs_addr, shutdown) =
            kitsune_p2p_bootstrap::run("127.0.0.1:0".parse::<SocketAddr>().unwrap(), vec![])
                .await
                .expect("Could not start bootstrap server");

        let abort_handle = tokio::spawn(async move {
            let _shutdown_cb = shutdown;
            bs_driver.await;
        })
        .abort_handle();

        (bs_addr, abort_handle)
    }

    fn create_test_keypair() -> SigningKey {
        let mut csprng = OsRng;
        SigningKey::generate(&mut csprng)
    }
}



================================================
File: crates/kitsune_p2p/dht/README.md
================================================
# kitsune_dht

DHT subcrate for kitsune-p2p.

This crate defines the "quantized DHT", the objects which inhabit it, and the operations on these objects.

The `kitsune_p2p_dht_arc` crate will probably be absorbed into this one eventually.

License: Apache-2.0



================================================
File: crates/kitsune_p2p/dht/Cargo.toml
================================================
[package]
name = "kitsune_p2p_dht"
version = "0.5.0-dev.3"
description = "Kitsune P2p DHT definition"
license = "Apache-2.0"
homepage = "https://github.com/holochain/holochain"
documentation = "https://docs.rs/kitsune_p2p_dht"
authors = ["Holochain Core Dev Team <devcore@holochain.org>"]
keywords = ["holochain", "holo", "p2p", "dht", "networking"]
categories = ["network-programming"]
edition = "2021"

# reminder - do not use workspace deps
[dependencies]
colored = { version = "2.1", optional = true }
derivative = "2.2.0"
derive_more = "0.99"
kitsune_p2p_dht_arc = { version = "^0.5.0-dev.2", path = "../dht_arc" }
kitsune_p2p_timestamp = { version = "^0.5.0-dev.1", path = "../timestamp" }
num-traits = "0.2.14"
rand = "0.8.4"
serde = { version = "1.0", features = ["derive"] }
statrs = "0.16.0"
thiserror = "1.0"
tracing = "0.1.29"

futures = { version = "0.3", optional = true }
must_future = { version = "0.1", optional = true }

proptest = { version = "1", optional = true }
proptest-derive = { version = "0", optional = true }

[dev-dependencies]
kitsune_p2p_dht = { path = ".", features = ["test_utils", "sqlite"] }

kitsune_p2p_dht_arc = { path = "../dht_arc", features = ["test_utils"] }
holochain_serialized_bytes = "=0.0.55"
maplit = "1"
holochain_trace = { version = "^0.5.0-dev.1", path = "../../holochain_trace" }
pretty_assertions = "1.4.0"
proptest = "1"
test-case = "3.3"

[lints]
workspace = true

[features]
default = ["kitsune_p2p_timestamp/now"]
test_utils = ["dep:colored", "dep:futures", "dep:must_future", "fuzzing"]
fuzzing = [
  "proptest",
  "proptest-derive",
  "kitsune_p2p_dht_arc/fuzzing",
  "kitsune_p2p_timestamp/fuzzing",
]
sqlite-encrypted = [
  "kitsune_p2p_timestamp/sqlite-encrypted",
  "kitsune_p2p_dht_arc/sqlite-encrypted",
]
sqlite = ["kitsune_p2p_timestamp/sqlite", "kitsune_p2p_dht_arc/sqlite"]



================================================
File: crates/kitsune_p2p/dht/CHANGELOG.md
================================================
---
default_semver_increment_mode: !pre_minor dev
---
# Changelog

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/). This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## \[Unreleased\]

## 0.5.0-dev.3

- Prevent TODO comments from being rendered in cargo docs.

## 0.5.0-dev.2

## 0.5.0-dev.1

## 0.5.0-dev.0

## 0.4.0

## 0.4.0-dev.10

## 0.4.0-dev.9

## 0.4.0-dev.8

## 0.4.0-dev.7

## 0.4.0-dev.6

## 0.4.0-dev.5

## 0.4.0-dev.4

## 0.4.0-dev.3

## 0.4.0-dev.2

## 0.4.0-dev.1

## 0.4.0-dev.0

## 0.3.0

## 0.3.0-beta-dev.24

## 0.3.0-beta-dev.23

## 0.3.0-beta-dev.22

## 0.3.0-beta-dev.21

## 0.3.0-beta-dev.20

## 0.3.0-beta-dev.19

## 0.3.0-beta-dev.18

## 0.3.0-beta-dev.17

## 0.3.0-beta-dev.16

## 0.3.0-beta-dev.15

## 0.3.0-beta-dev.14

## 0.3.0-beta-dev.13

## 0.3.0-beta-dev.12

## 0.3.0-beta-dev.11

## 0.3.0-beta-dev.10

## 0.3.0-beta-dev.9

## 0.3.0-beta-dev.8

## 0.3.0-beta-dev.7

## 0.3.0-beta-dev.6

## 0.3.0-beta-dev.5

## 0.3.0-beta-dev.4

## 0.3.0-beta-dev.3

## 0.3.0-beta-dev.2

## 0.3.0-beta-dev.1

## 0.3.0-beta-dev.0

## 0.2.0

## 0.2.0-beta-rc.3

## 0.2.0-beta-rc.2

## 0.2.0-beta-rc.1

## 0.2.0-beta-rc.0

## 0.1.0

## 0.1.0-beta-rc.1

## 0.1.0-beta-rc.0

## 0.0.11

## 0.0.10

## 0.0.9

## 0.0.8

## 0.0.7

## 0.0.6

## 0.0.5

## 0.0.4

## 0.0.3

## 0.0.2

## 0.0.1



================================================
File: crates/kitsune_p2p/dht/proptest-regressions/arq.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc 86f86d813d6b9bfc7b9121bcd1ec79e5186593a147cbbc4a07fb75ac56d7f589 # shrinks to center = 0, pow = 93, count = 240266593
cc 82adecb24fea474a946f1a5fe059c8150bf0f88bf259c2184c861a3e7a1e164e # shrinks to center = 0, pow = 29, count = 9
cc 8b613f39f49019b5b85ac888e442fc78a4a3c5b08d4a6ac552f25f43532feb34 # shrinks to center = 0, pow = 0, count = 0
cc 28d4acd22890773cf8c2251c3998e960f26e2773472b96cf15bb69705b6caaca # shrinks to center = 0, pow = 1, count = 0
cc 0189bf01f9c65a3d3763c95921689f5a1ac1f57152e94d9ecb866fcbfbf6971b # shrinks to center = 0, pow = 1, count = 1
cc 4ff091d2c68ae13cbb50704d7e1b7ce09d5dd8eb0a18a9e330060ead9922908e # shrinks to center = 0, pow = 2, count = 1
cc 2114cf98e01fc238006b6570d03c3cd3929d5883606c763dc4b149aad6a0b75a # shrinks to center = 0, pow = 3, count = 1
cc 90bdeafe40d63355ed1b0f165443968553acc1a3bdc2090eae8b4cae7035cd8d # shrinks to center = 0, pow = 4, count = 1
cc 66cdbfa73902d7a834f8548740171d678f3f190c1da2eabaa445adc55783b61f # shrinks to center = 0, pow = 3, count = 3
cc 7576fd018881c41bb07d294439b3b806f29bfa1544c00952e79a22e859d41462 # shrinks to center = 0, pow = 11, count = 3



================================================
File: crates/kitsune_p2p/dht/proptest-regressions/coords.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc f6d5cb1b1ba9986417608c23f37ccc72fc0d9b43f73cb00eefdece231401d3df # shrinks to now = 0



================================================
File: crates/kitsune_p2p/dht/proptest-regressions/quantum.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc 07bc33e9a92991ff0c17c3dd45cdaf4e62c03eb75e35328a3f6ecda54f2f8955 # shrinks to now = 5



================================================
File: crates/kitsune_p2p/dht/proptest-regressions/test_utils.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc f3a78399f776e21823c32d69968c22bed68a705ed91975088257822e37584b75 # shrinks to min_coverage = 40.0, buffer = 0.1
cc a16b08b7b7657e6654ce6428892eee99ce8aafd9db3096f6e96d15cb71502bc0 # shrinks to min_coverage = 40.0, buffer = 0.24312421515429483, num_peers = 134
cc b0a45edf9a8bfbf14d1d4da6fa5947dbe2909fce3ee0222f5315b5f492084709 # shrinks to center = 0.0, len = 0.9903702803455133
cc 5a470257bf951809b9797ccdec1e7b47547083f678454f99d25e6708d452416a # shrinks to min_coverage = 46.654015826773346, buffer = 0.49587848817064173, num_peers = 117
cc 2c9fb936731be40577ea0773108bc28c9550b403bada03e55695d20d885a4b6e # shrinks to min_coverage = 76.78284244250997, buffer = 0.21370848915762652, num_peers = 171
cc c71eb7f1059c4a1c394c1ed165ccb822314cda0ef5d9050cde33307000217fbd # shrinks to min_coverage = 72.73102186324675, buffer = 0.22155720182450622, num_peers = 141
cc 353592e63f0b6696d9696fbe44d7110c234cbbc8744155a3cdcbd79ef84ab1ac # shrinks to min_coverage = 89.6003555072335, buffer = 0.41911123197780786, num_peers = 100
cc 3a21af02ff33b94e027015da03d0f456bc2d77d314bf0ed92f834e51a3570760 # shrinks to min_coverage = 40.149731511533375, buffer = 0.14880552721088203, num_peers = 164



================================================
File: crates/kitsune_p2p/dht/proptest-regressions/arq/arq_set.txt
================================================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc c5a173f602e7a0da8b012678a4220f89c56bea3af116d5a7a58d35c16cc0a1eb # shrinks to p1 = 0, s1 = 0, c1 = 0, p2 = 18, s2 = 0, c2 = 0
cc b2f3015c667472b6a68829a61e0cec05dc6fb8400f4a5253160f35cb7c0a1201 # shrinks to mut p1 = 0, s1 = 0, c1 = 1, mut p2 = 0, s2 = 3560210432, c2 = 3089284281
cc 759f52a1b22a74e7693990c215ee5f69e6fa93beb1a212663c080692c7b81bae # shrinks to mut p1 = 0, s1 = 0, c1 = 0, mut p2 = 94, s2 = 0, c2 = 0
cc e4718d3feed040cb3aaf7b6e4e3bf5e288c3739ab4ee8f89ad0fb831d14a7073 # shrinks to p1 = 11, s1 = 3992977408, c1 = 36, p2 = 14, s2 = 2952790016, c2 = 20
cc 813e7081bc8f288bc3291869dddb56a741c157004fbd30d1d298abf5ad9f1c9d # shrinks to p1 = 12, s1 = 117440512, c1 = 334157140, p2 = 13, s2 = 3959422976, c2 = 473799178, p3 = 12, s3 = 0, c3 = 0



================================================
File: crates/kitsune_p2p/dht/src/arq.rs
================================================
//! The Quantized DHT Arc type
//!
//! Arq coordinates are expressed in terms of powers-of-two, representing
//! the "chunk" or "segment" size to work with. The actual extent of the Arq
//! is expressed as a `start` and an `offset`, in terms of the chunk size.
//! So, Arq boundaries can only ever fall on a quantized grid which is determined
//! by the `power` setting.

mod arq_set;
mod peer_view;
mod strat;

#[cfg(feature = "test_utils")]
pub mod ascii;

pub use arq_set::*;

pub use peer_view::*;
pub use strat::*;

use kitsune_p2p_dht_arc::{DhtArc, DhtArcRange};

use crate::{op::Loc, spacetime::*};

/// Convenience method for taking the power of 2 in u32.
///
/// If `p` is greater than 31, this function will return 2^31.
pub fn pow2(p: u8) -> u32 {
    debug_assert!(p < 32);
    2u32.pow((p as u32).min(31))
}

/// Convenience method for taking the power of 2 in f64
pub fn pow2f(p: u8) -> f64 {
    debug_assert!(p < 32);
    2f64.powf(p as f64)
}

/// Maximum number of values that a u32 can represent.
pub(crate) const U32_LEN: u64 = u32::MAX as u64 + 1;

/// Represents the start point or "left edge" of an Arq.
///
/// This helps us generalize over the two use cases of Arq:
/// 1. An Arq which is defined at a definite absolute DhtLocation corresponding
///    to an Agent's location, and which can be requantized, resized, etc.
/// 2. An Arq which has no absolute location defined, and which simply represents
///    a (quantized) range.
pub trait ArqStart: Sized + Copy + std::fmt::Debug {
    /// Get the DhtLocation representation
    fn to_loc(&self, dim: impl SpaceDim, power: u8) -> Loc;
    /// Get the exponential SpaceOffset representation
    fn to_offset(&self, dim: impl SpaceDim, power: u8) -> SpaceOffset;
    /// Requantize to a higher power, using the precalculated multiplicative factor.
    fn requantize_up(&self, factor: u32) -> Option<Self>;
    /// Requantize to a lower power, using the precalculated multiplicative factor.
    fn requantize_down(&self, factor: u32) -> Self;
    /// Zero value
    fn zero() -> Self;
}

impl ArqStart for Loc {
    fn to_loc(&self, _dim: impl SpaceDim, _power: u8) -> Loc {
        *self
    }

    fn to_offset(&self, dim: impl SpaceDim, power: u8) -> SpaceOffset {
        SpaceOffset::from_absolute_rounded(*self, dim.get(), power)
    }

    fn requantize_up(&self, _factor: u32) -> Option<Self> {
        Some(*self)
    }

    fn requantize_down(&self, _factor: u32) -> Self {
        *self
    }

    fn zero() -> Self {
        0.into()
    }
}

impl ArqStart for SpaceOffset {
    fn to_loc(&self, dim: impl SpaceDim, power: u8) -> Loc {
        self.to_absolute(dim.get(), power)
    }

    fn to_offset(&self, _dim: impl SpaceDim, _power: u8) -> SpaceOffset {
        *self
    }

    fn requantize_up(&self, factor: u32) -> Option<Self> {
        ((**self) % factor == 0).then(|| *self / factor)
    }

    fn requantize_down(&self, factor: u32) -> Self {
        *self * factor
    }

    fn zero() -> Self {
        0.into()
    }
}

/// A quantized DHT arc.
///
/// ## Coordinates
///
/// Arq coordinates are expressed in terms of powers-of-two, representing
/// the "chunk" or "segment" size to work with.
/// The chunk size is determined by the [`Topology`] of the space it is in,
/// as well as the `power` of the Arq. The actual chunk size is given by:
///
/// `chunk size = topology.space.quantum * 2^power`
///
/// So, the `power` represents the amount of quantization *on top* of the quantum
/// size set by the Topology, not the total quantization level.
///
/// The `start` is generic, because there are actually two flavors of Arq:
/// - one which has a definite starting DhtLocation associated with it,
/// - and one which does not.
///
/// The first flavor is used to represent Arqs which belong to Agents. It's important
/// to record the actual absolute Location of the Arq, because the exact location
/// determines the starting Chunk when requantizing to higher and lower levels.
///
/// The second flavor is mainly used to represent the intersections and unions of Arqs.
/// In this case, there is no definite location associated, so we want to forget
/// about the original Location data associated with each Arq.
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub struct Arq<S: ArqStart = Loc> {
    /// The "start" defines the left edge of the arq
    pub start: S,
    /// The level of quantization. Total length is `2^power * count`.
    /// The power must be between 0 and 31, inclusive (power of 32 causes overflow)
    pub power: u8,
    /// The number of unit lengths.
    /// We never expect the count to be less than 4 or so, and not much larger
    /// than 32.
    pub count: SpaceOffset,
}

/// Alias for Arq with an Loc start
pub type ArqLocated = Arq<Loc>;

/// Alias for Arq with an SpaceOffset start
pub type ArqBounds = Arq<SpaceOffset>;

impl<S: ArqStart> Arq<S> {
    /// Constructor from individual parts
    pub fn new(power: u8, start: S, count: SpaceOffset) -> Self {
        Self {
            power,
            start,
            count,
        }
    }

    /// The number of quanta to use for each segment
    #[inline]
    fn quantum_chunk_width(&self) -> u32 {
        pow2(self.power)
    }

    /// The absolute length of each segment, the "chunk size"
    #[inline]
    pub(crate) fn absolute_chunk_width(&self, dim: impl SpaceDim) -> u32 {
        let len = self
            .quantum_chunk_width()
            .saturating_mul(dim.get().quantum)
            .min(u32::MAX / 2);
        let max = u32::MAX / 4 + 2;
        // this really shouldn't ever be larger than MAX / 8
        debug_assert!(
            len <= max,
            "chunk width is much larger than expected: {len} vs {max}",
        );
        len
    }

    /// The absolute length of the entire arq.
    pub fn absolute_length(&self, dim: impl SpaceDim) -> u64 {
        let len = (self.absolute_chunk_width(dim) as u64 * (*self.count as u64)).min(U32_LEN);
        debug_assert_eq!(
            len,
            self.to_dht_arc_range(dim).length(),
            "lengths don't match {:?}",
            self
        );
        len
    }

    /// Convert to [`DhtArcRange`] using standard topology
    pub fn to_dht_arc_range_std(&self) -> DhtArcRange {
        self.to_dht_arc_range(SpaceDimension::standard())
    }

    /// Convert to [`DhtArcRange`]
    pub fn to_dht_arc_range(&self, dim: impl SpaceDim) -> DhtArcRange {
        if is_full(dim, self.power, *self.count) {
            DhtArcRange::Full
        } else if *self.count == 0 {
            DhtArcRange::Empty
        } else {
            let (a, b) = self.to_edge_locs(dim);
            DhtArcRange::from_bounds(a, b)
        }
    }

    /// Determine the edges of this Arq in absolute coordinates ([`Loc`])
    pub fn to_edge_locs(&self, dim: impl SpaceDim) -> (Loc, Loc) {
        let start = self.start.to_offset(dim, self.power);
        let left = start.to_loc(dim, self.power);
        let right = (start + self.count).to_loc(dim, self.power) - Loc::from(1);
        (left, right)
    }

    /// Accessor
    pub fn power(&self) -> u8 {
        self.power
    }

    /// Accessor
    pub fn count(&self) -> u32 {
        self.count.into()
    }

    /// What portion of the whole circle does this arq cover?
    pub fn coverage(&self, dim: impl SpaceDim) -> f64 {
        self.absolute_length(dim) as f64 / 2f64.powf(32.0)
    }

    /// Requantize to a different power. If requantizing to a higher power,
    /// only requantize if there is no information loss due to rounding.
    /// Otherwise, return None.
    pub fn requantize(&self, new_power: u8) -> Option<Self> {
        let old_power = self.power;
        let old_count = self.count;
        if old_power < new_power {
            let factor = 2u32.pow((new_power - old_power) as u32);
            self.start.requantize_up(factor).and_then(|start| {
                let new_count = old_count / factor;
                if old_count == new_count * factor {
                    Some((start, new_power, new_count))
                } else {
                    None
                }
            })
        } else {
            let factor = 2u32.pow((old_power - new_power) as u32);
            let new_count = old_count * factor;
            Some((self.start.requantize_down(factor), new_power, new_count))
        }
        .map(|(start, power, count)| Self {
            start,
            power,
            count,
        })
    }

    /// Construct a full arq at the given power.
    /// The `count` is calculated accordingly.
    pub fn new_full(dim: impl SpaceDim, start: S, power: u8) -> Self {
        let count = pow2(32u8.saturating_sub(power + dim.get().quantum_power));
        assert!(is_full(dim, power, count));
        Self {
            start,
            power,
            count: count.into(),
        }
    }

    /// Construct a full arq at the maximum power.
    pub fn new_full_max(dim: impl SpaceDim, strat: &ArqStrat, start: S) -> Self {
        Self::new_full(dim, start, dim.get().max_power(strat))
    }

    /// This arq has full coverage
    pub fn is_full(&self, dim: impl SpaceDim) -> bool {
        is_full(dim, self.power(), self.count())
    }

    /// This arq has zero coverage
    pub fn is_empty(&self) -> bool {
        self.count() == 0
    }
}

impl Arq<Loc> {
    /// Construct an empty arq (count = 0) at the minimum power.
    pub fn new_empty(dim: impl SpaceDim, start: Loc) -> Self {
        Self {
            start,
            power: dim.get().min_power(),
            count: 0.into(),
        }
    }

    /// Reduce the power by 1
    pub fn downshift(&self) -> Self {
        Self {
            start: self.start,
            power: self.power - 1,
            count: self.count * 2,
        }
    }

    /// Increase the power by 1. If this results in rounding, return None,
    /// unless `force` is true, in which case always return Some.
    pub fn upshift(&self, force: bool) -> Option<Self> {
        let count = if force && *self.count % 2 == 1 {
            self.count + SpaceOffset(1)
        } else {
            self.count
        };
        (*count % 2 == 0).then(|| Self {
            start: self.start,
            power: self.power + 1,
            count: count / 2,
        })
    }

    /// Convert to the [`ArqBounds`] representation, which forgets about the
    /// [`Loc`] associated with this arq. Uses standard topology.
    pub fn to_bounds_std(&self) -> ArqBounds {
        self.to_bounds(SpaceDimension::standard())
    }

    /// Convert to the [`ArqBounds`] representation, which forgets about the
    /// [`Loc`] associated with this arq.
    pub fn to_bounds(&self, dim: impl SpaceDim) -> ArqBounds {
        ArqBounds {
            start: SpaceOffset::from(self.start.as_u32() / self.absolute_chunk_width(dim)),
            power: self.power,
            count: self.count,
        }
    }

    /// Get a reference to the arq's left edge in absolute coordinates.
    pub fn start_loc(&self) -> Loc {
        self.start
    }

    /// Get a mutable reference to the arq's count.
    pub fn count_mut(&mut self) -> &mut u32 {
        &mut self.count
    }

    /// Convert to [`DhtArc`]
    pub fn to_dht_arc(&self, dim: impl SpaceDim) -> DhtArc {
        let len = self.absolute_length(dim);
        DhtArc::from_start_and_len(self.start, len)
    }

    /// Convert to [`DhtArc`] using the standard SpaceDimension
    pub fn to_dht_arc_std(&self) -> DhtArc {
        self.to_dht_arc(SpaceDimension::standard())
    }

    /// Computes the Arq which most closely matches the given [`DhtArc`]
    #[cfg(feature = "test_utils")]
    pub fn from_dht_arc_approximate(
        dim: impl SpaceDim,
        strat: &ArqStrat,
        dht_arc: &DhtArc,
    ) -> Self {
        approximate_arq(dim, strat, dht_arc.start_loc(), dht_arc.length())
    }

    /// The two arqs represent the same interval despite having potentially different terms
    pub fn equivalent(dim: impl SpaceDim, a: &Self, b: &Self) -> bool {
        let qa = a.absolute_chunk_width(dim);
        let qb = b.absolute_chunk_width(dim);
        a.start == b.start && (a.count.wrapping_mul(qa) == b.count.wrapping_mul(qb))
    }

    /// Computes the Arq which most closely matches the given params
    #[cfg(feature = "test_utils")]
    pub fn from_start_and_half_len_approximate(
        dim: impl SpaceDim,
        strat: &ArqStrat,
        start: Loc,
        half_len: u32,
    ) -> Self {
        let arc = DhtArc::from_start_and_half_len(start, half_len);
        Self::from_dht_arc_approximate(dim, strat, &arc)
    }
}

impl From<&ArqBounds> for ArqBounds {
    fn from(a: &ArqBounds) -> Self {
        *a
    }
}

impl ArqBounds {
    /// The two arqs represent the same interval despite having potentially different terms
    pub fn equivalent(dim: impl SpaceDim, a: &Self, b: &Self) -> bool {
        let qa = a.absolute_chunk_width(dim);
        let qb = b.absolute_chunk_width(dim);
        *a.count == 0 && *b.count == 0
            || (a.start.wrapping_mul(qa) == b.start.wrapping_mul(qb)
                && a.count.wrapping_mul(qa) == b.count.wrapping_mul(qb))
    }

    /// Return the ArqBounds which most closely matches the given [`DhtArcRange`]
    pub fn from_interval_rounded(
        dim: impl SpaceDim,
        power: u8,
        interval: DhtArcRange,
    ) -> (Self, bool) {
        Self::from_interval_inner(dim, power, interval, true).unwrap()
    }

    /// Return the ArqBounds which is equivalent to the given [`DhtArcRange`] if it exists.
    pub fn from_interval(dim: impl SpaceDim, power: u8, interval: DhtArcRange) -> Option<Self> {
        Self::from_interval_inner(dim, power, interval, false).map(|(a, _)| a)
    }

    /// Upcast this ArqBounds to an Arq that has knowledge of its [`Loc`]
    #[cfg(any(test, feature = "test_utils"))]
    pub fn to_arq<F: FnOnce(Loc) -> Loc>(&self, dim: impl SpaceDim, f: F) -> Arq {
        Arq {
            start: f(self.start.to_loc(dim, self.power)),
            power: self.power,
            count: self.count,
        }
    }

    /// An arbitrary zero-coverage arq.
    pub fn empty(dim: impl SpaceDim, power: u8) -> Self {
        Self::from_interval(dim, power, DhtArcRange::Empty).unwrap()
    }

    fn from_interval_inner(
        dim: impl SpaceDim,
        power: u8,
        interval: DhtArcRange,
        always_round: bool,
    ) -> Option<(Self, bool)> {
        let dim = dim.get();
        match interval {
            DhtArcRange::Empty => Some((
                Self {
                    start: 0.into(),
                    power,
                    count: 0.into(),
                },
                false,
            )),
            DhtArcRange::Full => {
                assert!(power > 0);
                let full_count = 2u32.pow(32 - power as u32 - dim.quantum_power as u32);
                Some((
                    Self {
                        start: 0.into(),
                        power,
                        count: full_count.into(),
                    },
                    false,
                ))
            }
            DhtArcRange::Bounded(lo, hi) => {
                let lo = lo.as_u32();
                let hi = hi.as_u32();
                let q = dim.quantum;
                let s = 2u32.pow(power as u32) * q;
                let offset = lo / s;
                let len = if lo <= hi {
                    hi - lo + 1
                } else {
                    (2u64.pow(32) - (lo as u64) + (hi as u64) + 1) as u32
                };
                let count = len / s;
                // XXX: this is kinda wrong. The right bound of the interval
                // should be 1 less, but we'll accept if it bleeds over by 1 too.
                let rem = len % s;
                let diff = rem.min(s - rem);
                let lossless = lo == offset * s && (diff <= 1);
                if always_round || lossless {
                    Some((
                        Self {
                            start: offset.into(),
                            power,
                            count: count.into(),
                        },
                        !lossless,
                    ))
                } else {
                    tracing::warn!("{} =?= {} == {} * {}", lo, offset * s, offset, s);
                    tracing::warn!("{} =?= {} == {} * {}", len, count * s, count, s);
                    None
                }
            }
        }
    }

    /// Iterate over each segment (chunk) in the Arq
    pub fn segments(&self) -> impl Iterator<Item = SpaceSegment> + '_ {
        (0..*self.count).map(|c| SpaceSegment::new(self.power, c.wrapping_add(*self.start)))
    }

    /// Get a reference to the arq bounds's offset.
    pub fn offset(&self) -> SpaceOffset {
        self.start
    }
}

/// Just the size of a quantized arc, without a start location
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ArqSize {
    /// The power
    pub power: u8,
    /// The count
    pub count: SpaceOffset,
}

impl ArqSize {
    /// Data for an empty arc
    pub fn empty() -> Self {
        Self {
            count: 0.into(),
            power: 0,
        }
    }

    /// Convert to Arq
    pub fn to_arq(&self, start: Loc) -> Arq {
        Arq::new(self.power, start, self.count)
    }

    /// Construct approximate quantized info from an arc half-length
    #[cfg(feature = "test_utils")]
    pub fn from_half_len(half_len: u32) -> Self {
        Arq::from_start_and_half_len_approximate(
            SpaceDimension::standard(),
            &ArqStrat::default(),
            0.into(),
            half_len,
        )
        .into()
    }
}

impl From<Arq> for ArqSize {
    fn from(arq: Arq) -> Self {
        Self {
            count: arq.count,
            power: arq.power,
        }
    }
}

/// Calculate whether a given combination of power and count corresponds to
/// full DHT coverage.
///
/// e.g. if the space quantum is 2^12, and the power is 14,
/// then the max power is (32 - 12) = 24. Any power 24 or greater implies fullness,
/// since even a count of 1 would be greater than 2^32.
/// Any power lower than 24 will result in full coverage with
/// count >= 2^(32 - 12 - 14) = 2^6 = 64, since it would take 64 chunks of
/// size 2^(12 + 14) to cover the full space.
pub fn is_full(dim: impl SpaceDim, power: u8, count: u32) -> bool {
    let max = 32u8.saturating_sub(dim.get().quantum_power);
    if power == 0 {
        false
    } else if power >= 32 {
        true
    } else {
        count >= pow2(max.saturating_sub(power))
    }
}

/// Calculate the unique pairing of power and count implied by a given length
/// and max number of chunks. Gives the nearest value that satisfies the constraints,
/// but may not be exact.
#[cfg(feature = "test_utils")]
pub fn power_and_count_from_length(dim: impl SpaceDim, len: u64, max_chunks: u32) -> ArqSize {
    let dim = dim.get();
    assert!(len <= U32_LEN);
    let mut power = 0;
    let mut count = (len as f64 / dim.quantum as f64).ceil();
    let max = max_chunks as f64;

    while count.round() > max {
        power += 1;
        count /= 2.0;
    }
    let count = count.round() as u32;
    ArqSize {
        power,
        count: count.into(),
    }
}

/// Calculate the highest power and lowest count such that the given length is
/// represented exactly. If the length is not representable even at the quantum
/// level (power==0), return None.
#[cfg(feature = "test_utils")]
pub fn power_and_count_from_length_exact(
    dim: impl SpaceDim,
    len: u64,
    min_chunks: u32,
) -> Option<ArqSize> {
    let dim = dim.get();
    assert!(len <= U32_LEN);

    let z = len.trailing_zeros();

    if z < dim.quantum_power.into() {
        return None;
    }
    let mut power = z as u8 - dim.quantum_power;
    let mut count = len >> z;

    while (count as u32) < min_chunks {
        count *= 2;
        power -= 1;
    }
    Some(ArqSize {
        power,
        count: (count as u32).into(),
    })
}

/// Given a center and a length, give Arq which matches most closely given the provided strategy
#[cfg(feature = "test_utils")]
pub fn approximate_arq(dim: impl SpaceDim, strat: &ArqStrat, start: Loc, len: u64) -> Arq {
    let dim = dim.get();
    if len == 0 {
        Arq::new(dim.min_power(), start, 0.into())
    } else {
        let ArqSize { power, count } = power_and_count_from_length(dim, len, strat.max_chunks());
        let count = count.0;

        let min = strat.min_chunks() as f64;
        let max = strat.max_chunks() as f64;

        debug_assert!(
            power == 0 || count >= min as u32,
            "count < min: {} < {}",
            count,
            min
        );
        debug_assert!(
            power == 0 || count <= max as u32,
            "count > max: {} > {}",
            count,
            max
        );
        debug_assert!(count == 0 || count - 1 <= u32::MAX / dim.quantum);
        debug_assert!(power <= dim.max_power(strat), "power too large: {}", power);
        Arq::new(power, start, count.into())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    use test_case::test_case;

    #[test]
    fn test_is_full() {
        {
            let topo = Topology::unit_zero();
            assert!(!is_full(&topo, 31, 1));
            assert!(is_full(&topo, 31, 2));
            assert!(is_full(&topo, 31, 3));

            assert!(!is_full(&topo, 30, 3));
            assert!(is_full(&topo, 30, 4));
            assert!(is_full(&topo, 29, 8));

            assert!(is_full(&topo, 1, 2u32.pow(31)));
            assert!(!is_full(&topo, 1, 2u32.pow(31) - 1));
            assert!(is_full(&topo, 2, 2u32.pow(30)));
            assert!(!is_full(&topo, 2, 2u32.pow(30) - 1));
        }
        {
            let topo = Topology::standard_epoch_full();
            assert!(!is_full(&topo, 31 - 12, 1));
            assert!(is_full(&topo, 31 - 12, 2));

            // power too high, doesn't panic
            assert!(is_full(&topo, 31, 2));
            // power too low, doesn't panic
            assert!(!is_full(&topo, 1, 2));
        }
    }

    #[test]
    fn test_full_intervals() {
        let topo = Topology::unit_zero();
        let full1 = Arq::<Loc>::new_full(&topo, 0u32.into(), 29);
        let full2 = Arq::<Loc>::new_full(&topo, 2u32.pow(31).into(), 25);
        assert!(matches!(full1.to_dht_arc_range(&topo), DhtArcRange::Full));
        assert!(matches!(full2.to_dht_arc_range(&topo), DhtArcRange::Full));
    }

    #[test]
    fn arq_requantize() {
        let c = Arq {
            start: Loc::from(42u32),
            power: 20,
            count: SpaceOffset(10),
        };

        let rq = |c: &Arq, p| (*c).requantize(p);

        assert_eq!(rq(&c, 18).map(|c| *c.count), Some(40));
        assert_eq!(rq(&c, 19).map(|c| *c.count), Some(20));
        assert_eq!(rq(&c, 20).map(|c| *c.count), Some(10));
        assert_eq!(rq(&c, 21).map(|c| *c.count), Some(5));
        assert_eq!(rq(&c, 22).map(|c| *c.count), None);
        assert_eq!(rq(&c, 23).map(|c| *c.count), None);
        assert_eq!(rq(&c, 24).map(|c| *c.count), None);

        let c = Arq {
            start: Loc::from(42u32),
            power: 20,
            count: SpaceOffset(256),
        };

        assert_eq!(rq(&c, 12).map(|c| *c.count), Some(256 * 256));
        assert_eq!(rq(&c, 28).map(|c| *c.count), Some(1));
        assert_eq!(rq(&c, 29).map(|c| *c.count), None);
    }

    #[test]
    fn test_to_bounds() {
        let topo = Topology::unit_zero();
        let pow: u8 = 4;
        {
            let a = Arq::new(pow, (2u32.pow(pow.into()) - 1).into(), 16.into());
            let b = a.to_bounds(&topo);
            assert_eq!(b.offset(), SpaceOffset(0));
            assert_eq!(b.count(), 16);
        }
        {
            let a = Arq::new(pow, 4u32.into(), 18.into());
            let b = a.to_bounds(&topo);
            assert_eq!(b.count(), 18);
        }
    }

    #[test]
    fn from_interval_regression() {
        let topo = Topology::unit_zero();
        let i = DhtArcRange::Bounded(4294967040u32.into(), 511.into());
        assert!(ArqBounds::from_interval(&topo, 8, i).is_some());
    }

    #[test_case(2u64.pow(30), (14, 16))]
    #[test_case(2u64.pow(31), (15, 16))]
    #[test_case(2u64.pow(32), (16, 16))]
    #[test_case(128 * 2u64.pow(24), (15, 16))]
    #[test_case((128 + 16) * 2u64.pow(24), (16, 9))]
    fn test_power_and_count_from_length(len: u64, expected: (u8, u32)) {
        let topo = Topology::standard_epoch_full();
        let ArqSize { power, count } = power_and_count_from_length(&topo, len, 16);
        assert_eq!((power, count.0), expected);
        assert_eq!(
            2u64.pow(power as u32 + topo.space.quantum_power as u32) * count.0 as u64,
            len
        );
    }

    #[test_case(2u64.pow(30), (15, 8))]
    #[test_case(2u64.pow(31), (16, 8))]
    #[test_case(2u64.pow(32), (17, 8))]
    #[test_case((128 + 16 + 8) * 2u64.pow(24), (15, 19))]
    #[test_case((128 + 16 + 8 + 4 + 2) * 2u64.pow(24), (13, 79))]
    fn test_power_and_count_from_length_exact(len: u64, expected: (u8, u32)) {
        let topo = Topology::standard_epoch_full();
        let ArqSize { power, count } = power_and_count_from_length_exact(&topo, len, 8).unwrap();
        assert_eq!((power, count.0), expected);
        assert_eq!(
            2u64.pow(power as u32 + topo.space.quantum_power as u32) * count.0 as u64,
            len
        );
    }

    proptest::proptest! {

        #[test]
        fn test_to_edge_locs(power in 0u8..16, count in 8u32..16, loc: u32) {
            // We use powers from 0 to 16 because with standard space topology,
            // the quantum size is 2^12, and the max count is 16 which is 2^4,
            // so any power greater than 16 could result in an overflow.
            let topo = Topology::standard_epoch_full();
            let a = Arq::new(power, Loc::from(loc), SpaceOffset(count));
            let (left, right) = a.to_edge_locs(&topo);
            let p = pow2(power);
            assert_eq!(left.as_u32() % p, 0);
            assert_eq!(right.as_u32().wrapping_add(1) % p, 0);

            assert_eq!(a.absolute_length(&topo), (right - left).as_u32() as u64 + 1);
        }

        #[test]
        fn test_preserve_ordering_for_bounds(mut centers: Vec<u32>, count in 0u32..8, power in 0u8..16) {
            let topo = Topology::standard_epoch_full();

            // given a list of sorted centerpoints
            centers.sort();

            // build identical arqs at each centerpoint and convert them to ArqBounds
            let arqs: Vec<_> = centers.into_iter().map(|c| Arq::new(power, c.into(), count.into())).collect();
            let mut bounds: Vec<_> = arqs.into_iter().map(|a| a.to_bounds(&topo)).enumerate().collect();

            // Ensure the list of ArqBounds also grows monotonically.
            // However, there may be one point at which monotonicity is broken,
            // corresponding to the left edge wrapping around.
            bounds.sort_by_key(|(_, b)| b.to_edge_locs(&topo).0);

            let mut prev = 0;
            let mut split = None;
            for (i, (ix, _)) in bounds.iter().enumerate() {
                if prev > *ix {
                    split = Some(i);
                    break;
                }
                prev = *ix;
            }

            // Split the list of bounds in two, if a discontinuity was found,
            // and check the monotonicity of each piece separately.
            let (b1, b2) = bounds.split_at(split.unwrap_or(0));
            let ix1: Vec<_> = b1.iter().map(|(i, _)| i).collect();
            let ix2: Vec<_> = b2.iter().map(|(i, _)| i).collect();
            let mut ix1s = ix1.clone();
            let mut ix2s = ix2.clone();
            ix1s.sort();
            ix2s.sort();
            assert_eq!(ix1, ix1s);
            assert_eq!(ix2, ix2s);
        }

        #[test]
        fn dht_arc_roundtrip_unit_topo(center: u32, pow in 4..29u8, count in 0..8u32) {
            let topo = Topology::unit_zero();
            let length = count as u64 * 2u64.pow(pow as u32) / 2 * 2;
            let strat = ArqStrat::default();
            let arq = approximate_arq(&topo, &strat, center.into(), length);
            let arc = arq.to_dht_arc(&topo);
            assert_eq!(arq.absolute_length(&topo), arc.length());
            let arq2 = Arq::from_dht_arc_approximate(&topo, &strat, &arc);
            assert_eq!(arq, arq2);
            let arc2 = arq2.to_dht_arc(&topo);
            assert_eq!(arc.range(), arc2.range());
        }

        #[test]
        fn dht_arc_roundtrip_standard_topo(center: u32, pow in 0..16u8, count in 0..16u32) {
            let topo = Topology::standard_epoch_full();
            let length = count as u64 * 2u64.pow(pow as u32) / 2 * 2;
            let strat = ArqStrat::default();
            let arq = approximate_arq(&topo, &strat, center.into(), length);
            let arc = arq.to_dht_arc(&topo);
            assert_eq!(arq.absolute_length(&topo), arc.length());
            let arq2 = Arq::from_dht_arc_approximate(&topo, &strat, &arc);
            assert!(Arq::<Loc>::equivalent(&topo, &arq, &arq2));
            let arc2 = arq2.to_dht_arc(&topo);
            assert_eq!(arc.range(), arc2.range());
        }

        #[test]
        fn arc_interval_roundtrip(center: u32, pow in 0..16u8, count in 0..8u32) {
            let topo = Topology::standard_epoch_full();
            let length = count as u64 * 2u64.pow(pow as u32) / 2 * 2;
            let strat = ArqStrat::default();
            let arq = approximate_arq(&topo, &strat, center.into(), length).to_bounds(&topo);
            let interval = arq.to_dht_arc_range(&topo);
            let arq2 = ArqBounds::from_interval(&topo, arq.power(), interval).unwrap();
            assert!(ArqBounds::equivalent(&topo, &arq, &arq2));
        }
    }
}



================================================
File: crates/kitsune_p2p/dht/src/error.rs
================================================
#![allow(missing_docs)]

#[derive(Debug, thiserror::Error)]
pub enum GossipError {
    #[error("The fundamental parameters of Op region spacetime are mismatched between nodes.")]
    TopologyMismatch,
    #[error("System times between nodes are too far apart to be able to gossip.")]
    TimesOutOfSync,
    #[error("Attempting to gossip with too large a discrepancy in chunk size")]
    ArqPowerDiffTooLarge,
    #[error("Attempting to gossip with a mismatch in the common arc set")]
    ArqSetMismatchForDiff,
}

pub type GossipResult<T> = Result<T, GossipError>;



================================================
File: crates/kitsune_p2p/dht/src/hash.rs
================================================
//! Simple hash types.
// TODO: unify with hashes from `kitsune_p2p_types::bin_types`

/// 32 bytes
pub type Hash32 = [u8; 32];

/// Get the 32 byte slice of a larger slice representing hash data
///
/// # Panics
///
/// Panics if the slice length is not 32, 36, or 39.
pub fn hash_slice_32(v: &[u8]) -> &[u8] {
    let len = v.len();
    if len == 36 || len == 39 {
        &v[4..36]
    } else if len == 32 {
        v
    } else {
        panic!("hash_slice_32 function must only be used with a slice 32, 36, or 39 bytes long")
    }
}

/// Get a fake hash, for testing only.
#[cfg(feature = "test_utils")]
pub fn fake_hash() -> Hash32 {
    use rand::distributions::*;

    let mut rng = rand::thread_rng();
    let uni = Uniform::from(u8::MIN..=u8::MAX);
    let bytes: Vec<u8> = uni.sample_iter(&mut rng).take(32).collect();
    let bytes: [u8; 32] = bytes.try_into().unwrap();
    bytes
}

/// The hash of an Op
#[derive(
    Clone, Hash, PartialEq, Eq, PartialOrd, Ord, derive_more::Constructor, derive_more::From,
)]
pub struct OpHash(pub Hash32);

#[cfg(feature = "test_utils")]
impl OpHash {
    /// Random fake hash for testing
    pub fn fake() -> Self {
        Self(fake_hash())
    }
}

/// The hash of an Agent
#[derive(
    Clone, Hash, PartialEq, Eq, PartialOrd, Ord, derive_more::Constructor, derive_more::From,
)]
pub struct AgentKey(pub Hash32);

#[cfg(feature = "test_utils")]
impl AgentKey {
    /// Random fake hash for testing
    pub fn fake() -> Self {
        Self(fake_hash())
    }
}

/// The hash of a Region, which is the XOR of all OpHashes contained in this region.
#[derive(
    Clone,
    PartialEq,
    Eq,
    derive_more::Constructor,
    derive_more::Deref,
    derive_more::DerefMut,
    derive_more::From,
    serde::Serialize,
    serde::Deserialize,
)]
pub struct RegionHash(pub Hash32);

impl RegionHash {
    /// If the Vec is 32/36/39 long, construct a RegionHash from it
    pub fn from_vec(v: Vec<u8>) -> Option<Self> {
        if v.len() == 36 || v.len() == 39 {
            v[4..36].try_into().map(Self).ok()
        } else {
            v[..].try_into().map(Self).ok()
        }
    }
}

impl std::fmt::Debug for OpHash {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!("{}(0x", "OpHash"))?;
        for byte in &self.0 {
            f.write_fmt(format_args!("{:02x}", byte))?;
        }
        f.write_fmt(format_args!(")"))?;
        Ok(())
    }
}

impl std::fmt::Debug for AgentKey {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!("{}(0x", "AgentKey"))?;
        for byte in &self.0 {
            f.write_fmt(format_args!("{:02x}", byte))?;
        }
        f.write_fmt(format_args!(")"))?;
        Ok(())
    }
}

impl std::fmt::Debug for RegionHash {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_fmt(format_args!("{}(0x", "RegionHash"))?;
        for byte in &self.0 {
            f.write_fmt(format_args!("{:02x}", byte))?;
        }
        f.write_fmt(format_args!(")"))?;
        Ok(())
    }
}



================================================
File: crates/kitsune_p2p/dht/src/lib.rs
================================================
//! Defines the structure of the DHT, the objects which inhabit it, and the operations
//! on these objects.
//!
//! ## Structure and contents of the DHT
//!
//! The DHT is populated by Agents and Ops. Agents introduce new Ops to the DHT,
//! and are responsible for storing their own Ops and the Ops of other Agents.
//!
//! Each Agent has a fixed Location, and each Op has both a fixed
//! Location and a fixed Timestamp.
//!
//! Locations are one-dimensional, and are best thought of as positions
//! on a circle. They are represented by `Wrapping<u32>`.
//! Timestamps are best thought of as positions on a timeline, and are represented
//! by an i64 which denotes microseconds since the UNIX epoch.
//! The Location dimension is circular, and the Time dimension is linear.
//!
//! Each Agent has an Arc which marks out a portion of the circle of Locations.
//! Arcs have a fixed starting point, and extend "clockwise" by some length which
//! is chosen by Kitsune and changes over time. Every Agent is responsible for
//! holding a copy of any Ops whose location overlaps their Arc. As we will see
//! later, Arcs are *quantized*.
//!
//! ### Geometrical interpretation
//!
//! This crate works heavily with the concept of a two-dimensional cylindrical
//! "spacetime" surface -- all terminology in this crate is built with this
//! model in mind, so it helps to explain it a bit here.
//!
//! If we think of Location as a spatial dimension,
//! and Time as a temporal dimension, we can think of the DHT as a
//! two-dimensional *spacetime*.
//!
//! The Location dimension is circular, i.e. the endpoints are connected,
//! but the Time dimension is linear and constantly expanding. So, these two
//! dimensions form a cylindrical surface, with its base at the origin time of
//! the network, and a constantly increasing height as time marches forward.
//!
//! So:
//! - An Op can be thought of as a point somewhere on the surface of this cylinder,
//! - An Agent can be thought of as a line perpendicular to the base, extending
//!   across the entire length of the cylinder.
//! - An Arc can be thought of as a rectangular "stripe" made by sweeping the
//!   Agent line across the distance specified by the length of the Arc.
//! - A *Region* is an arbitrary rectangular area of spacetime.
//!
//! ## Quantization
//!
//! One key feature of this DHT space is that it is *quantized* in both dimensions.
//! You can think of this as a grid which overlays spacetime, which has the effect
//! of grouping Agents and Ops into buckets specified by the grid cells. Another
//! way to think of this is we only refer to items by which grid cell they appear in,
//! not by their absolute coordinates.
//!
//! ### Topology and Quantum Coordinates
//!
//! The quantization is specified by a parameter called the [`Topology`](spacetime::Topology). This
//! construct defines how the quantized grid is overlaid onto spacetime, and specifies
//! how to transform between "absolute" coordinates (Locations and Timestamps) and
//! "quantum" coordinates (grid cells).
//!
//! Each Kitsune Space is free to choose its own Topology (and all nodes in that space
//! must agree to use the same Topology). However, in practice we currently use the same
//! topology for all spaces:
//! - the standard space quantum size is 2^12
//! - the standard time quantum size is 5 minutes
//!
//! Additionally, we include a "time origin" component in the topology, which describes
//! a shift of the origin of the time dimension of the quantized grid. In other words,
//! the time origin specifies what Timestamp corresponds to the origin coordinate of
//! the grid's time dimension.
//!
//! For the purpose of terminology, we refer to these rectangular grid cells as the
//! spacetime quanta. The sides of these rectangles are the space quanta and time quanta.
//!
//! ### Segments (Chunks) and Exponential Coordinates (TODO land on a name for this?)
//!
//! For the purposes of our gossip algorithm, we specify Arcs in terms of "segments"
//! or "chunks". A Segment is a set of `2^p` contiguous quanta, where `p` is a whole number.
//! So, segments are always a power-of-two multiple of the quantum size. Additionally, the offset
//! from the origin must also be a power of two. So, the set of all possible segments
//! implies a collection of uniform grids layered on top of the quantum grid, each one twice
//! as coarse as the one below it.
//!
//! Thus, we can refer to the successive levels of coarseness of quantization by a single
//! whole number, referred to as the "power", since it takes the form of `2^p`.
//! A "power" of 0 refers to the quantum grid itself. A power of 1 is twice as coarse, and so on.
//!
//! Arcs are specified as a set of contiguous Segments all of the same power.
//!
//! ## Quantized Gossip
//!
//! The reason for this detailed quantization of spacetime is to implement historical gossip between
//! agents efficiently. Each Agent has its own Arc which represents which Ops it is responsible
//! for storing and gossiping to other agents. When two agents gossip, they have to quickly and
//! efficiently determine which Ops they already have in common, and which they need to send
//! to each other.
//!
//! This is accomplished by agents exchanging information about the Regions of spacetime that they
//! hold in common. This needs to be done with a minimum of coordination, which is why we use a
//! uniform quantization of spacetime -- if there are only so many ways to split up spacetime into
//! Regions, there is a greater chance that agents will have information on the exact same regions.
//!
//! Concretely, when agents gossip, they exchange the "fingerprint" of each region they have in common
//! with their counterpart, and for any regions whose fingerprints mismatch, each agent sends all
//! the ops in that region to their counterpart. For the regions which match, it is understood that
//! both parties hold the same data in those regions and need no further gossip on those regions.
//! Therefore, it is important to split spacetime into regions in such a way that when mismatches in
//! region fingerprints occur, they are for small amounts of data; or if they are for large amounts
//! of data, then the mismatches should be rare. At the same time, we don't want to split into too
//! many regions, because that would lead to a larger overhead with minimal gains.
//!
//! ### Quantizing space
//!
//! Each agent selects a power level for its arc based on its observation of the network conditions.
//! Ideally, each agent sees a similar view of the network and chooses the same power level as the
//! other nearby agents. When the power levels match, the two agents are tracking space at the same
//! level of granularity and can more easily coordinate. Inasmuch as the power levels differ,
//! one of the agents will have to do some extra computation to reconcile the difference.
//!
//! Without going into too much detail, the power level is mainly a function of how many other
//! Agents are in the vicinity. Agents start with a high power level, covering large sections of
//! the DHT with a coarsely subdivided arc, and as more agents join, they reduce their power level,
//! having smaller segments and tracking ops at a finer level of resolution.
//!
//! With the standard parameters for Arc resizing, it is guaranteed that an agent's Arc will always
//! contain between 8 and 15 segments.
//!
//! ### Quantizing time
//!
//! There is also the time dimension to consider. Agents subdivide space according to who else is
//! around. They subdivide time differently, and somewhat more simply: the older a particular
//! time is, the larger the time segment it will be a part of. The rationale for this is that
//! older data experiences changes far less often than newer data, so it is very likely for
//! older regions to be similar between two agents, and thus they can compare older data in larger
//! swathes than newer data, which is expected to experience changes more frequently.
//!
//! This also has the nice property that the number of segments we use in time only increases
//! logarithmically with the passing of time itself, since older regions are twice as long
//! in the time dimension as newer regions.
//!
//! Since the number of space segments in an arc has constant upper and lower bounds, and the
//! number of time segments increases logarithmically, the overall overhead of coordination
//! during gossip increases only logarithmically as the network grows. Thus, even long-lived
//! networks have the ability to do historical gossip efficiently.

#![warn(missing_docs)]

pub mod arq;
pub mod error;
pub mod hash;
pub mod op;
pub mod region;
pub mod region_set;
pub mod spacetime;

pub use arq::{actual_coverage, Arq, ArqBounds, ArqSet, ArqStrat, PeerStrat, PeerView, PeerViewQ};

// The persistence traits are currently unused except for test implementations of
// a kitsune host. If we ever use them in actual host implementations, we can
// take the feature flag off.
#[cfg(feature = "test_utils")]
pub mod persistence;

#[cfg(feature = "test_utils")]
pub mod test_utils;

pub use kitsune_p2p_dht_arc::DhtLocation as Loc;

pub use kitsune_p2p_timestamp::Timestamp;

/// Common exports
pub mod prelude {
    pub use super::arq::*;
    pub use super::error::*;
    pub use super::hash::*;
    pub use super::op::*;
    pub use super::region::*;
    pub use super::region_set::*;
    pub use super::spacetime::*;

    #[cfg(feature = "test_utils")]
    pub use super::persistence::*;
}



================================================
File: crates/kitsune_p2p/dht/src/op.rs
================================================
//! Defines the trait which represents everything Kitsune needs to know about Ops

use crate::{
    region::RegionData,
    spacetime::{SpacetimeQuantumCoords, Topology},
};

pub use kitsune_p2p_dht_arc::DhtLocation as Loc;

pub use kitsune_p2p_timestamp::Timestamp;

/// Everything that Kitsune needs to know about an Op.
/// Intended to be implemented by the host.
pub trait OpRegion<D = RegionData>: PartialOrd + Ord + Send + Sync + std::fmt::Debug {
    /// The op's Location
    fn loc(&self) -> Loc;
    /// The op's Timestamp
    fn timestamp(&self) -> Timestamp;
    /// The RegionData that would be produced if this op were the only op
    /// in the region. The sum of these produces the RegionData for the whole
    /// region.
    fn region_data(&self) -> D;

    /// The quantized space and time coordinates, based on the location and timestamp.
    fn coords(&self, topo: &Topology) -> SpacetimeQuantumCoords {
        SpacetimeQuantumCoords {
            space: topo.space_quantum(self.loc()),
            time: topo.time_quantum(self.timestamp()),
        }
    }

    /// Create an Op with arbitrary data but that has the given timestamp and location.
    /// Used for bounded range queries based on the PartialOrd impl of the op.
    fn bound(timestamp: Timestamp, loc: Loc) -> Self;
}



================================================
File: crates/kitsune_p2p/dht/src/persistence.rs
================================================
//! Represents data structures outside of kitsune, on the "host" side,
//! i.e. methods which would be called via ghost_actors, i.e. Holochain.
//!
//! XXX: These traits are an artifact of the original prototype of the quantized gossip
//! algorithm, which started as its own crate separate from the rest of Kitsune,
//! and created these traits to build up a suitable interface from scratch.
//! Eventually these should be better integrated with the existing KitsuneP2pEvent
//! interface, and the KitsuneHost interface: the integration of those two is
//! still a work in progress, and these traits should be considered as part of
//! that work.

use std::sync::Arc;

use must_future::MustBoxFuture;

use crate::{
    arq::ArqSet,
    hash::AgentKey,
    op::*,
    region::*,
    region_set::*,
    spacetime::{GossipParams, TelescopingTimes, TimeQuantum, Topology},
    test_utils::OpData,
    Arq,
};

/// All methods involved in accessing the op store, to be implemented by the host.
// TODO: make async
pub trait AccessOpStore<O: OpRegion<D>, D: RegionDataConstraints = RegionData>: Send {
    /// Query the actual ops inside a region
    fn query_op_data(&self, region: &RegionCoords) -> Vec<Arc<O>>;

    /// Query the RegionData of a region, including the hash of all ops, size, and count
    fn query_region_data(&self, region: &RegionCoords) -> D;

    /// Fetch a set of Regions (the coords and the data) given the set of coords
    fn fetch_region_set(
        &self,
        coords: RegionCoordSetLtcs,
    ) -> MustBoxFuture<Result<RegionSetLtcs<D>, ()>>;

    /// Integrate incoming ops, updating the necessary stores
    fn integrate_ops<Ops: Clone + Iterator<Item = Arc<O>>>(&mut self, ops: Ops);

    /// Integrate a single op
    fn integrate_op(&mut self, op: Arc<O>) {
        self.integrate_ops([op].into_iter())
    }

    /// Get the GossipParams associated with this store
    fn gossip_params(&self) -> GossipParams;

    /// Get the Topology associated with this store
    fn topo(&self) -> &Topology;

    /// Get the RegionSet for this node, suitable for gossiping
    fn region_set(&self, arq_set: ArqSet, now: TimeQuantum) -> RegionSet<D> {
        let coords = RegionCoordSetLtcs::new(TelescopingTimes::new(now), arq_set);
        coords
            .into_region_set_infallible(|(_, coords)| self.query_region_data(&coords))
            .into()
    }
}

/// All methods involved in accessing the peer store, to be implemented by the host.
// TODO: make async
pub trait AccessPeerStore {
    /// Get the arq for an agent
    fn get_agent_arq(&self, agent: &AgentKey) -> Arq;

    /// Get the set of all arqs for this node
    fn get_arq_set(&self) -> ArqSet;
}

/// Represents all methods implemented by the host.
pub trait HostAccess<O: OpRegion<D>, D: RegionDataConstraints = RegionData>:
    AccessOpStore<O, D> + AccessPeerStore
{
}
impl<T, O: OpRegion<D>, D: RegionDataConstraints> HostAccess<O, D> for T where
    T: AccessOpStore<O, D> + AccessPeerStore
{
}

/// Represents all methods implemented by the host.
pub trait HostAccessTest: HostAccess<OpData, RegionData> {}
impl<T> HostAccessTest for T where T: HostAccess<OpData, RegionData> {}



================================================
File: crates/kitsune_p2p/dht/src/region.rs
================================================
//! A Region is a bounded section of spacetime containing zero or more Ops.
//!
//! It consists of a [`RegionCoords`] object which defines the space and time
//! boundaries of the region, and some [`RegionData`] which contains a summary
//! of what is inside that region, including:
//! - the number of ops
//! - the total size of op data
//! - the XOR of all OpHashes within this region.
//!
//! The actual [`Region`] struct is generic over the data type, but in all
//! cases, we simply use RegionData, the default. (The type is generic for
//! the possibility of simpler testing in the future.)
//!
//! RegionData is composable: The sum of the RegionData of two *disjoint* (nonoverlapping)
//! Regions represents the union of those two Regions. The sum of hashes is defined
//! as the XOR of hashes, which allows this compatibility.

mod region_coords;
mod region_data;

pub use region_coords::*;
pub use region_data::*;

use num_traits::Zero;
use std::ops::{AddAssign, Sub};

/// The constant size in bytes of a region, used for calculating bandwidth usage during
/// gossip. All regions require the same number of bytes.
pub const REGION_MASS: u32 = std::mem::size_of::<Region<RegionData>>() as u32;

/// The coordinates defining the Region, along with the calculated [`RegionData`]
#[derive(Debug, Clone, PartialEq, Eq, derive_more::Constructor)]
pub struct Region<D: RegionDataConstraints = RegionData> {
    /// The coords
    pub coords: RegionCoords,
    /// The data
    pub data: D,
}

impl<D: RegionDataConstraints> Region<D> {}

/// The constraints necessary for any RegionData
pub trait RegionDataConstraints:
    Eq
    + Zero
    + AddAssign
    + Sub<Output = Self>
    + Clone
    + Send
    + Sync
    + std::fmt::Debug
    + serde::Serialize
    + serde::de::DeserializeOwned
{
    /// The number of ops in this region
    fn count(&self) -> u32;

    /// The size of all ops in this region
    fn size(&self) -> u32;

    // TODO: hash (not currently needed to be generic)
}



================================================
File: crates/kitsune_p2p/dht/src/region_set.rs
================================================
//! A RegionSet is a compact representation of a set of Regions -- the [`RegionCoords`]
//! defining the regions, and their associated [`RegionData`].
//!
//! Currently we have only one scheme for specifying RegionSets, called "LTCS".
//! "LTCS" is an acronym standing for "Logarithmic Time, Constant Space",
//! and it refers to our current scheme of partitioning spacetime during gossip, which
//! is to use a constant number of SpaceSegments (8..16) and a logarithmically
//! growing number of TimeSegments, with larger segments to cover older times.
//! In the future we may have other schemes.

mod ltcs;

pub use ltcs::*;

use crate::{error::GossipResult, spacetime::*};

use crate::region::{Region, RegionBounds, RegionCoords, RegionData, RegionDataConstraints};

/// The generic definition of a set of Regions.
/// The current representation is very specific to our current algorithm,
/// but this is an enum to make room for a more generic representation, e.g.
/// a simple `Vec<Region>`, if we want a more intricate algorithm later.
#[derive(Debug, derive_more::From)]
#[cfg_attr(feature = "test_utils", derive(Clone))]
pub enum RegionSet<T: RegionDataConstraints = RegionData> {
    /// Logarithmic Time, Constant Space.
    Ltcs(RegionSetLtcs<T>),
}

impl<D: RegionDataConstraints> RegionSet<D> {
    /// The number of regions specified
    pub fn count(&self) -> usize {
        match self {
            Self::Ltcs(set) => set.count(),
        }
    }

    /// Iterator over all Regions
    pub fn regions(&self) -> impl Iterator<Item = Region<D>> + '_ {
        match self {
            Self::Ltcs(set) => set.regions(),
        }
    }

    /// The RegionSet can be used to answer questions about more regions than
    /// just the ones specified: If a larger region is queried, and this set contains
    /// a set of regions which over that larger region, then the larger region
    /// can be dynamically constructed.
    ///
    /// This allows agents with differently computed RegionSets to still engage
    /// in gossip without needing to recompute regions.
    pub fn query(&self, _bounds: &RegionBounds) -> ! {
        unimplemented!("only implement after trying naive database-only approach")
    }

    /// In order for this RegionSet to be queryable, new data needs to be
    /// integrated into it to avoid needing to recompute it from the database
    /// on each query.
    pub fn update(&self, _c: SpacetimeQuantumCoords, _d: D) -> ! {
        unimplemented!("only implement after trying naive database-only approach")
    }

    /// Find a set of Regions which represents the intersection of the two
    /// input RegionSets.
    pub fn diff(self, other: Self) -> GossipResult<Vec<Region<D>>> {
        match (self, other) {
            (Self::Ltcs(left), Self::Ltcs(right)) => left.diff(right),
        }
        // Notes on a generic algorithm for the diff of generic regions:
        // can we use a Fenwick tree to look up regions?
        // idea:
        // sort the regions by power (problem, there are two power)
        // lookup the region to see if there's already a direct hit (most efficient if the sorting guarantees that larger regions get looked up later)
        // PROBLEM: we *can't* resolve rectangles where one is not a subset of the other
    }
}

#[cfg(feature = "test_utils")]
impl RegionSet {
    /// Return only the regions which have ops in them. Useful for testing
    /// sparse scenarios.
    pub fn nonzero_regions(
        &self,
    ) -> impl '_ + Iterator<Item = ((usize, usize, usize), RegionCoords, RegionData)> {
        match self {
            Self::Ltcs(set) => set.nonzero_regions(),
        }
    }
}

#[cfg(test)]
#[cfg(feature = "test_utils")]
mod tests {

    use kitsune_p2p_timestamp::Timestamp;

    use crate::{
        op::*,
        persistence::*,
        prelude::{ArqLocated, ArqSet, ArqStart},
        test_utils::{Op, OpData, OpStore},
        Arq, ArqBounds, Loc,
    };

    use super::*;

    /// Create a uniform grid of ops:
    /// - one gridline per arq segment
    /// - one gridline per time specified in the iterator
    ///
    /// Only works for arqs that don't span `u32::MAX / 2`
    fn op_grid<S: ArqStart>(
        topo: &Topology,
        arq: &Arq<S>,
        trange: impl Iterator<Item = i64> + Clone,
    ) -> Vec<Op> {
        let (left, right) = arq.to_edge_locs(topo);
        let left = left.as_u32();
        let right = right.as_u32();
        let mid = u32::MAX / 2;
        assert!(
            !(left < mid && right > mid),
            "This hacky logic does not work for arqs which span `u32::MAX / 2`"
        );
        let xstep = (arq.absolute_length(topo) / arq.count() as u64) as usize;
        (left as i32..=right as i32)
            .step_by(xstep)
            .flat_map(|x| {
                trange.clone().map(move |t| {
                    let x = SpaceQuantum::from(x as u32).to_loc_bounds(topo).0;
                    let t = TimeQuantum::from(t as u32).to_timestamp_bounds(topo).0;
                    OpData::fake(x, t, 10)
                })
            })
            .collect()
    }

    #[test]
    fn test_count() {
        use num_traits::Zero;
        let arqs = ArqSet::new(vec![
            ArqBounds::new(12, 11.into(), 8.into()),
            ArqBounds::new(12, 11.into(), 7.into()),
            ArqBounds::new(12, 11.into(), 5.into()),
        ]);
        let tt = TelescopingTimes::new(TimeQuantum::from(11));
        let nt = tt.segments().len();
        let expected = (8 + 7 + 5) * nt;
        let coords = RegionCoordSetLtcs::new(tt, arqs);
        assert_eq!(coords.count(), expected);
        let regions = coords.into_region_set_infallible(|_| RegionData::zero());
        assert_eq!(regions.count(), expected);
    }

    #[test]
    fn test_regions() {
        let topo = Topology::unit(Timestamp::from_micros(1000));
        let pow = 8;
        let arq = Arq::new(pow, 0u32.into(), 4.into());
        assert_eq!(
            arq.to_edge_locs(&topo),
            (Loc::from(0u32), Loc::from(1023u32))
        );

        let mut store = OpStore::new(topo.clone(), GossipParams::zero());

        // Create a nx by nt grid of ops and integrate into the store
        let nx = 8;
        let nt = 10;
        let ops = op_grid(
            &topo,
            &ArqLocated::new(pow, 0u32.into(), 8.into()),
            (1000..11000_i64).step_by(1000),
        );
        assert_eq!(ops.len(), nx * nt);
        store.integrate_ops(ops.into_iter());

        // Calculate region data for all ops.
        // The total count should be half of what's in the op store,
        // since the arq covers exactly half of the ops
        let times = TelescopingTimes::new(TimeQuantum::from(11000));
        let coords = RegionCoordSetLtcs::new(times, ArqSet::single(arq.to_bounds(&topo)));
        let rset = RegionSetLtcs::from_store(&store, coords);
        assert_eq!(
            rset.data()
                .concat()
                .concat()
                .iter()
                .map(|r| r.count)
                .sum::<u32>() as usize,
            nx * nt / 2
        );
    }

    #[test]
    fn test_rectify() {
        let topo = Topology::unit_zero();
        let arq = Arq::new(8, 0u32.into(), 4.into()).to_bounds(&topo);
        let mut store = OpStore::new(topo.clone(), GossipParams::zero());
        store.integrate_ops(op_grid(&topo, &arq, 10..20).into_iter());

        let tt_a = TelescopingTimes::new(TimeQuantum::from(20));
        let tt_b = TelescopingTimes::new(TimeQuantum::from(30));
        let coords_a = RegionCoordSetLtcs::new(tt_a, ArqSet::single(arq));
        let coords_b = RegionCoordSetLtcs::new(tt_b, ArqSet::single(arq));

        let mut rset_a = RegionSetLtcs::from_store(&store, coords_a);
        let mut rset_b = RegionSetLtcs::from_store(&store, coords_b);
        assert_ne!(rset_a.data(), rset_b.data());

        rset_a.rectify(&mut rset_b).unwrap();

        assert_eq!(rset_a, rset_b);

        let coords: Vec<Vec<_>> = rset_a
            .coords
            .region_coords_nested()
            // The outer layer of iterators corresponds to arqs in the ArqSet.
            // There is only one arq, so just take the first item.
            .next()
            .unwrap()
            // The other two layers are for space and time segments
            .map(|col| col.collect())
            .collect();

        assert_eq!(coords.len(), arq.count() as usize);
        for col in coords.iter() {
            assert_eq!(col.len(), rset_a.coords.times.segments().len());
        }
        let nt = coords[0].len();
        assert_eq!(tt_b.segments()[0..nt], rset_a.coords.times.segments());
        assert_eq!(tt_b.segments()[0..nt], rset_b.coords.times.segments());
    }

    #[test]
    fn test_diff() {
        let topo = Topology::unit_zero();
        let arq = Arq::new(8, Loc::from(-512i32 as u32), 4.into()).to_bounds(&topo);
        dbg!(&arq, arq.to_dht_arc_range(&topo));

        let mut store1 = OpStore::new(topo.clone(), GossipParams::zero());
        store1.integrate_ops(op_grid(&topo, &arq, 10..20).into_iter());

        let extra_ops = [
            OpData::fake(Loc::from(-300i32), Timestamp::from_micros(18), 4),
            OpData::fake(Loc::from(12u32), Timestamp::from_micros(12), 4),
        ];
        let mut store2 = store1.clone();
        store2.integrate_ops(extra_ops.clone().into_iter());

        let coords_a = RegionCoordSetLtcs::new(
            TelescopingTimes::new(TimeQuantum::from(20)),
            ArqSet::single(arq),
        );
        let coords_b = RegionCoordSetLtcs::new(
            TelescopingTimes::new(TimeQuantum::from(21)),
            ArqSet::single(arq),
        );

        let rset_a = RegionSetLtcs::from_store(&store1, coords_a);
        let rset_b = RegionSetLtcs::from_store(&store2, coords_b);
        assert_ne!(rset_a.data(), rset_b.data());

        let diff = rset_a.clone().diff(rset_b.clone()).unwrap();
        dbg!(&diff, &extra_ops);
        assert_eq!(diff.len(), 2);

        assert!(diff[0].coords.contains(&topo, &extra_ops[0].coords(&topo)));
        assert!(diff[1].coords.contains(&topo, &extra_ops[1].coords(&topo)));

        // Adding the region data from each extra op to the region data of the
        // diff which was missing those ops should be the same as the query
        // of the store which contains the extra ops over the same region
        // TODO: proptest this
        assert_eq!(
            diff[0].data.clone() + extra_ops[0].region_data(),
            store2.query_region_data(&diff[0].coords)
        );
        assert_eq!(
            diff[1].data.clone() + extra_ops[1].region_data(),
            store2.query_region_data(&diff[1].coords)
        );
    }

    #[test]
    fn test_diff_standard_topo() {
        let topo = Topology::standard_zero();
        let pow: u8 = 4;
        // This arq goes from -2^17 to 2^17, with a chunk size of 2^16
        let left_edge = Loc::from(-(2i32.pow(pow as u32 + 12 + 1)));
        let arq = Arq::new(pow, left_edge, 4.into()).to_bounds(&topo);
        dbg!(&arq, arq.to_dht_arc_range(&topo));

        let mut store1 = OpStore::new(topo.clone(), GossipParams::zero());
        store1.integrate_ops(op_grid(&topo, &arq, 10..20).into_iter());

        let extra_ops = [
            OpData::fake(
                left_edge,
                TimeQuantum::from(18).to_timestamp_bounds(&topo).0,
                13,
            ),
            OpData::fake(
                Loc::from(11111u32),
                TimeQuantum::from(12).to_timestamp_bounds(&topo).0,
                11,
            ),
        ];
        // Store 2 has everything store 1 has, plus 2 extra ops
        let mut store2 = store1.clone();
        store2.integrate_ops(extra_ops.clone().into_iter());

        let coords_a = RegionCoordSetLtcs::new(
            TelescopingTimes::new(TimeQuantum::from(20)),
            ArqSet::single(arq),
        );
        let coords_b = RegionCoordSetLtcs::new(
            TelescopingTimes::new(TimeQuantum::from(21)),
            ArqSet::single(arq),
        );

        let rset_a = RegionSetLtcs::from_store(&store1, coords_a);
        let rset_b = RegionSetLtcs::from_store(&store2, coords_b);
        assert_ne!(rset_a.data(), rset_b.data());

        let diff = rset_a.clone().diff(rset_b.clone()).unwrap();
        dbg!(&diff, &extra_ops);
        assert_eq!(diff.len(), 2);

        assert!(diff[0].coords.contains(&topo, &extra_ops[0].coords(&topo)));
        assert!(diff[1].coords.contains(&topo, &extra_ops[1].coords(&topo)));

        // Adding the region data from each extra op to the region data of the
        // diff which was missing those ops should be the same as the query
        // of the store which contains the extra ops over the same region
        // TODO: proptest this
        assert_eq!(
            (diff[0].data).clone() + extra_ops[0].region_data(),
            store2.query_region_data(&diff[0].coords)
        );
        assert_eq!(
            (diff[1].data).clone() + extra_ops[1].region_data(),
            store2.query_region_data(&diff[1].coords)
        );
    }
}



================================================
File: crates/kitsune_p2p/dht/src/spacetime.rs
================================================
//! Data types representing the structure of spacetime and the various ways
//! space and time can be quantized.
//!
//! Kitsune deals with spacetime coordinates on three different levels:
//!
//! ### Absolute coordinates
//!
//! At the absolute level, space coordinates are represented by `u32` (via `DhtLocation`),
//! and time coordinates by `i64` (via `Timestamp`). The timestamp and DHT location
//! of each op is measured in absolute coordinates, as well as the DHT locations of
//! agents
//!
//! ### Quantized coordinates
//!
//! Some data types represent quantized space/time. The `Topology` for a network
//! determines the quantum size for both the time and space dimensions, meaning
//! that any absolute coordinate will always be a multiple of this quantum size.
//! Hence, quantized coordinates are expressed in terms of multiples of the
//! quantum size.
//!
//! `SpaceQuantum` and `TimeQuantum` express quantized coordinates. They refer
//! to a specific quantum-sized portion of space/time.
//!
//! Note that any transformation between Absolute and Quantized coordinates
//! requires the information contained in the `Topology` of the network.
//!
//! ### Segment coordinates (or, Exponential coordinates)
//!
//! The spacetime we are interested in has dimensions that are not only quantized,
//! but are also hierarchically organized into non-overlapping segments.
//! When expressing segments of space larger than a single quantum, we only ever talk about
//! groupings of 2, 4, 8, 16, etc. quanta at a time, and these groupings are
//! always aligned so that no two segments of a given size ever overlap. Moreover,
//! any two segments of different sizes either overlap completely (one is a strict
//! superset of the other), or they don't overlap at all (they are disjoint sets).
//!
//! Segment coordinates are expressed in terms of:
//! - a *power* (exponent of 2) which determines the length of the segment *expressed as a Quantized coordinate*
//! - an *offset*, which is a multiple of the length of this segment to determine
//!   the "left" edge's distance from the origin *as a Quantized coordinate*
//!
//! You must still convert from these Quantized coordinates to get to the actual
//! Absolute coordinates.
//!
//! The pairing of any `SpaceSegment` with any `TimeSegment` forms a `Region`,
//! a bounded rectangle of spacetime.
//!

use std::{
    ops::{AddAssign, Deref},
    time::Duration,
};

use crate::{
    op::{Loc, Timestamp},
    prelude::pow2,
    ArqStrat,
};
use derivative::Derivative;

mod quantum;
mod segment;
mod telescoping_times;
mod topology;

pub use quantum::*;
pub use segment::*;
pub use telescoping_times::*;
pub use topology::*;



================================================
File: crates/kitsune_p2p/dht/src/test_utils.rs
================================================
//! Utils for testing

mod gossip_direct;
mod min_redundancy;
mod op_data;
mod op_store;
mod test_node;

pub use gossip_direct::*;
pub use min_redundancy::*;
pub use op_data::*;
pub use op_store::*;
pub use test_node::*;

use crate::arq::*;
use crate::spacetime::Topology;

use kitsune_p2p_dht_arc::DhtLocation as Loc;
use rand::prelude::StdRng;
use rand::thread_rng;
use rand::Rng;
use rand::SeedableRng;

/// Wait for input, to slow down overwhelmingly large iterations
pub fn get_input() {
    let mut input_string = String::new();
    std::io::stdin()
        .read_line(&mut input_string)
        .expect("Failed to read line");
}

/// A RNG suitable for testing, if no seed is passed, uses standard random seed.
pub fn seeded_rng(seed: Option<u64>) -> StdRng {
    let seed = seed.unwrap_or_else(|| thread_rng().gen());
    tracing::info!("RNG seed: {}", seed);
    StdRng::seed_from_u64(seed)
}

fn full_len() -> f64 {
    2f64.powf(32.0)
}

#[allow(dead_code)]
type DataVec = statrs::statistics::Data<Vec<f64>>;

/// Your peers just look like a bunch of Arqs
pub type Peers = Vec<Arq>;

/// Construct an arq from start and length specified in the interval [0, 1]
pub fn unit_arq(topo: &Topology, strat: &ArqStrat, unit_start: f64, unit_len: f64) -> Arq {
    assert!(
        (0.0..1.0).contains(&unit_start),
        "center out of bounds {}",
        unit_start
    );
    assert!(
        (0.0..=1.0).contains(&unit_len),
        "len out of bounds {}",
        unit_len
    );

    approximate_arq(
        topo.space,
        strat,
        Loc::from((unit_start * full_len()) as u32),
        (unit_len * full_len()) as u64,
    )
}

/// Each agent is perfectly evenly spaced around the DHT (+/- some jitter),
/// with stable arc lengths that are sized to meet the minimum coverage target
pub fn generate_ideal_coverage(
    topo: &Topology,
    rng: &mut StdRng,
    strat: &ArqStrat,
    cov: Option<f64>,
    n: u32,
    jitter: f64,
) -> Peers {
    tracing::info!("N = {}, J = {}", n, jitter);
    tracing::info!("ArqStrat: = {:#?}", strat);

    let nf = n as f64;
    // aim for the middle of the coverage target range
    let target = cov.unwrap_or_else(|| strat.midline_coverage());
    let len = (target / nf).min(1.0);

    let peers: Vec<_> = (0..n)
        .map(|i| {
            let center =
                ((i as f64 / nf) + (2.0 * jitter * rng.gen::<f64>()) - jitter).rem_euclid(1.0);
            unit_arq(topo, strat, center, len)
        })
        .collect();

    let cov = actual_coverage(topo, peers.iter());
    // this equation for min is derived from solving for min in terms of buf and target, using
    // the system of equations:
    //     max = min * (buf + 1)
    //     target = (min + max) / 2
    let min = 2.0 * target / (strat.buffer + 2.0);
    let max = min * (strat.buffer + 1.0);
    // When this is perfect, fudge factor can be 1.0. Otherwise, choose a small value >1 like 1.05,
    // to accomodate values slightly out of range.
    let fudge = 1.0;
    let min = (min / fudge).floor();
    let max = (max * fudge).ceil();
    assert!(
        min <= cov && cov <= max,
        "Ideal coverage was generated incorrectly: !({} <= {} <= {})",
        min,
        cov,
        max
    );
    peers
}

/// Generates arqs with lengths according to a standard distribution, to test
/// wildly unbalanced initial conditions
pub fn generate_messy_coverage(
    topo: &Topology,
    rng: &mut StdRng,
    strat: &ArqStrat,
    len_mean: f64,
    len_std: f64,
    n: u32,
    jitter: f64,
) -> Peers {
    use rand::distributions::*;

    tracing::info!("N = {}, J = {}", n, jitter);
    tracing::info!("ArqStrat: = {:#?}", strat);

    let len_dist = statrs::distribution::Normal::new(len_mean, len_std).unwrap();

    let nf = n as f64;

    let peers: Vec<_> = (0..n)
        .map(|i| {
            let center =
                ((i as f64 / nf) + (2.0 * jitter * rng.gen::<f64>()) - jitter).rem_euclid(1.0);
            let len = len_dist.sample(rng).clamp(0.0, 1.0);
            unit_arq(topo, strat, center, len)
        })
        .collect();

    peers
}

#[test]
fn test_unit_arc() {
    let topo = Topology::unit_zero();
    let strat = ArqStrat {
        min_coverage: 10.0,
        buffer: 0.2,
        ..ArqStrat::default()
    };
    let expected_chunks = 8;

    {
        let a = unit_arq(&topo, &strat, 0.0, 0.0);
        assert_eq!(a.power(), topo.space.min_power());
        assert_eq!(a.count(), 0);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0);
        assert_eq!(a.power(), topo.space.max_power(&strat));
        assert_eq!(a.count(), 8);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0 / 2.0);
        assert_eq!(a.power(), 28);
        assert_eq!(a.count(), expected_chunks);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0 / 4.0);
        assert_eq!(a.power(), 27);
        assert_eq!(a.count(), expected_chunks);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0 / 8.0);
        assert_eq!(a.power(), 26);
        assert_eq!(a.count(), expected_chunks);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0 / 16.0);
        assert_eq!(a.power(), 25);
        assert_eq!(a.count(), expected_chunks);
    }
    {
        let a = unit_arq(&topo, &strat, 0.0, 1.0 / 32.0);
        assert_eq!(a.power(), 24);
        assert_eq!(a.count(), expected_chunks);
    }
}

#[cfg(test)]
mod tests {
    use crate::arq::PeerViewQ;

    use super::*;
    use proptest::proptest;
    use test_case::test_case;

    #[test_case(21.62, 0.1, 100)]
    #[test_case(89.6, 0.25, 100)]
    fn test_ideal_coverage_cases(min_coverage: f64, buffer: f64, num_peers: u32) {
        let topo = Topology::unit_zero();

        let strat = ArqStrat {
            min_coverage,
            buffer,
            ..ArqStrat::default()
        };

        let mut rng = seeded_rng(None);
        let peers = generate_ideal_coverage(&topo, &mut rng, &strat, None, num_peers, 0.0);

        let view = PeerViewQ::new(topo, strat.clone(), peers);
        let cov = view.actual_coverage();

        let min = strat.min_coverage;
        let max = strat.max_coverage();
        assert!(min <= cov);
        assert!(cov <= max);
    }

    proptest! {
        /// Ensure that something close to the ideal coverage is generated under a
        /// range of ArqStrat parameters.
        /// NOTE: this is not perfect. The final assertion has to be fudged a bit,
        /// so this test asserts that the extrapolated coverage falls within the
        /// range.
        #[test]
        fn test_ideal_coverage(min_coverage in 40f64..100.0, buffer in 0.1f64..0.25, num_peers in 100u32..200) {
            let topo = Topology::unit_zero();
            let strat = ArqStrat {
                min_coverage,
                buffer,
                ..ArqStrat::default()
            };
            let mut rng = seeded_rng(None);
            let peers = generate_ideal_coverage(&topo, &mut rng, &strat, None, num_peers, 0.0);
            let view = PeerViewQ::new(topo, strat.clone(), peers);
            let cov = view.actual_coverage();

            let min = strat.min_coverage;
            let max = strat.max_coverage();
            assert!(min <= cov, "extrapolated less than min {} <= {}", min, cov);
            assert!(cov <= max, "extrapolated greater than max {} <= {}", cov, max);
        }

        #[test]
        fn chunk_count_is_always_within_bounds(center in 0.0f64..0.999, len in 0.001f64..1.0) {
            let topo = Topology::unit_zero();
            let strat = ArqStrat {
                min_coverage: 10.0,
                buffer: 0.144,
                ..ArqStrat::default()
            };
            let a = unit_arq(&topo, &strat, center, len);

            assert!(a.count() >= strat.min_chunks());
            assert!(a.count() <= strat.max_chunks());
        }

        #[test]
        fn power_is_always_within_bounds(center in 0.0f64..0.999, len in 0.001f64..1.0) {
            let topo = Topology::unit_zero();
            let strat = ArqStrat {
                min_coverage: 10.0,
                buffer: 0.144,
                ..ArqStrat::default()
            };
            let a = unit_arq(&topo, &strat, center, len);
            assert!(a.power() >= topo.space.min_power());
            assert!(a.power() <= topo.space.max_power(&strat));
        }

        #[test]
        fn length_is_always_close(center in 0.0f64..0.999, len in 0.001f64..1.0) {
            let topo = Topology::unit_zero();
            let strat = ArqStrat {
                min_coverage: 10.0,
                buffer: 0.144,
                ..ArqStrat::default()
            };
            let a = unit_arq(&topo, &strat, center, len);
            let target_len = (len * 2f64.powf(32.0)) as i64;
            let true_len = a.to_dht_arc_range(&topo).length() as i64;
            assert!((true_len - target_len).abs() < a.absolute_chunk_width(&topo) as i64);
        }
    }
}



================================================
File: crates/kitsune_p2p/dht/src/arq/arq_set.rs
================================================
//! Types representing a set of Arqs all of the same "power".

use kitsune_p2p_dht_arc::DhtArcSet;

use crate::{arq::ArqBounds, spacetime::*};

use super::{Arq, ArqStart};

/// A collection of ArqBounds.
/// All bounds are guaranteed to be quantized to the same power
/// (the lowest common power).
#[derive(
    Debug,
    Clone,
    PartialEq,
    Eq,
    Hash,
    derive_more::Deref,
    derive_more::DerefMut,
    derive_more::IntoIterator,
    derive_more::Index,
    derive_more::IndexMut,
    serde::Serialize,
    serde::Deserialize,
)]
pub struct ArqSet<S: ArqStart = SpaceOffset> {
    #[into_iterator]
    #[deref]
    #[deref_mut]
    #[index]
    #[index_mut]
    #[serde(bound(deserialize = "S: serde::de::DeserializeOwned"))]
    pub(crate) arqs: Vec<Arq<S>>,
    power: u8,
}

impl<S: ArqStart> ArqSet<S> {
    /// Normalize all arqs to be of the same power (use the minimum power)
    pub fn new(arqs: Vec<Arq<S>>) -> Self {
        if let Some(pow) = arqs.iter().map(|a| a.power()).min() {
            Self {
                arqs: arqs
                    .into_iter()
                    .map(|a| a.requantize(pow).unwrap())
                    .collect(),
                power: pow,
            }
        } else {
            Self {
                arqs: vec![],
                power: 1,
            }
        }
    }

    /// Empty set
    pub fn empty() -> Self {
        Self::new(vec![])
    }

    /// Singleton set
    pub fn single(arq: Arq<S>) -> Self {
        Self::new(vec![arq])
    }

    /// Singleton set
    #[cfg(feature = "test_utils")]
    pub fn full_std() -> Self {
        use crate::ArqStrat;

        Self::new(vec![Arq::<S>::new_full_max(
            SpaceDimension::standard(),
            &ArqStrat::default(),
            S::zero(),
        )])
    }

    /// Get a reference to the arq set's power.
    pub fn power(&self) -> u8 {
        self.power
    }

    /// Get a reference to the arq set's arqs.
    pub fn arqs(&self) -> &[Arq<S>] {
        self.arqs.as_ref()
    }

    /// Convert to a set of "continuous" arcs using standard topology
    pub fn to_dht_arc_set_std(&self) -> DhtArcSet {
        self.to_dht_arc_set(SpaceDimension::standard())
    }

    /// Convert to a set of "continuous" arcs
    pub fn to_dht_arc_set(&self, dim: impl SpaceDim) -> DhtArcSet {
        DhtArcSet::from(
            self.arqs
                .iter()
                .map(|a| a.to_dht_arc_range(dim))
                .collect::<Vec<_>>(),
        )
    }

    /// Requantize each arq in the set.
    pub fn requantize(&self, power: u8) -> Option<Self> {
        self.arqs
            .iter()
            .map(|a| a.requantize(power))
            .collect::<Option<Vec<_>>>()
            .map(|arqs| Self { arqs, power })
    }

    /// Intersection of all arqs contained within
    pub fn intersection(&self, dim: impl SpaceDim, other: &Self) -> ArqSet<SpaceOffset> {
        let power = self.power.min(other.power());
        let a1 = self.requantize(power).unwrap().to_dht_arc_set(dim);
        let a2 = other.requantize(power).unwrap().to_dht_arc_set(dim);
        ArqSet {
            arqs: DhtArcSet::intersection(&a1, &a2)
                .intervals()
                .into_iter()
                .map(|interval| {
                    ArqBounds::from_interval(dim, power, interval).expect("cannot fail")
                })
                .collect(),
            power,
        }
    }

    /// View ascii for all arq bounds
    #[cfg(feature = "test_utils")]
    pub fn print_arqs(&self, dim: impl SpaceDim, len: usize) {
        println!("{} arqs, power: {}", self.arqs().len(), self.power());
        for (i, arq) in self.arqs().iter().enumerate() {
            println!(
                "{:>3}: |{}| {} {}/{} @ {:?}",
                i,
                arq.to_ascii(dim, len),
                arq.absolute_length(dim),
                arq.power(),
                arq.count(),
                arq.start
            );
        }
    }
}

impl ArqSet {
    /// Convert back from a continuous arc set to a quantized one.
    /// If any information is lost (the match is not exact), return None.
    /// This is necessary because an arcset which is the union of many agents'
    /// arcs may be much longer than any one agent's arcs, which would not quantize
    /// properly to an arq that fits the bounds of the ArqStrat (num chunks between
    /// 8 and 16), so if we want an exact match (which we often do!) we need to
    /// allow the power to be lower and the chunk size to be greater to provide
    /// the exact match.
    //
    // TODO: XXX: revisit this when power levels really matter, because this
    //   does entail a loss of info about the original power levels of the original
    //   arqs, or even of the original arqset minimum power level. For instance we
    //   may need to refactor agent info to include power level so as not to lose
    //   this info.
    #[cfg(feature = "test_utils")]
    pub fn from_dht_arc_set_exact(
        dim: impl SpaceDim,
        strat: &crate::ArqStrat,
        dht_arc_set: &DhtArcSet,
    ) -> Option<Self> {
        Some(Self::new(
            dht_arc_set
                .intervals()
                .into_iter()
                .map(|i| {
                    let len = i.length();
                    let super::ArqSize { power, .. } =
                        super::power_and_count_from_length_exact(dim, len, strat.min_chunks())?;
                    ArqBounds::from_interval(dim, power, i)
                })
                .collect::<Option<Vec<_>>>()?,
        ))
    }
}

/// Print ascii for arq bounds
#[cfg(feature = "test_utils")]
pub fn print_arq<S: ArqStart>(dim: impl SpaceDim, arq: &Arq<S>, len: usize) {
    println!(
        "|{}| {} *2^{}",
        arq.to_ascii(dim, len),
        arq.count(),
        arq.power()
    );
}

/// Print a collection of arqs
#[cfg(feature = "test_utils")]
pub fn print_arqs<S: ArqStart>(dim: impl SpaceDim, arqs: &[Arq<S>], len: usize) {
    for (i, arq) in arqs.iter().enumerate() {
        println!(
            "|{}| {}:\t{} +{} *2^{}",
            arq.to_ascii(dim, len),
            i,
            *arq.start.to_offset(dim, arq.power()),
            arq.count(),
            arq.power()
        );
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{prelude::pow2, ArqStrat};
    use kitsune_p2p_dht_arc::DhtArcRange;

    #[test]
    fn intersect_arqs() {
        holochain_trace::test_run();
        let topo = Topology::unit_zero();
        let a = Arq::new(27, 536870912u32.into(), 11.into());
        let b = Arq::new(27, 805306368u32.into(), 11.into());
        dbg!(a.to_bounds(&topo).offset());

        let a = ArqSet::single(a);
        let b = ArqSet::single(b);
        let c = a.intersection(&topo, &b);
        print_arqs(&topo, &a, 64);
        print_arqs(&topo, &b, 64);
        print_arqs(&topo, &c, 64);
    }

    #[test]
    fn intersect_arqs_multi() {
        holochain_trace::test_run();
        let topo = Topology::unit_zero();

        let pow = 26;
        let sa1 = (u32::MAX - 4 * pow2(pow) + 1).into();
        let sa2 = (13 * pow2(pow - 1)).into();
        let sb1 = 0u32.into();
        let sb2 = (20 * pow2(pow - 1)).into();

        let a = ArqSet::new(vec![
            Arq::new(pow, sa1, 8.into()).to_bounds(&topo),
