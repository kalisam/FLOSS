                                )
                                .await
                                .map_err(TaskManagerError::internal)?;
                        }
                        tracing::error!("Apps disabled.");
                    }
                }
                TaskOutcome::StopAppsWithDna(dna_hash, error, context) => {
                    tracing::error!("About to automatically stop apps with dna {}", dna_hash);
                    let app_ids = conductor
                        .list_running_apps_for_dependent_dna_hash(dna_hash.as_ref())
                        .await
                        .map_err(TaskManagerError::internal)?;
                    if error.is_recoverable() {
                        let cells_with_same_dna: Vec<_> = conductor
                            .running_cell_ids()
                            .into_iter()
                            .filter(|id| id.dna_hash() == dna_hash.as_ref())
                            .collect();
                        conductor.remove_cells(&cells_with_same_dna).await;

                        // The following message assumes that only the app_ids calculated will be paused, but other apps
                        // may have been paused as well.
                        tracing::error!(
                            "PAUSING the following apps due to a recoverable error: {:?}\nError: {:?}\nContext: {}",
                            app_ids,
                            error,
                            context
                        );

                        // MAYBE: it could be helpful to modify this function so that when providing Some(app_ids),
                        //   you can also pass in a PausedAppReason override, so that the reason for the apps being paused
                        //   can be set to the specific error message encountered here, rather than having to read it from
                        //   the logs.
                        let delta = conductor
                            .reconcile_app_status_with_cell_status(None)
                            .await
                            .map_err(TaskManagerError::internal)?;
                        tracing::debug!(delta = ?delta);

                        tracing::error!("Apps paused.");
                    } else {
                        // Since the error is unrecoverable, we don't expect to be able to use this Cell anymore.
                        // Therefore, we disable every app which requires that cell.
                        tracing::error!(
                            "DISABLING the following apps due to an unrecoverable error: {:?}\nError: {:?}\nContext: {}",
                            app_ids,
                            error,
                            context
                        );
                        for app_id in app_ids.iter() {
                            conductor
                                .clone()
                                .disable_app(
                                    app_id.to_string(),
                                    DisabledAppReason::Error(error.to_string()),
                                )
                                .await
                                .map_err(TaskManagerError::internal)?;
                        }
                        tracing::error!("Apps disabled.");
                    }
                }
            };
        }
        Ok(())
    }.instrument(span))
}

#[cfg_attr(feature = "instrument", tracing::instrument(skip(kind)))]
fn produce_task_outcome(kind: &TaskKind, result: ManagedTaskResult, name: String) -> TaskOutcome {
    use TaskOutcome::*;
    match kind {
        TaskKind::Ignore => match result {
            Ok(_) => LogInfo(format!("task completed: {}", name)),
            Err(err) => MinorError(Box::new(err), name),
        },
        TaskKind::Unrecoverable => match result {
            Ok(_) => LogInfo(format!("task completed: {}", name)),
            Err(err) => ShutdownConductor(Box::new(err), name),
        },
        TaskKind::CellCritical(cell_id) => match result {
            Ok(_) => LogInfo(format!("task completed: {}", name)),
            Err(err) => StopApps(cell_id.to_owned(), Box::new(err), name),
        },
        TaskKind::DnaCritical(dna_hash) => match result {
            Ok(_) => LogInfo(format!("task completed: {}", name)),
            Err(err) => StopAppsWithDna(dna_hash.to_owned(), Box::new(err), name),
        },
    }
}

/// Handle the result of shutting down the main thread.
pub fn handle_shutdown(result: Result<TaskManagerResult, tokio::task::JoinError>) {
    let result = result.inspect_err(|e| {
        error!(
            error = e as &dyn std::error::Error,
            "Failed to join the main task"
        );
    });
    match result {
        Ok(result) => result.expect("Conductor shutdown error"),
        Err(error) => match error.try_into_panic() {
            Ok(reason) => {
                // Resume the panic on the main task
                std::panic::resume_unwind(reason);
            }
            Err(error) => panic!("Error while joining threads during shutdown {:?}", error),
        },
    }
}

/// Each task has a group, and here they are
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum TaskGroup {
    /// Tasks which are associated with the conductor as a whole
    Conductor,
    /// Tasks which are associated with a particular DNA space
    Dna(Arc<DnaHash>),
    /// Tasks which are associated with a particular running Cell
    Cell(CellId),
}

/// Channel sender for task outcomes
pub type OutcomeSender = futures::channel::mpsc::Sender<(TaskGroup, TaskOutcome)>;
/// Channel receiver for task outcomes
pub type OutcomeReceiver = futures::channel::mpsc::Receiver<(TaskGroup, TaskOutcome)>;

/// A future which awaits the completion of all managed tasks
pub type ShutdownHandle = JoinHandle<()>;

#[cfg(test)]
mod test {
    use super::*;
    use crate::conductor::{error::ConductorError, Conductor};
    use holochain_state::test_utils::test_db_dir;
    use holochain_trace;

    #[tokio::test(flavor = "multi_thread")]
    async fn unrecoverable_error() {
        holochain_trace::test_run();
        let db_dir = test_db_dir();
        let handle = Conductor::builder()
            .with_data_root_path(db_dir.path().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();
        let tm = handle.task_manager();
        tm.add_conductor_task_unrecoverable("unrecoverable", |_stop| async {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            Err(Box::new(ConductorError::Other(
                anyhow::anyhow!("Unrecoverable task failed").into(),
            ))
            .into())
        });

        let main_task = handle.outcomes_task.share_mut(|o| o.take().unwrap());

        // the outcome channel sender lives on the TaskManager, so we need to drop it
        // so that the main_task will end

        // tm.shutdown();
        drop(tm);

        main_task
            .await
            .expect("Failed to join the main task")
            .expect_err("The main task should return an error");
    }

    #[tokio::test(flavor = "multi_thread")]
    #[ignore = "panics in tokio break other tests, this test is here to confirm behavior but cannot be run on ci"]
    async fn unrecoverable_panic() {
        holochain_trace::test_run();
        let db_dir = test_db_dir();
        let handle = Conductor::builder()
            .with_data_root_path(db_dir.as_ref().to_path_buf().into())
            .test(&[])
            .await
            .unwrap();
        let tm = handle.task_manager();

        tm.add_conductor_task_unrecoverable("unrecoverable", |_stop| async {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            panic!("Task has panicked")
        });

        let main_task = handle.outcomes_task.share_mut(|o| o.take().unwrap());
        drop(tm);
        handle_shutdown(main_task.await);
    }
}



================================================
File: crates/holochain/src/conductor/tests/app_info.rs
================================================
use holochain_conductor_api::CellInfo;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;
use matches::matches;

use crate::sweettest::{SweetConductor, SweetDnaFile};

#[tokio::test(flavor = "multi_thread")]
async fn app_info_returns_all_cells_with_info() {
    // set up app with two provisioned cells and one clone cell of each of them
    let (dna_1, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let (dna_2, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let mut conductor = SweetConductor::from_standard_config().await;

    let app_id: InstalledAppId = "app".into();
    let role_name_1: RoleName = "role_1".into();
    let role_name_2: RoleName = "role_2".into();
    let app = conductor
        .setup_app(
            &app_id,
            [
                &(role_name_1.clone(), dna_1.clone()),
                &(role_name_2.clone(), dna_2.clone()),
            ],
        )
        .await
        .unwrap();
    let agent_pub_key = app.agent().clone();

    // create 1 clone cell for role 1 = clone cell 1
    let clone_cell_1 = conductor
        .clone()
        .create_clone_cell(
            &app_id,
            CreateCloneCellPayload {
                role_name: role_name_1.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed numero uno".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
    assert_eq!(clone_cell_1.original_dna_hash, dna_1.dna_hash().clone());

    // create 1 clone cell for role 2 = clone cell 2
    let clone_cell_2 = conductor
        .clone()
        .create_clone_cell(
            &app_id,
            CreateCloneCellPayload {
                role_name: role_name_2.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed numero dos".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
    assert_eq!(clone_cell_2.original_dna_hash, dna_2.dna_hash().clone());

    // disable clone cell 2
    conductor
        .disable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: CloneCellId::DnaHash(clone_cell_2.cell_id.dna_hash().clone()),
            },
        )
        .await
        .unwrap();

    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();

    // agent pub key matches
    assert_eq!(app_info.agent_pub_key, agent_pub_key);

    // app info has cell info for two role names
    assert_eq!(app_info.cell_info.len(), 2);

    // check cell info for role name 1
    let cell_info_for_role_1 = app_info.cell_info.get(&role_name_1).unwrap();
    // cell 1 in cell info is provisioned cell
    matches!(cell_info_for_role_1[0], CellInfo::Provisioned(_));
    // cell 2 in cell info is clone cell
    matches!(cell_info_for_role_1[1], CellInfo::Cloned(_));

    // check cell info for role name 2
    let cell_info_for_role_2 = app_info.cell_info.get(&role_name_2).unwrap();
    // cell 1 in cell info is provisioned cell
    matches!(cell_info_for_role_2[0], CellInfo::Provisioned(_));
    // cell 2 in cell info is clone cell
    matches!(cell_info_for_role_2[1], CellInfo::Cloned(_));

    // clone cell ids match
    assert!(if let CellInfo::Cloned(cell) = &cell_info_for_role_1[1] {
        cell.cell_id == clone_cell_1.cell_id.clone()
    } else {
        false
    });

    assert!(if let CellInfo::Cloned(cell) = &cell_info_for_role_2[1] {
        cell.cell_id == clone_cell_2.cell_id.clone()
    } else {
        false
    });

    conductor.shutdown().await;
    conductor.startup().await;

    // make sure app info is identical after conductor restart
    let app_info_after_restart = conductor
        .clone()
        .get_app_info(&app_id)
        .await
        .unwrap()
        .unwrap();
    assert_eq!(app_info, app_info_after_restart);

    // make sure the re-enabled clone cell's original DNA hash matches
    // tests that the enable_clone_cell fn returns the right DNA hash
    let reenabled_clone_cell = conductor
        .clone()
        .enable_clone_cell(
            &app_id,
            &EnableCloneCellPayload {
                clone_cell_id: CloneCellId::DnaHash(clone_cell_2.cell_id.dna_hash().clone()),
            },
        )
        .await
        .unwrap();
    assert_eq!(
        reenabled_clone_cell.original_dna_hash,
        dna_2.dna_hash().clone()
    );
}



================================================
File: crates/holochain/src/conductor/tests/cell_cloning.rs
================================================
use crate::{
    conductor::{api::error::ConductorApiError, error::ConductorError, CellError},
    sweettest::*,
};
use holo_hash::ActionHash;
use holochain_conductor_api::CellInfo;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;
use matches::matches;

#[tokio::test(flavor = "multi_thread")]
async fn create_clone_cell_without_modifiers_fails() {
    let conductor = SweetConductor::from_standard_config().await;
    let result = conductor
        .clone()
        .create_clone_cell(
            &"".into(),
            CreateCloneCellPayload {
                role_name: "".to_string(),
                modifiers: DnaModifiersOpt::none(),
                membrane_proof: None,
                name: None,
            },
        )
        .await;
    assert!(result.is_err());
}

#[tokio::test(flavor = "multi_thread")]
async fn create_clone_cell_with_wrong_app_or_role_name_fails() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor
        .setup_app("app", [&(role_name.clone(), dna)])
        .await
        .unwrap();

    let result = conductor
        .clone()
        .create_clone_cell(
            &"wrong_app_id".into(),
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await;
    assert!(result.is_err());

    let result = conductor
        .clone()
        .create_clone_cell(
            app.installed_app_id(),
            CreateCloneCellPayload {
                role_name: "wrong_role_name".to_string(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await;
    assert!(result.is_err());
}

#[tokio::test(flavor = "multi_thread")]
async fn create_clone_cell_creates_callable_cell() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor
        .setup_app("app", [&(role_name.clone(), dna.clone())])
        .await
        .unwrap();

    let clone_name = "test_name".to_string();
    let clone_cell = conductor
        .clone()
        .create_clone_cell(
            app.installed_app_id(),
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed".to_string()),
                membrane_proof: None,
                name: Some(clone_name.clone()),
            },
        )
        .await
        .unwrap();
    assert!(clone_cell.enabled);
    assert_eq!(clone_cell.name, clone_name);

    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    assert!(zome_call_response.is_ok());
}

#[tokio::test(flavor = "multi_thread")]
async fn create_clone_cell_run_twice_returns_correct_clones() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor
        .setup_app("app", [&(role_name.clone(), dna.clone())])
        .await
        .unwrap();

    let clone_cell_0 = conductor
        .clone()
        .create_clone_cell(
            app.installed_app_id(),
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed_1".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
    assert_eq!(clone_cell_0.clone_id, CloneId::new(&role_name, 0)); // clone index starts at 0
    assert_eq!(clone_cell_0.original_dna_hash, dna.dna_hash().to_owned());

    let clone_cell_1 = conductor
        .clone()
        .create_clone_cell(
            app.installed_app_id(),
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed_2".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
    assert_eq!(clone_cell_1.clone_id, CloneId::new(&role_name, 1));
    assert_eq!(clone_cell_1.original_dna_hash, dna.dna_hash().to_owned());
}

#[tokio::test(flavor = "multi_thread")]
async fn create_identical_clone_cell_twice_fails() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let apps = conductor
        .setup_apps("app", 2, [&(role_name.clone(), dna.clone())])
        .await
        .unwrap()
        .into_inner();
    let alice_app = &apps[0];
    let bob_app = &apps[1];
    let clone_cell_payload = CreateCloneCellPayload {
        role_name: role_name.clone(),
        modifiers: DnaModifiersOpt::none().with_network_seed("seed".to_string()),
        membrane_proof: None,
        name: None,
    };

    let alice_clone_cell = conductor
        .clone()
        .create_clone_cell(alice_app.installed_app_id(), clone_cell_payload.clone())
        .await
        .unwrap();

    let identical_clone_cell_err = conductor
        .clone()
        .create_clone_cell(alice_app.installed_app_id(), clone_cell_payload.clone())
        .await;
    matches!(
        identical_clone_cell_err,
        Err(ConductorError::AppError(AppError::DuplicateCellId(cell_id))) if cell_id == alice_clone_cell.cell_id
    );

    // disable clone cell and try again to create an identical clone
    conductor
        .clone()
        .disable_clone_cell(
            alice_app.installed_app_id(),
            &DisableCloneCellPayload {
                clone_cell_id: CloneCellId::DnaHash(alice_clone_cell.cell_id.dna_hash().clone()),
            },
        )
        .await
        .unwrap();
    let identical_clone_cell_err = conductor
        .clone()
        .create_clone_cell(alice_app.installed_app_id(), clone_cell_payload.clone())
        .await;
    matches!(
        identical_clone_cell_err,
        Err(ConductorError::AppError(AppError::DuplicateCellId(cell_id))) if cell_id == alice_clone_cell.cell_id
    );

    // ensure that bob can clone cell with identical hash in same conductor
    let bob_clone_cell = conductor
        .clone()
        .create_clone_cell(bob_app.installed_app_id(), clone_cell_payload)
        .await
        .unwrap();
    assert_eq!(
        alice_clone_cell.original_dna_hash,
        bob_clone_cell.original_dna_hash
    );
    assert_eq!(
        alice_clone_cell.cell_id.dna_hash(),
        bob_clone_cell.cell_id.dna_hash()
    );
}

#[tokio::test(flavor = "multi_thread")]
async fn clone_cell_deletion() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app_id = "app".to_string();
    conductor
        .setup_app(&app_id, [&(role_name.clone(), dna)])
        .await
        .unwrap();
    let clone_cell = conductor
        .clone()
        .create_clone_cell(
            &app_id,
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed_1".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();
    let clone_id = CloneCellId::CloneId(clone_cell.clone().clone_id);

    // disable clone cell
    conductor
        .raw_handle()
        .disable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_id.clone(),
            },
        )
        .await
        .unwrap();

    // calling the cell after disabling fails
    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    assert!(zome_call_response.is_err());

    // enable the disabled clone cell by clone id
    let enabled_clone_cell = conductor
        .raw_handle()
        .enable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_id.clone(),
            },
        )
        .await
        .unwrap();

    // assert the enabled clone cell is the previously created clone cell
    assert_eq!(enabled_clone_cell, clone_cell);

    // assert that the cell appears in app info's cell data again
    let app_info = conductor.get_app_info(&app_id).await.unwrap().unwrap();
    let cell_info_for_role = app_info.cell_info.get(&role_name).unwrap();
    assert!(cell_info_for_role
        .iter()
        .any(|cell_info| if let CellInfo::Cloned(cell) = cell_info {
            cell.cell_id == clone_cell.cell_id.clone()
        } else {
            false
        }));

    // calling the cell after restoring succeeds
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    assert!(zome_call_response.is_ok());

    // disable clone cell again
    conductor
        .raw_handle()
        .disable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_id.clone(),
            },
        )
        .await
        .unwrap();

    // enable clone cell by cell id
    let enabled_clone_cell = conductor
        .raw_handle()
        .enable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: CloneCellId::DnaHash(clone_cell.cell_id.dna_hash().clone()),
            },
        )
        .await
        .unwrap();

    // assert the enabled clone cell is the previously created clone cell
    assert_eq!(enabled_clone_cell, clone_cell);

    // disable and delete clone cell
    conductor
        .raw_handle()
        .disable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_id.clone(),
            },
        )
        .await
        .unwrap();
    conductor
        .raw_handle()
        .delete_clone_cell(&DeleteCloneCellPayload {
            app_id: app_id.clone(),
            clone_cell_id: CloneCellId::DnaHash(clone_cell.cell_id.dna_hash().clone()),
        })
        .await
        .unwrap();
    // assert the deleted cell cannot be enabled
    let disable_result = conductor
        .raw_handle()
        .enable_clone_cell(
            &app_id,
            &DisableCloneCellPayload {
                clone_cell_id: clone_id.clone(),
            },
        )
        .await;
    assert!(disable_result.is_err());
}

#[tokio::test(flavor = "multi_thread")]
async fn conductor_can_startup_with_cloned_cell() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let role_name: RoleName = "dna_1".to_string();
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor
        .setup_app("app", [&(role_name.clone(), dna)])
        .await
        .unwrap();

    let clone_cell = conductor
        .clone()
        .create_clone_cell(
            app.installed_app_id(),
            CreateCloneCellPayload {
                role_name: role_name.clone(),
                modifiers: DnaModifiersOpt::none().with_network_seed("seed_1".to_string()),
                membrane_proof: None,
                name: None,
            },
        )
        .await
        .unwrap();

    // calling the cell works
    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    assert!(zome_call_response.is_ok());

    conductor.shutdown().await;
    conductor.startup().await;

    // calling the cell works after restart
    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    assert!(zome_call_response.is_ok());

    conductor
        .clone()
        .disable_clone_cell(
            app.installed_app_id(),
            &DisableCloneCellPayload {
                clone_cell_id: CloneCellId::DnaHash(clone_cell.cell_id.dna_hash().clone()),
            },
        )
        .await
        .unwrap();

    // calling the cell after disabling fails
    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    matches!(zome_call_response, Err(ConductorApiError::CellError(CellError::CellDisabled(cell_id))) if cell_id == clone_cell.cell_id.clone());

    conductor.shutdown().await;
    conductor.startup().await;

    // calling the cell still fails after restart, cell still disabled
    let zome = SweetZome::new(
        clone_cell.cell_id.clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let zome_call_response: Result<ActionHash, _> = conductor
        .call_fallible(&zome, "call_create_entry", ())
        .await;
    matches!(zome_call_response, Err(ConductorApiError::CellError(CellError::CellDisabled(cell_id))) if cell_id == clone_cell.cell_id.clone());
}



================================================
File: crates/holochain/src/conductor/tests/install_app_bundle.rs
================================================
use std::collections::{BTreeSet, HashMap};
use std::path::PathBuf;

use crate::{conductor::error::ConductorError, sweettest::*};
use holo_hash::DnaHash;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;
use maplit::btreeset;
use matches::assert_matches;

#[tokio::test(flavor = "multi_thread")]
async fn clone_only_provisioning_creates_no_cell_and_allows_cloning() {
    holochain_trace::test_run();

    let mut conductor = SweetConductor::from_standard_config().await;

    async fn make_payload(clone_limit: u32) -> InstallAppPayload {
        // The integrity zome in this WASM will fail if the properties are not set. This helps verify that genesis
        // is not being run for the clone-only cell and will only run for the cloned cells.
        let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![
            TestWasm::GenesisSelfCheckRequiresProperties,
        ])
        .await;
        let path = PathBuf::from(format!("{}", dna.dna_hash()));
        let modifiers = DnaModifiersOpt::none();

        let roles = vec![AppRoleManifest {
            name: "name".into(),
            dna: AppRoleDnaManifest {
                location: Some(DnaLocation::Bundled(path.clone())),
                modifiers: modifiers.clone(),
                installed_hash: None,
                clone_limit,
            },
            provisioning: Some(CellProvisioning::CloneOnly),
        }];

        let manifest = AppManifestCurrentBuilder::default()
            .name("test_app".into())
            .description(None)
            .roles(roles)
            .build()
            .unwrap();
        let dna_bundle = DnaBundle::from_dna_file(dna.clone()).unwrap();
        let resources = vec![(path.clone(), dna_bundle)];
        let bundle = AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
            .await
            .unwrap();

        let bundle_bytes = bundle.encode().unwrap();
        InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle_bytes),
            installed_app_id: Some("app_1".into()),
            network_seed: None,
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        }
    }

    // Fails due to clone limit of 0
    assert_matches!(
        conductor
            .clone()
            .install_app_bundle(make_payload(0).await)
            .await
            .unwrap_err(),
        ConductorError::AppBundleError(AppBundleError::AppManifestError(
            AppManifestError::InvalidStrategyCloneOnly(_)
        ))
    );

    {
        // Succeeds with clone limit of 1
        let app = conductor
            .clone()
            .install_app_bundle(make_payload(1).await)
            .await
            .unwrap();

        // No cells in this app due to CloneOnly provisioning strategy
        assert_eq!(app.all_cells().count(), 0);
        assert_eq!(app.role_assignments().len(), 1);
    }
    {
        let clone_cell = conductor
            .create_clone_cell(
                &"app_1".into(),
                CreateCloneCellPayload {
                    role_name: "name".into(),
                    modifiers: DnaModifiersOpt::none()
                        .with_network_seed("1".into())
                        .with_properties(YamlProperties::new(serde_yaml::Value::String(
                            "foo".into(),
                        ))),
                    membrane_proof: None,
                    name: Some("Johnny".into()),
                },
            )
            .await
            .unwrap();

        let state = conductor.get_state().await.unwrap();
        let app = state.get_app(&"app_1".to_string()).unwrap();

        assert_eq!(clone_cell.name, "Johnny".to_string());
        assert_eq!(app.role_assignments().len(), 1);
        assert_eq!(app.clone_cells().count(), 1);
    }
    {
        let err = conductor
            .create_clone_cell(
                &"app_1".into(),
                CreateCloneCellPayload {
                    role_name: "name".into(),
                    modifiers: DnaModifiersOpt::none()
                        .with_network_seed("1".into())
                        .with_properties(YamlProperties::new(serde_yaml::Value::String(
                            "foo".into(),
                        ))),
                    membrane_proof: None,
                    name: None,
                },
            )
            .await
            .unwrap_err();
        assert_matches!(
            err,
            ConductorError::AppError(AppError::CloneLimitExceeded(1, _))
        );
        let state = conductor.get_state().await.unwrap();
        let app = state.get_app(&"app_1".to_string()).unwrap();

        assert_eq!(app.all_cells().count(), 1);
    }
    // TODO: test that the cell can't be provisioned later
}

#[tokio::test(flavor = "multi_thread")]
async fn reject_duplicate_app_for_same_agent() {
    let conductor = SweetConductor::from_standard_config().await;

    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let path = PathBuf::from(format!("{}", dna.dna_hash()));
    let modifiers = DnaModifiersOpt::none();

    let roles = vec![AppRoleManifest {
        name: "name".into(),
        dna: AppRoleDnaManifest {
            location: Some(DnaLocation::Bundled(path.clone())),
            modifiers: modifiers.clone(),
            installed_hash: None,
            clone_limit: 0,
        },
        provisioning: Some(CellProvisioning::Create { deferred: false }),
    }];

    let manifest = AppManifestCurrentBuilder::default()
        .name("test_app".into())
        .description(None)
        .roles(roles)
        .build()
        .unwrap();
    let resources = vec![(path.clone(), DnaBundle::from_dna_file(dna.clone()).unwrap())];
    let bundle = AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
        .await
        .unwrap();

    let bundle_bytes = bundle.encode().unwrap();
    let app = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle_bytes),
            installed_app_id: Some("app_1".into()),
            network_seed: None,
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();
    let alice = app.agent_key().clone();

    let cell_id = CellId::new(dna.dna_hash().to_owned(), app.agent_key().clone());

    let resources = vec![(path.clone(), DnaBundle::from_dna_file(dna.clone()).unwrap())];
    let bundle = AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
        .await
        .unwrap();
    let bundle_bytes = bundle.encode().unwrap();
    let duplicate_install_with_app_disabled = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            source: AppBundleSource::Bytes(bundle_bytes),
            agent_key: Some(alice.clone()),
            installed_app_id: Some("app_2".into()),
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
            network_seed: None,
        })
        .await;
    assert_matches!(
        duplicate_install_with_app_disabled.unwrap_err(),
        ConductorError::CellAlreadyExists(id) if id == cell_id
    );

    // enable app
    conductor.enable_app("app_1".into()).await.unwrap();

    let resources = vec![(path.clone(), DnaBundle::from_dna_file(dna.clone()).unwrap())];
    let bundle = AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
        .await
        .unwrap();
    let bundle_bytes = bundle.encode().unwrap();
    let duplicate_install_with_app_enabled = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            source: AppBundleSource::Bytes(bundle_bytes),
            agent_key: Some(alice.clone()),
            installed_app_id: Some("app_2".into()),
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
            network_seed: None,
        })
        .await;
    assert_matches!(
        duplicate_install_with_app_enabled.unwrap_err(),
        ConductorError::CellAlreadyExists(id) if id == cell_id
    );

    let resources = vec![(path, DnaBundle::from_dna_file(dna.clone()).unwrap())];
    let bundle = AppBundle::new(manifest.into(), resources, PathBuf::from("."))
        .await
        .unwrap();
    let bundle_bytes = bundle.encode().unwrap();
    let valid_install_of_second_app = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            source: AppBundleSource::Bytes(bundle_bytes),
            agent_key: Some(alice.clone()),
            installed_app_id: Some("app_2".into()),
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
            network_seed: Some("network".into()),
        })
        .await;
    assert!(valid_install_of_second_app.is_ok());
}

#[tokio::test(flavor = "multi_thread")]
async fn can_install_app_a_second_time_using_nothing_but_the_manifest_from_app_info() {
    let conductor = SweetConductor::from_standard_config().await;

    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let path = PathBuf::from(format!("{}", dna.dna_hash()));
    let modifiers = DnaModifiersOpt::default()
        .with_network_seed("initial seed".into())
        .with_origin_time(Timestamp::now());

    let roles = vec![AppRoleManifest {
        name: "name".into(),
        dna: AppRoleDnaManifest {
            location: Some(DnaLocation::Bundled(path.clone())),
            modifiers: modifiers.clone(),
            // Note that there is no installed hash provided. We'll check that this changes later.
            installed_hash: None,
            clone_limit: 0,
        },
        provisioning: Some(CellProvisioning::Create { deferred: false }),
    }];

    let manifest = AppManifestCurrentBuilder::default()
        .name("test_app".into())
        .description(None)
        .roles(roles)
        .build()
        .unwrap();

    let resources = vec![(path.clone(), DnaBundle::from_dna_file(dna.clone()).unwrap())];

    let bundle = AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
        .await
        .unwrap();

    let bundle_bytes = bundle.encode().unwrap();
    conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle_bytes),
            installed_app_id: Some("app_1".into()),
            network_seed: Some("final seed".into()),
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();

    let manifest = conductor
        .get_app_info(&"app_1".to_string())
        .await
        .unwrap()
        .unwrap()
        .manifest;

    let installed_dna = dna.update_modifiers(
        modifiers
            .with_network_seed("final seed".into())
            .serialized()
            .unwrap(),
    );
    let installed_dna_hash = DnaHash::with_data_sync(installed_dna.dna_def());

    // Check that the returned manifest has the installed DNA hash properly set
    assert_eq!(
        manifest.app_roles()[0].dna.installed_hash,
        Some(installed_dna_hash.into())
    );

    assert_eq!(
        manifest.app_roles()[0].dna.modifiers.network_seed,
        Some("final seed".into())
    );

    let bundle = AppBundle::new(manifest, vec![], PathBuf::from("."))
        .await
        .unwrap();

    let bundle_bytes = bundle.encode().unwrap();
    conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle_bytes),
            installed_app_id: Some("app_2".into()),
            network_seed: None,
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();
}

#[tokio::test(flavor = "multi_thread")]
async fn cells_by_dna_lineage() {
    let mut conductor = SweetConductor::from_standard_config().await;

    async fn mk_dna(lineage: &[&DnaHash]) -> DnaFile {
        let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
        let (def, code) = dna.into_parts();
        let mut def = def.into_content();
        def.lineage = lineage.iter().map(|h| (**h).to_owned()).collect();
        DnaFile::from_parts(def.into_hashed(), code)
    }

    // The lineage of a DNA includes the DNA itself
    let dna1 = mk_dna(&[]).await;
    let dna2 = mk_dna(&[dna1.dna_hash()]).await;
    let dna3 = mk_dna(&[dna1.dna_hash(), dna2.dna_hash()]).await;
    // dna1 is removed from the lineage
    let dna4 = mk_dna(&[dna2.dna_hash(), dna3.dna_hash()]).await;
    let dnax = mk_dna(&[]).await;

    let app1 = conductor.setup_app("app1", [&dna1, &dnax]).await.unwrap();
    let app2 = conductor.setup_app("app2", [&dna2]).await.unwrap();
    let app3 = conductor.setup_app("app3", [&dna3]).await.unwrap();
    let app4 = conductor.setup_app("app4", [&dna4]).await.unwrap();

    let lin1 = conductor
        .cells_by_dna_lineage(dna1.dna_hash())
        .await
        .unwrap();
    let lin2 = conductor
        .cells_by_dna_lineage(dna2.dna_hash())
        .await
        .unwrap();
    let lin3 = conductor
        .cells_by_dna_lineage(dna3.dna_hash())
        .await
        .unwrap();
    let lin4 = conductor
        .cells_by_dna_lineage(dna4.dna_hash())
        .await
        .unwrap();
    let linx = conductor
        .cells_by_dna_lineage(dnax.dna_hash())
        .await
        .unwrap();

    fn app_cells(app: &SweetApp, indices: &[usize]) -> (String, BTreeSet<CellId>) {
        (
            app.installed_app_id().clone(),
            indices
                .iter()
                .map(|i| app.cells()[*i].cell_id().clone())
                .collect(),
        )
    }

    pretty_assertions::assert_eq!(
        lin1,
        btreeset![
            app_cells(&app1, &[0]),
            app_cells(&app2, &[0]),
            app_cells(&app3, &[0]),
            // no dna4: dna1 was "removed"
        ]
    );
    pretty_assertions::assert_eq!(
        lin2,
        btreeset![
            // no dna1: it's in the past
            app_cells(&app2, &[0]),
            app_cells(&app3, &[0]),
            app_cells(&app4, &[0]),
        ]
    );
    pretty_assertions::assert_eq!(
        lin3,
        btreeset![
            // no dna1 or dna2: they're in the past
            app_cells(&app3, &[0]),
            app_cells(&app4, &[0]),
        ]
    );
    pretty_assertions::assert_eq!(
        lin4,
        btreeset![
            // all other dnas are in the past
            app_cells(&app4, &[0]),
        ]
    );
    pretty_assertions::assert_eq!(linx, btreeset![app_cells(&app1, &[1]),]);
}

#[tokio::test(flavor = "multi_thread")]
async fn use_existing_integration() {
    let conductor = SweetConductor::from_standard_config().await;

    let (dna1, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::WhoAmI]).await;
    let (dna2, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::WhoAmI]).await;

    let bundle1 = {
        let path = PathBuf::from(format!("{}", dna1.dna_hash()));

        let roles = vec![AppRoleManifest {
            name: "created".into(),
            dna: AppRoleDnaManifest {
                location: Some(DnaLocation::Bundled(path.clone())),
                modifiers: DnaModifiersOpt::none(),
                installed_hash: None,
                clone_limit: 0,
            },
            provisioning: Some(CellProvisioning::Create { deferred: false }),
        }];

        let manifest = AppManifestCurrentBuilder::default()
            .name("test_app".into())
            .description(None)
            .roles(roles)
            .build()
            .unwrap();

        let resources = vec![(
            path.clone(),
            DnaBundle::from_dna_file(dna1.clone()).unwrap(),
        )];
        AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
            .await
            .unwrap()
            .encode()
            .unwrap()
    };

    let bundle2 = |correct: bool| {
        let dna2 = dna2.clone();
        async move {
            let path = PathBuf::from(format!("{}", dna2.dna_hash()));
            let installed_hash = if correct {
                Some(dna2.dna_hash().clone().into())
            } else {
                None
            };

            let roles = vec![
                AppRoleManifest {
                    name: "created".into(),
                    dna: AppRoleDnaManifest {
                        location: Some(DnaLocation::Bundled(path.clone())),
                        modifiers: DnaModifiersOpt::none(),
                        installed_hash: None,
                        clone_limit: 0,
                    },
                    provisioning: Some(CellProvisioning::Create { deferred: false }),
                },
                AppRoleManifest {
                    name: "extant".into(),
                    dna: AppRoleDnaManifest {
                        location: None,
                        modifiers: DnaModifiersOpt::none(),
                        installed_hash,
                        clone_limit: 0,
                    },
                    provisioning: Some(CellProvisioning::UseExisting { protected: true }),
                },
            ];

            let manifest = AppManifestCurrentBuilder::default()
                .name("test_app".into())
                .description(None)
                .roles(roles)
                .build()
                .unwrap();

            let resources = vec![(
                path.clone(),
                DnaBundle::from_dna_file(dna2.clone()).unwrap(),
            )];
            AppBundle::new(manifest.clone().into(), resources, PathBuf::from("."))
                .await
                .unwrap()
                .encode()
                .unwrap()
        }
    };

    // Install the "dependency" app
    let app_1 = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle1),
            installed_app_id: Some("app_1".into()),
            network_seed: None,
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();

    {
        // Fail to install the "dependent" app because the dependent DNA hash is not set in the manifest
        let err = conductor
            .clone()
            .install_app_bundle(InstallAppPayload {
                agent_key: None,
                source: AppBundleSource::Bytes(bundle2(false).await),
                installed_app_id: Some("app_2".into()),
                network_seed: None,
                roles_settings: Default::default(),
                ignore_genesis_failure: false,
                allow_throwaway_random_agent_key: true,
            })
            .await
            .unwrap_err();

        assert!(matches!(
            err,
            ConductorError::AppBundleError(AppBundleError::AppManifestError(_))
        ));
    }
    {
        // Fail to install the dependent app because the existing CellId is not specified
        let err = conductor
            .clone()
            .install_app_bundle(InstallAppPayload {
                agent_key: None,
                source: AppBundleSource::Bytes(bundle2(true).await),
                installed_app_id: Some("app_2".into()),
                network_seed: None,
                roles_settings: Default::default(),
                ignore_genesis_failure: false,
                allow_throwaway_random_agent_key: true,
            })
            .await
            .unwrap_err();

        assert!(matches!(
            err,
            ConductorError::AppBundleError(AppBundleError::CellResolutionFailure(_, _))
        ));
    }

    // Get the existing cell id through the normal means
    let appmap = conductor
        .cells_by_dna_lineage(dna1.dna_hash())
        .await
        .unwrap();
    assert_eq!(appmap.len(), 1);
    let (app_name, cells) = appmap.first().unwrap();
    assert_eq!(app_name, "app_1");
    assert_eq!(cells.len(), 1);
    let cell_id = cells.first().unwrap().clone();

    let role_settings = ("extant".into(), RoleSettings::UseExisting { cell_id });

    let app_2 = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle2(true).await),
            installed_app_id: Some("app_2".into()),
            network_seed: None,
            roles_settings: Some(HashMap::from([role_settings])),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();

    let cell_id_1 = app_1.all_cells().next().unwrap().clone();
    let cell_id_2 = app_2.all_cells().next().unwrap().clone();
    let zome2 = SweetZome::new(cell_id_2.clone(), "whoami".into());

    conductor.enable_app("app_1".into()).await.unwrap();
    conductor.enable_app("app_2".into()).await.unwrap();
    {
        // - Call the existing dependency cell via the dependent cell, which fails
        // because the proper capability has not been granted
        let r: Result<AgentInfo, _> = conductor
            .call_fallible(&zome2, "who_are_they_role", "extant".to_string())
            .await;
        assert!(r.is_err());
    }

    {
        // - Grant the capability
        let secret = CapSecret::from([1; 64]);
        conductor
            .grant_zome_call_capability(GrantZomeCallCapabilityPayload {
                cell_id: cell_id_1.clone(),
                cap_grant: ZomeCallCapGrant {
                    tag: "tag".into(),
                    // access: CapAccess::Unrestricted,
                    access: CapAccess::Transferable { secret },
                    functions: GrantedFunctions::All,
                },
            })
            .await
            .unwrap();

        // - Call the existing dependency cell via the dependent cell
        let r: AgentInfo = conductor
            .call_from_fallible(
                cell_id_2.agent_pubkey(),
                None,
                &zome2,
                "who_are_they_role_secret",
                ("extant".to_string(), Some(secret)),
            )
            .await
            .unwrap();
        assert_eq!(r.agent_initial_pubkey, *cell_id_1.agent_pubkey());
    }

    // Ideally, we shouldn't be able to disable app_1 because it's depended on by enabled app_2.
    // For now, we are just emitting warnings about this.
    conductor
        .disable_app("app_1".into(), DisabledAppReason::User)
        .await
        .unwrap();
    conductor
        .disable_app("app_2".into(), DisabledAppReason::User)
        .await
        .unwrap();
    conductor
        .disable_app("app_1".into(), DisabledAppReason::User)
        .await
        .unwrap();

    // Can't uninstall app because of dependents
    let err = conductor
        .clone()
        .uninstall_app(&"app_1".to_string(), false)
        .await
        .unwrap_err();
    assert_matches!(
        err,
        ConductorError::AppHasDependents(a, b) if a == *"app_1" && b == vec!["app_2".to_string()]
    );

    // Can still uninstall app with force
    conductor
        .clone()
        .uninstall_app(&"app_1".to_string(), true)
        .await
        .unwrap();
}



================================================
File: crates/holochain/src/conductor/tests/mod.rs
================================================
mod app_info;
mod cell_cloning;
mod install_app_bundle;
mod network_info;
mod network_seed_regression;
mod request_dna_def;
mod signed_zome_call;



================================================
File: crates/holochain/src/conductor/tests/network_info.rs
================================================
use holo_hash::ActionHash;
use holochain_types::prelude::{InstalledAppId, NetworkInfoRequestPayload};
use holochain_wasm_test_utils::TestWasm;
use holochain_zome_types::prelude::Timestamp;

use crate::sweettest::*;

#[tokio::test(flavor = "multi_thread")]
async fn network_info() {
    holochain_trace::test_run();

    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let number_of_peers = 3;
    let config = SweetConductorConfig::standard().no_dpki();
    let mut conductors = SweetConductorBatch::from_config(number_of_peers, config).await;
    let app_id: InstalledAppId = "app".into();
    let app_batch = conductors.setup_app(&app_id, &[dna.clone()]).await.unwrap();
    let cells = app_batch.cells_flattened();
    let apps = app_batch.into_inner();
    let alice_app = &apps[0];
    let bob_app = &apps[1];

    conductors.exchange_peer_info().await;

    // query since beginning of unix epoch
    let payload = NetworkInfoRequestPayload {
        agent_pub_key: alice_app.agent().clone(),
        dnas: vec![dna.dna_hash().clone()],
        last_time_queried: None,
    };
    let network_info = conductors[0].network_info(&app_id, &payload).await.unwrap();

    assert_eq!(network_info[0].current_number_of_peers, 3);
    assert_eq!(network_info[0].arc_size, 1.0);
    assert_eq!(network_info[0].total_network_peers, 3);
    assert_eq!(network_info[0].completed_rounds_since_last_time_queried, 0);
    assert!(network_info[0].bytes_since_last_time_queried > 0);

    // query since previous query should return 0 received bytes
    let last_time_queried = Timestamp::now();
    let payload = NetworkInfoRequestPayload {
        agent_pub_key: alice_app.agent().clone(),
        dnas: vec![dna.dna_hash().clone()],
        last_time_queried: Some(last_time_queried),
    };
    let network_info = conductors[0].network_info(&app_id, &payload).await.unwrap();

    assert_eq!(network_info[0].bytes_since_last_time_queried, 0);

    let cell = alice_app.cells()[0].clone();
    // alice creates one entry
    let zome = SweetZome::new(
        cell.cell_id().clone(),
        TestWasm::Create.coordinator_zome_name(),
    );
    let _: ActionHash = conductors[0].call(&zome, "create_entry", ()).await;

    await_consistency(10, &cells).await.unwrap();

    // wait_for_integration(
    //     &conductors[1].get_dht_db(dna.dna_hash()).unwrap(),
    //     28,
    //     100,
    //     std::time::Duration::from_millis(100),
    // )
    // .await;

    // query bob's DB for bytes since last time queried
    let payload = NetworkInfoRequestPayload {
        agent_pub_key: bob_app.agent().clone(),
        dnas: vec![dna.dna_hash().clone()],
        last_time_queried: Some(last_time_queried),
    };
    let network_info = conductors[1].network_info(&app_id, &payload).await.unwrap();
    assert!(network_info[0].bytes_since_last_time_queried > 0);
}



================================================
File: crates/holochain/src/conductor/tests/network_seed_regression.rs
================================================
// `cargo clippy --tests` emits warnings without this
#![allow(dead_code)]

use ::fixt::prelude::strum_macros;
use std::{fmt::Display, path::PathBuf};
use tempfile::{tempdir, TempDir};

use crate::sweettest::*;
use holochain_types::prelude::*;
use holochain_wasm_test_utils::TestWasm;

#[tokio::test(flavor = "multi_thread")]
async fn network_seed_regression() {
    let conductor = SweetConductor::from_standard_config().await;
    let tmp = tempdir().unwrap();
    let (dna, _, _) = SweetDnaFile::from_test_wasms(
        "".into(),
        vec![TestWasm::Create],
        holochain_serialized_bytes::SerializedBytes::default(),
    )
    .await;

    let dna_path = tmp.as_ref().join("the.dna");
    DnaBundle::from_dna_file(dna)
        .unwrap()
        .write_to_file(&dna_path)
        .await
        .unwrap();

    let manifest = {
        let roles = vec![AppRoleManifest {
            name: "rolename".into(),
            dna: AppRoleDnaManifest {
                location: Some(DnaLocation::Path(dna_path)),
                modifiers: DnaModifiersOpt::default(),
                installed_hash: None,
                clone_limit: 0,
            },
            provisioning: None,
        }];

        AppManifestCurrentBuilder::default()
            .name("app".into())
            .description(None)
            .roles(roles)
            .build()
            .unwrap()
    };

    let bundle1 = AppBundle::new(manifest.clone().into(), vec![], PathBuf::from("."))
        .await
        .unwrap()
        .encode()
        .unwrap();
    let bundle2 = AppBundle::new(manifest.into(), vec![], PathBuf::from("."))
        .await
        .unwrap()
        .encode()
        .unwrap();

    // if both of these apps can be installed under the same agent, the
    // network seed change was successful -- otherwise there will be a
    // CellAlreadyInstalled error.

    let _app1 = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle1),
            installed_app_id: Some("no-seed".into()),
            network_seed: None,
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();

    let _app2 = conductor
        .clone()
        .install_app_bundle(InstallAppPayload {
            agent_key: None,
            source: AppBundleSource::Bytes(bundle2),
            installed_app_id: Some("yes-seed".into()),
            network_seed: Some("seed".into()),
            roles_settings: Default::default(),
            ignore_genesis_failure: false,
            allow_throwaway_random_agent_key: true,
        })
        .await
        .unwrap();
}

/// Test all possible combinations of Locations and network seeds:
#[tokio::test(flavor = "multi_thread")]
async fn network_seed_affects_dna_hash_when_app_bundle_is_installed() {
    let conductor = SweetConductor::from_standard_config().await;
    let tmp = tempdir().unwrap();
    let (dna, _, _) = SweetDnaFile::from_test_wasms(
        "".to_string(),
        vec![TestWasm::Create],
        holochain_serialized_bytes::SerializedBytes::default(),
    )
    .await;

    let write_dna = |seed: Seed| {
        let mut dna = dna.clone();
        let path = tmp.as_ref().join(format!("{seed}.dna"));
        async move {
            if seed != Seed::None {
                dna = dna.with_network_seed(seed.to_string()).await;
            }
            DnaBundle::from_dna_file(dna.clone())
                .unwrap()
                .write_to_file(&path)
                .await
                .unwrap();
            dna
        }
    };

    let dnas = futures::future::join_all(vec![write_dna(None), write_dna(A), write_dna(B)]).await;

    let c = TestcaseCommon {
        conductor,
        dnas: dnas.clone(),
        tmp,
        _start: std::time::Instant::now(),
    };

    use Location::*;
    use Seed::*;

    let both = [Bundle, Path];

    let all_locs = both.iter().flat_map(|a| both.iter().map(|b| (*a, *b)));

    // Build up two equality groups. All outcomes in each group should have equal hashes,
    // and each group's hash should be different from the other group's hash.

    // Hashes when using empty network seed
    let mut group_0 = vec![];
    // Hashes when using network seed "A"
    let mut group_a = vec![];
    // There is no need for a group_b since "A" and "B" are essentially interchangeable

    // Populate the groups with all (most) possible combinations of seed values and location specifiers
    for (app_loc, dna_loc) in all_locs {
        group_0.extend([TestCase(None, None, None, app_loc, dna_loc)
            .install(&c)
            .await]);
        group_a.extend([
            TestCase(A, None, None, app_loc, dna_loc).install(&c).await,
            TestCase(None, A, None, app_loc, dna_loc).install(&c).await,
            TestCase(None, None, A, app_loc, dna_loc).install(&c).await,
            //
            TestCase(A, A, None, app_loc, dna_loc).install(&c).await,
            TestCase(A, None, A, app_loc, dna_loc).install(&c).await,
            TestCase(None, A, A, app_loc, dna_loc).install(&c).await,
            //
            TestCase(A, B, None, app_loc, dna_loc).install(&c).await,
            TestCase(A, None, B, app_loc, dna_loc).install(&c).await,
            TestCase(None, A, B, app_loc, dna_loc).install(&c).await,
            //
            TestCase(A, B, B, app_loc, dna_loc).install(&c).await,
        ]);
    }

    // It would be preferable to use join_all here to let all installations happen
    // in parallel, but it causes timeouts in macos tests. If it's ever determined
    // that we can parallelize this again, just remove all `.await` in the
    // above group construction and use join_all here to await them all.
    //
    // let group_0 = futures::future::join_all(group_0).await;
    // let group_a = futures::future::join_all(group_a).await;

    let (hash_0, case_0) = &group_0[0];
    let (hash_a, case_a) = &group_a[0];

    dbg!(mapvec(dnas.iter(), |d| d.dna_hash()));
    dbg!(&hash_0, mapvec(group_0.iter(), |(h, c)| (h, c.to_string())));
    dbg!(&hash_a, mapvec(group_a.iter(), |(h, c)| (h, c.to_string())));

    assert_eq!(hash_0, dnas[0].dna_hash());
    assert_eq!(hash_a, dnas[1].dna_hash());
    assert_ne!(hash_0, hash_a);

    for (h, c) in group_0.iter() {
        assert_eq!(hash_0, h, "case mismatch: {case_0}, {c}");
    }
    for (h, c) in group_a.iter() {
        assert_eq!(hash_a, h, "case mismatch: {case_a}, {c}");
    }
}

struct TestcaseCommon {
    conductor: SweetConductor,
    dnas: Vec<DnaFile>,
    tmp: TempDir,
    _start: std::time::Instant,
}

#[derive(Clone, Copy, Debug, strum_macros::Display)]
enum Location {
    Path,
    Bundle,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, strum_macros::Display)]
enum Seed {
    None,
    A,
    B,
}

#[derive(Debug)]
struct TestCase(Seed, Seed, Seed, Location, Location);

impl Display for TestCase {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}-{}-{}-{}-{}", self.0, self.1, self.2, self.3, self.4,)
    }
}

impl TestCase {
    async fn install(self, common: &TestcaseCommon) -> (DnaHash, Self) {
        let case = self;
        let case_str = case.to_string();
        let TestCase(app_seed, role_seed, dna_seed, app_loc, dna_loc) = case;
        let dna = match dna_seed {
            Seed::None => common.dnas[0].clone(),
            Seed::A => common.dnas[1].clone(),
            Seed::B => common.dnas[2].clone(),
        };
        let dna_hash = dna.dna_hash();
        let agent_key = Some(SweetAgents::one(common.conductor.keystore()).await);

        let dna_modifiers = match role_seed {
            Seed::None => DnaModifiersOpt::none(),
            Seed::A => DnaModifiersOpt::none().with_network_seed(Seed::A.to_string()),
            Seed::B => DnaModifiersOpt::none().with_network_seed(Seed::B.to_string()),
        };

        let dna_path = common.tmp.as_ref().join(format!("{dna_seed}.dna"));

        let bundle = match dna_loc {
            Location::Bundle => {
                let hashpath = PathBuf::from(dna_hash.to_string());
                let roles = vec![AppRoleManifest {
                    name: "rolename".into(),
                    dna: AppRoleDnaManifest {
                        location: Some(DnaLocation::Bundled(hashpath.clone())),
                        modifiers: dna_modifiers.clone(),
                        installed_hash: None,
                        clone_limit: 10,
                    },
                    provisioning: Some(CellProvisioning::Create { deferred: false }),
                }];
                let manifest = AppManifestCurrentBuilder::default()
                    .name(case_str.clone())
                    .description(None)
                    .roles(roles)
                    .build()
                    .unwrap();
                let resources = vec![(hashpath, DnaBundle::from_dna_file(dna.clone()).unwrap())];

                AppBundle::new(manifest.into(), resources, PathBuf::from("."))
                    .await
                    .unwrap()
            }
            Location::Path => {
                // use path
                let roles = vec![AppRoleManifest {
                    name: "rolename".into(),
                    dna: AppRoleDnaManifest {
                        location: Some(DnaLocation::Path(dna_path.clone())),
                        modifiers: dna_modifiers.clone(),
                        installed_hash: Some(dna_hash.clone().into()),
                        clone_limit: 0,
                    },
                    provisioning: None,
                }];

                let manifest = AppManifestCurrentBuilder::default()
                    .name(case_str.clone())
                    .description(None)
                    .roles(roles)
                    .build()
                    .unwrap();
                AppBundle::new(manifest.into(), vec![], PathBuf::from("."))
                    .await
                    .unwrap()
            }
        };

        let network_seed = match app_seed {
            Seed::None => None,
            Seed::A => Some(Seed::A.to_string()),
            Seed::B => Some(Seed::B.to_string()),
        };

        let source = match app_loc {
            Location::Path => {
                // Unnecessary duplication, but it's ok.
                let happ_path = common.tmp.as_ref().join(&case_str);
                bundle.write_to_file(&happ_path).await.unwrap();
                AppBundleSource::Path(happ_path)
            }
            Location::Bundle => {
                let bundle_bytes = bundle.encode().unwrap();
                AppBundleSource::Bytes(bundle_bytes)
            }
        };

        let app = common
            .conductor
            .clone()
            .install_app_bundle(InstallAppPayload {
                agent_key,
                source,
                installed_app_id: Some(case_str.clone()),
                network_seed,
                roles_settings: Default::default(),
                ignore_genesis_failure: false,
                allow_throwaway_random_agent_key: true,
            })
            .await
            .unwrap();

        let installed_hash = app.all_cells().next().unwrap().dna_hash().clone();
        (installed_hash, case)
    }
}



================================================
File: crates/holochain/src/conductor/tests/request_dna_def.rs
================================================
use holochain_wasm_test_utils::TestWasm;

use crate::sweettest::{SweetConductor, SweetDnaFile};

#[tokio::test(flavor = "multi_thread")]
async fn request_dna_def() {
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![TestWasm::Create]).await;
    let mut conductor = SweetConductor::from_standard_config().await;
    conductor.setup_app("app", [&dna]).await.unwrap();

    let dna_def = conductor.get_dna_def(dna.dna_hash());

    assert!(dna_def.is_some());
    assert!(dna_def.unwrap() == *dna.dna_def());
}



================================================
File: crates/holochain/src/conductor/tests/signed_zome_call.rs
================================================
use crate::fixt::AgentPubKeyFixturator;
use crate::sweettest::{SweetConductor, SweetDnaFile};
use ::fixt::fixt;
use holochain_nonce::fresh_nonce;
use holochain_nonce::Nonce256Bits;
use holochain_state::source_chain::SourceChainRead;
use holochain_wasm_test_utils::TestWasm;
use holochain_zome_types::prelude::*;
use matches::assert_matches;
use std::collections::BTreeSet;

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "test_utils")]
async fn signed_zome_call() {
    let zome = TestWasm::Create;
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![zome]).await;
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor.setup_app("app", [&dna]).await.unwrap();
    let cell_id = app.cells()[0].cell_id();
    let agent_pub_key = cell_id.agent_pubkey().clone();

    // generate a cap access public key
    let cap_access_public_key = fixt!(AgentPubKey, ::fixt::Predictable, 1);

    // compute a cap access secret
    let cap_access_secret = fixt!(CapSecret);

    // set up functions to grant access to
    let mut functions = BTreeSet::new();
    let granted_function: GrantedFunction = ("create_entry".into(), "get_entry".into());
    functions.insert(granted_function.clone());
    let granted_functions = GrantedFunctions::Listed(functions);
    // set up assignees which is only the agent key
    let mut assignees = BTreeSet::new();
    assignees.insert(cap_access_public_key.clone());

    let cap_grant = ZomeCallCapGrant {
        tag: "signing_key".into(),
        functions: granted_functions,
        access: CapAccess::Assigned {
            secret: cap_access_secret,
            assignees,
        },
    };

    // request authorization of signing key for agent's own cell should succeed
    let grant_action_hash = conductor
        .grant_zome_call_capability(GrantZomeCallCapabilityPayload {
            cell_id: cell_id.clone(),
            cap_grant: cap_grant.clone(),
        })
        .await
        .unwrap();

    // create a source chain read to query for the cap grant
    let authored_db = conductor
        .get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())
        .unwrap();
    let dht_db = conductor.get_dht_db(cell_id.dna_hash()).unwrap();
    let dht_db_cache = conductor.get_dht_db_cache(cell_id.dna_hash()).unwrap();

    let chain = SourceChainRead::new(
        authored_db.into(),
        dht_db.into(),
        dht_db_cache,
        conductor.keystore(),
        agent_pub_key.clone(),
    )
    .await
    .unwrap();

    let head = chain.chain_head_nonempty().unwrap();
    let dump = chain.dump().await.unwrap();

    dump.records.into_iter().for_each(|r| {
        let seq = r.action.action_seq();
        let hash = r.action_address;
        let ty = r.action.action_type();
        if let Some(e) = r.entry {
            println!("{seq:3} {ty:16 } {hash} {e:?}");
        } else {
            println!("{seq:3} {ty:16 } {hash}");
        }
    });

    // Genesis entries are 0, 1, and 2.
    // 3 is the cap grant created during init in the test wasm.
    // 4 is InitZomesComplete.
    // 5 is this grant added via admin call.
    // This checks that init ran before the grant was created.
    assert_eq!(head.seq, 5);
    assert_eq!(head.action, grant_action_hash);

    let actual_cap_grant = chain
        .valid_cap_grant(
            granted_function.clone(),
            cap_access_public_key.clone(),
            Some(cap_access_secret),
        )
        .await
        .unwrap();
    assert!(actual_cap_grant.is_some());
    assert!(actual_cap_grant.unwrap().is_valid(
        &granted_function,
        &cap_access_public_key,
        Some(&cap_access_secret)
    ));

    // a zome call without the cap secret that enables lookup of the authorized
    // signing key should be rejected
    let response = conductor
        .call_zome(ZomeCallParams {
            provenance: cap_access_public_key.clone(), // N.B.: using agent key would bypass capgrant lookup
            cell_id: cell_id.clone(),
            zome_name: zome.coordinator_zome_name(),
            fn_name: "get_entry".into(),
            cap_secret: None,
            payload: ExternIO::encode(()).unwrap(),
            nonce: Nonce256Bits::from([0; 32]),
            expires_at: Timestamp(Timestamp::now().as_micros() + 100000),
        })
        .await
        .unwrap()
        .unwrap();
    assert_matches!(response, ZomeCallResponse::Unauthorized(..));

    // a zome call with the cap secret of the authorized signing key should succeed
    let (nonce, expires_at) = fresh_nonce(Timestamp::now()).unwrap();
    let response = conductor
        .call_zome(ZomeCallParams {
            provenance: cap_access_public_key.clone(), // N.B.: using agent key would bypass capgrant lookup
            cell_id: cell_id.clone(),
            zome_name: zome.coordinator_zome_name(),
            fn_name: "get_entry".into(),
            cap_secret: Some(cap_access_secret),
            payload: ExternIO::encode(()).unwrap(),
            nonce,
            expires_at,
        })
        .await
        .unwrap()
        .unwrap();
    assert_matches!(response, ZomeCallResponse::Ok(_));
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "test_utils")]
async fn signed_zome_call_wildcard() {
    let zome = TestWasm::Create;
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![zome]).await;
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor.setup_app("app", [&dna]).await.unwrap();
    let cell_id = app.cells()[0].cell_id();
    let agent_pub_key = app.agent().clone();

    // generate a cap access public key
    let cap_access_public_key = fixt!(AgentPubKey, ::fixt::Predictable, 1);

    // compute a cap access secret
    let cap_access_secret = fixt!(CapSecret);

    // set up functions to grant access to
    let granted_functions = GrantedFunctions::All;

    // set up assignees which is only the agent key
    let mut assignees = BTreeSet::new();
    assignees.insert(cap_access_public_key.clone());

    let cap_grant = ZomeCallCapGrant {
        tag: "signing_key".into(),
        functions: granted_functions,
        access: CapAccess::Assigned {
            secret: cap_access_secret,
            assignees,
        },
    };

    // request authorization of signing key for agent's own cell should succeed
    conductor
        .grant_zome_call_capability(GrantZomeCallCapabilityPayload {
            cell_id: cell_id.clone(),
            cap_grant: cap_grant.clone(),
        })
        .await
        .unwrap();

    // create a source chain read to query for the cap grant
    let authored_db = conductor
        .get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())
        .unwrap();
    let dht_db = conductor.get_dht_db(cell_id.dna_hash()).unwrap();
    let dht_db_cache = conductor.get_dht_db_cache(cell_id.dna_hash()).unwrap();

    let source_chain_read = SourceChainRead::new(
        authored_db.into(),
        dht_db.into(),
        dht_db_cache,
        conductor.keystore(),
        agent_pub_key.clone(),
    )
    .await
    .unwrap();

    let called_function: GrantedFunction = ("create_entry".into(), "get_entry".into());

    let actual_cap_grant = source_chain_read
        .valid_cap_grant(
            called_function.clone(),
            cap_access_public_key.clone(),
            Some(cap_access_secret),
        )
        .await
        .unwrap();
    assert!(actual_cap_grant.is_some());
    assert!(actual_cap_grant.unwrap().is_valid(
        &called_function,
        &cap_access_public_key,
        Some(&cap_access_secret)
    ));

    // a zome call with the cap secret of the authorized signing key should succeed
    let (nonce, expires_at) = fresh_nonce(Timestamp::now()).unwrap();
    let response = conductor
        .call_zome(ZomeCallParams {
            provenance: cap_access_public_key.clone(), // N.B.: using agent key would bypass capgrant lookup
            cell_id: cell_id.clone(),
            zome_name: zome.coordinator_zome_name(),
            fn_name: "get_entry".into(),
            cap_secret: Some(cap_access_secret),
            payload: ExternIO::encode(()).unwrap(),
            nonce,
            expires_at,
        })
        .await
        .unwrap()
        .unwrap();
    assert_matches!(response, ZomeCallResponse::Ok(_));
}

#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "test_utils")]
async fn cap_grant_info_call() {
    use crate::test_utils::host_fn_caller::HostFnCaller;
    use std::collections::HashSet;

    let zome = TestWasm::Create;
    let (dna, _, _) = SweetDnaFile::unique_from_test_wasms(vec![zome]).await;
    let mut conductor = SweetConductor::from_standard_config().await;
    let app = conductor.setup_app("app", [&dna]).await.unwrap();
    let cell_id = app.cells()[0].cell_id();
    let agent_pub_key = cell_id.agent_pubkey().clone();

    // generate a cap access public key
    let cap_access_public_key = fixt!(AgentPubKey, ::fixt::Predictable, 1);

    // compute a cap access secret
    let cap_access_secret: CapSecret = [0; 64].into();

    // set up functions to grant access to
    let mut functions = BTreeSet::new();
    let granted_function: GrantedFunction = ("create_entry".into(), "get_entry".into());
    functions.insert(granted_function.clone());
    let granted_functions = GrantedFunctions::Listed(functions);
    // set up assignees which is only the agent key
    let mut assignees = BTreeSet::new();
    assignees.insert(cap_access_public_key.clone());

    let cap_grant = ZomeCallCapGrant {
        tag: "signing_key".into(),
        functions: granted_functions,
        access: CapAccess::Assigned {
            secret: cap_access_secret,
            assignees,
        },
    };

    // create a new cap grant entry
    let grant_action_hash = conductor
        .grant_zome_call_capability(GrantZomeCallCapabilityPayload {
            cell_id: cell_id.clone(),
            cap_grant: cap_grant.clone(),
        })
        .await
        .unwrap();

    // println!("grant_action_hash: {:?}\n", grant_action_hash);

    let mut cell_set = HashSet::new();
    cell_set.insert(cell_id.clone());

    // get the grant info, not including revoked grants
    let cap_info = conductor.capability_grant_info(&cell_set, false).await;
    assert!(cap_info.is_ok());
    // println!("{:?}\n", cap_info);

    // host call delete of cap grant
    let host_caller = HostFnCaller::create(cell_id, &conductor.raw_handle(), &dna).await;
    let _deletehash = host_caller
        .delete_entry(DeleteInput {
            deletes_action_hash: grant_action_hash.clone(),
            chain_top_ordering: Default::default(),
        })
        .await;
    // println!("deletehash: {:?}\n", _deletehash);

    // create a source chain read to query for the deleted cap grant
    let authored_db = conductor
        .get_or_create_authored_db(cell_id.dna_hash(), cell_id.agent_pubkey().clone())
        .unwrap();
    let dht_db = conductor.get_dht_db(cell_id.dna_hash()).unwrap();
    let dht_db_cache = conductor.get_dht_db_cache(cell_id.dna_hash()).unwrap();

    let chain = SourceChainRead::new(
        authored_db.into(),
        dht_db.into(),
        dht_db_cache,
        conductor.keystore(),
        agent_pub_key.clone(),
    )
    .await
    .unwrap();

    let delete_query: ChainQueryFilter = ChainQueryFilter::new()
        .include_entries(true)
        .action_type(ActionType::Delete);

    let delete_list = chain.query(delete_query.clone()).await.unwrap();

    // ensure that delete_address is same as cap_grant_address
    if let Action::Delete(delete) = delete_list[0].action().clone() {
        let delete_action_address = delete.deletes_address.clone();
        assert_eq!(delete_action_address, grant_action_hash);
    } else {
        panic!("Expected delete_address to be the same as the grant address");
    }

    // get the grant info, including revoked grants
    let cap_info = conductor
        .capability_grant_info(&cell_set, true)
        .await
        .unwrap();

    // println!("after delete: {:?}\n", cap_info);
    let cap_cell_info = cap_info.0.get(cell_id).unwrap().get(1).unwrap();
    assert_eq!(cap_cell_info.action_hash.clone(), grant_action_hash);
    assert!(cap_cell_info.revoked_at.is_some());
    assert!(cap_cell_info
        .created_at
        .lt(&cap_cell_info.revoked_at.unwrap()));
}



================================================
File: crates/holochain/src/core/metrics.rs
================================================
use std::sync::Arc;

use holo_hash::{AgentPubKey, DnaHash};
use opentelemetry_api::{global::meter_with_version, metrics::*, KeyValue};

pub type WorkflowDurationMetric = Histogram<f64>;

pub fn create_workflow_duration_metric(
    workflow_name: String,
    dna_hash: Arc<DnaHash>,
    agent: Option<AgentPubKey>,
) -> WorkflowDurationMetric {
    let mut attr = vec![
        KeyValue::new("workflow", workflow_name),
        KeyValue::new("dna_hash", format!("{:?}", dna_hash)),
    ];

    if let Some(agent) = agent {
        attr.push(KeyValue::new("agent", format!("{:?}", agent)));
    }

    meter_with_version(
        "hc.conductor",
        None::<&'static str>,
        None::<&'static str>,
        Some(attr),
    )
    .f64_histogram("hc.conductor.workflow.duration")
    .with_unit(Unit::new("s"))
    .with_description("The time spent running a workflow")
    .init()
}



================================================
File: crates/holochain/src/core/queue_consumer.rs
================================================
//! Manages the spawning of tasks which process the various work queues in
//! the system, as well as notifying subsequent queue processors to pick up the
//! work that was left off.
//!
//! The following table lays out the queues and the workflows that consume them,
//! as well as the follow-up workflows. A "source" queue is a database which
//! feeds data to the workflow, and a "destination" queue is a database which
//! said workflow writes to as part of its processing of its source queue.
//!
//! | workflow       | source queue     | dest. queue      | notifies       |
//! |----------------|------------------|------------------|----------------|
//! |                        **gossip path**                                |
//! | HandleGossip   | *n/a*            | ValidationQueue  | SysValidation  |
//! | SysValidation  | ValidationQueue  | ValidationQueue  | AppValidation  |
//! | AppValidation  | ValidationQueue  | ValidationQueue  | DhtOpIntegr.   |
//! |                       **authoring path**                              |
//! | CallZome       | *n/a*            | ChainSequence    | ProduceDhtOps  |
//! | ProduceDhtOps  | ChainSequence    | Auth'd + IntQ   | DhtOpIntegr.   |
//! |                 **integration, common to both paths**                 |
//! | DhtOpIntegr.   | IntegrationLimbo | IntegratedDhtOps | SysVal + VR    |
//! | ValReceipt.    | IntegratedDhtOps | IntegratedDhtOps | *n/a           |
//! | Publish        | AuthoredDhtOps   | *n/a*            | *n/a*          |
//!
//! ( Auth'd + IntQ is short for: AuthoredDhtOps + IntegrationLimbo)
//!
//! Implicitly, every workflow also writes to its own source queue, i.e. to
//! remove the item it has just processed.

use super::metrics::create_workflow_duration_metric;
use super::workflow::app_validation_workflow::AppValidationWorkspace;
use super::workflow::sys_validation_workflow::SysValidationWorkspace;
use super::workflow::{WorkflowError, WorkflowResult};
use crate::conductor::conductor::{RwShare, StopReceiver};
use crate::conductor::manager::TaskManagerClient;
use crate::conductor::space::Space;
use crate::conductor::ConductorHandle;
use crate::conductor::{error::ConductorError, manager::ManagedTaskResult};
use derive_more::Display;
use futures::future::Either;
use futures::{Future, Stream, StreamExt};
use holochain_p2p::HolochainP2pDna;
use holochain_p2p::*;
use holochain_types::prelude::*;
use publish_dht_ops_consumer::*;
use std::collections::HashMap;
use std::ops::Range;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::broadcast;
use witnessing_consumer::*;

// MAYBE: move these to workflow mod
mod integrate_dht_ops_consumer;
use integrate_dht_ops_consumer::*;
mod sys_validation_consumer;
use sys_validation_consumer::*;
mod app_validation_consumer;
use app_validation_consumer::*;

mod publish_dht_ops_consumer;
use crate::conductor::error::ConductorResult;
use crate::core::queue_consumer::countersigning_consumer::spawn_countersigning_consumer;
use crate::core::workflow::countersigning_workflow::CountersigningWorkspace;
use validation_receipt_consumer::*;

mod countersigning_consumer;
mod validation_receipt_consumer;

mod witnessing_consumer;

#[cfg(test)]
mod tests;

/// Spawns several long-running tasks which are responsible for processing work
/// which shows up on various databases.
///
/// Waits for the initial loop to complete before returning, to prevent causing
/// a race condition by trying to run a workflow too soon after cell creation.
#[allow(clippy::too_many_arguments)]
pub async fn spawn_queue_consumer_tasks(
    cell_id: CellId,
    network: HolochainP2pDna,
    space: &Space,
    conductor: ConductorHandle,
) -> ConductorResult<(QueueTriggers, InitialQueueTriggers)> {
    let Space {
        dht_db,
        cache_db: cache,
        dht_query_cache,
        ..
    } = space;

    let keystore = conductor.keystore().clone();
    let dna_hash = Arc::new(cell_id.dna_hash().clone());
    let queue_consumer_map = conductor.get_queue_consumer_workflows();
    let authored_db = space.get_or_create_authored_db(cell_id.agent_pubkey().clone())?;

    // Publish
    let tx_publish = spawn_publish_dht_ops_consumer(
        cell_id.clone(),
        authored_db.clone(),
        conductor.clone(),
        network.clone(),
    );

    // Validation Receipt
    // One per space.

    let tx_receipt = queue_consumer_map.spawn_once_validation_receipt(dna_hash.clone(), || {
        spawn_validation_receipt_consumer(
            dna_hash.clone(),
            dht_db.clone(),
            conductor.clone(),
            network.clone(),
        )
    });

    // Integration
    // One per space.
    let tx_integration = queue_consumer_map.spawn_once_integration(dna_hash.clone(), || {
        spawn_integrate_dht_ops_consumer(
            dna_hash.clone(),
            dht_db.clone(),
            dht_query_cache.clone(),
            conductor.task_manager(),
            tx_receipt.clone(),
            network.clone(),
        )
    });

    let dna_def = conductor
        .get_dna_def(&dna_hash)
        .expect("Dna must be in store");

    // App validation
    // One per space.
    let tx_app = queue_consumer_map.spawn_once_app_validation(dna_hash.clone(), || {
        spawn_app_validation_consumer(
            dna_hash.clone(),
            AppValidationWorkspace::new(
                authored_db.clone(),
                dht_db.clone(),
                space.dht_query_cache.clone(),
                cache.clone(),
                keystore.clone(),
                Arc::new(dna_def),
            ),
            conductor.clone(),
            tx_integration.clone(),
            tx_publish.clone(),
            network.clone(),
            dht_query_cache.clone(),
        )
    });

    let dna_def = conductor
        .get_dna_def(&dna_hash)
        .expect("Dna must be in store");

    // Sys validation
    // One per space.
    let tx_sys = queue_consumer_map.spawn_once_sys_validation(dna_hash.clone(), || {
        spawn_sys_validation_consumer(
            SysValidationWorkspace::new(
                authored_db.clone(),
                dht_db.clone(),
                dht_query_cache.clone(),
                cache.clone(),
                Arc::new(dna_def),
                conductor.running_services().dpki.clone(),
                conductor
                    .get_config()
                    .conductor_tuning_params()
                    .sys_validation_retry_delay(),
            ),
            space.clone(),
            conductor.clone(),
            tx_app.clone(),
            tx_publish.clone(),
            network.clone(),
            conductor.keystore().clone(),
        )
    });

    let workspace = {
        let mut guard = space.countersigning_workspaces.lock();
        guard
            .entry(cell_id.clone())
            .or_insert_with(|| {
                Arc::new(CountersigningWorkspace::new(
                    conductor
                        .config
                        .conductor_tuning_params()
                        .countersigning_resolution_retry_delay(),
                    conductor
                        .config
                        .conductor_tuning_params()
                        .countersigning_resolution_retry_limit,
                ))
            })
            .clone()
    };
    let tx_countersigning = spawn_countersigning_consumer(
        space.clone(),
        workspace,
        cell_id,
        conductor.clone(),
        tx_integration.clone(),
        tx_publish.clone(),
    );

    let tx_witnessing = queue_consumer_map.spawn_once_witnessing(dna_hash, || {
        spawn_witnessing_consumer(
            space.clone(),
            conductor.task_manager(),
            network.clone(),
            tx_sys.clone(),
        )
    });

    Ok((
        QueueTriggers {
            sys_validation: tx_sys.clone(),
            publish_dht_ops: tx_publish.clone(),
            countersigning: tx_countersigning.clone(),
            witnessing: tx_witnessing,
            integrate_dht_ops: tx_integration.clone(),
        },
        InitialQueueTriggers::new(
            tx_sys,
            tx_publish,
            tx_app,
            tx_integration,
            tx_receipt,
            tx_countersigning,
        ),
    ))
}

#[derive(Clone)]
/// Map of running queue consumers workflows per dna space.
pub struct QueueConsumerMap {
    map: RwShare<HashMap<QueueEntry, TriggerSender>>,
}

impl Default for QueueConsumerMap {
    fn default() -> Self {
        Self::new()
    }
}

impl QueueConsumerMap {
    /// Create a new queue consumer map.
    pub fn new() -> Self {
        Self {
            map: RwShare::new(HashMap::new()),
        }
    }

    fn spawn_once_validation_receipt<S>(&self, dna_hash: Arc<DnaHash>, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.spawn_once(QueueEntry(dna_hash, QueueType::Receipt), spawn)
    }

    fn spawn_once_integration<S>(&self, dna_hash: Arc<DnaHash>, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.spawn_once(QueueEntry(dna_hash, QueueType::Integration), spawn)
    }

    fn spawn_once_sys_validation<S>(&self, dna_hash: Arc<DnaHash>, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.spawn_once(QueueEntry(dna_hash, QueueType::SysValidation), spawn)
    }

    fn spawn_once_app_validation<S>(&self, dna_hash: Arc<DnaHash>, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.spawn_once(QueueEntry(dna_hash, QueueType::AppValidation), spawn)
    }

    fn spawn_once_witnessing<S>(&self, dna_hash: Arc<DnaHash>, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.spawn_once(QueueEntry(dna_hash, QueueType::Witnessing), spawn)
    }

    /// Get the validation receipt trigger for this dna hash.
    pub fn validation_receipt_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::Receipt))
    }

    /// Get the integration trigger for this dna hash.
    pub fn integration_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::Integration))
    }

    /// Get the sys validation trigger for this dna hash.
    pub fn sys_validation_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::SysValidation))
    }

    /// Get the app validation trigger for this dna hash.
    pub fn app_validation_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::AppValidation))
    }

    /// Get the countersigning trigger for this dna hash.
    pub fn countersigning_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::Countersigning))
    }

    /// Get the witnessing trigger for this dna hash.
    pub fn witnessing_trigger(&self, dna_hash: Arc<DnaHash>) -> Option<TriggerSender> {
        self.get_trigger(&QueueEntry(dna_hash, QueueType::Witnessing))
    }

    fn get_trigger(&self, key: &QueueEntry) -> Option<TriggerSender> {
        self.map.share_ref(|map| map.get(key).cloned())
    }

    fn spawn_once<S>(&self, key: QueueEntry, spawn: S) -> TriggerSender
    where
        S: FnOnce() -> TriggerSender,
    {
        self.map.share_mut(|map| match map.entry(key) {
            std::collections::hash_map::Entry::Occupied(o) => o.get().clone(),
            std::collections::hash_map::Entry::Vacant(v) => {
                let ts = spawn();
                v.insert(ts).clone()
            }
        })
    }
}

#[derive(Hash, Eq, PartialEq, Clone, Debug)]
struct QueueEntry(Arc<DnaHash>, QueueType);

#[derive(Hash, Eq, PartialEq, Clone, Debug)]
enum QueueType {
    Receipt,
    Integration,
    AppValidation,
    SysValidation,
    Countersigning,
    Witnessing,
}

/// The entry points for kicking off a chain reaction of queue activity
#[derive(Clone)]
pub struct QueueTriggers {
    /// Notify the SysValidation workflow to run, i.e. after handling gossip
    pub sys_validation: TriggerSender,
    /// Notify the ProduceDhtOps workflow to run, i.e. after InvokeCallZome
    pub publish_dht_ops: TriggerSender,
    /// Notify the countersigning workflow to run and manage sessions state.
    pub countersigning: TriggerSender,
    /// Notify the witnessing workflow to run and check for complete signature sets for sessions.
    pub witnessing: TriggerSender,
    /// Notify the IntegrateDhtOps workflow to run, i.e. after InvokeCallZome
    pub integrate_dht_ops: TriggerSender,
}

/// The triggers to run once at the start of a cell
#[derive(Clone)]
pub struct InitialQueueTriggers {
    /// These triggers can only be run once
    /// so they are private
    sys_validation: TriggerSender,
    publish_dht_ops: TriggerSender,
    app_validation: TriggerSender,
    integrate_dht_ops: TriggerSender,
    validation_receipt: TriggerSender,
    countersigning: TriggerSender,
}

impl InitialQueueTriggers {
    fn new(
        sys_validation: TriggerSender,
        publish_dht_ops: TriggerSender,
        app_validation: TriggerSender,
        integrate_dht_ops: TriggerSender,
        validation_receipt: TriggerSender,
        countersigning: TriggerSender,
    ) -> Self {
        Self {
            sys_validation,
            publish_dht_ops,
            app_validation,
            integrate_dht_ops,
            validation_receipt,
            countersigning,
        }
    }

    /// Initialize all the workflows once.
    pub fn initialize_workflows(self) {
        self.sys_validation.trigger(&"init");
        self.app_validation.trigger(&"init");
        self.integrate_dht_ops.trigger(&"init");
        self.publish_dht_ops.trigger(&"init");
        self.validation_receipt.trigger(&"init");
        self.countersigning.trigger(&"init");
    }
}

/// The means of nudging a queue consumer to tell it to look for more work
#[derive(Clone)]
pub struct TriggerSender {
    /// The actual trigger sender.
    trigger: broadcast::Sender<&'static &'static str>,
    /// Reset the back off loop if there is one.
    reset_back_off: Option<Arc<AtomicBool>>,
    /// Pause / resume the back off loop if there is one.
    pause_back_off: Option<Arc<AtomicBool>>,
}

/// The receiving end of a queue trigger channel
pub struct TriggerReceiver {
    /// The actual trigger.
    rx: broadcast::Receiver<&'static &'static str>,
    /// If there is a back off loop, should
    /// the trigger reset the back off.
    reset_on_trigger: bool,
    /// The optional back off loop.
    back_off: Option<BackOff>,
}

/// A loop that can optionally back off, pause and resume.
struct BackOff {
    /// The starting duration for the back off.
    /// This allows resetting the range.
    start: Duration,
    /// The range of duration for the back off.
    range: Range<Duration>,
    /// If we should reset the range on next iteration.
    reset_back_off: Arc<AtomicBool>,
    /// If we should pause the loop on next iteration.
    paused: Arc<AtomicBool>,
}

impl TriggerSender {
    /// Create a new channel for waking a consumer
    pub fn new() -> (TriggerSender, TriggerReceiver) {
        let (tx, rx) = broadcast::channel(1);
        (
            TriggerSender {
                trigger: tx,
                reset_back_off: None,
                pause_back_off: None,
            },
            TriggerReceiver {
                rx,
                back_off: None,
                reset_on_trigger: false,
            },
        )
    }

    /// Create a new channel trigger that will also trigger
    /// on a loop.
    /// The duration takes a range so that the loop  can
    /// be set to back off from the lowest to the highest duration.
    /// If you do not want a back off, set the duration range
    /// to the same value like: `Duration::from_millis(10)..Duration::from_millis(10)`
    /// If reset_on_trigger is true, the back off will be reset whenever a
    /// trigger is received.
    pub fn new_with_loop(
        range: Range<Duration>,
        reset_on_trigger: bool,
    ) -> (TriggerSender, TriggerReceiver) {
        let (tx, rx) = broadcast::channel(1);
        let reset_back_off = Arc::new(AtomicBool::new(false));
        let pause_back_off = Arc::new(AtomicBool::new(false));
        (
            TriggerSender {
                trigger: tx,
                reset_back_off: Some(reset_back_off.clone()),
                pause_back_off: Some(pause_back_off.clone()),
            },
            TriggerReceiver {
                rx,
                reset_on_trigger,
                back_off: Some(BackOff::new(range, reset_back_off, pause_back_off)),
            },
        )
    }

    /// Lazily nudge the consumer task, ignoring the case where the consumer
    /// already has a pending trigger signal
    pub fn trigger(&self, context: &'static &'static str) {
        if self.trigger.send(context).is_err() {
            tracing::warn!(
                "Queue consumer trigger was sent while Cell is shutting down: ignoring."
            );
        };
    }

    /// Reset the back off to the lowest duration.
    /// If no back off is set this is a no-op.
    pub fn reset_back_off(&self) {
        if let Some(tx) = &self.reset_back_off {
            tx.store(true, Ordering::Relaxed);
        }
    }

    /// Pause the trigger loop if there is one.
    pub fn pause_loop(&self) {
        if let Some(pause) = &self.pause_back_off {
            pause.store(true, Ordering::Relaxed);
        }
    }

    /// Resume the trigger loop now if there is one.
    ///
    /// This will resume the loop even if it is currently
    /// listening (the workflow is not running).
    /// The downside to this call is that if the workflow
    /// is running it will immediately run a second time.
    ///
    /// This call is a no-op if the loop is not paused.
    pub fn resume_loop_now(&self) {
        if let Some(pause) = &self.pause_back_off {
            if pause.fetch_and(false, Ordering::AcqRel) {
                self.trigger(&"resume_loop_now");
            }
        }
    }

    /// Resume the trigger loop if there is one.
    ///
    /// This will cause the loop to resume after the
    /// next trigger (or if the workflow is currently in progress).
    /// It will not cause the loop to resume immediately.
    /// If the loop is currently listening (the workflow is not running)
    /// then nothing will happen until the next trigger.
    /// See `resume_loop_now` for a version that will resume immediately.
    ///
    /// This call is a no-op if the loop is not paused.
    pub fn resume_loop(&self) {
        if let Some(pause) = &self.pause_back_off {
            pause.store(false, Ordering::Release);
        }
    }
}

impl TriggerReceiver {
    /// Listen for one or more items to come through, draining the channel
    /// each time. Bubble up errors on empty channel.
    pub async fn listen(&mut self) -> Result<(), QueueTriggerClosedError> {
        let Self {
            back_off,
            rx,
            reset_on_trigger,
        } = self;

        let mut was_trigger = true;
        {
            // Create the trigger future
            let trigger_fut = rx_fut(rx);
            match back_off {
                // We have a back off loop that is running.
                Some(back_off) if !back_off.is_paused() => {
                    let paused = back_off.paused.clone();
                    {
                        // Get the back off future.
                        let back_off_fut = back_off.wait();
                        futures::pin_mut!(back_off_fut, trigger_fut);

                        // Race between either a trigger or the loop.
                        match futures::future::select(trigger_fut, back_off_fut).await {
                            Either::Left((result, _)) => {
                                // We got a trigger, check the result and drop the wait future.
                                result?;
                            }
                            Either::Right((_, trigger_fut)) => {
                                // We got the loop future.
                                if paused.load(Ordering::Acquire) {
                                    // If we are now paused then we should wait for a trigger.
                                    trigger_fut.await?;
                                } else {
                                    // We are not pause so this was not a trigger.
                                    was_trigger = false;
                                }
                            }
                        }
                    }
                }
                _ => {
                    // We either have no back off loop or it's paused
                    // so wait for a trigger.
                    trigger_fut.await?;
                }
            }
        }
        // We want to flush the buffer if a trigger
        // that woke the listen.
        if was_trigger {
            // Do one try recv to empty the buffer.
            // This is needed as we can't have an empty buffer
            // but we don't want a second trigger to be stored in
            // the buffer and cause the workflow to run twice.
            let _ = self.rx.try_recv();

            // If we have a back off loop and got a trigger then
            // we should reset the back off if that flag is on.
            if *reset_on_trigger {
                if let Some(back_off) = back_off {
                    back_off.reset();
                }
            }
        }
        Ok(())
    }

    /// Check whether the backoff loop is paused. Will always return false if there is no backoff for this receiver.
    pub fn is_paused(&self) -> bool {
        self.back_off.as_ref().map_or(false, |b| b.is_paused())
    }

    /// Try to receive a trigger without blocking.
    #[cfg(test)]
    pub fn try_recv(&mut self) -> Option<&'static &'static str> {
        self.rx.try_recv().ok()
    }
}

/// Create a future that will be ok with either a recv or a lagged.
async fn rx_fut(
    rx: &mut broadcast::Receiver<&'static &'static str>,
) -> Result<(), QueueTriggerClosedError> {
    match rx.recv().await {
        Ok(context) => {
            tracing::trace!(msg = "trigger received", ?context);
            Ok(())
        }
        Err(broadcast::error::RecvError::Closed) => Err(QueueTriggerClosedError),
        Err(broadcast::error::RecvError::Lagged(_)) => Ok(()),
    }
}

impl BackOff {
    fn new(
        range: Range<Duration>,
        reset_back_off: Arc<AtomicBool>,
        pause_back_off: Arc<AtomicBool>,
    ) -> Self {
        Self {
            start: range.start,
            range,
            reset_back_off,
            paused: pause_back_off,
        }
    }

    async fn wait(&mut self) {
        // Check if we should reset the back off.
        if self.reset_back_off.fetch_and(false, Ordering::Relaxed) {
            self.reset();
        }
        // If the range is empty we are just looping.
        let dur = if self.range.is_empty() {
            self.range.end
        } else {
            // If not we take the current start value.
            self.range.start
        };
        // Sleep this task for the chosen duration.
        // This future may be cancelled during this await,
        // and any code following will not be executed.
        tokio::time::sleep(dur).await;
        // If the sleep completes then we bump the start of the range
        // or take the end if we have reached the end.
        self.range.start = std::cmp::min(self.range.start * 2, self.range.end);
    }

    fn reset(&mut self) {
        self.range.start = self.start;
    }

    fn is_paused(&self) -> bool {
        self.paused.load(Ordering::Acquire)
    }
}

/// Declares whether a workflow has exhausted the queue or not
#[derive(Clone, Debug, PartialEq)]
pub enum WorkComplete {
    /// The queue has been exhausted
    Complete,
    /// Items still remain on the queue. Optionally specify a delay in ms before retriggering.
    Incomplete(Option<Duration>),
}

/// The only error possible when attempting to trigger: the channel is closed
#[derive(Debug, Display, thiserror::Error)]
pub struct QueueTriggerClosedError;

/// Get a stream of triggers which can be terminated by a received Stop
pub(super) fn trigger_stream(rx: TriggerReceiver, stop: StopReceiver) -> impl Stream<Item = ()> {
    stop.fuse_with(Box::pin(futures::stream::unfold(rx, |mut rx| async move {
        match rx.listen().await {
            Ok(()) => Some(((), rx)),
            Err(_) => None,
        }
    })))
}

async fn queue_consumer_main_task_impl<
    Fut: 'static + Send + Future<Output = WorkflowResult<WorkComplete>>,
>(
    name: String,
    dna_hash: Arc<DnaHash>,
    agent: Option<AgentPubKey>,
    (tx, rx): (TriggerSender, TriggerReceiver),
    stop: StopReceiver,
    mut fut: impl 'static + Send + FnMut() -> Fut,
) -> ManagedTaskResult {
    let mut triggers = trigger_stream(rx, stop);
    let duration_metric = create_workflow_duration_metric(name.clone(), dna_hash, agent);
    loop {
        if let Some(()) = triggers.next().await {
            let start = Instant::now();
            match fut().await {
                Ok(WorkComplete::Incomplete(delay)) => {
                    tracing::debug!("Work incomplete, re-triggering workflow - {}.", name);
                    if let Some(dly) = delay {
                        tracing::debug!(
                            "Sleeping for {} ms before re-triggering - {}.",
                            dly.as_millis(),
                            name
                        );
                        tokio::time::sleep(dly).await;
                    }
                    tx.trigger(&"retrigger")
                }
                Err(err) => handle_workflow_error(&name, err)?,
                _ => (),
            }

            duration_metric.record(start.elapsed().as_secs_f64(), &[]);
        } else {
            tracing::info!("Cell is shutting down: stopping queue consumer '{}'", name);
            break;
        }
    }
    ManagedTaskResult::Ok(())
}

fn queue_consumer_dna_bound<Fut: 'static + Send + Future<Output = WorkflowResult<WorkComplete>>>(
    name: &str,
    dna_hash: Arc<DnaHash>,
    tm: TaskManagerClient,
    (tx, rx): (TriggerSender, TriggerReceiver),
    fut: impl 'static + Send + FnMut() -> Fut,
) {
    let workflow_name = name.to_string();
    let task_dna_hash = dna_hash.clone();
    tm.add_dna_task_critical(name, dna_hash, {
        move |stop| {
            queue_consumer_main_task_impl(workflow_name, task_dna_hash, None, (tx, rx), stop, fut)
        }
    });
}

fn queue_consumer_cell_bound<
    Fut: 'static + Send + Future<Output = WorkflowResult<WorkComplete>>,
>(
    name: &str,
    cell_id: CellId,
    tm: TaskManagerClient,
    (tx, rx): (TriggerSender, TriggerReceiver),
    fut: impl 'static + Send + FnMut() -> Fut,
) {
    let workflow_name = name.to_string();
    let dna_hash = cell_id.dna_hash().clone();
    let agent = cell_id.agent_pubkey().clone();
    tm.add_cell_task_critical(name, cell_id, {
        move |stop| {
            queue_consumer_main_task_impl(
                workflow_name,
                Arc::new(dna_hash),
                Some(agent),
                (tx, rx),
                stop,
                fut,
            )
        }
    });
}

/// Does nothing.
/// Does extra nothing and logs about it if the error shouldn't bail the
/// workflow.
fn handle_workflow_error(workflow_name: &String, err: WorkflowError) -> ManagedTaskResult {
    if err.workflow_should_bail() {
        Err(Box::new(ConductorError::from(err)).into())
    } else {
        tracing::error!(?workflow_name, ?err);
        Ok(())
    }
}



================================================
File: crates/holochain/src/core/ribosome.rs
================================================
//! A Ribosome is a structure which knows how to execute hApp code.
//!
//! We have only one instance of this: [crate::core::ribosome::real_ribosome::RealRibosome]. The abstract trait exists
//! so that we can write mocks against the `RibosomeT` interface, as well as
//! opening the possiblity that we might support applications written in other
//! languages and environments.

// This allow is here because #[automock] automaticaly creates a struct without
// documentation, and there seems to be no way to add docs to it after the fact
#[allow(missing_docs)]
pub mod error;

/// How to version guest callbacks.
/// See `genesis_self_check` for an example.
///
/// - Create unversioned structs in the root of the callback module
///   - Invocation, result, host access
///   - The unversioned structs should thinly wrap all their versioned structs
/// - Create versioned submodules for the callback
///   - In these, create versioned structs for the unversioned structs
///   - Write/keep all tests for the versioned copies of the callbacks, test
///     wasms can expose externs directly without the macros for explicit
///     legacy identities if needed.
/// - On the ribosome make sure the trait uses the unversioned struct
///   - Inside the callback method loop over the versioned callbacks, and
///     dispatch each that is found in the wasm
///   - Figure out how to merge/handle results if multiple versions of a callback
///     are found in the target wasm
/// - The ribosome method caller will now be forced by types to provide the
///   unversioned struct, which means they cannot forget to provide and dispatch
///   everything required for each version
/// - Update the `map_extern` macro so that the unversioned name of the callback
///   maps to the latest version of the callback, e.g. `genesis_self_check` is
///   rewritten to `genesis_self_check_2` at the time of writing
///   - This has the effect of newly compiled wasms implementing the callback
///     that is newest when they compile, without polluting the unversioned
///     callback, which is effectively legacy/deprecated behaviour to call it
///     directly.
pub mod guest_callback;

/// How to version host_fns.
/// See `dna_info_1` and `dna_info_2` for an example.
///
/// - Create new versions of the host fn and related IO structs
///   - Any change to an IO struct implies/necessitates a new host fn version
///   - Changes to structs MAY also trigger a new callback version if there is
///     a partially shared data structure in their interfaces
///   - Update the IO type aliases to point to the newest version of all structs
/// - Map both the old and new host functions in the ribosome
/// - Define both of the host functions in the wasm externs in HDI/HDK
/// - Test all versions of every host fn
/// - Ensure the convenience wrapper in the HDI/HDK references the latest version
///   of the host_fn
pub mod host_fn;
pub mod real_ribosome;

mod check_clone_access;

use crate::conductor::api::CellConductorHandle;
use crate::conductor::api::CellConductorReadHandle;
use crate::conductor::api::DpkiApi;
use crate::conductor::api::ZomeCallParamsSigned;
use crate::core::ribosome::guest_callback::entry_defs::EntryDefsResult;
use crate::core::ribosome::guest_callback::genesis_self_check::v1::GenesisSelfCheckHostAccessV1;
use crate::core::ribosome::guest_callback::genesis_self_check::v2::GenesisSelfCheckHostAccessV2;
use crate::core::ribosome::guest_callback::init::InitInvocation;
use crate::core::ribosome::guest_callback::init::InitResult;
use crate::core::ribosome::guest_callback::post_commit::PostCommitInvocation;
use crate::core::ribosome::guest_callback::validate::ValidateInvocation;
use crate::core::ribosome::guest_callback::validate::ValidateResult;
use crate::core::ribosome::guest_callback::CallStream;
use derive_more::Constructor;
use error::RibosomeResult;
use ghost_actor::dependencies::must_future::MustBoxFuture;
use guest_callback::entry_defs::EntryDefsHostAccess;
use guest_callback::init::InitHostAccess;
use guest_callback::post_commit::PostCommitHostAccess;
use guest_callback::validate::ValidateHostAccess;
use holo_hash::AgentPubKey;
use holochain_conductor_services::DpkiImpl;
use holochain_keystore::MetaLairClient;
use holochain_nonce::*;
use holochain_p2p::HolochainP2pDna;
use holochain_p2p::HolochainP2pDnaT;
use holochain_serialized_bytes::prelude::*;
use holochain_state::host_fn_workspace::HostFnWorkspace;
use holochain_state::host_fn_workspace::HostFnWorkspaceRead;
use holochain_state::nonce::WitnessNonceResult;
use holochain_types::prelude::*;
use holochain_types::zome_types::GlobalZomeTypes;
use holochain_zome_types::block::BlockTargetId;
use mockall::automock;
use std::iter::Iterator;
use std::sync::Arc;
use tokio::sync::broadcast;

use self::guest_callback::{
    entry_defs::EntryDefsInvocation, genesis_self_check::GenesisSelfCheckResult,
};
use self::{
    error::RibosomeError,
    guest_callback::genesis_self_check::{GenesisSelfCheckHostAccess, GenesisSelfCheckInvocation},
};

#[derive(Clone)]
pub struct CallContext {
    pub(crate) zome: Zome,
    pub(crate) function_name: FunctionName,
    pub(crate) auth: InvocationAuth,
    pub(crate) host_context: HostContext,
}

impl CallContext {
    pub fn new(
        zome: Zome,
        function_name: FunctionName,
        host_context: HostContext,
        auth: InvocationAuth,
    ) -> Self {
        Self {
            zome,
            function_name,
            host_context,
            auth,
        }
    }

    pub fn zome(&self) -> &Zome {
        &self.zome
    }

    pub fn function_name(&self) -> &FunctionName {
        &self.function_name
    }

    pub fn host_context(&self) -> HostContext {
        self.host_context.clone()
    }

    pub fn auth(&self) -> InvocationAuth {
        self.auth.clone()
    }
}

#[derive(Clone, Debug)]
pub enum HostContext {
    EntryDefs(EntryDefsHostAccess),
    GenesisSelfCheckV1(GenesisSelfCheckHostAccessV1),
    GenesisSelfCheckV2(GenesisSelfCheckHostAccessV2),
    Init(InitHostAccess),
    PostCommit(PostCommitHostAccess), // MAYBE: add emit_signal access here?
    Validate(ValidateHostAccess),
    ZomeCall(ZomeCallHostAccess),
}

impl From<&HostContext> for HostFnAccess {
    fn from(host_access: &HostContext) -> Self {
        match host_access {
            HostContext::ZomeCall(access) => access.into(),
            HostContext::GenesisSelfCheckV1(access) => access.into(),
            HostContext::GenesisSelfCheckV2(access) => access.into(),
            HostContext::Validate(access) => access.into(),
            HostContext::Init(access) => access.into(),
            HostContext::EntryDefs(access) => access.into(),
            HostContext::PostCommit(access) => access.into(),
        }
    }
}

impl HostContext {
    /// Get the workspace, panics if none was provided
    pub fn workspace(&self) -> HostFnWorkspaceRead {
        self.maybe_workspace().expect(
            "Gave access to a host function that uses the workspace without providing a workspace",
        )
    }

    /// Get the workspace if it was provided.
    pub fn maybe_workspace(&self) -> Option<HostFnWorkspaceRead> {
        match self.clone() {
            Self::ZomeCall(ZomeCallHostAccess { workspace, .. })
            | Self::Init(InitHostAccess { workspace, .. })
            | Self::PostCommit(PostCommitHostAccess { workspace, .. }) => Some(workspace.into()),
            Self::Validate(ValidateHostAccess { workspace, .. }) => Some(workspace),
            _ => None,
        }
    }

    /// Get the DPKI service if installed.
    pub fn maybe_dpki(&self) -> DpkiApi {
        match self.clone() {
            Self::ZomeCall(ZomeCallHostAccess { dpki, .. }) => dpki,
            Self::Init(InitHostAccess { dpki, .. }) => dpki,
            _ => {
                panic!("Gave access to a host function that accesses DPKI without providing DPKI.")
            }
        }
    }

    /// Get the workspace, panics if none was provided
    pub fn workspace_write(&self) -> &HostFnWorkspace {
        match self {
            Self::ZomeCall(ZomeCallHostAccess { workspace, .. })
            | Self::Init(InitHostAccess { workspace, .. })
            | Self::PostCommit(PostCommitHostAccess { workspace, .. }) => workspace,
            _ => panic!(
                "Gave access to a host function that writes to the workspace without providing a workspace"
            ),
        }
    }

    /// Get the keystore, panics if none was provided
    pub fn keystore(&self) -> &MetaLairClient {
        match self {
            Self::ZomeCall(ZomeCallHostAccess { keystore, .. })
            | Self::Init(InitHostAccess { keystore, .. })
            | Self::PostCommit(PostCommitHostAccess { keystore, .. }) => keystore,
            _ => panic!(
                "Gave access to a host function that uses the keystore without providing a keystore"
            ),
        }
    }

    /// Get the network, panics if none was provided
    pub fn network(&self) -> Arc<dyn HolochainP2pDnaT> {
        match self {
            Self::ZomeCall(ZomeCallHostAccess { network, .. })
            | Self::Init(InitHostAccess { network, .. })
            | Self::PostCommit(PostCommitHostAccess { network, .. }) => Arc::new(network.clone()),
            Self::Validate(ValidateHostAccess { network, .. }) => network.clone(),
            _ => panic!(
                "Gave access to a host function that uses the network without providing a network"
            ),
        }
    }

    /// Get the signal sender, panics if none was provided
    pub fn signal_tx(&mut self) -> &mut broadcast::Sender<Signal> {
        match self {
            Self::ZomeCall(ZomeCallHostAccess { signal_tx, .. })
            | Self::Init(InitHostAccess { signal_tx, .. })
            | Self::PostCommit(PostCommitHostAccess { signal_tx, .. })
            => signal_tx,
            _ => panic!(
                "Gave access to a host function that uses the signal broadcaster without providing one"
            ),
        }
    }

    /// Get the call zome handle, panics if none was provided
    pub fn call_zome_handle(&self) -> &CellConductorReadHandle {
        match self {
            Self::ZomeCall(ZomeCallHostAccess {
                call_zome_handle, ..
            })
            | Self::Init(InitHostAccess { call_zome_handle, .. })
            => call_zome_handle,
            _ => panic!(
                "Gave access to a host function that uses the call zome handle without providing a call zome handle"
            ),
        }
    }
}

#[derive(Clone, Debug)]
pub struct FnComponents(pub Vec<String>);

/// iterating over FnComponents isn't as simple as returning the inner Vec iterator
/// we return the fully joined vector in specificity order
/// specificity is defined as consisting of more components
/// e.g. FnComponents(Vec("foo", "bar", "baz")) would return:
/// - Some("foo_bar_baz")
/// - Some("foo_bar")
/// - Some("foo")
/// - None
impl Iterator for FnComponents {
    type Item = String;
    fn next(&mut self) -> Option<String> {
        match self.0.len() {
            0 => None,
            _ => {
                let ret = self.0.join("_");
                self.0.pop();
                Some(ret)
            }
        }
    }
}

impl From<Vec<String>> for FnComponents {
    fn from(vs: Vec<String>) -> Self {
        Self(vs)
    }
}

impl FnComponents {
    pub fn into_inner(self) -> Vec<String> {
        self.0
    }
}

#[derive(Clone, Debug, PartialEq)]
pub enum ZomesToInvoke {
    /// All the integrity zomes.
    AllIntegrity,
    /// All integrity and coordinator zomes.
    All,
    /// A single zome of unknown type.
    One(Zome),
    /// A single integrity zome.
    OneIntegrity(IntegrityZome),
    /// A single coordinator zome.
    OneCoordinator(CoordinatorZome),
}

impl ZomesToInvoke {
    pub fn one(zome: Zome) -> Self {
        Self::One(zome)
    }
    pub fn one_integrity(zome: IntegrityZome) -> Self {
        Self::OneIntegrity(zome)
    }
    pub fn one_coordinator(zome: CoordinatorZome) -> Self {
        Self::OneCoordinator(zome)
    }
}

#[derive(Clone, Debug)]
pub enum InvocationAuth {
    LocalCallback,
    Cap(AgentPubKey, Option<CapSecret>),
}

impl InvocationAuth {
    pub fn new(agent_pubkey: AgentPubKey, cap_secret: Option<CapSecret>) -> Self {
        Self::Cap(agent_pubkey, cap_secret)
    }
}

pub trait Invocation: Clone + Send + Sync {
    /// Some invocations call into a single zome and some call into many or all zomes.
    /// An example of an invocation that calls across all zomes is init. Init must pass for every
    /// zome in order for the Dna overall to successfully init.
    /// An example of an invocation that calls a single zome is validation of an entry, because
    /// the entry is only defined in a single zome, so it only makes sense for that exact zome to
    /// define the validation logic for that entry.
    /// In the future this may be expanded to support a subset of zomes that is larger than one.
    /// For example, we may want to trigger a callback in all zomes that implement a
    /// trait/interface, but this doesn't exist yet, so the only valid options are All or One.
    fn zomes(&self) -> ZomesToInvoke;
    /// Invocations execute in a "sparse" manner of decreasing specificity. In technical terms this
    /// means that the list of strings in FnComponents will be concatenated into a single function
    /// name to be called, then the last string will be removed and a shorter function name will
    /// be attempted and so on until all variations have been attempted.
    /// For example, if FnComponents was vec!["foo", "bar", "baz"] it would loop as "foo_bar_baz"
    /// then "foo_bar" then "foo". All of those three callbacks that are defined will be called
    /// _unless a definitive callback result is returned_.
    /// See [ `CallbackResult::is_definitive` ] in zome_types.
    /// All of the individual callback results are then folded into a single overall result value
    /// as a From implementation on the invocation results structs (e.g. zome results vs. ribosome
    /// results).
    fn fn_components(&self) -> FnComponents;
    /// the serialized input from the host for the wasm call
    /// this is intentionally NOT a reference to self because ExternIO may be huge we want to be
    /// careful about cloning invocations
    fn host_input(self) -> Result<ExternIO, SerializedBytesError>;
    fn auth(&self) -> InvocationAuth;
}

impl ZomeCallInvocation {
    /// to decide if a zome call grant is authorized:
    /// - we need to find a live (committed and not deleted) cap grant that matches the secret
    /// - if the live cap grant is for the current author the call is ALWAYS authorized ELSE
    /// - the live cap grant needs to include the invocation's provenance AND zome/function name
    pub async fn verify_grant(
        &self,
        host_access: &ZomeCallHostAccess,
    ) -> RibosomeResult<ZomeCallAuthorization> {
        let check_function = (self.zome.zome_name().clone(), self.fn_name.clone());
        let check_agent = self.provenance.clone();
        let check_secret = self.cap_secret;
        let maybe_grant: Option<CapGrant> = host_access
            .workspace
            .source_chain()
            .as_ref()
            .expect("Must have source chain to make zome calls")
            .valid_cap_grant(check_function, check_agent, check_secret)
            .await?;
        Ok(if maybe_grant.is_some() {
            ZomeCallAuthorization::Authorized
        } else {
            ZomeCallAuthorization::BadCapGrant
        })
    }

    pub async fn verify_nonce(
        &self,
        host_access: &ZomeCallHostAccess,
    ) -> RibosomeResult<ZomeCallAuthorization> {
        Ok(
            match host_access
                .call_zome_handle
                .witness_nonce_from_calling_agent(
                    self.provenance.clone(),
                    self.nonce,
                    self.expires_at,
                )
                .await
                .map_err(Box::new)?
            {
                WitnessNonceResult::Fresh => ZomeCallAuthorization::Authorized,
                nonce_result => ZomeCallAuthorization::BadNonce(format!("{:?}", nonce_result)),
            },
        )
    }

    pub async fn verify_blocked_provenance(
        &self,
        host_access: &ZomeCallHostAccess,
    ) -> RibosomeResult<ZomeCallAuthorization> {
        if host_access
            .call_zome_handle
            .is_blocked(
                BlockTargetId::Cell(CellId::new(
                    (*self.cell_id.dna_hash()).clone(),
                    self.provenance.clone(),
                )),
                Timestamp::now(),
            )
            .await?
        {
            Ok(ZomeCallAuthorization::BlockedProvenance)
        } else {
            Ok(ZomeCallAuthorization::Authorized)
        }
    }

    /// to verify if the zome call is authorized:
    /// - the nonce must not have already been seen
    /// - the grant must be valid
    /// - the provenance must not have any active blocks against them right now
    ///
    /// The checks MUST be done in this order as witnessing the nonce is a write operation,
    /// and so we MUST NOT write nonces until after we verify the signature.
    #[allow(clippy::extra_unused_lifetimes)]
    pub async fn is_authorized<'a>(
        &self,
        host_access: &ZomeCallHostAccess,
    ) -> RibosomeResult<ZomeCallAuthorization> {
        Ok(match self.verify_nonce(host_access).await? {
            ZomeCallAuthorization::Authorized => match self.verify_grant(host_access).await? {
                ZomeCallAuthorization::Authorized => {
                    self.verify_blocked_provenance(host_access).await?
                }
                unauthorized => unauthorized,
            },
            unauthorized => unauthorized,
        })
    }
}

mockall::mock! {
    Invocation {}
    impl Invocation for Invocation {
        fn zomes(&self) -> ZomesToInvoke;
        fn fn_components(&self) -> FnComponents;
        fn host_input(self) -> Result<ExternIO, SerializedBytesError>;
        fn auth(&self) -> InvocationAuth;
    }
    impl Clone for Invocation {
        fn clone(&self) -> Self;
    }
}

/// A top-level call into a zome function,
/// i.e. coming from outside the Cell from an external Interface
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct ZomeCallInvocation {
    /// The Id of the `Cell` in which this Zome-call would be invoked
    pub cell_id: CellId,
    /// The Zome containing the function that would be invoked
    pub zome: Zome,
    /// The capability request authorization.
    /// This can be `None` and still succeed in the case where the function
    /// in the zome being called has been given an Unrestricted status
    /// via a `CapGrant`. Otherwise, it will be necessary to provide a `CapSecret` for every call.
    pub cap_secret: Option<CapSecret>,
    /// The name of the Zome function to call
    pub fn_name: FunctionName,
    /// The serialized data to pass as an argument to the Zome call
    pub payload: ExternIO,
    /// The provenance of the call. Provenance means the 'source'
    /// so this expects the `AgentPubKey` of the agent calling the Zome function
    pub provenance: AgentPubKey,
    /// The nonce of the call. Must be unique and monotonic.
    /// If a higher nonce has been seen then older zome calls will be discarded.
    pub nonce: Nonce256Bits,
    /// This call MUST NOT be respected after this time, in the opinion of the callee.
    pub expires_at: Timestamp,
}

impl Invocation for ZomeCallInvocation {
    fn zomes(&self) -> ZomesToInvoke {
        ZomesToInvoke::One(self.zome.to_owned())
    }
    fn fn_components(&self) -> FnComponents {
        vec![self.fn_name.to_owned().into()].into()
    }
    fn host_input(self) -> Result<ExternIO, SerializedBytesError> {
        Ok(self.payload)
    }
    fn auth(&self) -> InvocationAuth {
        InvocationAuth::Cap(self.provenance.clone(), self.cap_secret)
    }
}

impl ZomeCallInvocation {
    pub async fn try_from_params(
        conductor_api: CellConductorHandle,
        params: ZomeCallParams,
    ) -> RibosomeResult<Self> {
        let ZomeCallParams {
            cap_secret,
            cell_id,
            expires_at,
            fn_name,
            nonce,
            payload,
            provenance,
            zome_name,
        } = params;
        let zome = conductor_api
            .get_zome(cell_id.dna_hash(), &zome_name)
            .map_err(|conductor_api_error| RibosomeError::from(Box::new(conductor_api_error)))?;
        Ok(Self {
            cell_id,
            zome,
            cap_secret,
            fn_name,
            payload,
            provenance,
            nonce,
            expires_at,
        })
    }
}

impl From<ZomeCallInvocation> for ZomeCallParams {
    fn from(inv: ZomeCallInvocation) -> Self {
        let ZomeCallInvocation {
            cell_id,
            zome,
            fn_name,
            cap_secret,
            payload,
            provenance,
            nonce,
            expires_at,
        } = inv;
        Self {
            cell_id,
            provenance,
            zome_name: zome.zome_name().clone(),
            fn_name,
            cap_secret,
            payload,
            nonce,
            expires_at,
        }
    }
}

#[derive(Clone, Constructor)]
pub struct ZomeCallHostAccess {
    pub workspace: HostFnWorkspace,
    pub keystore: MetaLairClient,
    pub dpki: Option<DpkiImpl>,
    pub network: HolochainP2pDna,
    pub signal_tx: broadcast::Sender<Signal>,
    pub call_zome_handle: CellConductorReadHandle,
}

impl std::fmt::Debug for ZomeCallHostAccess {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("ZomeCallHostAccess").finish()
    }
}

impl From<ZomeCallHostAccess> for HostContext {
    fn from(zome_call_host_access: ZomeCallHostAccess) -> Self {
        Self::ZomeCall(zome_call_host_access)
    }
}

impl From<&ZomeCallHostAccess> for HostFnAccess {
    fn from(_: &ZomeCallHostAccess) -> Self {
        Self::all()
    }
}

/// Interface for a Ribosome. Currently used only for mocking, as our only
/// real concrete type is [`RealRibosome`](crate::core::ribosome::real_ribosome::RealRibosome)
#[automock]
#[allow(async_fn_in_trait)]
pub trait RibosomeT: Sized + std::fmt::Debug + Send + Sync {
    fn dna_def(&self) -> &DnaDefHashed;

    fn dna_hash(&self) -> &DnaHash;

    fn dna_file(&self) -> &DnaFile;

    async fn zome_info(&self, zome: Zome) -> RibosomeResult<ZomeInfo>;

    fn zomes_to_invoke(&self, zomes_to_invoke: ZomesToInvoke) -> Vec<Zome> {
        match zomes_to_invoke {
            ZomesToInvoke::AllIntegrity => self
                .dna_def()
                .integrity_zomes
                .iter()
                .map(|(n, d)| (n.clone(), d.clone().erase_type()).into())
                .collect(),
            ZomesToInvoke::All => self
                .dna_def()
                .all_zomes()
                .map(|(n, d)| (n.clone(), d.clone()).into())
                .collect(),
            ZomesToInvoke::One(zome) => vec![zome],
            ZomesToInvoke::OneIntegrity(zome) => vec![zome.erase_type()],
            ZomesToInvoke::OneCoordinator(zome) => vec![zome.erase_type()],
        }
    }

    fn zome_name_to_id(&self, zome_name: &ZomeName) -> RibosomeResult<ZomeIndex> {
        match self
            .dna_def()
            .all_zomes()
            .position(|(name, _)| name == zome_name)
        {
            Some(index) => Ok(holochain_zome_types::action::ZomeIndex::from(index as u8)),
            None => Err(RibosomeError::ZomeNotExists(zome_name.to_owned())),
        }
    }

    fn get_integrity_zome(&self, zome_index: &ZomeIndex) -> Option<IntegrityZome>;

    fn call_stream<I: Invocation + 'static>(
        &self,
        host_context: HostContext,
        invocation: I,
    ) -> CallStream;

    fn maybe_call<I: Invocation + 'static>(
        &self,
        host_context: HostContext,
        invocation: &I,
        zome: &Zome,
        to_call: &FunctionName,
    ) -> MustBoxFuture<'static, Result<Option<ExternIO>, RibosomeError>>
    where
        Self: 'static;

    /// Get a value from a const wasm function.
    ///
    /// This is really a stand in until Rust can properly support
    /// const wasm values.
    ///
    /// This allows getting values from wasm without the need for any translation.
    /// The same technique can be used with the wasmer cli to validate these
    /// values without needing to make holochain a dependency.
    async fn get_const_fn(&self, zome: &Zome, name: &str) -> Result<Option<i32>, RibosomeError>;

    // @todo list out all the available callbacks and maybe cache them somewhere
    fn list_callbacks(&self) {
        unimplemented!()
        // pseudocode
        // self.instance().exports().filter(|e| e.is_callback())
    }

    // @todo list out all the available zome functions and maybe cache them somewhere
    fn list_zome_fns(&self) {
        unimplemented!()
        // pseudocode
        // self.instance().exports().filter(|e| !e.is_callback())
    }

    async fn run_genesis_self_check(
        &self,
        access: GenesisSelfCheckHostAccess,
        invocation: GenesisSelfCheckInvocation,
    ) -> RibosomeResult<GenesisSelfCheckResult>;

    async fn run_init(
        &self,
        access: InitHostAccess,
        invocation: InitInvocation,
    ) -> RibosomeResult<InitResult>;

    async fn run_entry_defs(
        &self,
        access: EntryDefsHostAccess,
        invocation: EntryDefsInvocation,
    ) -> RibosomeResult<EntryDefsResult>;

    async fn run_post_commit(
        &self,
        access: PostCommitHostAccess,
        invocation: PostCommitInvocation,
    ) -> RibosomeResult<()>;

    /// Helper function for running a validation callback. Calls
    /// private fn `do_callback!` under the hood.
    async fn run_validate(
        &self,
        access: ValidateHostAccess,
        invocation: ValidateInvocation,
    ) -> RibosomeResult<ValidateResult>;

    /// Runs the specified zome fn. Returns the cursor used by HDK,
    /// so that it can be passed on to source chain manager for transactional writes
    async fn call_zome_function(
        &self,
        access: ZomeCallHostAccess,
        invocation: ZomeCallInvocation,
    ) -> RibosomeResult<ZomeCallResponse>;

    fn zome_types(&self) -> &Arc<GlobalZomeTypes>;
}

/// Placeholder for weighing. Currently produces zero weight.
pub fn weigh_placeholder() -> EntryRateWeight {
    EntryRateWeight::default()
}

#[cfg(test)]
pub mod wasm_test {
    use crate::core::ribosome::FnComponents;
    use crate::sweettest::SweetCell;
    use crate::sweettest::SweetConductor;
    use crate::sweettest::SweetDnaFile;
    use crate::sweettest::SweetZome;
    use crate::test_utils::host_fn_caller::HostFnCaller;
    use core::time::Duration;
    use hdk::prelude::*;
    use holo_hash::AgentPubKey;
    use holochain_nonce::fresh_nonce;
    use holochain_wasm_test_utils::TestWasm;
    use holochain_zome_types::zome_io::ZomeCallParams;

    pub fn now() -> Duration {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .expect("Time went backwards")
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn verify_zome_call_test() {
        holochain_trace::test_run();
        let RibosomeTestFixture {
            conductor,
            alice,
            alice_pubkey,
            bob_pubkey,
            ..
        } = RibosomeTestFixture::new(TestWasm::Capability).await;

        let now = Timestamp::now();
        let (nonce, expires_at) = fresh_nonce(now).unwrap();
        let alice_zome_call_params = ZomeCallParams {
            provenance: alice_pubkey.clone(),
            cell_id: alice.cell_id().clone(),
            zome_name: TestWasm::Capability.coordinator_zome_name(),
            fn_name: "needs_cap_claim".into(),
            cap_secret: None,
            payload: ExternIO::encode(()).unwrap(),
            nonce,
            expires_at,
        };

        // Bob observes or forges a valid zome call from alice.
        // He removes Alice's signature but leaves her provenance and adds his own signature.
        let mut bob_zome_call_params = alice_zome_call_params.clone();
        bob_zome_call_params.provenance = bob_pubkey.clone();

        // The call should fail for bob.
        let bob_call_result = conductor.raw_handle().call_zome(bob_zome_call_params).await;

        match bob_call_result {
            Ok(Ok(ZomeCallResponse::Unauthorized(..))) => { /* (   ) */ }
            _ => panic!("{:?}", bob_call_result),
        }

        // The call should NOT fail for alice (e.g. bob's forgery should not consume alice's nonce).
        let alice_call_result_0 = conductor
            .raw_handle()
            .call_zome(alice_zome_call_params.clone())
            .await;

        match alice_call_result_0 {
            Ok(Ok(ZomeCallResponse::Ok(_))) => { /*  */ }
            _ => panic!("{:?}", alice_call_result_0),
        }

        // The same call cannot be used a second time.
        let alice_call_result_1 = conductor
            .raw_handle()
            .call_zome(alice_zome_call_params)
            .await;

        match alice_call_result_1 {
            Ok(Ok(ZomeCallResponse::Unauthorized(..))) => { /* () */ }
            _ => panic!("{:?}", bob_call_result),
        }
    }

    #[test]
    fn fn_components_iterate() {
        let fn_components = FnComponents::from(vec!["foo".into(), "bar".into(), "baz".into()]);
        let expected = vec!["foo_bar_baz", "foo_bar", "foo"];

        assert_eq!(fn_components.into_iter().collect::<Vec<String>>(), expected,);
    }

    pub struct RibosomeTestFixture {
        pub conductor: SweetConductor,
        pub alice_pubkey: AgentPubKey,
        pub bob_pubkey: AgentPubKey,
        pub alice: SweetZome,
        pub bob: SweetZome,
        pub alice_cell: SweetCell,
        pub bob_cell: SweetCell,
        pub alice_host_fn_caller: HostFnCaller,
        pub bob_host_fn_caller: HostFnCaller,
    }

    impl RibosomeTestFixture {
        pub async fn new(test_wasm: TestWasm) -> Self {
            let (dna_file, _, _) = SweetDnaFile::unique_from_test_wasms(vec![test_wasm]).await;

            let mut conductor = SweetConductor::from_standard_config().await;

            let apps = conductor.setup_apps("app-", 2, [&dna_file]).await.unwrap();

            let ((alice_cell,), (bob_cell,)) = apps.into_tuples();

            let alice_host_fn_caller = HostFnCaller::create_for_zome(
                alice_cell.cell_id(),
                &conductor.raw_handle(),
                &dna_file,
                0,
            )
            .await;

            let bob_host_fn_caller = HostFnCaller::create_for_zome(
                bob_cell.cell_id(),
                &conductor.raw_handle(),
                &dna_file,
                0,
            )
            .await;

            let alice = alice_cell.zome(test_wasm);
            let bob = bob_cell.zome(test_wasm);

            let alice_pubkey = alice_cell.agent_pubkey().clone();
            let bob_pubkey = bob_cell.agent_pubkey().clone();

            Self {
                conductor,
                alice_pubkey,
                bob_pubkey,
                alice,
                bob,
                alice_cell,
                bob_cell,
                alice_host_fn_caller,
                bob_host_fn_caller,
            }
        }
    }
}



================================================
File: crates/holochain/src/core/share.rs
================================================
use std::sync::Arc;

pub type ShareResult<T> = Result<T, ShareError>;

#[derive(Debug, thiserror::Error)]
#[non_exhaustive]
pub enum ShareError {
    /// This object is closed, calls on it are invalid.
    #[error("This object is closed, calls on it are invalid.")]
    Closed,
    /// An error ocurred in a closure accessing the Share state
    #[cfg(feature = "unstable-countersigning")]
    #[error(transparent)]
    ClosureFailed(Box<dyn std::error::Error + Send + Sync>),
}

/// Synchronized droppable share-lock around internal state date.
pub struct Share<T: 'static + Send>(Arc<parking_lot::Mutex<Option<T>>>);

impl<T: 'static + Send> Clone for Share<T> {
    fn clone(&self) -> Self {
        Self(self.0.clone())
    }
}

impl<T: 'static + Send + std::fmt::Debug> std::fmt::Debug for Share<T> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        self.share_ref(|s| Ok(f.debug_tuple("Share").field(s).finish()))
            .unwrap()
    }
}

impl<T: 'static + Send> PartialEq for Share<T> {
    fn eq(&self, oth: &Self) -> bool {
        Arc::ptr_eq(&self.0, &oth.0)
    }
}

impl<T: 'static + Send> Eq for Share<T> {}

impl<T: 'static + Send> std::hash::Hash for Share<T> {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        Arc::as_ptr(&self.0).hash(state);
    }
}

impl<T: 'static + Send + Default> Default for Share<T> {
    fn default() -> Self {
        Self::new(T::default())
    }
}

impl<T: 'static + Send> Share<T> {
    /// Create a new share lock.
    pub fn new(t: T) -> Self {
        Self(Arc::new(parking_lot::Mutex::new(Some(t))))
    }

    /// Execute code with immutable access to the internal state.
    pub fn share_ref<R, F>(&self, f: F) -> ShareResult<R>
    where
        F: FnOnce(&T) -> ShareResult<R>,
    {
        let t = self.0.lock();
        if t.is_none() {
            return Err(ShareError::Closed);
        }
        f(t.as_ref().unwrap())
    }

    /// Execute code with mut access to the internal state.
    /// The second param, if set to true, will drop the shared state,
    /// any further access will `Err(ShareError::Closed)`.
    /// E.g. `share.share_mut(|_state, close| *close = true).unwrap();`
    pub fn share_mut<R, F>(&self, f: F) -> ShareResult<R>
    where
        F: FnOnce(&mut T, &mut bool) -> ShareResult<R>,
    {
        let mut t = self.0.lock();
        if t.is_none() {
            return Err(ShareError::Closed);
        }
        let mut close = false;
        let r = f(t.as_mut().unwrap(), &mut close);
        if close {
            *t = None;
        }
        r
    }
}



================================================
File: crates/holochain/src/core/sys_validate.rs
================================================
//! # System Validation Checks
//! This module contains all the checks we run for sys validation

use super::queue_consumer::TriggerSender;
use super::workflow::incoming_dht_ops_workflow::incoming_dht_ops_workflow;
use super::workflow::sys_validation_workflow::SysValidationWorkspace;
use crate::conductor::space::Space;
use holochain_conductor_services::DpkiService;
use holochain_conductor_services::KeyState;
use holochain_keystore::AgentPubKeyExt;
use holochain_types::prelude::*;
use std::sync::Arc;

pub use error::*;
pub use holo_hash::*;
pub use holochain_state::source_chain::SourceChainError;
pub use holochain_state::source_chain::SourceChainResult;

#[allow(missing_docs)]
mod error;
#[cfg(test)]
mod tests;

/// 16mb limit on Entries due to websocket limits.
/// 4mb limit to constrain bandwidth usage on uploading.
/// (Assuming a baseline 5mbps upload for now... update this
/// as consumer internet connections trend toward more upload)
/// Consider splitting large entries up.
pub const MAX_ENTRY_SIZE: usize = ENTRY_SIZE_LIMIT;

/// 1kb limit on LinkTags.
/// Tags are used as keys to the database to allow
/// fast lookup so they should be small.
pub const MAX_TAG_SIZE: usize = 1000;

/// Verify the signature for this action
pub async fn verify_action_signature(sig: &Signature, action: &Action) -> SysValidationResult<()> {
    if action.signer().verify_signature(sig, action).await? {
        Ok(())
    } else {
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::CounterfeitAction((*sig).clone(), (*action).clone()),
        ))
    }
}

/// Verify the signature for this warrant
pub async fn verify_warrant_signature(warrant_op: &WarrantOp) -> SysValidationResult<()> {
    if warrant_op
        .author
        .verify_signature(warrant_op.signature(), warrant_op.warrant().clone())
        .await?
    {
        Ok(())
    } else {
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::CounterfeitWarrant(warrant_op.warrant().clone()),
        ))
    }
}

/// Verify the author key was valid at the time
/// of signing with dpki
/// TODO: This is just a stub until we have dpki.
pub async fn author_key_is_valid(_author: &AgentPubKey) -> SysValidationResult<()> {
    Ok(())
}

/// Verify the countersigning session contains the specified action.
pub fn check_countersigning_session_data_contains_action(
    entry_hash: EntryHash,
    session_data: &CounterSigningSessionData,
    action: NewEntryActionRef<'_>,
) -> SysValidationResult<()> {
    let weight = match action {
        NewEntryActionRef::Create(h) => h.weight.clone(),
        NewEntryActionRef::Update(h) => h.weight.clone(),
    };
    let action_is_in_session = session_data
        .build_action_set(entry_hash, weight)
        .map_err(SysValidationError::from)?
        .iter()
        .any(|session_action| match (&action, session_action) {
            (NewEntryActionRef::Create(create), Action::Create(session_create)) => {
                create == &session_create
            }
            (NewEntryActionRef::Update(update), Action::Update(session_update)) => {
                update == &session_update
            }
            _ => false,
        });
    if !action_is_in_session {
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::ActionNotInCounterSigningSession(
                session_data.to_owned(),
                action.to_new_entry_action(),
            ),
        ))
    } else {
        Ok(())
    }
}

/// Verify that the signature on a preflight request is valid.
pub async fn check_countersigning_preflight_response_signature(
    preflight_response: &PreflightResponse,
) -> SysValidationResult<()> {
    let signature_is_valid = preflight_response
        .request()
        .signing_agents
        .get(*preflight_response.agent_state().agent_index() as usize)
        .ok_or_else(|| {
            SysValidationError::ValidationOutcome(ValidationOutcome::PreflightResponseSignature(
                (*preflight_response).clone(),
            ))
        })?
        .0
        .verify_signature_raw(
            preflight_response.signature(),
            preflight_response
                .encode_for_signature()
                .map_err(|_| {
                    SysValidationError::ValidationOutcome(
                        ValidationOutcome::PreflightResponseSignature(
                            (*preflight_response).clone(),
                        ),
                    )
                })?
                .into(),
        )
        .await?;
    if signature_is_valid {
        Ok(())
    } else {
        Err(SysValidationError::ValidationOutcome(
            ValidationOutcome::PreflightResponseSignature((*preflight_response).clone()),
        ))
    }
}

/// Verify all the countersigning session data together.
pub async fn check_countersigning_session_data(
    entry_hash: EntryHash,
    session_data: &CounterSigningSessionData,
    action: NewEntryActionRef<'_>,
) -> SysValidationResult<()> {
    session_data.check_integrity()?;
    check_countersigning_session_data_contains_action(entry_hash, session_data, action)?;

    let tasks: Vec<_> = session_data
        .responses()
        .iter()
        .map(|(response, signature)| async move {
            let preflight_response = PreflightResponse::try_new(
                session_data.preflight_request().clone(),
                response.clone(),
                signature.clone(),
            )?;
            check_countersigning_preflight_response_signature(&preflight_response).await
        })
        .collect();

    let results: Vec<SysValidationResult<()>> = futures::future::join_all(tasks).await;
    let results: SysValidationResult<()> = results.into_iter().collect();
    match results {
        Ok(_) => Ok(()),
        Err(e) => Err(e),
    }
}

/// Check that the correct actions have the correct setting for prev_action:
/// - Dna can never have a prev_action, and must have seq == 0.
/// - All other actions must have prev_action, and seq > 0.
pub fn check_prev_action(action: &Action) -> SysValidationResult<()> {
    let is_dna = matches!(action, Action::Dna(_));
    let has_prev = action.prev_action().is_some();
    let is_first = action.action_seq() == 0;
    #[allow(clippy::collapsible_else_if)]
    if is_first {
        if is_dna && !has_prev {
            Ok(())
        } else {
            // Note that the implementation of the action types and `prev_action` should prevent this being hit
            // but this is useful as a defensive check.
            Err(PrevActionErrorKind::InvalidRoot)
        }
    } else {
        if !is_dna && has_prev {
            Ok(())
        } else {
            Err(PrevActionErrorKind::MissingPrev)
        }
    }
    .map_err(|e| ValidationOutcome::PrevActionError((e, action.clone()).into()).into())
}

/// Check that Dna actions are only added to empty source chains
pub fn check_valid_if_dna(action: &Action, dna_def: &DnaDefHashed) -> SysValidationResult<()> {
    match action {
        Action::Dna(a) => {
            let dna_hash = dna_def.as_hash();
            if a.hash != *dna_hash {
                Err(ValidationOutcome::WrongDna(a.hash.clone(), dna_hash.clone()).into())
            } else if action.timestamp() < dna_def.modifiers.origin_time {
                // If the Dna timestamp is ahead of the origin time, every other action
                // will be inductively so also due to the prev_action check
                Err(PrevActionErrorKind::InvalidRootOriginTime).map_err(|e| {
                    ValidationOutcome::PrevActionError((e, action.clone()).into()).into()
                })
            } else {
                Ok(())
            }
        }
        _ => Ok(()),
    }
}

/// Check if there are other actions at this
/// sequence number
pub async fn check_chain_rollback(
    action: &Action,
    workspace: &SysValidationWorkspace,
) -> SysValidationResult<()> {
    let empty = workspace.action_seq_is_empty(action).await?;

    // Ok or log warning
    if empty {
        Ok(())
    } else {
        // TODO: implement real rollback detection once we know what that looks like
        tracing::error!(
            "Chain rollback detected at position {} for agent {:?} from action {:?}",
            action.action_seq(),
            action.author(),
            action,
        );
        Ok(())
    }
}

/// Placeholder for future spam check.
/// Check action timestamps don't exceed MAX_PUBLISH_FREQUENCY
pub async fn check_spam(_action: &Action) -> SysValidationResult<()> {
    Ok(())
}

/// Check that created agents are always paired with an AgentValidationPkg and vice versa
pub fn check_agent_validation_pkg_predecessor(
    action: &Action,
    prev_action: &Action,
) -> SysValidationResult<()> {
    let maybe_error = match (prev_action, action) {
        (
            Action::AgentValidationPkg(AgentValidationPkg { .. }),
            Action::Create(Create {
                entry_type: EntryType::AgentPubKey,
                ..
            })
            | Action::Update(Update {
                entry_type: EntryType::AgentPubKey,
                ..
            }),
        ) => None,
        (Action::AgentValidationPkg(AgentValidationPkg { .. }), _) => Some(
            "Every AgentValidationPkg must be followed by a Create or Update for an AgentPubKey",
        ),
        (
            _,
            Action::Create(Create {
                entry_type: EntryType::AgentPubKey,
                ..
            })
            | Action::Update(Update {
                entry_type: EntryType::AgentPubKey,
                ..
            }),
        ) => Some(
            "Every Create or Update for an AgentPubKey must be preceded by an AgentValidationPkg",
        ),
        _ => None,
    };

    if let Some(error) = maybe_error {
        Err(PrevActionErrorKind::InvalidSuccessor(
            error.to_string(),
            Box::new((prev_action.clone(), action.clone())),
        ))
        .map_err(|e| ValidationOutcome::PrevActionError((e, action.clone()).into()).into())
    } else {
        Ok(())
    }
}

/// Check that the author didn't change between actions
pub fn check_prev_author(action: &Action, prev_action: &Action) -> SysValidationResult<()> {
    let a1 = prev_action.author().clone();
    let a2 = action.author();
    if a1 == *a2 {
        Ok(())
    } else {
        Err(PrevActionErrorKind::Author(a1, a2.clone()))
            .map_err(|e| ValidationOutcome::PrevActionError((e, action.clone()).into()).into())
    }
}

/// Check previous action timestamp is before this action
pub fn check_prev_timestamp(action: &Action, prev_action: &Action) -> SysValidationResult<()> {
    let t1 = prev_action.timestamp();
    let t2 = action.timestamp();
    if t2 >= t1 {
        Ok(())
    } else {
        Err(PrevActionErrorKind::Timestamp(t1, t2))
            .map_err(|e| ValidationOutcome::PrevActionError((e, action.clone()).into()).into())
    }
}

/// Check the previous action is one less than the current
pub fn check_prev_seq(action: &Action, prev_action: &Action) -> SysValidationResult<()> {
    let action_seq = action.action_seq();
    let prev_seq = prev_action.action_seq();
    if action_seq > 0 && prev_seq == action_seq - 1 {
        Ok(())
    } else {
        Err(PrevActionErrorKind::InvalidSeq(action_seq, prev_seq))
            .map_err(|e| ValidationOutcome::PrevActionError((e, action.clone()).into()).into())
    }
}

/// Check the entry variant matches the variant in the actions entry type
pub fn check_entry_type(entry_type: &EntryType, entry: &Entry) -> SysValidationResult<()> {
    entry_type_matches(entry_type, entry)
        .then_some(())
        .ok_or_else(|| ValidationOutcome::EntryTypeMismatch.into())
}

/// Check that the EntryVisibility is congruous with the presence or absence of entry data
pub fn check_entry_visibility(op: &ChainOp) -> SysValidationResult<()> {
    use EntryVisibility::*;
    use RecordEntry::*;

    let err = |reason: &str| {
        Err(ValidationOutcome::MalformedDhtOp(
            Box::new(op.action()),
            op.get_type(),
            reason.to_string(),
        )
        .into())
    };

    match (op.action().entry_type().map(|t| t.visibility()), op.entry()) {
        (Some(Public), Present(_)) => Ok(()),
        (Some(Private), Hidden) => Ok(()),
        (Some(Private), NotStored) => Ok(()),

        (Some(Public), Hidden) => err("RecordEntry::Hidden is only for Private entry type"),
        (Some(_), NA) => err("There is action entry data but the entry itself is N/A"),
        (Some(Private), Present(_)) => Err(ValidationOutcome::PrivateEntryLeaked.into()),
        (Some(Public), NotStored) => {
            if op.get_type() == ChainOpType::RegisterAgentActivity
                || op.action().entry_type() == Some(&EntryType::AgentPubKey)
            {
                // RegisterAgentActivity is a special case, where the entry data can be omitted.
                // Agent entries are also a special case. The "entry data" is already present in
                // the action as the entry hash, so no external entry data is needed.
                Ok(())
            } else {
                err("Op has public entry type but is missing its data")
            }
        }
        (None, NA) => Ok(()),
        (None, _) => err("Entry must be N/A for action with no entry type"),
    }
}

/// Check that the record is valid from the perspective of DPKI.
pub async fn check_dpki_agent_validity_for_record(
    dpki: &DpkiService,
    record: &Record,
) -> SysValidationResult<()> {
    let author = record.action().author().clone();
    let timestamp = if record.action().is_genesis() {
        None
    } else {
        Some(record.action().timestamp())
    };
    check_dpki_agent_validity(dpki, author, timestamp).await
}

/// Check that the op is valid from the perspective of DPKI.
pub async fn check_dpki_agent_validity_for_op(
    dpki: &DpkiService,
    op: &ChainOp,
) -> SysValidationResult<()> {
    let author = op.author().clone();
    let timestamp = if op.is_genesis() {
        None
    } else {
        Some(op.timestamp())
    };
    let agent_validity_result = check_dpki_agent_validity(dpki, author, timestamp).await;
    // If agent key is invalid in Dpki and the op being validated is a `Delete` of that agent key, it must pass
    // for the delete to succeed. Otherwise Dpki would prevent deletion of an agent key on the source chain.
    if let Err(SysValidationError::ValidationOutcome(ValidationOutcome::DpkiAgentInvalid(_, _))) =
        &agent_validity_result
    {
        if let Action::Delete(d) = &op.action() {
            if d.deletes_entry_address == op.author().clone().into() {
                return Ok(());
            }
        }
    }
    agent_validity_result
}

/// Check that the agent is valid from the perspective of DPKI.
///
/// There are different rules for genesis actions and all other actions:
/// - For genesis actions, we use None for the timestamp, and we only check that the key is the
///   first key in the app's keyset, i.e. that key_index == 0.
/// - For all other actions, we include the timestamp, and use `key_state` to check that the key
///   is valid as-at that timestamp.
async fn check_dpki_agent_validity(
    dpki: &DpkiService,
    author: AgentPubKey,
    timestamp: Option<Timestamp>,
) -> SysValidationResult<()> {
    if let Some(timestamp) = timestamp {
        let key_state = dpki
            .state()
            .await
            .key_state(author.clone(), timestamp)
            .await?;

        match key_state {
            KeyState::Valid(_) => Ok(()),
            KeyState::Invalid(_) => {
                Err(ValidationOutcome::DpkiAgentInvalid(author, timestamp).into())
            }
            KeyState::NotFound => Err(ValidationOutcome::DpkiAgentMissing(author).into()),
        }
    } else {
        // TODO: add check for key_index == 0 once updates are possible
        tracing::info!("Skipping DPKI check for genesis action until updates are possible");
        Ok(())
    }
}

/// Check the actions entry hash matches the hash of the entry
pub fn check_entry_hash(hash: &EntryHash, entry: &Entry) -> SysValidationResult<()> {
    if *hash == EntryHash::with_data_sync(entry) {
        Ok(())
    } else {
        Err(ValidationOutcome::EntryHash.into())
    }
}

/// Check the action should have an entry.
/// Is either a Create or Update
pub fn check_new_entry_action(action: &Action) -> SysValidationResult<()> {
    match action {
        Action::Create(_) | Action::Update(_) => Ok(()),
        _ => Err(ValidationOutcome::NotNewEntry(action.clone()).into()),
    }
}

/// Check the entry size is under the MAX_ENTRY_SIZE
pub fn check_entry_size(entry: &Entry) -> SysValidationResult<()> {
    match entry {
        Entry::App(bytes) | Entry::CounterSign(_, bytes) => {
            let size = std::mem::size_of_val(&bytes.bytes()[..]);
            if size <= MAX_ENTRY_SIZE {
                Ok(())
            } else {
                Err(ValidationOutcome::EntryTooLarge(size).into())
            }
        }
        _ => {
            // TODO: size checks on other types (cap grant and claim)
            Ok(())
        }
    }
}

/// Check the link tag size is under the MAX_TAG_SIZE
pub fn check_tag_size(tag: &LinkTag) -> SysValidationResult<()> {
    let size = std::mem::size_of_val(&tag.0[..]);
    if size <= MAX_TAG_SIZE {
        Ok(())
    } else {
        Err(ValidationOutcome::TagTooLarge(size).into())
    }
}

/// Check a Update's entry type is the same for
/// original and new entry.
pub fn check_update_reference(
    update: &Update,
    original_entry_action: &NewEntryActionRef<'_>,
) -> SysValidationResult<()> {
    if update.entry_type != *original_entry_action.entry_type() {
        return Err(ValidationOutcome::UpdateTypeMismatch(
            original_entry_action.entry_type().clone(),
            update.entry_type.clone(),
        )
        .into());
    }

    if update.original_entry_address != *original_entry_action.entry_hash() {
        return Err(ValidationOutcome::UpdateHashMismatch(
            original_entry_action.entry_hash().clone(),
            update.original_entry_address.clone(),
        )
        .into());
    }

    Ok(())
}

/// Allows DhtOps to be sent to some receiver
#[async_trait::async_trait]
#[cfg_attr(test, mockall::automock)]
pub trait DhtOpSender {
    /// Sends an op
    async fn send_op(&self, op: DhtOp) -> SysValidationResult<()>;

    /// Send a StoreRecord DhtOp
    async fn send_store_record(&self, record: Record) -> SysValidationResult<()>;

    /// Send a StoreEntry DhtOp
    async fn send_store_entry(&self, record: Record) -> SysValidationResult<()>;

    /// Send a RegisterAddLink DhtOp
    async fn send_register_add_link(&self, record: Record) -> SysValidationResult<()>;

    /// Send a RegisterAgentActivity DhtOp
    async fn send_register_agent_activity(&self, record: Record) -> SysValidationResult<()>;
}

/// Allows you to send an op to the
/// incoming_dht_ops_workflow if you
/// found it on the network and were supposed
/// to be holding it.
#[derive(derive_more::Constructor, Clone)]
pub struct IncomingDhtOpSender {
    space: Arc<Space>,
    sys_validation_trigger: TriggerSender,
}

#[async_trait::async_trait]
impl DhtOpSender for IncomingDhtOpSender {
    async fn send_op(&self, op: DhtOp) -> SysValidationResult<()> {
        let ops = vec![op];
        Ok(incoming_dht_ops_workflow(
            self.space.as_ref().clone(),
            self.sys_validation_trigger.clone(),
            ops,
            false,
        )
        .await
        .map_err(Box::new)?)
    }

    async fn send_store_record(&self, record: Record) -> SysValidationResult<()> {
        self.send_op(make_store_record(record).into()).await
    }

    async fn send_store_entry(&self, record: Record) -> SysValidationResult<()> {
        // TODO: MD: isn't it already too late if we've received a private entry from the network at this point?
        let is_public_entry = record.action().entry_type().map_or(false, |et| {
            matches!(et.visibility(), EntryVisibility::Public)
        });
        if is_public_entry {
            if let Some(op) = make_store_entry(record) {
                self.send_op(op.into()).await?;
            }
        }
        Ok(())
    }

    async fn send_register_add_link(&self, record: Record) -> SysValidationResult<()> {
        if let Some(op) = make_register_add_link(record) {
            self.send_op(op.into()).await?;
        }

        Ok(())
    }

    async fn send_register_agent_activity(&self, record: Record) -> SysValidationResult<()> {
        self.send_op(make_register_agent_activity(record).into())
            .await
    }
}

/// Make a StoreRecord ChainOp from a Record.
/// Note that this can fail if the op is missing an
/// Entry when it was supposed to have one.
///
/// Because adding ops to incoming limbo while we are checking them
/// is only faster then waiting for them through gossip we don't care enough
/// to return an error.
fn make_store_record(record: Record) -> ChainOp {
    // Extract the data
    let (shh, record_entry) = record.privatized().0.into_inner();
    let (action, signature) = shh.into_inner();
    let action = action.into_content();

    // Create the op
    ChainOp::StoreRecord(signature, action, record_entry)
}

/// Make a StoreEntry ChainOp from a Record.
/// Note that this can fail if the op is missing an Entry or
/// the action is the wrong type.
///
/// Because adding ops to incoming limbo while we are checking them
/// is only faster then waiting for them through gossip we don't care enough
/// to return an error.
fn make_store_entry(record: Record) -> Option<ChainOp> {
    // Extract the data
    let (shh, record_entry) = record.into_inner();
    let (action, signature) = shh.into_inner();

    // Check the entry and exit early if it's not there
    let entry_box = record_entry.into_option()?;
    // If the action is the wrong type exit early
    let action = action.into_content().try_into().ok()?;

    // Create the op
    let op = ChainOp::StoreEntry(signature, action, entry_box);
    Some(op)
}

/// Make a RegisterAddLink ChainOp from a Record.
/// Note that this can fail if the action is the wrong type
///
/// Because adding ops to incoming limbo while we are checking them
/// is only faster then waiting for them through gossip we don't care enough
/// to return an error.
fn make_register_add_link(record: Record) -> Option<ChainOp> {
    // Extract the data
    let (shh, _) = record.into_inner();
    let (action, signature) = shh.into_inner();

    // If the action is the wrong type exit early
    let action = action.into_content().try_into().ok()?;

    // Create the op
    let op = ChainOp::RegisterAddLink(signature, action);
    Some(op)
}

/// Make a RegisterAgentActivity ChainOp from a Record.
/// Note that this can fail if the action is the wrong type
///
/// Because adding ops to incoming limbo while we are checking them
/// is only faster then waiting for them through gossip we don't care enough
/// to return an error.
fn make_register_agent_activity(record: Record) -> ChainOp {
    // Extract the data
    let (shh, _) = record.into_inner();
    let (action, signature) = shh.into_inner();

    // TODO something seems to have changed here, should this not be able to fail?
    // If the action is the wrong type exit early
    let action = action.into_content();

    // Create the op
    ChainOp::RegisterAgentActivity(signature, action)
}

#[cfg(test)]
mod test {
    use super::check_countersigning_preflight_response_signature;
    use crate::core::sys_validate::error::SysValidationError;
    use crate::core::ValidationOutcome;
    use crate::prelude::EntryTypeFixturator;
    use crate::prelude::{ActionBase, CounterSigningAgentState, CounterSigningSessionTimes};
    use fixt::fixt;
    use hdk::prelude::{PreflightBytes, Signature, SIGNATURE_BYTES};
    use holo_hash::fixt::ActionHashFixturator;
    use holo_hash::fixt::EntryHashFixturator;
    use holochain_keystore::AgentPubKeyExt;
    use holochain_timestamp::Timestamp;
    use holochain_types::prelude::PreflightRequest;
    use holochain_zome_types::countersigning::PreflightResponse;
    use holochain_zome_types::prelude::CreateBase;
    use matches::assert_matches;
    use std::time::Duration;

    #[tokio::test(flavor = "multi_thread")]
    async fn test_check_countersigning_preflight_response_signature() {
        let keystore = holochain_keystore::test_keystore();

        let agent_1 = keystore.new_sign_keypair_random().await.unwrap();
        let agent_2 = keystore.new_sign_keypair_random().await.unwrap();

        let request = PreflightRequest::try_new(
            fixt!(EntryHash),
            vec![(agent_1.clone(), vec![]), (agent_2, vec![])],
            vec![],
            0,
            false,
            CounterSigningSessionTimes::try_new(
                Timestamp::now(),
                (Timestamp::now() + Duration::from_secs(30)).unwrap(),
            )
            .unwrap(),
            ActionBase::Create(CreateBase::new(fixt!(EntryType))),
            PreflightBytes(vec![1, 2, 3]),
        )
        .unwrap();

        let agent_state = [
            CounterSigningAgentState::new(0, fixt!(ActionHash), 100),
            CounterSigningAgentState::new(1, fixt!(ActionHash), 50),
        ];

        let preflight_response = PreflightResponse::try_new(
            request.clone(),
            agent_state[0].clone(),
            Signature(vec![0; SIGNATURE_BYTES].try_into().unwrap()),
        )
        .unwrap();

        assert_matches!(
            check_countersigning_preflight_response_signature(&preflight_response).await,
            Err(SysValidationError::ValidationOutcome(
                ValidationOutcome::PreflightResponseSignature(_)
            ))
        );

        let sig_data =
            PreflightResponse::encode_fields_for_signature(&request, &agent_state[0]).unwrap();
        let signature = agent_1.sign_raw(&keystore, sig_data.into()).await.unwrap();
        let preflight_response =
            PreflightResponse::try_new(request, agent_state[0].clone(), signature).unwrap();

        check_countersigning_preflight_response_signature(&preflight_response)
            .await
            .unwrap();
    }
}



================================================
File: crates/holochain/src/core/validation.rs
================================================
//! Types needed for all validation
use super::workflow::WorkflowResult;
use super::SourceChainError;
use super::SysValidationError;
use super::ValidationOutcome;
use holochain_state::prelude::IncompleteCommitReason;
use std::convert::TryFrom;

/// Exit early with either an outcome or an error
#[derive(Debug)]
pub enum OutcomeOrError<T, E> {
    Outcome(T),
    Err(E),
}

impl<T, E> OutcomeOrError<T, E> {
    /// Peel off an Outcome if that's what it is
    pub fn into_outcome(self) -> Option<T> {
        if let Self::Outcome(t) = self {
            Some(t)
        } else {
            None
        }
    }

    /// Peel off an Err if that's what it is
    pub fn into_err(self) -> Option<E> {
        if let Self::Err(e) = self {
            Some(e)
        } else {
            None
        }
    }
}

/// Helper macro for implementing from sub error types
/// for the error in OutcomeOrError
#[macro_export]
macro_rules! from_sub_error {
    ($error_type:ident, $sub_error_type:ident) => {
        impl<T> From<$sub_error_type> for OutcomeOrError<T, $error_type> {
            fn from(e: $sub_error_type) -> Self {
                OutcomeOrError::Err($error_type::from(e))
            }
        }
    };
}

impl OutcomeOrError<ValidationOutcome, SysValidationError> {
    /// Convert an OutcomeOrError<ValidationOutcome, SysValidationError> into
    /// a [crate::core::workflow::error::WorkflowError].
    ///
    /// The inner error will be a [SourceChainError] if sys validation ran successfully but produced
    /// an unsuccessful validation outcome. Otherwise, the error will be [SysValidationError] to
    /// explain why sys validation was not able to complete the validation request.
    pub fn into_workflow_error<T>(self) -> WorkflowResult<T> {
        let outcome = ValidationOutcome::try_from(self)?;
        match outcome {
            ValidationOutcome::DepMissingFromDht(deps) => Err(SourceChainError::IncompleteCommit(
                IncompleteCommitReason::DepMissingFromDht(vec![deps]),
            )
            .into()),
            outcome => Err(SourceChainError::InvalidCommit(outcome.to_string()).into()),
        }
    }
}



================================================
File: crates/holochain/src/core/workflow.rs
================================================
//! Workflows are the core building block of Holochain functionality.
//!
//! ## Properties
//!
//! Workflows are **transactional**, so that if any workflow fails to run to
//! completion, nothing will happen.
//!
//! In order to achieve this, workflow functions are **free of any side-effects
//! which modify cryptographic state**: they will not modify the source chain
//! nor send network messages which could cause other agents to update their own
//! source chain.
//!
//! Workflows are **never nested**. A workflow cannot call another workflow.
//! However, a workflow can specify that any number of other workflows should
//! be triggered after this one completes.
//!
//! Side effects and triggering of other workflows is specified declaratively
//! rather than imperatively. Each workflow returns a `WorkflowEffects` value
//! representing the side effects that should be run. The `finish` function
//! processes this value and performs the necessary actions, including
//! committing changes to the associated Workspace and triggering other
//! workflows.

mod error;
pub use error::*;

pub mod app_validation_workflow;
pub mod call_zome_workflow;
pub mod countersigning_workflow;
pub mod genesis_workflow;
pub mod incoming_dht_ops_workflow;
pub mod initialize_zomes_workflow;
pub mod integrate_dht_ops_workflow;
pub mod publish_dht_ops_workflow;
pub mod sys_validation_workflow;
pub mod validation_receipt_workflow;
pub mod witnessing_workflow;

// MAYBE: either remove wildcards or add wildcards for all above child modules
pub use call_zome_workflow::*;

pub use genesis_workflow::*;
pub use initialize_zomes_workflow::*;



================================================
File: crates/holochain/src/core/queue_consumer/app_validation_consumer.rs
================================================
//! The workflow and queue consumer for sys validation

use super::*;
use crate::core::workflow::app_validation_workflow::app_validation_workflow;
use crate::core::workflow::app_validation_workflow::AppValidationWorkspace;
use holochain_types::db_cache::DhtDbQueryCache;

/// Spawn the QueueConsumer for AppValidation workflow
#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(
        workspace,
        conductor,
        trigger_integration,
        trigger_publish,
        network,
        dht_query_cache
    ))
)]
pub fn spawn_app_validation_consumer(
    dna_hash: Arc<DnaHash>,
    workspace: AppValidationWorkspace,
    conductor: ConductorHandle,
    trigger_integration: TriggerSender,
    trigger_publish: TriggerSender,
    network: HolochainP2pDna,
    dht_query_cache: DhtDbQueryCache,
) -> TriggerSender {
    let (tx, rx) = TriggerSender::new();
    let workspace = Arc::new(workspace);

    queue_consumer_dna_bound(
        "app_validation_consumer",
        dna_hash.clone(),
        conductor.task_manager(),
        (tx.clone(), rx),
        move || {
            app_validation_workflow(
                dna_hash.clone(),
                workspace.clone(),
                trigger_integration.clone(),
                trigger_publish.clone(),
                conductor.clone(),
                network.clone(),
                dht_query_cache.clone(),
            )
        },
    );
    tx
}



================================================
File: crates/holochain/src/core/queue_consumer/countersigning_consumer.rs
================================================
//! The queue consumer for the countersigning workflow.

use super::*;
#[cfg(feature = "unstable-countersigning")]
use crate::core::workflow::countersigning_workflow::countersigning_workflow;
use tracing::*;

/// Spawn the QueueConsumer for the countersigning workflow
#[instrument(skip_all)]
#[allow(unused_variables)]
pub(crate) fn spawn_countersigning_consumer(
    space: Space,
    workspace: Arc<CountersigningWorkspace>,
    cell_id: CellId,
    conductor: ConductorHandle,
    integration_trigger: TriggerSender,
    publish_trigger: TriggerSender,
) -> TriggerSender {
    let (tx, rx) = TriggerSender::new();

    let self_trigger = tx.clone();
    queue_consumer_cell_bound(
        "countersigning_consumer",
        cell_id.clone(),
        conductor.task_manager(),
        (tx.clone(), rx),
        #[cfg(not(feature = "unstable-countersigning"))]
        move || async { WorkflowResult::Ok(WorkComplete::Complete) },
        #[cfg(feature = "unstable-countersigning")]
        move || {
            countersigning_workflow_fn(
                space.clone(),
                workspace.clone(),
                cell_id.clone(),
                conductor.clone(),
                self_trigger.clone(),
                integration_trigger.clone(),
                publish_trigger.clone(),
            )
        },
    );

    tx
}

#[cfg(feature = "unstable-countersigning")]
async fn countersigning_workflow_fn(
    space: Space,
    workspace: Arc<CountersigningWorkspace>,
    cell_id: CellId,
    conductor: ConductorHandle,
    self_trigger: TriggerSender,
    integration_trigger: TriggerSender,
    publish_trigger: TriggerSender,
) -> WorkflowResult<WorkComplete> {
    let signal_tx = conductor
        .get_signal_tx(&cell_id)
        .await
        .map_err(WorkflowError::other)?;

    let cell = conductor
        .cell_by_id(&cell_id)
        .await
        .map_err(WorkflowError::other)?;
    let cell_network = cell.holochain_p2p_dna();

    let keystore = conductor.keystore().clone();

    countersigning_workflow(
        space.clone(),
        workspace,
        Arc::new(cell_network.clone()),
        keystore,
        cell_id.clone(),
        signal_tx,
        self_trigger.clone(),
        integration_trigger.clone(),
        publish_trigger.clone(),
    )
    .await
}



================================================
File: crates/holochain/src/core/queue_consumer/integrate_dht_ops_consumer.rs
================================================
//! The workflow and queue consumer for DhtOp integration

use super::*;
use crate::conductor::manager::TaskManagerClient;
use crate::core::workflow::integrate_dht_ops_workflow::integrate_dht_ops_workflow;
use holochain_types::db_cache::DhtDbQueryCache;

/// Spawn the QueueConsumer for DhtOpIntegration workflow
#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(env, trigger_receipt, tm, network, dht_query_cache))
)]
pub fn spawn_integrate_dht_ops_consumer(
    dna_hash: Arc<DnaHash>,
    env: DbWrite<DbKindDht>,
    dht_query_cache: DhtDbQueryCache,
    tm: TaskManagerClient,
    trigger_receipt: TriggerSender,
    network: HolochainP2pDna,
) -> TriggerSender {
    let (tx, rx) = TriggerSender::new();

    super::queue_consumer_dna_bound(
        "integrate_dht_ops_consumer",
        dna_hash,
        tm,
        (tx.clone(), rx),
        move || {
            integrate_dht_ops_workflow(
                env.clone(),
                dht_query_cache.clone(),
                trigger_receipt.clone(),
                network.clone(),
            )
        },
    );

    tx
}



================================================
File: crates/holochain/src/core/queue_consumer/publish_dht_ops_consumer.rs
================================================
//! The workflow and queue consumer for sys validation

use super::*;

use crate::core::workflow::publish_dht_ops_workflow::publish_dht_ops_workflow;

/// Spawn the QueueConsumer for Publish workflow
#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(env, conductor, network))
)]
pub fn spawn_publish_dht_ops_consumer(
    cell_id: CellId,
    env: DbWrite<DbKindAuthored>,
    conductor: ConductorHandle,
    network: impl HolochainP2pDnaT + Clone + 'static,
) -> TriggerSender {
    // Create a trigger with an exponential back off starting at 1 minute
    // and maxing out at 5 minutes.
    // The back off is reset any time the trigger is called (when new data is committed)
    let (tx, rx) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60 * 5), true);
    let sender = tx.clone();
    super::queue_consumer_cell_bound(
        "publish_dht_ops_consumer",
        cell_id.clone(),
        conductor.task_manager(),
        (tx.clone(), rx),
        move || {
            let conductor = conductor.clone();
            let tx = tx.clone();
            let env = env.clone();
            let agent = cell_id.agent_pubkey().clone();
            let network = network.clone();
            let min_publish_interval = conductor
                .get_config()
                .conductor_tuning_params()
                .min_publish_interval();
            async move {
                if conductor.get_config().network.tuning_params.disable_publish {
                    Ok(WorkComplete::Complete)
                } else {
                    publish_dht_ops_workflow(
                        env,
                        Arc::new(network),
                        tx,
                        agent,
                        min_publish_interval,
                    )
                    .await
                }
            }
        },
    );
    sender
}



================================================
File: crates/holochain/src/core/queue_consumer/sys_validation_consumer.rs
================================================
//! The workflow and queue consumer for sys validation

use super::*;
use crate::core::workflow::sys_validation_workflow::validation_deps::SysValDeps;
use crate::core::workflow::sys_validation_workflow::SysValidationWorkspace;
use crate::core::workflow::sys_validation_workflow::{
    get_representative_agent, sys_validation_workflow,
};
use holochain_keystore::MetaLairClient;

/// Spawn the QueueConsumer for SysValidation workflow
#[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
pub fn spawn_sys_validation_consumer(
    workspace: SysValidationWorkspace,
    space: Space,
    conductor: ConductorHandle,
    trigger_app_validation: TriggerSender,
    trigger_publish: TriggerSender,
    network: HolochainP2pDna,
    keystore: MetaLairClient,
) -> TriggerSender {
    let (tx, rx) = TriggerSender::new();
    let trigger_self = tx.clone();
    let workspace = Arc::new(workspace);
    let space = Arc::new(space);

    let current_validation_dependencies = SysValDeps::default();

    super::queue_consumer_dna_bound(
        "sys_validation_consumer",
        space.dna_hash.clone(),
        conductor.task_manager(),
        (tx.clone(), rx),
        move || {
            if let Some(representative_agent) =
                get_representative_agent(&conductor, &network.dna_hash())
            {
                Either::Left(sys_validation_workflow(
                    workspace.clone(),
                    current_validation_dependencies.clone(),
                    trigger_app_validation.clone(),
                    trigger_publish.clone(),
                    trigger_self.clone(),
                    network.clone(),
                    keystore.clone(),
                    representative_agent,
                ))
            } else {
                tracing::warn!("No agent found for DNA, skipping sys validation");
                Either::Right(async move { Ok(WorkComplete::Complete) })
            }
        },
    );

    tx
}



================================================
File: crates/holochain/src/core/queue_consumer/tests.rs
================================================
use super::*;
use crate::core::workflow::publish_dht_ops_workflow::publish_dht_ops_workflow;
use ::fixt::*;
use holo_hash::fixt::ActionHashFixturator;
use holo_hash::fixt::AgentPubKeyFixturator;
use holo_hash::fixt::EntryHashFixturator;
use holochain_conductor_api::conductor::ConductorTuningParams;
use holochain_state::mutations;
use holochain_state::prelude::StateMutationResult;

#[tokio::test]
async fn test_trigger_receiver_waits_for_sender() {
    let (_tx, mut rx) = TriggerSender::new();
    let jh = tokio::spawn(async move { rx.listen().await.unwrap() });

    // This should timeout because the trigger was not called.
    let r = tokio::time::timeout(Duration::from_millis(10), jh).await;
    assert!(r.is_err());
}

#[tokio::test]
async fn test_trigger_send() {
    let (tx, mut rx) = TriggerSender::new();
    let jh = tokio::spawn(async move { rx.listen().await.unwrap() });
    tx.trigger(&"");

    // This should be joined because the trigger was called.
    let r = jh.await;
    assert!(r.is_ok());
}

#[tokio::test]
async fn test_trigger_only_permits_single_trigger() {
    holochain_trace::test_run();

    let (tx, mut rx) = TriggerSender::new();
    let jh = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(50)).await;
        rx.listen().await.unwrap();
        tokio::time::sleep(Duration::from_millis(10)).await;
        rx.listen().await.unwrap()
    });
    // Calling trigger twice before a listen should only
    // cause one listen to progress.
    tx.trigger(&"");
    tx.trigger(&"");

    // This should timeout because the second listen should not pass.
    let r = tokio::time::timeout(Duration::from_millis(100), jh).await;
    assert!(r.is_err());
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
async fn test_trigger_back_off() {
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60 * 5), false);

    // After 1m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );

    // After 2m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 2)
            && timer.elapsed() < Duration::from_secs(60 * 2 + 1)
    );

    // After 4m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 4)
            && timer.elapsed() < Duration::from_secs(60 * 4 + 1)
    );

    // After 5m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 5)
            && timer.elapsed() < Duration::from_secs(60 * 5 + 1)
    );

    // After 5m there should be another trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 5)
            && timer.elapsed() < Duration::from_secs(60 * 5 + 1)
    );

    tx.reset_back_off();

    // After 1m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
async fn test_trigger_loop() {
    let (_tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60), false);

    for _ in 0..100 {
        // After 1m there should be one trigger.
        let timer = tokio::time::Instant::now();
        rx.listen().await.unwrap();
        assert!(
            timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
        );
    }
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
async fn test_reset_on_trigger() {
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60 * 5), true);

    // After 1m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );

    // After 2m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 2)
            && timer.elapsed() < Duration::from_secs(60 * 2 + 1)
    );

    tx.trigger(&"");

    // There should be one trigger immediately.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(timer.elapsed() < Duration::from_secs(1));

    // After 1m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
async fn test_pause_resume() {
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60), false);

    for _ in 0..10 {
        // After 1m there should be one trigger.
        let timer = tokio::time::Instant::now();
        rx.listen().await.unwrap();
        assert!(
            timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
        );
    }

    tx.pause_loop();

    // After 1hr there should be no trigger.
    let r = tokio::time::timeout(Duration::from_secs(60 * 60), rx.listen()).await;
    assert!(r.is_err());

    tx.resume_loop();

    // After 1m there should be one trigger.
    let timer = tokio::time::Instant::now();
    rx.listen().await.unwrap();
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );
}

#[tokio::test]
#[ignore = "flaky due to dependence on timing"]
async fn test_concurrency() {
    // - Trigger overrides already waiting listen.
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_millis(60)..Duration::from_millis(60), false);
    let timer = tokio::time::Instant::now();
    let jh = tokio::spawn(async move { rx.listen().await.unwrap() });
    // - Make sure listen has been called already.
    tokio::time::sleep(Duration::from_millis(10)).await;
    tx.trigger(&"");
    jh.await.unwrap();
    assert!(timer.elapsed() < Duration::from_millis(20));

    // - Calling resume_loop_now doesn't override waiting listen when loop is running.
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_millis(60)..Duration::from_millis(60), false);
    let timer = tokio::time::Instant::now();
    let jh = tokio::spawn(async move { rx.listen().await.unwrap() });
    // - Make sure listen has been called already.
    tokio::time::sleep(Duration::from_millis(10)).await;
    tx.resume_loop_now();
    jh.await.unwrap();
    assert!(timer.elapsed() >= Duration::from_millis(60));

    // - Calling resume_loop_now does override waiting listen when loop is paused.
    let (tx, mut rx) =
        TriggerSender::new_with_loop(Duration::from_millis(60)..Duration::from_millis(60), false);
    tx.pause_loop();
    let timer = tokio::time::Instant::now();
    let jh = tokio::spawn(async move { rx.listen().await.unwrap() });
    // - Make sure listen has been called already.
    tokio::time::sleep(Duration::from_millis(10)).await;
    tx.resume_loop_now();
    jh.await.unwrap();
    assert!(timer.elapsed() < Duration::from_millis(20));
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
async fn publish_loop() {
    let kind = DbKindAuthored(Arc::new(fixt!(CellId)));
    let tmpdir = tempfile::Builder::new()
        .prefix("holochain-test-environments")
        .tempdir()
        .unwrap();
    let db = DbWrite::test(tmpdir.path(), kind).expect("Couldn't create test database");
    let action = Action::Create(Create {
        author: fixt!(AgentPubKey),
        timestamp: Timestamp::now(),
        action_seq: 5,
        prev_action: fixt!(ActionHash),
        entry_type: EntryType::App(AppEntryDef::new(
            0.into(),
            0.into(),
            EntryVisibility::Public,
        )),
        entry_hash: fixt!(EntryHash),
        weight: EntryRateWeight::default(),
    });
    let author = action.author().clone();
    let signature = Signature(vec![3; SIGNATURE_BYTES].try_into().unwrap());
    let op = ChainOp::RegisterAgentActivity(signature, action);
    let op = DhtOpHashed::from_content_sync(op);
    let op_hash = op.to_hash();
    db.write_async(move |txn| -> StateMutationResult<()> {
        mutations::insert_op_authored(txn, &op)
    })
    .await
    .unwrap();
    let mut dna_network = MockHolochainP2pDnaT::new();
    let (tx, mut op_published) = tokio::sync::mpsc::channel(100);
    dna_network
        .expect_publish()
        .returning(move |_, _, _, _, _, _, _| {
            tx.try_send(()).unwrap();
            Ok(())
        });
    let dna_network = Arc::new(dna_network);

    let (ts, mut trigger_recv) =
        TriggerSender::new_with_loop(Duration::from_secs(60)..Duration::from_secs(60 * 5), true);

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    // - Publish runs after a 1m.
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );

    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - Op was published.
    op_published.recv().await.unwrap();

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    // - Publish runs again after 2m.
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 2)
            && timer.elapsed() < Duration::from_secs(60 * 2 + 1)
    );
    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - But the op isn't published because it was published in the last five minutes.
    assert_eq!(
        op_published.try_recv(),
        Err(tokio::sync::mpsc::error::TryRecvError::Empty)
    );

    // - Triggering publish causes it to run again.
    ts.trigger(&"");

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    assert!(timer.elapsed() < Duration::from_secs(1));
    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - But still no op is published.
    assert_eq!(
        op_published.try_recv(),
        Err(tokio::sync::mpsc::error::TryRecvError::Empty)
    );

    // - Set the ops last publish time to five mins ago.
    let five_mins_ago = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .ok()
        .and_then(|epoch| {
            epoch.checked_sub(ConductorTuningParams::default().min_publish_interval())
        })
        .unwrap();

    db.write_async({
        let query_op_hash = op_hash.clone();
        move |txn| -> StateMutationResult<()> {
            mutations::set_last_publish_time(txn, &query_op_hash, five_mins_ago)
        }
    })
    .await
    .unwrap();

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    // - Publish runs after a 1m because the trigger reset the back off.
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );

    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - The data is published because of the last publish time being greater then the interval.
    op_published.recv().await.unwrap();

    // - Set receipts complete.
    db.write_async({
        let query_op_hash = op_hash.clone();
        move |txn| -> StateMutationResult<()> {
            mutations::set_receipts_complete(txn, &query_op_hash, true)
        }
    })
    .await
    .unwrap();

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    // - Publish runs after another 2ms.
    assert!(
        timer.elapsed() >= Duration::from_secs(60 * 2)
            && timer.elapsed() < Duration::from_secs(60 * 2 + 1)
    );
    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - But no op is published because receipts are complete.
    assert_eq!(
        op_published.try_recv(),
        Err(tokio::sync::mpsc::error::TryRecvError::Empty)
    );

    // - Confirm the publish loop doesn't run again.
    let r = tokio::time::timeout(Duration::from_secs(60 * 100), trigger_recv.listen()).await;
    assert!(r.is_err());
    assert_eq!(
        op_published.try_recv(),
        Err(tokio::sync::mpsc::error::TryRecvError::Empty)
    );

    // - Set the ops last publish time to five mins ago.
    // - Set receipts not complete.
    db.write_async({
        let query_op_hash = op_hash.clone();
        move |txn| -> StateMutationResult<()> {
            mutations::set_last_publish_time(txn, &query_op_hash, five_mins_ago)?;
            mutations::set_receipts_complete(txn, &query_op_hash, false)?;

            Ok(())
        }
    })
    .await
    .unwrap();

    // - Publish runs due to a trigger.
    ts.trigger(&"");
    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    assert!(timer.elapsed() < Duration::from_secs(1));
    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();

    // - Op was published.
    op_published.recv().await.unwrap();

    let timer = tokio::time::Instant::now();
    trigger_recv.listen().await.unwrap();
    // - Publish is looping again starting at 1m.
    assert!(
        timer.elapsed() >= Duration::from_secs(60) && timer.elapsed() < Duration::from_secs(61)
    );

    publish_dht_ops_workflow(
        db.clone(),
        dna_network.clone(),
        ts.clone(),
        author.clone(),
        ConductorTuningParams::default().min_publish_interval(),
    )
    .await
    .unwrap();
    // - The op is not published because of the time interval.
    assert_eq!(
        op_published.try_recv(),
        Err(tokio::sync::mpsc::error::TryRecvError::Empty)
    );
}



================================================
File: crates/holochain/src/core/queue_consumer/validation_receipt_consumer.rs
================================================
//! The workflow and queue consumer for validation receipt

use super::*;
use crate::core::workflow::validation_receipt_workflow::validation_receipt_workflow;
use futures::FutureExt;

/// Spawn the QueueConsumer for validation receipt workflow
#[cfg_attr(
    feature = "instrument",
    tracing::instrument(skip(env, conductor, network))
)]
pub fn spawn_validation_receipt_consumer(
    dna_hash: Arc<DnaHash>,
    env: DbWrite<DbKindDht>,
    conductor: ConductorHandle,
    network: HolochainP2pDna,
) -> TriggerSender {
