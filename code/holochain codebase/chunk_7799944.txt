            alice_initiate,
            &mut AgentInfoSession::new(
                bob.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();

    // - Check we always have at least one node not proceeding with initiate.
    assert!((bob_outgoing.is_empty() || alice_outgoing.is_empty()));
}

#[tokio::test(flavor = "multi_thread")]
/// This test checks that trying to initiate after a round with
/// a node is already in progress does not initiate a new round.
async fn initiate_after_target_is_set() {
    let agents = agents_with_infos(2).await;
    let all_agents: Vec<AgentInfoSigned> = agents.iter().map(|x| x.1.clone()).collect();
    let alice = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[0].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    let bob = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[1].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    // - Alice successfully initiates a round with bob.
    let (cert, _, alice_initiate) = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap()
        .unwrap();
    dbg!(&cert);
    dbg!(&agents);
    // - Bob accepts the round.
    let bob_outgoing = bob
        .process_incoming(
            cert.clone(),
            alice_initiate,
            &mut AgentInfoSession::new(
                bob.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();
    assert_eq!(bob_outgoing.len(), 2);

    bob.inner
        .share_mut(|i, _| {
            dbg!(&i.initiate_tgt);
            dbg!(i.round_map.current_rounds().len());
            Ok(())
        })
        .unwrap();
    // - Bob tries to initiate a round with alice.
    let bob_initiate = bob
        .try_initiate(&mut AgentInfoSession::new(
            bob.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap();
    bob.inner
        .share_mut(|i, _| {
            dbg!(&i.initiate_tgt);
            dbg!(i.round_map.current_rounds().len());
            Ok(())
        })
        .unwrap();
    // - Bob cannot initiate a round with anyone because he
    // already has a round with the only other player.
    assert!(bob_initiate.is_none());
}

#[tokio::test(flavor = "current_thread", start_paused = true)]
/// Test the initiates timeout after the round timeout has elapsed.
async fn initiate_times_out() {
    holochain_trace::test_run();

    let agents = agents_with_infos(3).await;
    let all_agents: Vec<AgentInfoSigned> = agents.iter().map(|x| x.1.clone()).collect();
    let alice_cert = cert_from_info(agents[0].1.clone());
    let alice = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[0].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;
    let bob = setup_empty_player(
        ShardedGossipLocalState {
            local_agents: maplit::hashset!(agents[1].0.clone()),
            ..Default::default()
        },
        agents.clone(),
    )
    .await;

    // Trying to initiate a round should succeed.
    let (tgt_cert, _, _) = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap()
        .expect("Failed to initiate");
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_some());
            Ok(())
        })
        .unwrap();
    let r = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap();

    // Doesn't re-initiate.
    assert!(r.is_none());
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_some());
            Ok(())
        })
        .unwrap();

    // Wait slightly longer than the timeout.
    tokio::time::sleep(
        alice.tuning_params.gossip_round_timeout() + std::time::Duration::from_millis(1),
    )
    .await;

    let (tgt2_cert, _, alice_initiate) = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap()
        .expect("Failed to initiate");

    // Now it should re-initiate with a different node.
    assert_ne!(tgt_cert, tgt2_cert);
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_some());
            Ok(())
        })
        .unwrap();

    // Process the initiate with Bob.
    let bob_outgoing = bob
        .process_incoming(
            alice_cert.clone(),
            alice_initiate,
            &mut AgentInfoSession::new(
                bob.query_agents_by_local_agents().await.unwrap(),
                all_agents.clone(),
            ),
        )
        .await
        .unwrap();

    // Process the Bob's accept with Alice.
    for bo in bob_outgoing {
        alice
            .process_incoming(
                tgt2_cert.clone(),
                bo,
                &mut AgentInfoSession::new(
                    alice.query_agents_by_local_agents().await.unwrap(),
                    all_agents.clone(),
                ),
            )
            .await
            .unwrap();
    }

    // Check the round is now active.
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_some());
            assert_eq!(i.round_map.current_rounds().len(), 1);
            Ok(())
        })
        .unwrap();

    // Wait slightly longer then the timeout but touch the round in between.
    tokio::time::sleep(alice.tuning_params.gossip_round_timeout() / 2).await;

    // Get the map so the round doesn't timeout
    alice
        .inner
        .share_mut(|i, _| {
            i.round_map.get(&tgt2_cert);
            Ok(())
        })
        .unwrap();

    tokio::time::sleep(
        alice.tuning_params.gossip_round_timeout() / 2 + std::time::Duration::from_millis(1),
    )
    .await;

    // Check that initiating again doesn't do anything.

    let r = alice
        .try_initiate(&mut AgentInfoSession::new(
            alice.query_agents_by_local_agents().await.unwrap(),
            all_agents.clone(),
        ))
        .await
        .unwrap();
    // Doesn't re-initiate.
    assert!(r.is_none());
    alice
        .inner
        .share_mut(|i, _| {
            assert!(i.initiate_tgt.is_some());
            Ok(())
        })
        .unwrap();
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/host_api/host_default_error.rs
================================================
use crate::dht::prelude::ArqSet;
use kitsune_p2p_fetch::FetchPoolConfig;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::box_fut;
use kitsune_p2p_types::dht::region_set::RegionSetLtcs;

use super::*;

/// A supertrait of KitsuneHost convenient for defining test handlers.
/// Allows only specifying the methods you care about, and letting all the rest
/// throw errors if called
#[allow(missing_docs)]
pub trait KitsuneHostDefaultError: KitsuneHost + FetchPoolConfig {
    /// Name to be printed out on unimplemented error
    const NAME: &'static str;

    fn block(&self, _input: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn unblock(&self, _input: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn is_blocked(
        &self,
        _input: kitsune_p2p_block::BlockTargetId,
        _timestamp: Timestamp,
    ) -> crate::KitsuneHostResult<bool> {
        box_fut(Ok(false))
    }

    fn put_agent_info_signed(
        &self,
        _input: Vec<crate::types::agent_store::AgentInfoSigned>,
    ) -> KitsuneHostResult<()> {
        box_fut(Ok(()))
    }

    fn get_agent_info_signed(
        &self,
        _input: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "get_agent_info_signed",
            Self::NAME
        )
        .into()))
    }

    fn remove_agent_info_signed(&self, _input: GetAgentInfoSignedEvt) -> KitsuneHostResult<bool> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "remove_agent_info_signed",
            Self::NAME
        )
        .into()))
    }

    fn peer_extrapolated_coverage(
        &self,
        _space: Arc<KitsuneSpace>,
        _dht_arc_set: DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "peer_extrapolated_coverage",
            Self::NAME
        )
        .into()))
    }

    fn record_metrics(
        &self,
        _space: Arc<KitsuneSpace>,
        _records: Vec<MetricRecord>,
    ) -> KitsuneHostResult<()> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "record_metrics",
            Self::NAME
        )
        .into()))
    }

    fn query_region_set(
        &self,
        _space: Arc<KitsuneSpace>,
        _arq_set: ArqSet,
    ) -> KitsuneHostResult<RegionSetLtcs> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "query_region_set",
            Self::NAME
        )
        .into()))
    }

    /// Given an input list of regions, return a list of equal or greater length
    /// such that each region's size is less than the `size_limit`, by recursively
    /// subdividing regions which are over the size limit.
    fn query_size_limited_regions(
        &self,
        _space: Arc<KitsuneSpace>,
        _size_limit: u32,
        _regions: Vec<Region>,
    ) -> KitsuneHostResult<Vec<Region>> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "query_size_limited_regions",
            Self::NAME
        )
        .into()))
    }

    fn get_topology(&self, _space: Arc<KitsuneSpace>) -> KitsuneHostResult<Topology> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "get_topology",
            Self::NAME
        )
        .into()))
    }

    fn op_hash(&self, _op_data: KOpData) -> KitsuneHostResult<KOpHash> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "op_hash",
            Self::NAME
        )
        .into()))
    }

    fn query_op_hashes_by_region(
        &self,
        _space: Arc<KitsuneSpace>,
        _region: RegionCoords,
    ) -> KitsuneHostResult<Vec<OpHashSized>> {
        box_fut(Err(format!(
            "error for unimplemented KitsuneHost test behavior: method {} of {}",
            "query_op_hashes_by_region",
            Self::NAME
        )
        .into()))
    }

    fn merge_fetch_contexts(&self, _a: u32, _b: u32) -> u32 {
        0
    }
}

impl<T: KitsuneHostDefaultError> KitsuneHost for T {
    fn block(&self, input: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        KitsuneHostDefaultError::block(self, input)
    }

    fn unblock(&self, input: kitsune_p2p_block::Block) -> crate::KitsuneHostResult<()> {
        KitsuneHostDefaultError::unblock(self, input)
    }

    fn is_blocked(
        &self,
        input: kitsune_p2p_block::BlockTargetId,
        timestamp: Timestamp,
    ) -> crate::KitsuneHostResult<bool> {
        KitsuneHostDefaultError::is_blocked(self, input, timestamp)
    }

    fn get_agent_info_signed(
        &self,
        input: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>> {
        KitsuneHostDefaultError::get_agent_info_signed(self, input)
    }

    fn remove_agent_info_signed(&self, input: GetAgentInfoSignedEvt) -> KitsuneHostResult<bool> {
        KitsuneHostDefaultError::remove_agent_info_signed(self, input)
    }

    fn peer_extrapolated_coverage(
        &self,
        space: Arc<KitsuneSpace>,
        dht_arc_set: DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>> {
        KitsuneHostDefaultError::peer_extrapolated_coverage(self, space, dht_arc_set)
    }

    fn record_metrics(
        &self,
        space: Arc<KitsuneSpace>,
        records: Vec<MetricRecord>,
    ) -> KitsuneHostResult<()> {
        KitsuneHostDefaultError::record_metrics(self, space, records)
    }

    fn query_size_limited_regions(
        &self,
        space: Arc<KitsuneSpace>,
        size_limit: u32,
        regions: Vec<Region>,
    ) -> crate::KitsuneHostResult<Vec<Region>> {
        KitsuneHostDefaultError::query_size_limited_regions(self, space, size_limit, regions)
    }

    fn query_region_set(
        &self,
        space: Arc<KitsuneSpace>,
        arq_set: ArqSet,
    ) -> KitsuneHostResult<RegionSetLtcs> {
        KitsuneHostDefaultError::query_region_set(self, space, arq_set)
    }

    fn get_topology(&self, space: Arc<KitsuneSpace>) -> KitsuneHostResult<Topology> {
        KitsuneHostDefaultError::get_topology(self, space)
    }

    fn op_hash(&self, op_data: KOpData) -> KitsuneHostResult<KOpHash> {
        KitsuneHostDefaultError::op_hash(self, op_data)
    }

    fn query_op_hashes_by_region(
        &self,
        space: Arc<KitsuneSpace>,
        region: RegionCoords,
    ) -> KitsuneHostResult<Vec<OpHashSized>> {
        KitsuneHostDefaultError::query_op_hashes_by_region(self, space, region)
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/host_api/host_stub.rs
================================================
use super::*;
use crate::event::KitsuneP2pEvent;
use crate::test_util::data::mk_agent_info;
use crate::{KitsuneBinType, KitsuneHostDefaultError};
use futures::FutureExt;
use kitsune_p2p_block::{Block, BlockTarget, BlockTargetId};
use kitsune_p2p_fetch::*;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::bin_types::KitsuneOpHash;
use kitsune_p2p_types::dht::arq::ArqSet;
use std::collections::HashSet;
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};

/// Signature for check_op_data_impl
pub type CheckOpDataImpl = Box<
    dyn Fn(
            Arc<KitsuneSpace>,
            Vec<KOpHash>,
            Option<FetchContext>,
        ) -> KitsuneHostResult<'static, Vec<bool>>
        + 'static
        + Send
        + Sync,
>;

#[derive(Debug)]
struct HostStubErr;

impl KitsuneHostDefaultError for HostStubErr {
    const NAME: &'static str = "HostStub";
}

impl FetchPoolConfig for HostStubErr {
    fn merge_fetch_contexts(&self, _a: u32, _b: u32) -> u32 {
        unimplemented!()
    }
}

/// Dummy host impl for plumbing
pub struct HostStub {
    err: HostStubErr,
    check_op_data_impl: Option<CheckOpDataImpl>,
    fail_next_request: Arc<AtomicBool>,
    fail_count: Arc<AtomicUsize>,
    blocks: Arc<parking_lot::Mutex<HashSet<Block>>>,
}

/// Manual implementation of debug to skip over underivable Debug field.
impl std::fmt::Debug for HostStub {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("HostStub").field("err", &self.err).finish()
    }
}

impl HostStub {
    /// Constructor
    pub fn new() -> std::sync::Arc<Self> {
        std::sync::Arc::new(Self {
            err: HostStubErr,
            check_op_data_impl: None,
            fail_next_request: Arc::new(AtomicBool::new(false)),
            fail_count: Arc::new(AtomicUsize::new(0)),
            blocks: Arc::new(parking_lot::Mutex::new(HashSet::new())),
        })
    }

    /// Constructor
    pub fn with_check_op_data(check_op_data_impl: CheckOpDataImpl) -> std::sync::Arc<Self> {
        std::sync::Arc::new(Self {
            err: HostStubErr,
            check_op_data_impl: Some(check_op_data_impl),
            fail_next_request: Arc::new(AtomicBool::new(false)),
            fail_count: Arc::new(AtomicUsize::new(0)),
            blocks: Arc::new(parking_lot::Mutex::new(HashSet::new())),
        })
    }

    /// Request that the next request will fail and respond with an error
    pub fn fail_next_request(&self) {
        self.fail_next_request.store(true, Ordering::SeqCst);
    }

    /// Get the count of requests that have failed due to `fail_next_request`.
    pub fn get_fail_count(&self) -> usize {
        self.fail_count.load(Ordering::SeqCst)
    }

    /// Wrap it up with a legacy sender
    pub fn legacy(
        self: Arc<Self>,
        sender: futures::channel::mpsc::Sender<KitsuneP2pEvent>,
    ) -> HostApiLegacy {
        HostApiLegacy::new(self, sender)
    }
}

impl KitsuneHost for HostStub {
    fn block(&self, input: Block) -> KitsuneHostResult<()> {
        let mut blocks = self.blocks.lock();
        blocks.insert(input);

        async move { Ok(()) }.boxed().into()
    }

    fn unblock(&self, input: Block) -> KitsuneHostResult<()> {
        let mut blocks = self.blocks.lock();
        blocks.remove(&input);

        async move { Ok(()) }.boxed().into()
    }

    fn is_blocked(
        &self,
        input: kitsune_p2p_block::BlockTargetId,
        timestamp: Timestamp,
    ) -> crate::KitsuneHostResult<bool> {
        let blocks = self.blocks.lock();

        let blocked = match &input {
            BlockTargetId::Node(check_node_id) => {
                let maybe_matched_block = blocks.iter().find(|b| match b.target() {
                    BlockTarget::Node(node_id, _) => node_id == check_node_id,
                    _ => false,
                });

                if let Some(block) = maybe_matched_block {
                    timestamp.0 > block.start().0 && timestamp.0 < block.end().0
                } else {
                    false
                }
            }
            _ => false,
        };

        async move { Ok(blocked) }.boxed().into()
    }

    fn get_agent_info_signed(
        &self,
        input: GetAgentInfoSignedEvt,
    ) -> KitsuneHostResult<Option<crate::types::agent_store::AgentInfoSigned>> {
        if let Ok(true) =
            self.fail_next_request
                .compare_exchange(true, false, Ordering::SeqCst, Ordering::SeqCst)
        {
            self.fail_count.fetch_add(1, Ordering::SeqCst);
            return KitsuneHostDefaultError::get_agent_info_signed(&self.err, input);
        }

        async move {
            let signed = mk_agent_info(*input.agent.0.to_vec().first().unwrap()).await;
            Ok(Some(signed))
        }
        .boxed()
        .into()
    }

    fn remove_agent_info_signed(&self, input: GetAgentInfoSignedEvt) -> KitsuneHostResult<bool> {
        KitsuneHostDefaultError::remove_agent_info_signed(&self.err, input)
    }

    fn peer_extrapolated_coverage(
        &self,
        space: Arc<KitsuneSpace>,
        dht_arc_set: DhtArcSet,
    ) -> KitsuneHostResult<Vec<f64>> {
        KitsuneHostDefaultError::peer_extrapolated_coverage(&self.err, space, dht_arc_set)
    }

    fn query_region_set(
        &self,
        space: Arc<KitsuneSpace>,
        arq_set: ArqSet,
    ) -> KitsuneHostResult<RegionSetLtcs> {
        KitsuneHostDefaultError::query_region_set(&self.err, space, arq_set)
    }

    fn query_size_limited_regions(
        &self,
        space: Arc<KitsuneSpace>,
        size_limit: u32,
        regions: Vec<Region>,
    ) -> crate::KitsuneHostResult<Vec<Region>> {
        KitsuneHostDefaultError::query_size_limited_regions(&self.err, space, size_limit, regions)
    }

    fn query_op_hashes_by_region(
        &self,
        space: Arc<KitsuneSpace>,
        region: RegionCoords,
    ) -> KitsuneHostResult<Vec<OpHashSized>> {
        KitsuneHostDefaultError::query_op_hashes_by_region(&self.err, space, region)
    }

    fn record_metrics(
        &self,
        space: Arc<KitsuneSpace>,
        records: Vec<MetricRecord>,
    ) -> KitsuneHostResult<()> {
        KitsuneHostDefaultError::record_metrics(&self.err, space, records)
    }

    fn get_topology(&self, space: Arc<KitsuneSpace>) -> KitsuneHostResult<Topology> {
        KitsuneHostDefaultError::get_topology(&self.err, space)
    }

    fn op_hash(&self, op_data: KOpData) -> KitsuneHostResult<KOpHash> {
        if let Ok(true) =
            self.fail_next_request
                .compare_exchange(true, false, Ordering::SeqCst, Ordering::SeqCst)
        {
            self.fail_count.fetch_add(1, Ordering::SeqCst);
            return KitsuneHostDefaultError::op_hash(&self.err, op_data);
        }

        async move {
            // Probably not important but we could compute a real hash here if a test needs it
            let hash_byte = op_data.0.first().cloned().unwrap_or(0);
            Ok(Arc::new(KitsuneOpHash::new(vec![hash_byte; 36])))
        }
        .boxed()
        .into()
    }

    fn check_op_data(
        &self,
        space: Arc<KitsuneSpace>,
        op_hash_list: Vec<KOpHash>,
        context: Option<kitsune_p2p_fetch::FetchContext>,
    ) -> KitsuneHostResult<Vec<bool>> {
        if let Some(i) = &self.check_op_data_impl {
            i(space, op_hash_list, context)
        } else {
            KitsuneHost::check_op_data(&self.err, space, op_hash_list, context)
        }
    }
}

impl FetchPoolConfig for HostStub {
    fn merge_fetch_contexts(&self, _a: u32, _b: u32) -> u32 {
        unimplemented!()
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor.rs
================================================
// this is largely a passthrough that routes to a specific space handler

use crate::actor::*;
use crate::event::*;
use crate::gossip::sharded_gossip::BandwidthThrottles;
use crate::gossip::sharded_gossip::KitsuneDiagnostics;
use crate::types::gossip::GossipModuleType;
use crate::wire::MetricExchangeMsg;
use crate::*;
use futures::future::FutureExt;
use kitsune_p2p_bootstrap_client::BootstrapNet;
use kitsune_p2p_fetch::*;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::async_lazy::AsyncLazy;
use kitsune_p2p_types::config::{KitsuneP2pConfig, TransportConfig};
use kitsune_p2p_types::dht::Arq;
use kitsune_p2p_types::*;
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::sync::Arc;

/// Default webrtc config if set to `None`.
/// TODO - set this to holochain stun servers once they exist!
const DEFAULT_WEBRTC_CONFIG: &str = r#"{
  "iceServers": [
    { "urls": ["stun:stun-0.main.infra.holo.host:443"] },
    { "urls": ["stun:stun-1.main.infra.holo.host:443"] }
  ]
}"#;

/// The bootstrap service is much more thoroughly documented in the default service implementation.
/// See <https://github.com/holochain/bootstrap>
mod discover;
pub(crate) mod meta_net;
use meta_net::*;
mod fetch;
mod meta_net_task;
mod space;
use ghost_actor::dependencies::tracing;
use space::*;

#[cfg(feature = "test_utils")]
pub mod test_util;

type EvtRcv = futures::channel::mpsc::Receiver<KitsuneP2pEvent>;
type KSpace = Arc<KitsuneSpace>;
type KAgent = Arc<KitsuneAgent>;
type KBasis = Arc<KitsuneBasis>;
type VecMXM = Vec<MetricExchangeMsg>;
type Payload = Box<[u8]>;
type OpHashList = Vec<OpHashSized>;
type MaybeDelegate = Option<(KBasis, u32, u32)>;

/// Random number.
const UNAUTHORIZED_DISCONNECT_CODE: u32 = 0x59ea599e;
const UNAUTHORIZED_DISCONNECT_REASON: &str = "unauthorized";

ghost_actor::ghost_chan! {
    #[allow(clippy::too_many_arguments)]
    pub chan Internal<crate::KitsuneP2pError> {
        /// Notification that we have a new address to be identified at
        fn new_address(local_url: String) -> ();

        /// Register space event handler
        fn register_space_event_handler(recv: EvtRcv) -> ();

        /// Incoming Delegate Broadcast
        /// We are being requested to delegate a broadcast to our neighborhood
        /// on behalf of an author. `mod_idx` / `mod_cnt` inform us which
        /// neighbors we are responsible for.
        /// (See comments in actual method impl for more detail.)
        fn incoming_delegate_broadcast(
            space: KSpace,
            basis: KBasis,
            to_agent: KAgent,
            mod_idx: u32,
            mod_cnt: u32,
            data: BroadcastData,
        ) -> ();

        /// This should be invoked instead of incoming_delegate_broadcast
        /// in the case of a publish data variant. It will, in turn, call
        /// into incoming_delegate_broadcast once we have the data to act
        /// as a fetch responder for the op data.
        fn incoming_publish(
            space: KSpace,
            to_agent: KAgent,
            source: KAgent,
            transfer_method: kitsune_p2p_fetch::TransferMethod,
            op_hash_list: OpHashList,
            context: kitsune_p2p_fetch::FetchContext,
            maybe_delegate: MaybeDelegate,
        ) -> ();

        /// We just received data for an op_hash. Check if we had a pending
        /// delegation action we need to continue now that we have the data.
        fn resolve_publish_pending_delegates(space: KSpace, op_hash: KOpHash) -> ();

        /// Incoming Gossip
        fn incoming_gossip(space: KSpace, con: MetaNetCon, remote_url: String, data: Payload, module_type: crate::types::gossip::GossipModuleType) -> ();

        /// Incoming Metric Exchange
        fn incoming_metric_exchange(space: KSpace, msgs: VecMXM) -> ();

        /// New Con
        fn new_con(url: String, con: MetaNetCon) -> ();

        /// Del Con
        fn del_con(url: String) -> ();

        /// Fetch an op from a remote
        fn fetch(key: FetchKey, space: KSpace, source: FetchSource) -> ();

        /// Get all local joined agent infos across all spaces.
        fn get_all_local_joined_agent_infos() -> Vec<AgentInfoSigned>;
    }
}

pub(crate) struct KitsuneP2pActor {
    channel_factory: ghost_actor::actor_builder::GhostActorChannelFactory<Self>,
    internal_sender: ghost_actor::GhostSender<Internal>,
    ep_hnd: MetaNet,
    host_api: HostApiLegacy,
    #[allow(clippy::type_complexity)]
    spaces: HashMap<
        Arc<KitsuneSpace>,
        AsyncLazy<(
            ghost_actor::GhostSender<KitsuneP2p>,
            ghost_actor::GhostSender<space::SpaceInternal>,
        )>,
    >,
    config: Arc<KitsuneP2pConfig>,
    bootstrap_net: BootstrapNet,
    bandwidth_throttles: BandwidthThrottles,
    parallel_notify_permit: Arc<tokio::sync::Semaphore>,
    fetch_pool: FetchPool,
    local_url: Arc<std::sync::Mutex<Option<String>>>,
}

impl KitsuneP2pActor {
    #[allow(clippy::too_many_arguments)]
    pub async fn new(
        config: KitsuneP2pConfig,
        channel_factory: ghost_actor::actor_builder::GhostActorChannelFactory<Self>,
        internal_sender: ghost_actor::GhostSender<Internal>,
        direct_host_api: HostApiLegacy,
        self_host_api: HostApiLegacy,
        ep_hnd: MetaNet,
        ep_evt: MetaNetEvtRecv,
        bootstrap_net: BootstrapNet,
        maybe_peer_url: Option<String>,
    ) -> KitsuneP2pResult<Self> {
        let local_url = Arc::new(std::sync::Mutex::new(maybe_peer_url));

        crate::types::metrics::init();

        let fetch_response_queue =
            FetchResponseQueue::new(FetchResponseConfig::new(config.tuning_params.clone()));

        // TODO - use a real config
        let fetch_pool = FetchPool::new_bitwise_or();

        // Start a loop to handle our fetch queue fetch items.
        FetchTask::spawn(
            config.clone(),
            fetch_pool.clone(),
            self_host_api.clone(),
            internal_sender.clone(),
        );

        let i_s = internal_sender.clone();

        let bandwidth_throttles = BandwidthThrottles::new(&config.tuning_params);
        let parallel_notify_permit = Arc::new(tokio::sync::Semaphore::new(
            config.tuning_params.concurrent_limit_per_thread,
        ));

        MetaNetTask::new(
            self_host_api.clone(),
            config.clone(),
            fetch_pool.clone(),
            fetch_response_queue,
            ep_evt,
            i_s,
        )
        .spawn();

        Ok(Self {
            channel_factory,
            internal_sender,
            ep_hnd,
            host_api: direct_host_api,
            spaces: HashMap::new(),
            config: Arc::new(config),
            bootstrap_net,
            bandwidth_throttles,
            parallel_notify_permit,
            fetch_pool,
            local_url,
        })
    }
}

pub(super) async fn create_meta_net(
    config: &KitsuneP2pConfig,
    _tls_config: tls::TlsConfig,
    internal_sender: ghost_actor::GhostSender<Internal>,
    host: HostApiLegacy,
    preflight_user_data: PreflightUserData,
) -> KitsuneP2pResult<(MetaNet, MetaNetEvtRecv, BootstrapNet, Option<String>)> {
    let mut ep_hnd = None;
    let mut ep_evt = None;
    let mut bootstrap_net = None;
    let mut maybe_peer_url = None;

    if ep_hnd.is_none() && config.is_tx5() {
        tracing::trace!("tx5");
        let mut tune: kitsune_p2p_types::config::tuning_params_struct::KitsuneP2pTuningParams =
            (*config.tuning_params).clone();
        let (signal_url, webrtc_config) = match config.transport_pool.first().unwrap() {
            TransportConfig::WebRTC {
                signal_url,
                webrtc_config,
            } => {
                let webrtc_config = webrtc_config
                    .as_ref()
                    .map(|c| serde_json::to_string(&c).expect("Can Serialize JSON"))
                    .unwrap_or_else(|| DEFAULT_WEBRTC_CONFIG.to_string());
                (signal_url.clone(), webrtc_config)
            }
            TransportConfig::Mem {} => {
                tune.tx5_backend_module = "mem".to_string();
                ("wss://fake.fake".to_string(), "{}".to_string())
            }
        };
        let (h, e, p) = MetaNet::new_tx5(
            Arc::new(tune),
            host.clone(),
            internal_sender.clone(),
            signal_url,
            webrtc_config,
            preflight_user_data,
        )
        .await?;
        ep_hnd = Some(h);
        ep_evt = Some(e);
        bootstrap_net = Some(BootstrapNet::Tx5);
        maybe_peer_url = p;
    }

    match (ep_hnd, ep_evt, bootstrap_net) {
        (Some(h), Some(e), Some(n)) => Ok((h, e, n, maybe_peer_url)),
        _ => Err("Network config has no valid transport".into()),
    }
}

use crate::spawn::actor::fetch::{FetchResponseConfig, FetchTask};
use crate::spawn::actor::meta_net_task::MetaNetTask;
use ghost_actor::dependencies::must_future::MustBoxFuture;
use kitsune_p2p_types::bootstrap::AgentInfoPut;

impl ghost_actor::GhostControlHandler for KitsuneP2pActor {
    fn handle_ghost_actor_shutdown(mut self) -> MustBoxFuture<'static, ()> {
        use futures::sink::SinkExt;
        use ghost_actor::GhostControlSender;
        async move {
            // The line below was added when migrating to rust edition 2021, per
            // https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html#migration
            let _ = &self;
            // this is a courtesy, ok if fails
            let _ = self.host_api.legacy.close().await;
            self.ep_hnd.close(500, "").await;
            for (_, space) in self.spaces.into_iter() {
                let (space, _) = space.get().await;
                let _ = space.ghost_actor_shutdown_immediate().await;
            }
        }
        .boxed()
        .into()
    }
}

impl ghost_actor::GhostHandler<Internal> for KitsuneP2pActor {}

impl InternalHandler for KitsuneP2pActor {
    fn handle_new_address(&mut self, local_url: String) -> InternalHandlerResult<()> {
        let spaces = self.spaces.values().map(|s| s.get()).collect::<Vec<_>>();
        Ok(async move {
            let mut all = Vec::new();
            for (_, space) in futures::future::join_all(spaces).await {
                all.push(space.new_address(local_url.clone()));
            }
            let _ = futures::future::join_all(all).await;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_register_space_event_handler(
        &mut self,
        recv: futures::channel::mpsc::Receiver<KitsuneP2pEvent>,
    ) -> InternalHandlerResult<()> {
        let f = self.channel_factory.attach_receiver(recv);
        Ok(async move {
            f.await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_incoming_delegate_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
        to_agent: Arc<KitsuneAgent>,
        mod_idx: u32,
        mod_cnt: u32,
        data: BroadcastData,
    ) -> InternalHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                tracing::warn!(
                    "received delegate_broadcast for unhandled space: {:?}",
                    space
                );
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            space_inner
                .incoming_delegate_broadcast(space, basis, to_agent, mod_idx, mod_cnt, data)
                .await
        }
        .boxed()
        .into())
    }

    fn handle_incoming_publish(
        &mut self,
        space: KSpace,
        to_agent: KAgent,
        source: KAgent,
        transfer_method: kitsune_p2p_fetch::TransferMethod,
        op_hash_list: OpHashList,
        context: kitsune_p2p_fetch::FetchContext,
        maybe_delegate: MaybeDelegate,
    ) -> InternalHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                tracing::warn!("received publish for unhandled space: {:?}", space);
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            space_inner
                .incoming_publish(
                    space,
                    to_agent,
                    source,
                    transfer_method,
                    op_hash_list,
                    context,
                    maybe_delegate,
                )
                .await
        }
        .boxed()
        .into())
    }

    fn handle_resolve_publish_pending_delegates(
        &mut self,
        space: KSpace,
        op_hash: KOpHash,
    ) -> InternalHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            space_inner
                .resolve_publish_pending_delegates(space, op_hash)
                .await
        }
        .boxed()
        .into())
    }

    fn handle_incoming_gossip(
        &mut self,
        space: Arc<KitsuneSpace>,
        con: MetaNetCon,
        remote_url: String,
        data: Box<[u8]>,
        module_type: GossipModuleType,
    ) -> InternalHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                tracing::warn!("received gossip for unhandled space: {:?}", space);
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            space_inner
                .incoming_gossip(space, con, remote_url, data, module_type)
                .await
        }
        .boxed()
        .into())
    }

    fn handle_incoming_metric_exchange(
        &mut self,
        space: Arc<KitsuneSpace>,
        msgs: Vec<MetricExchangeMsg>,
    ) -> InternalHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            space_inner.incoming_metric_exchange(space, msgs).await
        }
        .boxed()
        .into())
    }

    fn handle_new_con(&mut self, url: String, con: MetaNetCon) -> InternalHandlerResult<()> {
        let spaces = self.spaces.values().map(|s| s.get()).collect::<Vec<_>>();
        Ok(async move {
            let mut all = Vec::new();
            for (_, space) in futures::future::join_all(spaces).await {
                all.push(space.new_con(url.clone(), con.clone()));
            }
            let _ = futures::future::join_all(all).await;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_del_con(&mut self, url: String) -> InternalHandlerResult<()> {
        let spaces = self.spaces.values().map(|s| s.get()).collect::<Vec<_>>();
        Ok(async move {
            let mut all = Vec::new();
            for (_, space) in futures::future::join_all(spaces).await {
                all.push(space.del_con(url.clone()));
            }
            let _ = futures::future::join_all(all).await;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_fetch(
        &mut self,
        key: FetchKey,
        space: KSpace,
        source: FetchSource,
    ) -> InternalHandlerResult<()> {
        let FetchSource::Agent(agent) = source;

        let space_sender = match self.spaces.get_mut(&space) {
            None => {
                tracing::warn!("received fetch for unhandled space: {:?}", space);
                return unit_ok_fut();
            }
            Some(space) => space.get(),
        };
        Ok(async move {
            let (_, space_inner) = space_sender.await;
            let payload = wire::Wire::fetch_op(vec![(space, vec![key])]);
            space_inner.notify(agent, payload).await
        }
        .boxed()
        .into())
    }

    /// Best effort to retrieve all local agent infos across all spaces. If there
    /// is an error for some space we simply log it and ignore the error for that
    /// space and return local joined agent infos from the other spaces.
    fn handle_get_all_local_joined_agent_infos(
        &mut self,
    ) -> InternalHandlerResult<Vec<AgentInfoSigned>> {
        let spaces = self.spaces.values().map(|s| s.get()).collect::<Vec<_>>();
        Ok(async move {
            let mut all = Vec::new();
            for (_, space) in futures::future::join_all(spaces).await {
                all.push(space.get_all_local_joined_agent_infos());
            }
            let agent_infos = futures::future::join_all(all)
                .await
                .into_iter()
                .filter_map(|maybe_agent_infos| {
                    if let Err(err) = &maybe_agent_infos {
                        tracing::warn!(?err, "error reading agent infos from spaces");
                    }
                    maybe_agent_infos.ok()
                })
                .flatten()
                .collect();
            Ok(agent_infos)
        }
        .boxed()
        .into())
    }
}

impl ghost_actor::GhostHandler<KitsuneP2pEvent> for KitsuneP2pActor {}

impl KitsuneP2pEventHandler for KitsuneP2pActor {
    fn handle_put_agent_info_signed(
        &mut self,
        input: crate::event::PutAgentInfoSignedEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<AgentInfoPut>> {
        let legacy_host = self.host_api.legacy.clone();
        let ep_hnd = self.ep_hnd.clone();

        Ok(async move {
            let puts = legacy_host.put_agent_info_signed(input).await?;

            for removed_url in puts.iter().flat_map(|r| r.removed_urls.clone()) {
                tracing::debug!(?removed_url, "peer URL changed, closing connection");
                if let Err(e) = ep_hnd.close_peer_con(removed_url.clone()) {
                    tracing::debug!(?e, ?removed_url, "could not close peer connection");
                }
            }

            Ok(puts)
        }
        .boxed()
        .into())
    }

    fn handle_query_agents(
        &mut self,
        input: crate::event::QueryAgentsEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<crate::types::agent_store::AgentInfoSigned>> {
        Ok(self.host_api.legacy.query_agents(input))
    }

    fn handle_query_peer_density(
        &mut self,
        space: Arc<KitsuneSpace>,
        arq: kitsune_p2p_types::dht_arc::DhtArc,
    ) -> KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView> {
        Ok(self.host_api.legacy.query_peer_density(space, arq))
    }

    fn handle_call(
        &mut self,
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
    ) -> KitsuneP2pEventHandlerResult<Vec<u8>> {
        Ok(self.host_api.legacy.call(space, to_agent, payload))
    }

    fn handle_notify(
        &mut self,
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
    ) -> KitsuneP2pEventHandlerResult<()> {
        Ok(self.host_api.legacy.notify(space, to_agent, payload))
    }

    fn handle_receive_ops(
        &mut self,
        space: Arc<KitsuneSpace>,
        ops: Vec<KOp>,
        context: Option<FetchContext>,
    ) -> KitsuneP2pEventHandlerResult<()> {
        Ok(self.host_api.legacy.receive_ops(space, ops, context))
    }

    fn handle_query_op_hashes(
        &mut self,
        input: QueryOpHashesEvt,
    ) -> KitsuneP2pEventHandlerResult<Option<(Vec<Arc<KitsuneOpHash>>, TimeWindowInclusive)>> {
        Ok(self.host_api.legacy.query_op_hashes(input))
    }

    fn handle_fetch_op_data(
        &mut self,
        input: FetchOpDataEvt,
    ) -> KitsuneP2pEventHandlerResult<Vec<(Arc<KitsuneOpHash>, KOp)>> {
        Ok(self.host_api.legacy.fetch_op_data(input))
    }

    fn handle_sign_network_data(
        &mut self,
        input: SignNetworkDataEvt,
    ) -> KitsuneP2pEventHandlerResult<KitsuneSignature> {
        Ok(self.host_api.legacy.sign_network_data(input))
    }
}

impl ghost_actor::GhostHandler<KitsuneP2p> for KitsuneP2pActor {}

impl KitsuneP2pHandler for KitsuneP2pActor {
    fn handle_join(
        &mut self,
        space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
        maybe_agent_info: Option<AgentInfoSigned>,
        initial_arq: Option<Arq>,
    ) -> KitsuneP2pHandlerResult<()> {
        let internal_sender = self.internal_sender.clone();
        let space2 = space.clone();
        let ep_hnd = self.ep_hnd.clone();
        let host = self.host_api.clone().api;
        let config = Arc::clone(&self.config);
        let bootstrap_net = self.bootstrap_net;
        let bandwidth_throttles = self.bandwidth_throttles.clone();
        let parallel_notify_permit = self.parallel_notify_permit.clone();
        let fetch_pool = self.fetch_pool.clone();
        let local_url = self.local_url.clone();

        let space_sender = match self.spaces.entry(space.clone()) {
            Entry::Occupied(entry) => entry.into_mut(),
            Entry::Vacant(entry) => entry.insert(AsyncLazy::new(async move {
                let (send, send_inner, evt_recv) = spawn_space(
                    space2,
                    ep_hnd,
                    host,
                    config,
                    bootstrap_net,
                    bandwidth_throttles,
                    parallel_notify_permit,
                    fetch_pool,
                    local_url,
                )
                .await
                .expect("cannot fail to create space");
                internal_sender
                    .register_space_event_handler(evt_recv)
                    .await
                    .expect("FAIL");
                (send, send_inner)
            })),
        };
        let space_sender = space_sender.get();
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender
                .join(space, agent, maybe_agent_info, initial_arq)
                .await
        }
        .boxed()
        .into())
    }

    fn handle_leave(
        &mut self,
        space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
    ) -> KitsuneP2pHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return unit_ok_fut(),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.leave(space.clone(), agent).await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_rpc_single(
        &mut self,
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        payload: Vec<u8>,
        timeout_ms: Option<u64>,
    ) -> KitsuneP2pHandlerResult<Vec<u8>> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender
                .rpc_single(space, to_agent, payload, timeout_ms)
                .await
        }
        .boxed()
        .into())
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip(self, input)))]
    fn handle_rpc_multi(
        &mut self,
        input: actor::RpcMulti,
    ) -> KitsuneP2pHandlerResult<Vec<actor::RpcMultiResponse>> {
        let space_sender = match self.spaces.get_mut(&input.space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(input.space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.rpc_multi(input).await
        }
        .boxed()
        .into())
    }

    fn handle_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
        timeout: KitsuneTimeout,
        data: BroadcastData,
    ) -> KitsuneP2pHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.broadcast(space, basis, timeout, data).await
        }
        .boxed()
        .into())
    }

    fn handle_targeted_broadcast(
        &mut self,
        space: Arc<KitsuneSpace>,
        agents: Vec<Arc<KitsuneAgent>>,
        timeout: KitsuneTimeout,
        payload: Vec<u8>,
        drop_at_limit: bool,
    ) -> KitsuneP2pHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender
                .targeted_broadcast(space, agents, timeout, payload, drop_at_limit)
                .await
        }
        .boxed()
        .into())
    }

    fn handle_new_integrated_data(
        &mut self,
        space: Arc<KitsuneSpace>,
    ) -> KitsuneP2pHandlerResult<()> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return unit_ok_fut(),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.new_integrated_data(space).await?;
            Ok(())
        }
        .boxed()
        .into())
    }

    fn handle_authority_for_hash(
        &mut self,
        space: Arc<KitsuneSpace>,
        basis: Arc<KitsuneBasis>,
    ) -> KitsuneP2pHandlerResult<bool> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.authority_for_hash(space, basis).await
        }
        .boxed()
        .into())
    }

    fn handle_dump_network_metrics(
        &mut self,
        space: Option<Arc<KitsuneSpace>>,
    ) -> KitsuneP2pHandlerResult<serde_json::Value> {
        let spaces = self
            .spaces
            .iter()
            .filter_map(|(h, s)| {
                if let Some(space) = &space {
                    if h != space {
                        return None;
                    }
                }
                let h = h.clone();
                Some((h, s.get()))
            })
            .collect::<Vec<_>>();
        let results = async move {
            let mut all: Vec<KitsuneP2pFuture<serde_json::Value>> = Vec::new();
            for (h, (space, _)) in futures::future::join_all(
                spaces.into_iter().map(|(h, s)| async move { (h, s.await) }),
            )
            .await
            {
                all.push(space.dump_network_metrics(Some(h)));
            }
            Ok(futures::future::try_join_all(all).await?.into())
        }
        .boxed()
        .into();
        Ok(results)
    }

    fn handle_dump_network_stats(&mut self) -> KitsuneP2pHandlerResult<serde_json::Value> {
        let peer_fut_list = self
            .spaces
            .keys()
            .map(|space| {
                self.host_api
                    .legacy
                    .query_agents(QueryAgentsEvt::new(space.clone()))
            })
            .collect::<Vec<_>>();
        let stat_fut = self.ep_hnd.dump_network_stats();
        Ok(async move {
            let mut stats = stat_fut.await?;

            let this_id: String = stats
                .as_object()
                .and_then(|obj| obj.get("thisId"))
                .and_then(|v| v.as_str())
                .map(|s| s.to_string())
                .unwrap_or_else(String::new);

            let all_peers = futures::future::join_all(peer_fut_list).await;

            #[derive(serde::Serialize)]
            #[serde(rename_all = "camelCase")]
            struct Agent {
                pub expires_at_millis: u64,
            }

            for peer in all_peers {
                for peer in peer? {
                    if let Some(net_key) = peer
                        .url_list
                        .first()
                        .map(|u| {
                            KitsuneResult::Ok(
                                kitsune_p2p_types::tx_utils::ProxyUrl::from(u.as_url2())
                                    .digest()?
                                    .to_string(),
                            )
                        })
                        .transpose()?
                    {
                        if net_key == this_id {
                            continue;
                        }

                        let r = stats
                            .as_object_mut()
                            .ok_or(KitsuneP2pError::from("InvalidStats"))?
                            .entry(net_key)
                            .or_insert_with(|| serde_json::json!({}));

                        let r = r
                            .as_object_mut()
                            .ok_or(KitsuneP2pError::from("InvalidStats"))?
                            .entry("hcDnaHashesToAgents".to_string())
                            .or_insert_with(|| serde_json::json!({}));

                        use base64::Engine;

                        let dna_hash = format!(
                            "uhC0k{}",
                            base64::engine::general_purpose::URL_SAFE_NO_PAD.encode(&**peer.space)
                        );

                        let r = r
                            .as_object_mut()
                            .ok_or(KitsuneP2pError::from("InvalidStats"))?
                            .entry(dna_hash)
                            .or_insert_with(|| serde_json::json!({}));

                        let agent_pub_key = format!(
                            "uhCAk{}",
                            base64::engine::general_purpose::URL_SAFE_NO_PAD.encode(&**peer.agent)
                        );

                        let agent = Agent {
                            expires_at_millis: peer.expires_at_ms,
                        };

                        r.as_object_mut()
                            .ok_or(KitsuneP2pError::from("InvalidStats"))?
                            .insert(agent_pub_key, serde_json::json!(agent));
                    }
                }
            }

            Ok(stats)
        }
        .boxed()
        .into())
    }

    fn handle_get_diagnostics(
        &mut self,
        space: KSpace,
        // gossip_type: GossipModuleType,
    ) -> KitsuneP2pHandlerResult<KitsuneDiagnostics> {
        let space_sender = match self.spaces.get_mut(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.get_diagnostics(space).await
        }
        .boxed()
        .into())
    }

    fn handle_storage_arcs(
        &mut self,
        space: KSpace,
    ) -> KitsuneP2pHandlerResult<Vec<kitsune2_api::DhtArc>> {
        let space_sender = match self.spaces.get(&space) {
            None => return Err(KitsuneP2pError::RoutingSpaceError(space)),
            Some(space) => space.get(),
        };
        Ok(async move {
            let (space_sender, _) = space_sender.await;
            space_sender.storage_arcs(space).await
        }
        .boxed()
        .into())
    }
}

#[cfg(any(test, feature = "test_utils"))]
mockall::mock! {

    pub KitsuneP2pEventHandler {}

    impl KitsuneP2pEventHandler for KitsuneP2pEventHandler {

        fn handle_put_agent_info_signed(
            &mut self,
            input: crate::event::PutAgentInfoSignedEvt,
        ) -> KitsuneP2pEventHandlerResult<Vec<AgentInfoPut>>;

        fn handle_query_agents(
            &mut self,
            input: crate::event::QueryAgentsEvt,
        ) -> KitsuneP2pEventHandlerResult<Vec<crate::types::agent_store::AgentInfoSigned>>;

        fn handle_query_peer_density(
            &mut self,
            space: Arc<KitsuneSpace>,
            arq: kitsune_p2p_types::dht_arc::DhtArc,
        ) -> KitsuneP2pEventHandlerResult<kitsune_p2p_types::dht::PeerView>;

        fn handle_call(
            &mut self,
            space: Arc<KitsuneSpace>,
            to_agent: Arc<KitsuneAgent>,
            payload: Vec<u8>,
        ) -> KitsuneP2pEventHandlerResult<Vec<u8>>;

        fn handle_notify(
            &mut self,
            space: Arc<KitsuneSpace>,
            to_agent: Arc<KitsuneAgent>,
            payload: Vec<u8>,
        ) -> KitsuneP2pEventHandlerResult<()> ;

        fn handle_receive_ops(
            &mut self,
            space: Arc<KitsuneSpace>,
            ops: Vec<KOp>,
            context: Option<FetchContext>,
        ) -> KitsuneP2pEventHandlerResult<()>;

        fn handle_query_op_hashes(
            &mut self,
            input: QueryOpHashesEvt,
        ) -> KitsuneP2pEventHandlerResult<Option<(Vec<Arc<KitsuneOpHash>>, TimeWindowInclusive)>>;

        fn handle_fetch_op_data(
            &mut self,
            input: FetchOpDataEvt,
        ) -> KitsuneP2pEventHandlerResult<Vec<(Arc<KitsuneOpHash>, KOp)>> ;

        fn handle_sign_network_data(
            &mut self,
            input: SignNetworkDataEvt,
        ) -> KitsuneP2pEventHandlerResult<KitsuneSignature> ;

    }
}

#[cfg(any(test, feature = "test_utils"))]
impl ghost_actor::GhostHandler<KitsuneP2pEvent> for MockKitsuneP2pEventHandler {}
#[cfg(any(test, feature = "test_utils"))]
impl ghost_actor::GhostControlHandler for MockKitsuneP2pEventHandler {}

#[cfg(test)]
mod tests {
    use crate::meta_net::PreflightUserData;
    use crate::spawn::actor::create_meta_net;
    use crate::spawn::actor::test_util::InternalStub;
    use crate::spawn::actor::MetaNet;
    use crate::spawn::actor::MetaNetEvtRecv;
    use crate::spawn::Internal;
    use crate::test_util::start_signal_srv;
    use crate::HostStub;
    use crate::KitsuneP2pResult;
    use ghost_actor::actor_builder::GhostActorBuilder;
    use kitsune_p2p_bootstrap_client::BootstrapNet;
    use kitsune_p2p_types::config::KitsuneP2pConfig;
    use kitsune_p2p_types::tls::TlsConfig;
    use url2::url2;

    #[tokio::test(flavor = "multi_thread")]
    async fn create_tx5_with_mdns_meta_net() {
        let (signal_addr, _sig_hnd) = start_signal_srv().await;

        let config = KitsuneP2pConfig::from_signal_addr(signal_addr);

        let (meta_net, _, bootstrap_net) = test_create_meta_net(config).await.unwrap();

        // Not the most interesting check but we mostly care that the above function produces a result given a valid config.
        assert_eq!(BootstrapNet::Tx5, bootstrap_net);

        meta_net.close(0, "test").await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn create_tx5_with_bootstrap_meta_net() {
        let (signal_addr, _sig_hnd) = start_signal_srv().await;

        let mut config = KitsuneP2pConfig::from_signal_addr(signal_addr);
        config.bootstrap_service = Some(url2!("ws://not-a-bootstrap.test"));

        let (meta_net, _, bootstrap_net) = test_create_meta_net(config).await.unwrap();

        // Not the most interesting check but we mostly care that the above function produces a result given a valid config.
        assert_eq!(BootstrapNet::Tx5, bootstrap_net);

        meta_net.close(0, "test").await;
    }

    async fn test_create_meta_net(
        config: KitsuneP2pConfig,
    ) -> KitsuneP2pResult<(MetaNet, MetaNetEvtRecv, BootstrapNet)> {
        let builder = GhostActorBuilder::new();

        let internal_sender = builder
            .channel_factory()
            .create_channel::<Internal>()
            .await
            .unwrap();

        tokio::spawn(builder.spawn(InternalStub::new()));

        let (sender, _) = futures::channel::mpsc::channel(10);

        create_meta_net(
            &config,
            TlsConfig::new_ephemeral().await.unwrap(),
            internal_sender,
            HostStub::new().legacy(sender),
            PreflightUserData::default(),
        )
        .await
        .map(|(n, r, b, _)| (n, r, b))
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/discover.rs
================================================
use super::*;
use kitsune_p2p_types::{agent_info::AgentInfoSigned, dht_arc::DhtLocation};
use std::future::Future;

/// This enum represents the outcomes from peer discovery
/// - OkShortcut - the agent is locally joined, just mirror the request back out
/// - OkRemote - we were able to successfully establish a remote connection
/// - Err - we were not able to establish a connection within the timeout
pub(crate) enum PeerDiscoverResult {
    OkShortcut,
    OkRemote {
        #[allow(dead_code)]
        url: String,
        con_hnd: MetaNetCon,
    },
    Err(KitsuneP2pError),
}

pub(crate) trait SearchAndDiscoverPeerConnect: 'static + Send + Sync {
    fn is_agent_local(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>>;

    fn get_agent_info_signed(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'_, Result<Option<AgentInfoSigned>, Box<dyn Send + Sync + std::error::Error>>>;
}

impl SearchAndDiscoverPeerConnect for Arc<SpaceReadOnlyInner> {
    fn is_agent_local(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
        self.i_s.is_agent_local(agent)
    }

    fn get_agent_info_signed(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'_, Result<Option<AgentInfoSigned>, Box<dyn Send + Sync + std::error::Error>>>
    {
        self.host_api.get_agent_info_signed(GetAgentInfoSignedEvt {
            space: self.space.clone(),
            agent,
        })
    }
}

#[allow(clippy::enum_variant_names)]
pub(crate) enum SearchAndDiscoverPeerConnectLogicResult {
    ShouldReturn(PeerDiscoverResult),
    ShouldPeerConnect(AgentInfoSigned),
    ShouldSearchPeers,
}

pub(crate) struct SearchAndDiscoverPeerConnectLogic<S: SearchAndDiscoverPeerConnect> {
    inner: S,
    timeout: KitsuneTimeout,
    backoff: KitsuneBackoff,
    to_agent: Arc<KitsuneAgent>,
}

impl<S: SearchAndDiscoverPeerConnect> SearchAndDiscoverPeerConnectLogic<S> {
    pub fn new(
        inner: S,
        initial_delay_ms: u64,
        max_delay_ms: u64,
        to_agent: Arc<KitsuneAgent>,
        timeout: KitsuneTimeout,
    ) -> Self {
        let backoff = timeout.backoff(initial_delay_ms, max_delay_ms);
        Self {
            inner,
            timeout,
            backoff,
            to_agent,
        }
    }

    pub async fn wait(&self) {
        self.backoff.wait().await;
    }

    pub async fn check_state(&mut self) -> SearchAndDiscoverPeerConnectLogicResult {
        // see if the tgt agent is actually local
        if let Ok(true) = self.inner.is_agent_local(self.to_agent.clone()).await {
            return SearchAndDiscoverPeerConnectLogicResult::ShouldReturn(
                PeerDiscoverResult::OkShortcut,
            );
        }

        // see if we already know how to reach the tgt agent
        if let Ok(Some(agent_info_signed)) = self
            .inner
            .get_agent_info_signed(self.to_agent.clone())
            .await
        {
            return SearchAndDiscoverPeerConnectLogicResult::ShouldPeerConnect(agent_info_signed);
        }

        // the next step involves making network requests
        // so check our timeout first
        if self.timeout.is_expired() {
            return SearchAndDiscoverPeerConnectLogicResult::ShouldReturn(PeerDiscoverResult::Err(
                "timeout discovering peer".into(),
            ));
        }

        SearchAndDiscoverPeerConnectLogicResult::ShouldSearchPeers
    }
}

/// search for / discover / and open a connection to a specific remote agent
pub(crate) fn search_and_discover_peer_connect(
    inner: Arc<SpaceReadOnlyInner>,
    to_agent: Arc<KitsuneAgent>,
    timeout: KitsuneTimeout,
) -> impl Future<Output = PeerDiscoverResult> + 'static + Send {
    const INITIAL_DELAY_MS: u64 = 100;
    const MAX_DELAY_MS: u64 = 1000;

    let mut logic = SearchAndDiscoverPeerConnectLogic::new(
        inner.clone(),
        INITIAL_DELAY_MS,
        MAX_DELAY_MS,
        to_agent.clone(),
        timeout,
    );

    async move {
        loop {
            match logic.check_state().await {
                SearchAndDiscoverPeerConnectLogicResult::ShouldReturn(r) => return r,
                SearchAndDiscoverPeerConnectLogicResult::ShouldPeerConnect(agent_info_signed) => {
                    return peer_connect(inner, &agent_info_signed, timeout).await;
                }
                SearchAndDiscoverPeerConnectLogicResult::ShouldSearchPeers => (),
            }

            // let's do some discovery
            if let Ok(nodes) =
                search_remotes_covering_basis(inner.clone(), to_agent.get_loc(), timeout).await
            {
                for node in nodes {
                    // try connecting to the returned nodes
                    if let PeerDiscoverResult::OkRemote { con_hnd, .. } =
                        peer_connect(inner.clone(), &node, timeout).await
                    {
                        // make a peer query for the basis
                        let payload = wire::Wire::peer_get(inner.space.clone(), to_agent.clone());
                        match con_hnd.request(&payload, timeout).await {
                            Ok(wire::Wire::PeerGetResp(wire::PeerGetResp {
                                agent_info_signed: Some(agent_info_signed),
                            })) => {
                                if let Err(err) = inner
                                    .host_api
                                    .legacy
                                    .put_agent_info_signed(PutAgentInfoSignedEvt {
                                        peer_data: vec![agent_info_signed.clone()],
                                    })
                                    .await
                                {
                                    tracing::error!(
                                        ?err,
                                        "search_and_discover_peer_connect: error putting agent info"
                                    );
                                }

                                // hey, we got our target node info
                                // return the try-to-connect future
                                return peer_connect(inner, &agent_info_signed, timeout).await;
                            }
                            Ok(wire::Wire::PeerGetResp(wire::PeerGetResp {
                                agent_info_signed: None,
                            })) => {
                                // No agent found, move on to the next node.
                                continue;
                            }
                            peer_resp => {
                                // This node is sending us something unexpected, so let's warn about that.
                                tracing::warn!(
                                    ?peer_resp,
                                    "search_and_discover_peer_connect: unexpected peer response"
                                );
                            }
                        }
                    }
                }
            }

            tracing::info!(
                "search_and_discover_peer_connect: no peers found, retrying after delay."
            );

            logic.wait().await;
        }
    }
}

/// attempt to establish a connection to another peer within given timeout
pub(crate) fn peer_connect(
    inner: Arc<SpaceReadOnlyInner>,
    agent_info_signed: &AgentInfoSigned,
    timeout: KitsuneTimeout,
) -> impl Future<Output = PeerDiscoverResult> + 'static + Send {
    let agent = agent_info_signed.agent.clone();
    let url = agent_info_signed
        .url_list
        .first()
        .cloned()
        .ok_or_else(|| KitsuneP2pError::from("no url - agent is likely offline"));

    async move {
        let url = url?;

        // if they are local, return the shortcut result
        if inner.i_s.is_agent_local(agent).await? {
            return Ok(PeerDiscoverResult::OkShortcut);
        }

        // attempt an outgoing connection
        let con_hnd = inner
            .ep_hnd
            .get_connection(url.to_string(), timeout)
            .await?;

        // return the result
        Ok(PeerDiscoverResult::OkRemote {
            url: url.to_string(),
            con_hnd,
        })
    }
    .map(|r| match r {
        Ok(r) => r,
        Err(e) => PeerDiscoverResult::Err(e),
    })
}

#[derive(Debug)]
pub(crate) enum SearchRemotesCoveringBasisLogicResult {
    Success(Vec<AgentInfoSigned>),
    Error(KitsuneP2pError),
    ShouldWait,
    QueryPeers(Vec<AgentInfoSigned>),
}

pub(crate) struct SearchRemotesCoveringBasisLogic {
    timeout: KitsuneTimeout,
    backoff: KitsuneBackoff,
    check_node_count: usize,
    basis_loc: DhtLocation,
}

impl SearchRemotesCoveringBasisLogic {
    pub fn new(
        initial_delay_ms: u64,
        max_delay_ms: u64,
        check_node_count: usize,
        basis_loc: DhtLocation,
        timeout: KitsuneTimeout,
    ) -> Self {
        let backoff = timeout.backoff(initial_delay_ms, max_delay_ms);
        Self {
            timeout,
            backoff,
            check_node_count,
            basis_loc,
        }
    }

    pub async fn wait(&self) {
        self.backoff.wait().await;
    }

    pub fn check_nodes(
        &mut self,
        nodes: Vec<AgentInfoSigned>,
    ) -> SearchRemotesCoveringBasisLogicResult {
        let mut cover_nodes = Vec::new();
        let mut near_nodes = Vec::new();

        // first check our local peer store,
        // sort into nodes covering the basis and otherwise
        for node in nodes {
            // skip offline nodes
            if node.url_list.is_empty() {
                continue;
            }

            // skip nodes that aren't willing to store data
            if node.storage_arc().range().is_empty() {
                continue;
            }

            if node.storage_arc().contains(self.basis_loc) {
                cover_nodes.push(node);
            } else {
                near_nodes.push(node);
            }

            if cover_nodes.len() + near_nodes.len() >= self.check_node_count {
                break;
            }
        }

        // if we have any nodes covering the basis, return them
        if !cover_nodes.is_empty() {
            return SearchRemotesCoveringBasisLogicResult::Success(cover_nodes);
        }

        // if we've exhausted our timeout, we should exit
        if let Err(err) = self.timeout.ok("search_remotes_covering_basis") {
            return SearchRemotesCoveringBasisLogicResult::Error(err.into());
        }

        if near_nodes.is_empty() {
            // maybe just wait and try again?
            return SearchRemotesCoveringBasisLogicResult::ShouldWait;
        }

        // shuffle the returned nodes so we don't keep hammering the same one
        use rand::prelude::*;
        near_nodes.shuffle(&mut rand::thread_rng());

        SearchRemotesCoveringBasisLogicResult::QueryPeers(near_nodes)
    }
}

/// looping search for agents covering basis_loc
/// by requesting closer agents from remote nodes
pub(crate) fn search_remotes_covering_basis(
    inner: Arc<SpaceReadOnlyInner>,
    basis_loc: DhtLocation,
    timeout: KitsuneTimeout,
) -> impl Future<Output = KitsuneP2pResult<Vec<AgentInfoSigned>>> + 'static + Send {
    const INITIAL_DELAY_MS: u64 = 100;
    const MAX_DELAY_MS: u64 = 1000;
    const CHECK_NODE_COUNT: usize = 8;

    let mut logic = SearchRemotesCoveringBasisLogic::new(
        INITIAL_DELAY_MS,
        MAX_DELAY_MS,
        CHECK_NODE_COUNT,
        basis_loc,
        timeout,
    );

    async move {
        loop {
            let nodes = get_cached_remotes_near_basis(inner.clone(), basis_loc, timeout)
                .await
                .unwrap_or_else(|_| Vec::new());

            let near_nodes = match logic.check_nodes(nodes) {
                SearchRemotesCoveringBasisLogicResult::Success(out) => {
                    return Ok(out);
                }
                SearchRemotesCoveringBasisLogicResult::Error(err) => {
                    return Err(err);
                }
                SearchRemotesCoveringBasisLogicResult::ShouldWait => {
                    logic.wait().await;
                    continue;
                }
                SearchRemotesCoveringBasisLogicResult::QueryPeers(nodes) => nodes,
            };

            let mut added_data = false;
            for node in near_nodes {
                // try connecting to the returned nodes
                if let PeerDiscoverResult::OkRemote { con_hnd, .. } =
                    peer_connect(inner.clone(), &node, timeout).await
                {
                    // make a peer query for the basis
                    let payload = wire::Wire::peer_query(inner.space.clone(), basis_loc);
                    match con_hnd.request(&payload, timeout).await {
                        Ok(wire::Wire::PeerQueryResp(wire::PeerQueryResp { peer_list })) => {
                            if peer_list.is_empty() {
                                tracing::warn!("empty discovery peer list");
                                continue;
                            }
                            // if we got results, add them to our peer store
                            if let Err(err) = inner
                                .host_api
                                .legacy
                                .put_agent_info_signed(PutAgentInfoSignedEvt {
                                    peer_data: peer_list,
                                })
                                .await
                            {
                                tracing::error!(?err, "error storing peer_queried agent_info");
                            }
                            // then break, to pull up the local query
                            // that should now include these new results
                            added_data = true;
                            break;
                        }
                        peer_resp => {
                            tracing::warn!(
                                ?peer_resp,
                                "search_remotes_covering_basis: unexpected peer response"
                            );
                        }
                    }
                }
            }

            if !added_data {
                logic.wait().await;
            }
        }
    }
}

pub(crate) trait GetCachedRemotesNearBasisSpace: 'static + Send + Sync {
    fn space(&self) -> Arc<KitsuneSpace>;

    fn query_agents(
        &self,
        query: QueryAgentsEvt,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>>;

    fn is_agent_local(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>>;
}

impl GetCachedRemotesNearBasisSpace for Arc<SpaceReadOnlyInner> {
    fn space(&self) -> Arc<KitsuneSpace> {
        self.space.clone()
    }

    fn query_agents(
        &self,
        query: QueryAgentsEvt,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<Vec<AgentInfoSigned>>> {
        self.host_api.legacy.query_agents(query)
    }

    fn is_agent_local(
        &self,
        agent: Arc<KitsuneAgent>,
    ) -> MustBoxFuture<'static, KitsuneP2pResult<bool>> {
        self.i_s.is_agent_local(agent)
    }
}

/// local search for remote (non-local) agents closest to basis
pub(crate) fn get_cached_remotes_near_basis<S: GetCachedRemotesNearBasisSpace>(
    inner: S,
    basis_loc: DhtLocation,
    _timeout: KitsuneTimeout,
) -> impl Future<Output = KitsuneP2pResult<Vec<AgentInfoSigned>>> + 'static + Send {
    // as this is a local request, there isn't much cost to getting more
    // results than we strictly need
    const LIMIT: u32 = 20;

    async move {
        let mut nodes = Vec::new();

        let query = QueryAgentsEvt::new(inner.space())
            .near_basis(basis_loc)
            .limit(LIMIT);
        for node in inner.query_agents(query).await? {
            if !inner.is_agent_local(node.agent.clone()).await? {
                nodes.push(node);
            }
        }

        if nodes.is_empty() {
            return Err("no remote nodes found, abort discovery".into());
        }

        Ok(nodes)
    }
}

#[cfg(test)]
mod test_search_and_discover_peer_connect;

#[cfg(test)]
mod test_search_remotes_covering_basis;

#[cfg(test)]
mod test_get_cached_remotes_near_basis;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/fetch.rs
================================================
mod fetch_task;
mod response_config;

pub use fetch_task::FetchTask;
pub use response_config::FetchResponseConfig;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/meta_net.rs
================================================
// because of feature flipping
#![allow(dead_code)]
#![allow(irrefutable_let_patterns)]
#![allow(unused_variables)]
#![allow(unreachable_code)]
#![allow(unused_imports)]
#![allow(unreachable_patterns)]
#![allow(clippy::needless_return)]
//! Networking abstraction to handle feature flipping.

use crate::wire::WireData;
use crate::*;
use futures::sink::SinkExt;
use futures::stream::StreamExt;

use kitsune_p2p_types::tx_utils::TxUrl;

use crate::spawn::actor::InternalSender;
use crate::spawn::KitsuneP2pEvent;
use crate::spawn::PutAgentInfoSignedEvt;
use crate::types::event::KitsuneP2pEventSender;
use kitsune_p2p_block::BlockTargetId;
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::codec::Codec;
use kitsune_p2p_types::config::KitsuneP2pConfig;
use kitsune_p2p_types::config::KitsuneP2pTuningParams;
use kitsune_p2p_types::*;
use opentelemetry_api::metrics::Histogram;
use parking_lot::Mutex;
use std::collections::HashMap;
use std::sync::Arc;
use tx5::PeerUrl;

use crate::spawn::actor::UNAUTHORIZED_DISCONNECT_CODE;
use crate::spawn::actor::UNAUTHORIZED_DISCONNECT_REASON;

kitsune_p2p_types::write_codec_enum! {
    /// KitsuneP2p WebRTC wrapper enum.
    codec WireWrap {
        /// Notification not needing a response.
        Notify(0x00) {
            msg_id.0: u64,
            data.1: WireData,
        },

        /// Request that expects a response.
        Request(0x10) {
            msg_id.0: u64,
            data.1: WireData,
        },

        /// Response to a previous request.
        Response(0x11) {
            msg_id.0: u64,
            data.1: WireData,
        },
    }
}

kitsune_p2p_types::write_codec_enum! {
    /// Preflight data for tx5.
    /// Since this is all about compatibility, the codec itself contains versioned payloads,
    /// in case the preflight check needs to evolve over time.
    codec PreflightData {
        /// Version 0
        V0(0) {
            /// Kitsune protocol version which is bumped at every breaking change
            kitsune_protocol_version.0: u16,
            /// Our local peer info
            peer_list.1: Vec<AgentInfoSigned>,
            /// Data provided by the host, which must match across nodes in order
            /// for preflight to succeed
            user_data.2: Vec<u8>,
        },
    }
}

/// Host-defined data used to implement custom connection preflight checks.
///
/// The `bytes` are sent with every preflight, and the `comparator` is used to validate
/// the bytes sent by the remote peer. If the comparator returns an Err, the preflight
/// fails and no connection is made.
///
/// The string returned in the Err is logged from kitsune to indicate the point of failure.
pub struct PreflightUserData {
    /// The bytes to send with every preflight.
    pub bytes: Vec<u8>,
    /// The comparator function to use to validate the bytes sent by the remote peer.
    ///
    /// Typically this will be a closure that captures the bytes sent, so that the two values can
    /// be compared.
    #[allow(clippy::type_complexity)]
    pub comparator: Box<dyn Fn(&tx5::PeerUrl, &[u8]) -> Result<(), String> + Send + Sync + 'static>,
}

impl Default for PreflightUserData {
    fn default() -> Self {
        Self {
            bytes: Vec::new(),
            comparator: Box::new(|_, _| Ok(())),
        }
    }
}

fn next_msg_id() -> u64 {
    static MSG_ID: std::sync::atomic::AtomicU64 = std::sync::atomic::AtomicU64::new(1);
    // MAYBE - track these message ids at the connection level
    // to prevent mismatches
    MSG_ID.fetch_add(1, std::sync::atomic::Ordering::Relaxed)
}

pub type RespondFut = std::pin::Pin<Box<dyn std::future::Future<Output = ()> + 'static + Send>>;

pub type Respond = Box<dyn FnOnce(wire::Wire) -> RespondFut + 'static + Send>;

/// Events emitted by a meta net instance.
pub enum MetaNetEvt {
    /// This node has a new address at which it can be reached.
    NewAddress {
        /// The new address at which this node can be reached.
        local_url: String,
    },

    /// A connection has been established.
    Connected {
        /// Identifies the remote peer.
        remote_url: String,

        /// Handle to the connection.
        con: MetaNetCon,
    },

    /// A connection has been closed.
    Disconnected {
        /// Identifies the remote peer.
        remote_url: String,

        /// Handle to the connection.
        con: MetaNetCon,
    },

    /// An incoming request expecting a response.
    Request {
        /// Identifies the remote peer.
        remote_url: String,

        /// Handle to the connection.
        con: MetaNetCon,

        /// The request data sent by the remote peer.
        data: wire::Wire,

        /// Respond to this request.
        respond: Respond,
    },

    /// An incoming notification that doesn't require a direct response.
    Notify {
        /// Identifies the remote peer.
        remote_url: String,

        /// Handle to the connection.
        con: MetaNetCon,

        /// The request data sent by the remote peer.
        data: wire::Wire,
    },
}

impl std::fmt::Debug for MetaNetEvt {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::NewAddress { local_url } => f
                .debug_struct("NewAddress")
                .field("local_url", local_url)
                .finish(),
            Self::Connected { remote_url, .. } => f
                .debug_struct("Connected")
                .field("remote_url", remote_url)
                .finish(),
            Self::Disconnected { remote_url, .. } => f
                .debug_struct("Disconnected")
                .field("remote_url", remote_url)
                .finish(),
            Self::Request {
                remote_url, data, ..
            } => f
                .debug_struct("Request")
                .field("remote_url", remote_url)
                .field("data", data)
                .finish(),
            Self::Notify {
                remote_url, data, ..
            } => f
                .debug_struct("Notify")
                .field("remote_url", remote_url)
                .field("data", data)
                .finish(),
        }
    }
}

impl MetaNetEvt {
    pub fn maybe_con(&self) -> Option<&MetaNetCon> {
        match self {
            MetaNetEvt::NewAddress { .. } => None,
            MetaNetEvt::Connected { con, .. }
            | MetaNetEvt::Disconnected { con, .. }
            | MetaNetEvt::Request { con, .. }
            | MetaNetEvt::Notify { con, .. } => Some(con),
        }
    }

    pub fn maybe_space(&self) -> Option<Arc<KitsuneSpace>> {
        match self {
            MetaNetEvt::Request { data, .. } | MetaNetEvt::Notify { data, .. } => {
                data.maybe_space()
            }
            MetaNetEvt::NewAddress { .. }
            | MetaNetEvt::Connected { .. }
            | MetaNetEvt::Disconnected { .. } => None,
        }
    }
}

pub enum MetaNetAuth {
    Authorized,
    UnauthorizedIgnore,
    UnauthorizedDisconnect,
}

async fn node_is_authorized(host: &HostApi, node_id: NodeCert, now: Timestamp) -> MetaNetAuth {
    match host.is_blocked(BlockTargetId::Node(node_id), now).await {
        Ok(true) => MetaNetAuth::UnauthorizedDisconnect,
        Ok(false) => MetaNetAuth::Authorized,
        Err(_) => MetaNetAuth::UnauthorizedIgnore,
    }
}

pub async fn nodespace_is_authorized(
    host: &HostApi,
    node_id: NodeCert,
    maybe_space: Option<Arc<KitsuneSpace>>,
    now: Timestamp,
) -> MetaNetAuth {
    if let Some(space) = maybe_space {
        match node_is_authorized(host, node_id.clone(), now).await {
            MetaNetAuth::Authorized => {
                match host
                    .is_blocked(BlockTargetId::NodeSpace(node_id, space), now)
                    .await
                {
                    Ok(true) => MetaNetAuth::UnauthorizedIgnore,
                    Ok(false) => MetaNetAuth::Authorized,
                    Err(_) => MetaNetAuth::UnauthorizedIgnore,
                }
            }
            unauthorized => unauthorized,
        }
    } else {
        MetaNetAuth::Authorized
    }
}

pub type MetaNetEvtRecv = futures::channel::mpsc::Receiver<MetaNetEvt>;

type ResStore = Arc<Mutex<HashMap<u64, tokio::sync::oneshot::Sender<wire::Wire>>>>;

struct MetricSendGuard {
    rem_id: tx5::PubKey,
    is_error: bool,
    byte_count: u64,
    start_time: std::time::Instant,
}

impl MetricSendGuard {
    pub fn new(rem_id: tx5::PubKey, byte_count: u64) -> Self {
        Self {
            rem_id,
            is_error: true,
            byte_count,
            start_time: std::time::Instant::now(),
        }
    }

    pub fn set_is_error(&mut self, is_error: bool) {
        self.is_error = is_error;
    }
}

impl Drop for MetricSendGuard {
    fn drop(&mut self) {
        crate::metrics::METRIC_MSG_OUT_BYTE.record(
            self.byte_count,
            &[
                opentelemetry_api::KeyValue::new("remote_id", format!("{:?}", self.rem_id)),
                opentelemetry_api::KeyValue::new("is_error", self.is_error),
            ],
        );
        crate::metrics::METRIC_MSG_OUT_TIME.record(
            self.start_time.elapsed().as_secs_f64(),
            &[
                opentelemetry_api::KeyValue::new("remote_id", format!("{:?}", self.rem_id)),
                opentelemetry_api::KeyValue::new("is_error", self.is_error),
            ],
        );
    }
}

#[derive(Debug, Clone)]
pub enum MetaNetCon {
    Tx5 {
        host: HostApiLegacy,
        ep: Arc<tx5::Endpoint>,
        rem_url: tx5::PeerUrl,
        res: ResStore,
        tun: KitsuneP2pTuningParams,
    },

    #[cfg(test)]
    Test {
        state: Arc<parking_lot::RwLock<MetaNetConTest>>,
    },
}

impl PartialEq for MetaNetCon {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (MetaNetCon::Tx5 { ep: a, .. }, MetaNetCon::Tx5 { ep: b, .. }) => Arc::ptr_eq(a, b),
            _ => false,
        }
    }
}

impl Eq for MetaNetCon {}

impl MetaNetCon {
    pub async fn close(&self, code: u32, reason: &str) {
        #[cfg(test)]
        {
            if let MetaNetCon::Test { state } = self {
                state.write().closed = true;
                return;
            }
        }

        {
            if let MetaNetCon::Tx5 {
                ep, rem_url, tun, ..
            } = self
            {
                ep.close(rem_url);
                return;
            }
        }
    }

    pub fn is_closed(&self) -> bool {
        #[cfg(test)]
        {
            if let MetaNetCon::Test { state } = self {
                return state.read().closed;
            }
        }

        {
            // NOTE - tx5 connections are never exactly "closed"
            //        since it's more of a message queue...
            return false;
        }
    }

    async fn wire_is_authorized(&self, payload: &wire::Wire, now: Timestamp) -> MetaNetAuth {
        match self {
            MetaNetCon::Tx5 { host, .. } => {
                nodespace_is_authorized(host, self.peer_id(), payload.maybe_space(), now).await
            }
            #[cfg(test)]
            MetaNetCon::Test { .. } => MetaNetAuth::Authorized,
        }
    }

    #[cfg_attr(feature = "instrument", tracing::instrument(skip_all))]
    pub async fn notify(&self, payload: &wire::Wire, timeout: KitsuneTimeout) -> KitsuneResult<()> {
        let start = std::time::Instant::now();
        let msg_id = next_msg_id();

        let result = async move {
            match self.wire_is_authorized(payload, Timestamp::now()).await {
                MetaNetAuth::Authorized => {
                    #[cfg(test)]
                    {
                        if let MetaNetCon::Test { state } = self {
                            let mut state = state.write();
                            state.notify_call_count += 1;

                            return if state.notify_succeed {
                                Ok(())
                            } else {
                                Err("Test error while notifying".into())
                            };
                        }
                    }

                    {
                        if let MetaNetCon::Tx5 { ep, rem_url, .. } = self {
                            let wire = payload.encode_vec().map_err(KitsuneError::other)?;
                            let wrap = WireWrap::notify(msg_id, WireData(wire));

                            let data = wrap.encode_vec().map_err(KitsuneError::other)?;

                            let mut metric_guard =
                                MetricSendGuard::new(rem_url.pub_key().clone(), data.len() as u64);

                            ep.send(rem_url.clone(), data)
                                .await
                                .map_err(KitsuneError::other)?;

                            metric_guard.set_is_error(false);

                            return Ok(());
                        }
                    }

                    return Err("invalid features".into());
                }
                MetaNetAuth::UnauthorizedIgnore => {
                    return Ok(());
                }
                MetaNetAuth::UnauthorizedDisconnect => {
                    self.close(UNAUTHORIZED_DISCONNECT_CODE, UNAUTHORIZED_DISCONNECT_REASON)
                        .await;
                    return Ok(());
                }
            }
        }
        .await;

        let elapsed_s = start.elapsed().as_secs_f64();

        tracing::trace!(%elapsed_s, %msg_id, ?payload, ?result, "sent notify");

        result
    }

    pub async fn request(
        &self,
        payload: &wire::Wire,
        timeout: KitsuneTimeout,
    ) -> KitsuneResult<wire::Wire> {
        let start = std::time::Instant::now();
        let msg_id = next_msg_id();

        tracing::trace!(?payload, "initiating request");

        let result = async move {
            match self.wire_is_authorized(payload, Timestamp::now()).await {
                MetaNetAuth::Authorized => {
                    {
                        if let MetaNetCon::Tx5 {
                            ep,
                            rem_url,
                            res: res_store,
                            ..
                        } = self
                        {
                            let (s, r) = tokio::sync::oneshot::channel();
                            res_store.lock().insert(msg_id, s);

                            let res_store = res_store.clone();
                            tokio::task::spawn(async move {
                                tokio::time::sleep(timeout.time_remaining()).await;
                                res_store.lock().remove(&msg_id);
                            });

                            let wire = payload.encode_vec().map_err(KitsuneError::other)?;
                            let wrap = WireWrap::request(msg_id, WireData(wire));
                            let data = wrap.encode_vec().map_err(KitsuneError::other)?;

                            let mut metric_guard =
                                MetricSendGuard::new(rem_url.pub_key().clone(), data.len() as u64);

                            ep.send(rem_url.clone(), data)
                                .await
                                .map_err(KitsuneError::other)?;

                            let resp = r.await.map_err(|_| KitsuneError::other("timeout"))?;

                            metric_guard.set_is_error(false);
                            return Ok(resp);
                        }
                    }

                    return Err("invalid features".into());
                }
                MetaNetAuth::UnauthorizedIgnore => {
                    return Err(KitsuneErrorKind::Unauthorized.into());
                }
                MetaNetAuth::UnauthorizedDisconnect => {
                    self.close(UNAUTHORIZED_DISCONNECT_CODE, UNAUTHORIZED_DISCONNECT_REASON)
                        .await;
                    return Err(KitsuneErrorKind::Unauthorized.into());
                }
            }
        }
        .await;

        let elapsed_s = start.elapsed().as_secs_f64();

        tracing::trace!(%elapsed_s, %msg_id, ?payload, ?result, "sent request");

        result
    }

    pub fn peer_id(&self) -> NodeCert {
        #[cfg(test)]
        {
            if let MetaNetCon::Test { state } = self {
                return state.read().id();
            }
        }

        {
            if let MetaNetCon::Tx5 { rem_url, .. } = self {
                return rem_url.pub_key().0.clone().into();
            }
        }

        panic!("invalid features");
    }
}

#[cfg(test)]
#[derive(Debug)]
pub struct MetaNetConTest {
    pub id: NodeCert,
    pub closed: bool,

    pub notify_succeed: bool,
    pub notify_call_count: usize,
}

#[cfg(test)]
impl Default for MetaNetConTest {
    fn default() -> Self {
        Self {
            id: NodeCert::from(Arc::new([0; 32])),
            closed: false,
            notify_succeed: true,
            notify_call_count: 0,
        }
    }
}

#[cfg(test)]
impl MetaNetConTest {
    pub fn new_with_id(id: u8) -> Self {
        Self {
            id: NodeCert::from(Arc::new(vec![id; 32].try_into().unwrap())),
            ..Default::default()
        }
    }

    pub fn id(&self) -> NodeCert {
        self.id.clone()
    }
}

/// Networking abstraction to handle feature flipping.
#[derive(Clone)]
pub enum MetaNet {
    /// Tx5 Abstraction
    Tx5 {
        host: HostApiLegacy,
        ep: Arc<tx5::Endpoint>,
        res: ResStore,
        tun: KitsuneP2pTuningParams,
    },
}

impl MetaNet {
    /// Construct abstraction with tx5 backend.
    pub async fn new_tx5(
        tuning_params: KitsuneP2pTuningParams,
        host: HostApiLegacy,
        kitsune_internal_sender: ghost_actor::GhostSender<crate::spawn::Internal>,
        signal_url: String,
        webrtc_config: String,
        preflight_user_data: PreflightUserData,
    ) -> KitsuneP2pResult<(Self, MetaNetEvtRecv, Option<String>)> {
        use kitsune_p2p_types::codec::{rmp_decode, rmp_encode};

        let (mut evt_send, evt_recv) =
            futures::channel::mpsc::channel(tuning_params.concurrent_limit_per_thread);

        let PreflightUserData {
            bytes: user_data_sent,
            comparator: user_data_cmp,
        } = preflight_user_data;

        let backend_module = match tuning_params.tx5_backend_module.as_str() {
            "mem" => tx5::backend::BackendModule::Mem,
            _ => tx5::backend::BackendModule::GoPion,
        };

        let backend_module_config = Some(
            serde_json::from_str(&tuning_params.tx5_backend_module_config)
                .map_err(std::io::Error::other)?,
        );

        let evt_sender = host.legacy.clone();
        let tx5_config = tx5::Config {
            // TODO: once we implement local discovery, we should only
            //       allow plain text over local connections. But for now
            //       we cannot distinguish, so in order for run-local-services
            //       to work, we need to allow plain text on ALL connections
            signal_allow_plain_text: true,
            initial_webrtc_config: webrtc_config,
            connection_count_max: tuning_params.tx5_connection_count_max,
            send_buffer_bytes_max: tuning_params.tx5_send_buffer_bytes_max,
            recv_buffer_bytes_max: tuning_params.tx5_recv_buffer_bytes_max,
            incoming_message_bytes_max: tuning_params.tx5_incoming_message_bytes_max,
            message_size_max: tuning_params.tx5_message_size_max,
            internal_event_channel_size: tuning_params.tx5_internal_event_channel_size,
            timeout: std::time::Duration::from_secs(tuning_params.tx5_timeout_s as u64),
            backoff_start: std::time::Duration::from_secs(tuning_params.tx5_backoff_start_s as u64),
            backoff_max: std::time::Duration::from_secs(tuning_params.tx5_backoff_max_s as u64),
            preflight: Some((
                Arc::new(move |_| {
                    let i_s = kitsune_internal_sender.clone();
                    let user_data_sent = user_data_sent.clone();

                    Box::pin(async move {
                        let agent_list = i_s
                            .get_all_local_joined_agent_infos()
                            .await
                            .unwrap_or_default();
                        PreflightData::v0(KITSUNE_PROTOCOL_VERSION, agent_list, user_data_sent)
                            .encode_vec()
                    })
                }),
                Arc::new(move |url, data| {
                    let e_s = evt_sender.clone();
                    let url = url.clone();
                    match PreflightData::decode_ref(&data) {
                        Ok((
                            _,
                            PreflightData::V0(V0 {
                                kitsune_protocol_version,
                                peer_list,
                                user_data: user_data_bytes_received,
                            }),
                        )) => {
                            if kitsune_protocol_version != KITSUNE_PROTOCOL_VERSION {
                                tracing::warn!(
                                    ?url,
                                    "kitsune protocol version mismatch: ours = {}, theirs = {}",
                                    KITSUNE_PROTOCOL_VERSION,
                                    kitsune_protocol_version,
                                );
                                return box_fut_plain(Err(std::io::Error::other(
                                    "kitsune protocol version mismatch",
                                )));
                            }

                            if let Err(reason) = user_data_cmp(&url, &user_data_bytes_received) {
                                tracing::warn!(?url, "tx5 preflight user_data mismatch");
                                return box_fut_plain(Err(std::io::Error::other(
                                    "tx5 preflight user_data mismatch",
                                )));
                            }
                            Box::pin(async move {
                                if let Err(err) = e_s
                                    .put_agent_info_signed(PutAgentInfoSignedEvt {
                                        peer_data: peer_list,
                                    })
                                    .await
                                {
                                    tracing::warn!(
                                        ?err,
                                        "error processing incoming agent info unsolicited"
                                    );
                                }
                                Ok(())
                            })
                        }
                        Err(err) => {
                            tracing::warn!(?err, ?url, "Could not decode PreflightData");
                            box_fut_plain(Err(std::io::Error::other(
                                "Could not decode PreflightData",
                            )))
                        }
                        _ => box_fut_plain(Err(std::io::Error::other("Unexpected wire message"))),
                    }
                }),
            )),
            backend_module,
            backend_module_config,
            //..Default::default()
        };

        tracing::info!(?tx5_config, "meta net startup tx5");

        if let Err(err) = (tx5::Tx5InitConfig {
            tracing_enabled: tuning_params.tx5_backend_tracing_enabled,
            ephemeral_udp_port_min: tuning_params.tx5_min_ephemeral_udp_port,
            ephemeral_udp_port_max: tuning_params.tx5_max_ephemeral_udp_port,
            ..Default::default()
        })
        .set_as_global_default()
        {
            tracing::warn!(?err, "Tx5InitConfig failed, you must be running multiple conductors in the same process. Be aware they will all share whichever Tx5InitConfig was first to be registered.");
        }
        let (ep_hnd, mut ep_evt) = tx5::Endpoint::new(Arc::new(tx5_config));
        let ep_hnd = Arc::new(ep_hnd);

        let maybe_peer_url = ep_hnd
            .listen(tx5::SigUrl::parse(&signal_url)?)
            .await
            .map(|p| p.to_string());

        let res_store = Arc::new(Mutex::new(HashMap::new()));

        let ep_hnd2 = ep_hnd.clone();
        let res_store2 = res_store.clone();
        let tuning_params2 = tuning_params.clone();
        let spawn_host = host.clone();
        tokio::task::spawn(async move {
            while let Some(evt) = ep_evt.recv().await {
                match evt {
                    tx5::EndpointEvent::ListeningAddressOpen { local_url } => {
                        tracing::info!(%local_url, "listening open");
                        if evt_send
                            .send(MetaNetEvt::NewAddress {
                                local_url: local_url.to_string(),
                            })
                            .await
                            .is_err()
                        {
                            break;
                        }
                    }
                    tx5::EndpointEvent::ListeningAddressClosed { local_url } => {
                        tracing::info!(%local_url, "listening closed");
                        // TODO: publish close agent_info
                    }
                    tx5::EndpointEvent::Connected { peer_url } => {
                        tracing::debug!(%peer_url, "peer connected");
                        if evt_send
                            .send(MetaNetEvt::Connected {
                                remote_url: peer_url.to_string(),
                                con: MetaNetCon::Tx5 {
                                    host: spawn_host.clone(),
                                    ep: ep_hnd2.clone(),
                                    rem_url: peer_url,
                                    res: res_store2.clone(),
                                    tun: tuning_params2.clone(),
                                },
                            })
                            .await
                            .is_err()
                        {
                            break;
                        }
                    }
                    tx5::EndpointEvent::Disconnected { peer_url } => {
                        tracing::debug!(%peer_url, "peer disconnected");
                        if evt_send
                            .send(MetaNetEvt::Disconnected {
                                remote_url: peer_url.to_string(),
                                con: MetaNetCon::Tx5 {
                                    host: spawn_host.clone(),
                                    ep: ep_hnd2.clone(),
                                    rem_url: peer_url,
                                    res: res_store2.clone(),
                                    tun: tuning_params2.clone(),
                                },
                            })
                            .await
                            .is_err()
                        {
                            break;
                        }
                    }
                    tx5::EndpointEvent::Message { peer_url, message } => {
                        tracing::trace!(%peer_url, byte_count=?message.len(), "received bytes");

                        let mut message = std::io::Cursor::new(&message);
                        match WireWrap::decode(&mut message) {
                            Ok(WireWrap::Notify(Notify { msg_id, data })) => {
                                match wire::Wire::decode_ref(&data) {
                                    Ok((_, data)) => {
                                        tracing::trace!(%msg_id, ?data, "received notify");
                                        if evt_send
                                            .send(MetaNetEvt::Notify {
                                                remote_url: peer_url.to_string(),
                                                con: MetaNetCon::Tx5 {
                                                    host: spawn_host.clone(),
                                                    ep: ep_hnd2.clone(),
                                                    rem_url: peer_url,
                                                    res: res_store2.clone(),
                                                    tun: tuning_params2.clone(),
                                                },
                                                data,
                                            })
                                            .await
                                            .is_err()
                                        {
                                            break;
                                        }
                                    }
                                    Err(err) => {
                                        tracing::error!(?err, "decoding error");
                                        ep_hnd2.close(&peer_url);
                                    }
                                }
                            }
                            Ok(WireWrap::Request(Request { msg_id, data })) => {
                                match wire::Wire::decode_ref(&data) {
                                    Ok((_, data)) => {
                                        let ep_hnd = ep_hnd2.clone();
                                        let peer_url2 = peer_url.clone();
                                        let respond: Respond = Box::new(move |data| {
                                            let out: RespondFut = Box::pin(async move {
                                                let wire = match data.encode_vec() {
                                                    Ok(wire) => wire,
                                                    Err(_) => return,
                                                };
                                                let wrap =
                                                    WireWrap::response(msg_id, WireData(wire));
                                                let data = match wrap.encode_vec() {
                                                    Ok(data) => data,
                                                    Err(_) => return,
                                                };
                                                let _ = ep_hnd.send(peer_url2, data).await;
                                            });
                                            out
                                        });
                                        if evt_send
                                            .send(MetaNetEvt::Request {
                                                remote_url: peer_url.to_string(),
                                                con: MetaNetCon::Tx5 {
                                                    host: spawn_host.clone(),
                                                    ep: ep_hnd2.clone(),
                                                    rem_url: peer_url,
                                                    res: res_store2.clone(),
                                                    tun: tuning_params2.clone(),
                                                },
                                                data,
                                                respond,
                                            })
                                            .await
                                            .is_err()
                                        {
                                            break;
                                        }
                                    }
                                    Err(err) => {
                                        tracing::error!(?err, "decoding error");
                                        ep_hnd2.close(&peer_url);
                                    }
                                }
                            }
                            Ok(WireWrap::Response(Response { msg_id, data })) => {
                                if let Some(s) = res_store2.lock().remove(&msg_id) {
                                    match wire::Wire::decode_ref(&data) {
                                        Ok((_, data)) => {
                                            let _ = s.send(data);
                                        }
                                        Err(err) => {
                                            tracing::error!(?err, "decoding error");
                                            ep_hnd2.close(&peer_url);
                                        }
                                    }
                                } else {
                                    tracing::debug!(%msg_id, "response mismatch");
                                }
                            }
                            Err(err) => {
                                tracing::error!(?err, "decoding error");
                                ep_hnd2.close(&peer_url);
                                continue;
                            }
                        }
                    }
                }
            }
        });

        Ok((
            MetaNet::Tx5 {
                host,
                ep: ep_hnd,
                res: res_store,
                tun: tuning_params,
            },
            evt_recv,
            maybe_peer_url,
        ))
    }

    pub async fn broadcast(
        &self,
        payload: &wire::Wire,
        timeout: KitsuneTimeout,
    ) -> KitsuneResult<()> {
        let msg_id = next_msg_id();

        {
            if let MetaNet::Tx5 { ep, .. } = self {
                let wire = payload.encode_vec().map_err(KitsuneError::other)?;
                let wrap = WireWrap::notify(msg_id, WireData(wire));

                let data = wrap.encode_vec().map_err(KitsuneError::other)?;
                ep.broadcast(data.as_slice()).await;
                return Ok(());
            }
        }

        Err("invalid features".into())
    }

    pub async fn close(&self, code: u32, reason: &str) {

        // TODO - currently no way to shutdown tx5
    }

    pub fn close_peer_con(&self, peer_url: TxUrl) -> KitsuneResult<()> {
        {
            // Even if tx5 is enabled, check that the peer_url is a ws or wss url to the signal server
            if peer_url.scheme() == "ws" || peer_url.scheme() == "wss" {
                if let MetaNet::Tx5 { ep, .. } = self {
                    let peer_url =
                        PeerUrl::parse(peer_url.to_string()).map_err(KitsuneError::other)?;
                    ep.close(&peer_url);
                }
            }
        }

        Ok(())
    }

    pub async fn get_connection(
        &self,
        remote_url: String,
        timeout: KitsuneTimeout,
    ) -> KitsuneResult<MetaNetCon> {
        {
            if let MetaNet::Tx5 {
                host, ep, res, tun, ..
            } = self
            {
                return Ok(MetaNetCon::Tx5 {
                    host: host.clone(),
                    ep: ep.clone(),
                    rem_url: tx5::PeerUrl::parse(remote_url).map_err(KitsuneError::other)?,
                    res: res.clone(),
                    tun: tun.clone(),
                });
            }
        }

        Err("invalid features".into())
    }

    pub fn dump_network_stats(
        &self,
    ) -> impl std::future::Future<Output = KitsuneResult<serde_json::Value>> + 'static + Send {
        use futures::FutureExt;

        {
            if let MetaNet::Tx5 { ep, .. } = self {
                let ep = ep.clone();
                let stats = ep.get_stats();
                return async move {
                    serde_json::from_str(
                        &serde_json::to_string(&stats).map_err(KitsuneError::other)?,
                    )
                    .map_err(KitsuneError::other)
                }
                .boxed();
            }
        }

        async move { Err("invalid features".into()) }.boxed()
    }
}

#[cfg(test)]
mod tests;



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/meta_net_task.rs
================================================
use crate::actor::BroadcastData;
use crate::event::{
    FetchOpDataEvt, FetchOpDataEvtQuery, GetAgentInfoSignedEvt, KitsuneP2pEventSender,
    PutAgentInfoSignedEvt, QueryAgentsEvt,
};
use crate::spawn::actor::fetch::FetchResponseConfig;
use crate::spawn::actor::{
    Internal, InternalSender, UNAUTHORIZED_DISCONNECT_CODE, UNAUTHORIZED_DISCONNECT_REASON,
};
use crate::spawn::meta_net::{
    nodespace_is_authorized, MetaNetAuth, MetaNetCon, MetaNetEvt, MetaNetEvtRecv, Respond,
};
use crate::wire::WireData;
use crate::{wire, HostApiLegacy, KitsuneAgent, KitsuneP2pError, KitsuneSpace};
use futures::StreamExt;
use ghost_actor::{GhostError, GhostSender};
use kitsune_p2p_fetch::{FetchKey, FetchPool, FetchResponseQueue};
use kitsune_p2p_timestamp::Timestamp;
use kitsune_p2p_types::config::KitsuneP2pConfig;
use std::error::Error;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use tracing::Instrument;

struct DelegateBroadcastItem {
    space: Arc<KitsuneSpace>,
    basis: Arc<crate::KitsuneBasis>,
    to_agent: Arc<KitsuneAgent>,
    mod_idx: u32,
    mod_cnt: u32,
    data: BroadcastData,
}

pub struct MetaNetTask {
    host: HostApiLegacy,
    config: KitsuneP2pConfig,
    fetch_pool: FetchPool,
    fetch_response_queue: FetchResponseQueue<FetchResponseConfig>,
    ep_evt: Option<MetaNetEvtRecv>,
    i_s: GhostSender<Internal>,
    is_finished: Arc<AtomicBool>,
    delegate_broadcast_send: tokio::sync::mpsc::Sender<DelegateBroadcastItem>,
    delegate_broadcast_task: tokio::task::JoinHandle<()>,
}

impl Drop for MetaNetTask {
    fn drop(&mut self) {
        self.delegate_broadcast_task.abort();
    }
}

#[derive(thiserror::Error, Debug)]
enum MetaNetTaskError {
    #[error("A required channel has closed")]
    RequiredChannelClosed,

    #[error("Ignored error: {0}")]
    Ignored(Box<dyn Error>),
}

impl From<KitsuneP2pError> for MetaNetTaskError {
    fn from(err: KitsuneP2pError) -> Self {
        match err {
            KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                MetaNetTaskError::RequiredChannelClosed
            }
            e => MetaNetTaskError::Ignored(Box::new(e)),
        }
    }
}

type MetaNetTaskResult<T> = Result<T, MetaNetTaskError>;

impl MetaNetTask {
    pub fn new(
        host: HostApiLegacy,
        config: KitsuneP2pConfig,
        fetch_pool: FetchPool,
        fetch_response_queue: FetchResponseQueue<FetchResponseConfig>,
        ep_evt: MetaNetEvtRecv,
        i_s: GhostSender<Internal>,
    ) -> Self {
        const MAX_DELEGATE_BROADCAST: usize = 512;

        let (delegate_broadcast_send, mut db_recv) =
            tokio::sync::mpsc::channel(MAX_DELEGATE_BROADCAST);

        let i_s2 = i_s.clone();
        let delegate_broadcast_task = tokio::task::spawn(async move {
            while let Some(DelegateBroadcastItem {
                space,
                basis,
                to_agent,
                mod_idx,
                mod_cnt,
                data,
            }) = db_recv.recv().await
            {
                if let Err(err) = i_s2
                    .incoming_delegate_broadcast(space, basis, to_agent, mod_idx, mod_cnt, data)
                    .await
                {
                    tracing::warn!(?err, "failed to handle delegate broadcast");
                }
            }
        });

        Self {
            host,
            config,
            fetch_pool,
            fetch_response_queue,
            ep_evt: Some(ep_evt),
            i_s,
            is_finished: Arc::new(AtomicBool::new(false)),
            delegate_broadcast_send,
            delegate_broadcast_task,
        }
    }

    pub fn spawn(mut self) {
        let shutdown_notify = Arc::new(tokio::sync::Notify::new());
        let shutdown_notify_send = shutdown_notify.clone();

        let is_finished = self.is_finished.clone();

        tokio::task::spawn({
            let tuning_params = self.config.tuning_params.clone();
            let span = tracing::info_span!("MetaNetTask::spawn", scope = self.config.tracing_scope);
            let span_outer = span.clone();
            async move {
                let ep_evt = self
                    .ep_evt
                    .take()
                    .expect("There should always be an ep_evt");

                let this = Arc::new(self);
                let span = span.clone();

                let ep_evt_run = ep_evt.for_each_concurrent(
                    tuning_params.concurrent_limit_per_thread,
                    move |event| {
                        let this = this.clone();
                        let shutdown_notify = shutdown_notify_send.clone();
                        let span = span.clone();
                        async move {
                            if let Err(MetaNetTaskError::RequiredChannelClosed) = match event {
                                MetaNetEvt::NewAddress { local_url } => {
                                    this.handle_new_address(local_url).await
                                }
                                MetaNetEvt::Connected { remote_url, con } => {
                                    this.handle_connect(remote_url, con).await
                                }
                                MetaNetEvt::Disconnected { remote_url, con: _ } => {
                                    this.handle_disconnect(remote_url).await
                                }
                                MetaNetEvt::Request {
                                    remote_url: _,
                                    con,
                                    data,
                                    respond,
                                } => this.handle_request(con, data, respond).await,
                                MetaNetEvt::Notify {
                                    remote_url: url,
                                    con,
                                    data,
                                } => this.handle_notify(url, con, data).await,
                            } {
                                shutdown_notify.notify_one();
                            }
                        }
                        .instrument(span)
                    },
                );

                tokio::select! {
                    _ = ep_evt_run => {
                        // This will happen if all senders close
                    }
                    _ = shutdown_notify.notified() => {
                        // Got a shutdown signal
                    }
                }

                tracing::error!(
                    "KitsuneP2p: networking poll shutdown. Networking will no longer work!
                You can ignore this is if it happened during node shutdown.
                Otherwise please restart your node and report this error."
                );
                is_finished.fetch_or(true, Ordering::SeqCst)
            }
            .instrument(span_outer)
        });
    }

    async fn handle_new_address(&self, local_url: String) -> MetaNetTaskResult<()> {
        match self.i_s.new_address(local_url).await {
            Err(e) => Err(e.into()),
            Ok(_) => Ok(()),
        }
    }

    async fn handle_connect(&self, remote_url: String, con: MetaNetCon) -> MetaNetTaskResult<()> {
        match self.i_s.new_con(remote_url, con.clone()).await {
            Err(e) => Err(e.into()),
            Ok(_) => Ok(()),
        }
    }

    async fn handle_disconnect(&self, remote_url: String) -> MetaNetTaskResult<()> {
        match self.i_s.del_con(remote_url).await {
            Err(e) => Err(e.into()),
            Ok(_) => Ok(()),
        }
    }

    async fn handle_request(
        &self,
        con: MetaNetCon,
        data: wire::Wire,
        respond: Respond,
    ) -> MetaNetTaskResult<()> {
        match nodespace_is_authorized(
            &self.host,
            con.peer_id(),
            data.maybe_space(),
            Timestamp::now(),
        )
        .await
        {
            MetaNetAuth::UnauthorizedIgnore => {}
            MetaNetAuth::UnauthorizedDisconnect => {
                con.close(UNAUTHORIZED_DISCONNECT_CODE, UNAUTHORIZED_DISCONNECT_REASON)
                    .await;
            }
            MetaNetAuth::Authorized => {
                self.handle_request_authorized(data, respond).await?;
            }
        }

        Ok(())
    }

    async fn handle_request_authorized(
        &self,
        data: wire::Wire,
        respond: Respond,
    ) -> MetaNetTaskResult<()> {
        match data {
            wire::Wire::Call(wire::Call {
                space,
                to_agent,
                data,
                ..
            }) => {
                self.handle_call_request(space, to_agent, data, respond)
                    .await?;
            }
            wire::Wire::PeerGet(wire::PeerGet { space, agent }) => {
                self.handle_peer_get_request(space, agent, respond).await;
            }
            wire::Wire::PeerQuery(wire::PeerQuery { space, basis_loc }) => {
                // this *does* go over the network...
                // so we don't want it to be too many
                const LIMIT: u32 = 8;
                let query = QueryAgentsEvt::new(space)
                    .near_basis(basis_loc)
                    .limit(LIMIT);
                let resp = match self.host.legacy.query_agents(query).await {
                    Ok(list) => wire::Wire::peer_query_resp(list),
                    Err(err) => wire::Wire::failure(format!("Error querying agents: {:?}", err,)),
                };
                respond(resp).await;
            }
            _ => {
                tracing::warn!("received non-request data in a request");
            }
        }

        Ok(())
    }

    async fn handle_call_request(
        &self,
        space: Arc<KitsuneSpace>,
        to_agent: Arc<KitsuneAgent>,
        data: WireData,
        respond: Respond,
    ) -> MetaNetTaskResult<()> {
        let res = match self.host.legacy.call(space, to_agent, data.into()).await {
            Err(err) => {
                let reason = format!("{:?}", err);
                let fail = wire::Wire::failure(reason);
                respond(fail).await;

                return Err(err.into());
            }
            Ok(r) => r,
        };
        let resp = wire::Wire::call_resp(res.into());
        respond(resp).await;

        Ok(())
    }

    async fn handle_peer_get_request(
        &self,
        space: Arc<KitsuneSpace>,
        agent: Arc<KitsuneAgent>,
        respond: Respond,
    ) {
        let resp = match self
            .host
            .get_agent_info_signed(GetAgentInfoSignedEvt { space, agent })
            .await
        {
            Ok(info) => wire::Wire::peer_get_resp(info),
            Err(err) => wire::Wire::failure(format!("Error getting agent: {:?}", err,)),
        };
        respond(resp).await;
    }

    async fn handle_notify(
        &self,
        url: String,
        con: MetaNetCon,
        data: wire::Wire,
    ) -> MetaNetTaskResult<()> {
        match nodespace_is_authorized(
            &self.host,
            con.peer_id(),
            data.maybe_space(),
            Timestamp::now(),
        )
        .await
        {
            MetaNetAuth::UnauthorizedIgnore => {}
            MetaNetAuth::UnauthorizedDisconnect => {
                con.close(UNAUTHORIZED_DISCONNECT_CODE, UNAUTHORIZED_DISCONNECT_REASON)
                    .await;
            }
            MetaNetAuth::Authorized => {
                self.handle_notify_authorized(url, con, data).await?;
            }
        }

        Ok(())
    }

    async fn handle_notify_authorized(
        &self,
        url: String,
        con: MetaNetCon,
        data: wire::Wire,
    ) -> MetaNetTaskResult<()> {
        match data {
            wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                space,
                basis,
                to_agent,
                mod_idx,
                mod_cnt,
                data,
            }) => match data {
                BroadcastData::Publish {
                    source,
                    transfer_method,
                    op_hash_list,
                    context,
                } => {
                    if let Err(err) = self
                        .i_s
                        .incoming_publish(
                            space,
                            to_agent,
                            source,
                            transfer_method,
                            op_hash_list,
                            context,
                            Some((basis, mod_idx, mod_cnt)),
                        )
                        .await
                    {
                        tracing::warn!(?err, "failed to handle incoming delegate broadcast");
                        Err(err.into())
                    } else {
                        Ok(())
                    }
                }
                data => {
                    // one might be tempted to notify here
                    // as in Broadcast below... but we
                    // notify all relevant agents inside
                    // the space incoming_delegate_broadcast
                    // handler.
                    if let Err(err) = self
                        .delegate_broadcast_send
                        .try_send(DelegateBroadcastItem {
                            space,
                            basis,
                            to_agent,
                            mod_idx,
                            mod_cnt,
                            data,
                        })
                    {
                        tracing::warn!(?err, "failed to enqueue incoming delegate broadcast");
                    }
                    Ok(())
                }
            },
            wire::Wire::Broadcast(wire::Broadcast {
                space,
                to_agent,
                data,
                ..
            }) => match data {
                BroadcastData::User(data) => {
                    // TODO: Should we check if the basis is held before calling notify?
                    if let Err(err) = self.host.legacy.notify(space, to_agent, data).await {
                        tracing::warn!(?err, "error processing incoming broadcast");
                        Err(err.into())
                    } else {
                        Ok(())
                    }
                }
                BroadcastData::AgentInfo(agent_info) => {
                    // TODO: Should we check if the basis is
                    //       held before calling put_agent_info_signed?
                    if let Err(err) = self
                        .host
                        .legacy
                        .put_agent_info_signed(PutAgentInfoSignedEvt {
                            peer_data: vec![agent_info],
                        })
                        .await
                    {
                        tracing::warn!(?err, "error processing incoming agent info broadcast");
                        Err(err.into())
                    } else {
                        Ok(())
                    }
                }
                BroadcastData::Publish {
                    source,
                    transfer_method,
                    op_hash_list,
                    context,
                } => {
                    if let Err(err) = self
                        .i_s
                        .incoming_publish(
                            space,
                            to_agent,
                            source,
                            transfer_method,
                            op_hash_list,
                            context,
                            None,
                        )
                        .await
                    {
                        tracing::warn!(?err, "failed to handle incoming broadcast");
                        Err(err.into())
                    } else {
                        Ok(())
                    }
                }
            },
            wire::Wire::Gossip(wire::Gossip {
                space,
                data,
                module,
            }) => {
                let data: Vec<u8> = data.into();
                let data: Box<[u8]> = data.into_boxed_slice();
                if let Err(err) = self
                    .i_s
                    .incoming_gossip(space, con, url, data, module)
                    .await
                {
                    tracing::warn!(?err, "failed to handle incoming gossip");
                    Err(err.into())
                } else {
                    Ok(())
                }
            }
            wire::Wire::FetchOp(wire::FetchOp { fetch_list }) => {
                for (space, key_list) in fetch_list {
                    let mut hashes = Vec::new();
                    for key in key_list {
                        let FetchKey::Op(op_hash) = key;
                        hashes.push(op_hash);
                    }

                    if !hashes.is_empty() {
                        match self
                            .host
                            .legacy
                            .fetch_op_data(FetchOpDataEvt {
                                space: space.clone(),
                                query: FetchOpDataEvtQuery::Hashes {
                                    op_hash_list: hashes,
                                    include_limbo: true,
                                },
                            })
                            .await
                        {
                            Ok(list) => {
                                for (_hash, op) in list {
                                    self.fetch_response_queue.enqueue_op(
                                        space.clone(),
                                        (con.clone(), url.clone(), None),
                                        op,
                                    );
                                }
                            }
                            Err(KitsuneP2pError::GhostError(GhostError::Disconnected)) => {
                                return Err(MetaNetTaskError::RequiredChannelClosed)
                            }
                            _ => {
                                // Ignore other errors
                            }
                        }
                    }
                }

                Ok(())
            }
            wire::Wire::PushOpData(wire::PushOpData { op_data_list }) => {
                for (space, op_list) in op_data_list {
                    for op in op_list {
                        // hash the op
                        let op_hash = match self.host.op_hash(op.op_data.clone()).await {
                            Ok(op_hash) => op_hash,
                            Err(err) => {
                                tracing::warn!(
                                    ?err,
                                    "Dropping incoming op because the host failed to hash it {:?}",
                                    op
                                );
                                continue;
                            }
                        };

                        let key = FetchKey::Op(op_hash.clone());
                        let fetch_context = match self.fetch_pool.check_item(&key) {
                            (true, maybe_fetch_context) => maybe_fetch_context,
                            (false, _) => {
                                tracing::warn!(
                                    "Dropping incoming op because the fetch pool did not contain it, this may indicate a hashing mismatch or unsolicited pushes {:?}",
                                    op
                                );
                                continue;
                            }
                        };

                        // forward the received op
                        if let Err(err) = self
                            .host
                            .legacy
                            .receive_ops(space.clone(), vec![op.op_data], fetch_context)
                            .await
                        {
                            match err {
                                KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                                    return Err(MetaNetTaskError::RequiredChannelClosed)
                                }
                                err => {
                                    tracing::error!(?err, "Failed to receive op");
                                }
                            }

                            // In the case of an error we don't want to attempt to `resolve_publish_pending_delegates`
                            continue;
                        }

                        // Now that the host is holding the op, remove it from the fetch pool. Any sooner and we might queue the op for fetching again.
                        // We don't need to wait for validation to complete, at least with respect to gossip, because we don't ask for unvalidated
                        // ops during gossip. (See crates/holochain/src/conductor/kitsune_host_impl/query_region_set.rs)
                        self.fetch_pool.remove(&key);

                        // trigger any delegation that is pending on having this data
                        if let Err(err) = self
                            .i_s
                            .resolve_publish_pending_delegates(space.clone(), op_hash.clone())
                            .await
                        {
                            match err {
                                KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                                    return Err(MetaNetTaskError::RequiredChannelClosed);
                                }
                                err => {
                                    tracing::error!(
                                        ?err,
                                        "Failed to send notification to resolve pending delegates"
                                    );
                                }
                            }
                        }
                    }
                }

                Ok(())
            }
            wire::Wire::MetricExchange(wire::MetricExchange { space, msgs }) => {
                if let Err(err) = self.i_s.incoming_metric_exchange(space, msgs).await {
                    tracing::error!(?err, "Metric exchange failed to send");
                    Err(err.into())
                } else {
                    Ok(())
                }
            }
            wire::Wire::PeerUnsolicited(wire::PeerUnsolicited { peer_list }) => {
                if let Err(err) = self
                    .host
                    .legacy
                    .put_agent_info_signed(PutAgentInfoSignedEvt {
                        peer_data: peer_list,
                    })
                    .await
                {
                    tracing::warn!(?err, "error processing incoming agent info unsolicited");

                    match err {
                        KitsuneP2pError::GhostError(GhostError::Disconnected) => {
                            return Err(MetaNetTaskError::RequiredChannelClosed)
                        }
                        e => {
                            tracing::error!("Failed to put agent info: {:?}", e);
                        }
                    };
                }

                Ok(())
            }
            wire::Wire::Failure(_)
            | wire::Wire::Call(_)
            | wire::Wire::CallResp(_)
            | wire::Wire::PeerGet(_)
            | wire::Wire::PeerGetResp(_)
            | wire::Wire::PeerQuery(_)
            | wire::Wire::PeerQueryResp(_) => {
                tracing::warn!("received non-notify data in a notify");
                Ok(())
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::actor::BroadcastData;
    use crate::dht_arc::DhtLocation;
    use crate::spawn::actor::fetch::FetchResponseConfig;
    use crate::spawn::actor::meta_net_task::MetaNetTask;
    use crate::spawn::actor::test_util::InternalStub;
    use crate::spawn::actor::test_util::LegacyHostStub as HostReceiverStub;
    use crate::spawn::actor::Internal;
    use crate::spawn::meta_net::{MetaNetCon, MetaNetConTest, MetaNetEvt};
    use crate::test_util::data::mk_agent_info;
    use crate::types::wire;
    use crate::wire::PushOpItem;
    use crate::{
        GossipModuleType, HostStub, KitsuneAgent, KitsuneBasis, KitsuneHost, KitsuneOpData,
    };
    use futures::channel::mpsc::{channel, Sender};
    use futures::FutureExt;
    use futures::SinkExt;
    use ghost_actor::actor_builder::GhostActorBuilder;
    use ghost_actor::{GhostControlSender, GhostSender};
    use kitsune_p2p::KitsuneBinType;
    use kitsune_p2p_block::{Block, BlockTarget, NodeBlockReason, NodeId};
    use kitsune_p2p_fetch::test_utils::{test_key_op, test_req_op, test_source, test_space};
    use kitsune_p2p_fetch::TransferMethod;
    use kitsune_p2p_fetch::{FetchPool, FetchResponseQueue};
    use kitsune_p2p_timestamp::{InclusiveTimestampInterval, Timestamp};
    use kitsune_p2p_types::bin_types::NodeCert;
    use kitsune_p2p_types::config::KitsuneP2pConfig;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    use std::time::Duration;

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_connect() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        assert_eq!(0, internal_stub.connections.read().len());

        ep_evt_send
            .send(MetaNetEvt::Connected {
                remote_url: "".to_string(),
                con: mk_test_con(),
            })
            .await
            .unwrap();

        wait_for_condition(|| !internal_stub.connections.read().is_empty())
            .await
            .expect("Timed out waiting for connection to be added");

        assert_eq!(1, internal_stub.connections.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_connect_stops_task_if_internal_sender_closes() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, _, meta_net_task_finished) =
            setup().await;

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        ep_evt_send
            .send(MetaNetEvt::Connected {
                remote_url: "".to_string(),
                con: mk_test_con(),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_disconnect() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Connected {
                remote_url: "x".to_string(),
                con: mk_test_con(),
            })
            .await
            .unwrap();

        tokio::time::timeout(Duration::from_millis(100), async {
            while internal_stub.connections.read().is_empty() {
                tokio::time::sleep(Duration::from_millis(1)).await;
            }
        })
        .await
        .expect("Timed out waiting for connection to be removed");

        ep_evt_send
            .send(MetaNetEvt::Disconnected {
                remote_url: "x".to_string(),
                con: mk_test_con(),
            })
            .await
            .unwrap();

        wait_for_condition(|| internal_stub.connections.read().is_empty())
            .await
            .expect("Timed out waiting for connection to be removed");

        assert_eq!(0, internal_stub.connections.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_disconnect_stops_task_if_internal_sender_closes() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, _, meta_net_task_finished) =
            setup().await;

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        wait_for_condition(|| !meta_net_task_finished.load(Ordering::Acquire))
            .await
            .expect("Timed out waiting for task to shut down");

        ep_evt_send
            .send(MetaNetEvt::Disconnected {
                remote_url: "".to_string(),
                con: mk_test_con(),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    // TODO no disconnect event is sent if the connection is force closed by us.
    #[tokio::test(flavor = "multi_thread")]
    async fn make_request_while_blocked() {
        let (mut ep_evt_send, _, _, _, host_stub, _, _, _) = setup().await;

        host_stub
            .block(Block::new(
                BlockTarget::Node(test_node_id(1), NodeBlockReason::DOS),
                InclusiveTimestampInterval::try_new(
                    Timestamp::now(),
                    Timestamp::now()
                        .checked_add(&Duration::from_secs(10))
                        .unwrap(),
                )
                .unwrap(),
            ))
            .await
            .unwrap();

        let con = mk_test_con_with_id(1);
        let con_state = get_con_state(&con);

        ep_evt_send
            .send(MetaNetEvt::Request {
                remote_url: "".to_string(),
                con,
                data: wire::Wire::Call(wire::Call {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: wire::WireData(vec![]),
                }),
                respond: Box::new(|_| async move {}.boxed()),
            })
            .await
            .unwrap();

        wait_for_condition(|| con_state.read().closed)
            .await
            .expect("Timed out waiting for the connection to be closed");

        assert!(con_state.read().closed);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn make_call_request() {
        let (ep_evt_send, _, _, _, _, _, _, _) = setup().await;

        let request_data = vec![2, 7];

        let call_response = do_request(
            ep_evt_send,
            wire::Wire::Call(wire::Call {
                space: test_space(1),
                to_agent: test_agent(2),
                data: wire::WireData(request_data.clone()),
            }),
        )
        .await;

        let response_data = match call_response {
            wire::Wire::CallResp(res) => res.data.to_vec(),
            _ => panic!("Unexpected response"),
        };

        // Because the stub does an echo response
        assert_eq!(request_data, response_data);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn make_call_request_handles_error() {
        let (ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        let request_data = vec![2, 7];
        let call_response = do_request(
            ep_evt_send.clone(),
            wire::Wire::Call(wire::Call {
                space: test_space(1),
                to_agent: test_agent(2),
                data: wire::WireData(request_data.clone()),
            }),
        )
        .await;

        let reason = match call_response {
            wire::Wire::Failure(f) => f.reason,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!("Other(\"a test error\")".to_string(), reason);

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn make_call_request_handles_shutdown() {
        let (ep_evt_send, _, _, host_receiver_stub, _, _, _, _) = setup().await;

        host_receiver_stub.abort();

        let request_data = vec![2, 7];
        let call_response = do_request(
            ep_evt_send,
            wire::Wire::Call(wire::Call {
                space: test_space(1),
                to_agent: test_agent(2),
                data: wire::WireData(request_data.clone()),
            }),
        )
        .await;

        let reason = match call_response {
            wire::Wire::Failure(f) => f.reason,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!("GhostError(Disconnected)".to_string(), reason);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn make_peer_get_request() {
        let (ep_evt_send, _, _, _, _, _, _, _) = setup().await;

        let call_response = do_request(
            ep_evt_send,
            wire::Wire::PeerGet(wire::PeerGet {
                space: test_space(1),
                agent: test_agent(1),
            }),
        )
        .await;

        let agent_info_signed = match call_response {
            wire::Wire::PeerGetResp(res) => res.agent_info_signed,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!(test_agent(1), agent_info_signed.unwrap().agent);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_peer_get_request_error() {
        let (ep_evt_send, _, _, _, host_stub, _, _, meta_net_task_finished) = setup().await;

        // Set up the error response so that when we make a request we get an error
        host_stub.fail_next_request();

        let call_response = do_request(
            ep_evt_send.clone(),
            wire::Wire::PeerGet(wire::PeerGet {
                space: test_space(1),
                agent: test_agent(1),
            }),
        )
        .await;

        let reason = match call_response {
            wire::Wire::Failure(f) => f.reason,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!("Error getting agent: \"error for unimplemented KitsuneHost test behavior: method get_agent_info_signed of HostStub\"".to_string(), reason);

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn make_peer_query_request() {
        let (ep_evt_send, _, _, _, _, _, _, _) = setup().await;

        let response = do_request(
            ep_evt_send,
            wire::Wire::PeerQuery(wire::PeerQuery {
                space: test_space(1),
                basis_loc: DhtLocation::new(1),
            }),
        )
        .await;

        let peer_list = match response {
            wire::Wire::PeerQueryResp(r) => r.peer_list,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!(8, peer_list.len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn handle_peer_query_request_error() {
        let (ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        // Set up the error response so that when we make a request we get an error
        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        let response = do_request(
            ep_evt_send.clone(),
            wire::Wire::PeerQuery(wire::PeerQuery {
                space: test_space(1),
                basis_loc: DhtLocation::new(1),
            }),
        )
        .await;

        let reason = match response {
            wire::Wire::Failure(f) => f.reason,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!(
            "Error querying agents: Other(\"a test error\")".to_string(),
            reason
        );

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn ignores_unexpected_request_payload() {
        let (mut ep_evt_send, _, _, _, _, _, _, meta_net_task_finished) = setup().await;

        // Send a request but don't listen for a response
        ep_evt_send
            .send(MetaNetEvt::Request {
                remote_url: "".to_string(),
                con: mk_test_con_with_id(1),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(1),
                    data: BroadcastData::User(test_agent(2).to_vec()),
                }),
                respond: Box::new(|_| async move {}.boxed()),
            })
            .await
            .unwrap();

        // Now check that we can still use the task to send/receive messages.
        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    // TODO no disconnect event is sent if the connection is force closed by us.
    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_while_blocked() {
        let (mut ep_evt_send, _, _, _, host_stub, _, _, _) = setup().await;

        host_stub
            .block(Block::new(
                BlockTarget::Node(test_node_id(1), NodeBlockReason::DOS),
                InclusiveTimestampInterval::try_new(
                    Timestamp::now(),
                    Timestamp::now()
                        .checked_add(&Duration::from_secs(10))
                        .unwrap(),
                )
                .unwrap(),
            ))
            .await
            .unwrap();

        let con = mk_test_con_with_id(1);
        let con_state = get_con_state(&con);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con,
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| con_state.read().closed)
            .await
            .expect("Timed out waiting for the connection to be closed");

        assert!(con_state.read().closed);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_publish() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| !internal_stub.incoming_publish_calls.read().is_empty())
            .await
            .expect("Timed out waiting for a publish call");

        let args = internal_stub
            .incoming_publish_calls
            .read()
            .first()
            .unwrap()
            .clone();
        assert_eq!(test_space(1), args.0);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_publish_fails_to_forward() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, meta_net_task_finished) = setup().await;

        internal_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for a publish call error");

        assert_eq!(
            1,
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(internal_stub.incoming_publish_calls.read().is_empty());

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_publish_handles_shutdown() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, _, met_net_task_finished) =
            setup().await;

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(met_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_user() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            !internal_stub
                .incoming_delegate_broadcast_calls
                .read()
                .is_empty()
        })
        .await
        .expect("Timed out waiting for a publish call");

        let args = internal_stub
            .incoming_delegate_broadcast_calls
            .read()
            .first()
            .unwrap()
            .clone();
        assert_eq!(BroadcastData::User(test_agent(5).to_vec()), args.5);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_user_fails_to_forward() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, meta_net_task_finished) = setup().await;

        internal_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for a publish call");

        assert_eq!(
            1,
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(internal_stub
            .incoming_delegate_broadcast_calls
            .read()
            .is_empty());

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_agent_info() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            !internal_stub
                .incoming_delegate_broadcast_calls
                .read()
                .is_empty()
        })
        .await
        .expect("Timed out waiting for a delegate broadcast");

        let args = internal_stub
            .incoming_delegate_broadcast_calls
            .read()
            .first()
            .unwrap()
            .clone();
        assert_eq!(BroadcastData::AgentInfo(mk_agent_info(6).await), args.5);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_delegate_broadcast_agent_info_fails_to_forward() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, meta_net_task_finished) = setup().await;

        internal_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::DelegateBroadcast(wire::DelegateBroadcast {
                    space: test_space(1),
                    basis: Arc::new(KitsuneBasis::new(vec![0; 36])),
                    to_agent: test_agent(2),
                    mod_idx: 0,
                    mod_cnt: 0,
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for an error");

        assert_eq!(
            1,
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(internal_stub
            .incoming_delegate_broadcast_calls
            .read()
            .is_empty());

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_publish() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| !internal_stub.incoming_publish_calls.read().is_empty())
            .await
            .expect("Timed out waiting for a publish broadcast");

        let args = internal_stub
            .incoming_publish_calls
            .read()
            .first()
            .unwrap()
            .clone();
        assert_eq!(test_space(1), args.0);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_publish_fails_to_forward() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, meta_net_task_finished) = setup().await;

        internal_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for an error");

        assert_eq!(
            1,
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(internal_stub.incoming_publish_calls.read().is_empty());

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_publish_handles_shutdown() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, _, meta_net_task_finished) =
            setup().await;

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::Publish {
                        source: test_agent(5),
                        transfer_method: TransferMethod::Publish,
                        op_hash_list: vec![],
                        context: Default::default(),
                    },
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_user() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| !host_receiver_stub.notify_calls.read().is_empty())
            .await
            .expect("Timed out waiting for a notify");

        assert_eq!(1, host_receiver_stub.notify_calls.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_user_fails_to_forward() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for an error");

        assert_eq!(
            1,
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(host_receiver_stub.notify_calls.read().is_empty());
        assert!(!meta_net_task_finished.load(Ordering::Acquire));

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_user_handles_shutdown() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub.abort();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::User(test_agent(5).to_vec()),
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_agent_info() {
        let (mut ep_evt_send, _, _, mut host_receiver_stub, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        let agent_info_evt = host_receiver_stub
            .next_event(Duration::from_millis(1000))
            .await;
        assert_eq!(1, agent_info_evt.peer_data.len());
        assert_eq!(
            mk_agent_info(6).await,
            agent_info_evt.peer_data.first().unwrap().clone()
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_agent_info_fails_to_forward() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for a publish call");

        assert_eq!(
            1,
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );
        assert!(host_receiver_stub
            .put_agent_info_signed_calls
            .read()
            .is_empty());

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_broadcast_agent_info_handles_shutdown() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub.abort();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_gossip() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Gossip(wire::Gossip {
                    space: test_space(1),
                    data: wire::WireData(vec![1, 4, 6]),
                    module: GossipModuleType::ShardedRecent,
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| !internal_stub.incoming_gossip_calls.read().is_empty())
            .await
            .expect("Timed out waiting for incoming gossip");

        assert_eq!(1, internal_stub.incoming_gossip_calls.read().len());
        assert_eq!(
            vec![1, 4, 6],
            internal_stub
                .incoming_gossip_calls
                .read()
                .first()
                .unwrap()
                .3
                .to_vec()
        );
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_gossip_handles_error() {
        let (mut ep_evt_send, internal_stub, _, _, _, _, _, meta_net_task_finished) = setup().await;

        // Set up an error
        internal_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Gossip(wire::Gossip {
                    space: test_space(1),
                    data: wire::WireData(vec![1, 4, 6]),
                    module: GossipModuleType::ShardedRecent,
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| {
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for an error");

        assert_eq!(
            1,
            internal_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_gossip_handles_shutdown() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, _, meta_net_task_finished) =
            setup().await;

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Gossip(wire::Gossip {
                    space: test_space(1),
                    data: wire::WireData(vec![1, 4, 6]),
                    module: GossipModuleType::ShardedRecent,
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_fetch_op() {
        let (mut ep_evt_send, _, _, _, _, fetch_response_queue, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::FetchOp(wire::FetchOp {
                    fetch_list: vec![(test_space(1), vec![test_key_op(1), test_key_op(2)])],
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| fetch_response_queue.bytes_sent.load(Ordering::Acquire) == 6)
            .await
            .expect("Timed out waiting for op fetch");

        assert_eq!(6, fetch_response_queue.bytes_sent.load(Ordering::Acquire));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_fetch_op_fail_independently() {
        let (
            mut ep_evt_send,
            _,
            _,
            host_receiver_stub,
            _,
            fetch_response_queue,
            _,
            meta_net_task_finished,
        ) = setup().await;

        // The first call will fail, subsequent calls succeed
        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::FetchOp(wire::FetchOp {
                    fetch_list: vec![
                        (test_space(1), vec![test_key_op(1), test_key_op(2)]),
                        (test_space(2), vec![test_key_op(3), test_key_op(4)]),
                    ],
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| fetch_response_queue.bytes_sent.load(Ordering::Acquire) == 6)
            .await
            .expect("Timed out waiting for op fetch");

        // The list for the first space does not get sent due to an error fetching its op data but the second does succeed and gets sent
        assert_eq!(6, fetch_response_queue.bytes_sent.load(Ordering::Acquire));

        verify_task_live(ep_evt_send, meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_fetch_op_handles_shutdown() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub.abort();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::FetchOp(wire::FetchOp {
                    fetch_list: vec![(test_space(1), vec![test_key_op(1), test_key_op(2)])],
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_push_op_data() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, fetch_pool, _) = setup().await;

        fetch_pool.push(test_req_op(1, None, test_source(2)));
        assert_eq!(1, fetch_pool.len());

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PushOpData(wire::PushOpData {
                    op_data_list: vec![(
                        test_space(1),
                        vec![PushOpItem {
                            op_data: KitsuneOpData::new(vec![1, 4, 10]),
                            region: None,
                        }],
                    )],
                }),
            })
            .await
            .unwrap();

        wait_for_condition(|| fetch_pool.is_empty())
            .await
            .expect("Timed out waiting for op push");

        assert!(fetch_pool.is_empty());
        assert_eq!(1, host_receiver_stub.receive_ops_calls.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_push_op_data_fails_independently_on_op_hash_error() {
        let (mut ep_evt_send, _, _, host_receiver_stub, host_stub, _, fetch_pool, _) =
            setup().await;

        host_stub.fail_next_request();

        fetch_pool.push(test_req_op(0, None, test_source(2)));
        assert_eq!(1, fetch_pool.len());

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PushOpData(wire::PushOpData {
                    op_data_list: vec![
                        (
                            test_space(1),
                            vec![PushOpItem {
                                op_data: KitsuneOpData::new(vec![0, 4, 10]),
                                region: None,
                            }],
                        ),
                        (
                            test_space(1),
                            vec![PushOpItem {
                                op_data: KitsuneOpData::new(vec![0, 3, 90]),
                                region: None,
                            }],
                        ),
                    ],
                }),
            })
            .await
            .unwrap();

        // Check that there was an error
        wait_for_condition(|| host_stub.get_fail_count() != 0)
            .await
            .expect("Timed out waiting for an error");

        assert_eq!(1, host_stub.get_fail_count());

        // and also a successful op push
        wait_for_condition(|| {
            fetch_pool.is_empty() && !host_receiver_stub.receive_ops_calls.read().is_empty()
        })
        .await
        .expect("Timed out waiting for op push");

        assert!(fetch_pool.is_empty());
        assert_eq!(1, host_receiver_stub.receive_ops_calls.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_push_op_data_fails_independently_on_receive_ops_error() {
        holochain_trace::test_run();

        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, fetch_pool, _) = setup().await;

        host_receiver_stub
            .respond_with_error
            .store(true, Ordering::SeqCst);

        fetch_pool.push(test_req_op(0, None, test_source(2)));
        fetch_pool.push(test_req_op(1, None, test_source(3)));
        assert_eq!(2, fetch_pool.len());

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PushOpData(wire::PushOpData {
                    op_data_list: vec![
                        (
                            test_space(1),
                            vec![PushOpItem {
                                op_data: KitsuneOpData::new(vec![0, 4, 10]),
                                region: None,
                            }],
                        ),
                        (
                            test_space(1),
                            vec![PushOpItem {
                                op_data: KitsuneOpData::new(vec![1, 3, 90]),
                                region: None,
                            }],
                        ),
                    ],
                }),
            })
            .await
            .unwrap();

        // Check that there was an error
        wait_for_condition(|| {
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
                != 0
        })
        .await
        .expect("Timed out waiting for an error");

        assert_eq!(
            1,
            host_receiver_stub
                .respond_with_error_count
                .load(Ordering::Acquire)
        );

        // Manually drop the item from the pool that we failed to receive
        fetch_pool.remove(&test_key_op(0));

        // and also a successful op push
        wait_for_condition(|| {
            fetch_pool.is_empty() && !host_receiver_stub.receive_ops_calls.read().is_empty()
        })
        .await
        .expect("Timed out waiting for op push");

        assert!(fetch_pool.is_empty());
        assert_eq!(1, host_receiver_stub.receive_ops_calls.read().len());
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_push_op_data_handles_shutdown_on_receive_ops() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, fetch_pool, meta_net_task_finished) =
            setup().await;

        fetch_pool.push(test_req_op(0, None, test_source(2)));
        assert_eq!(1, fetch_pool.len());

        host_receiver_stub.abort();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PushOpData(wire::PushOpData {
                    op_data_list: vec![(
                        test_space(1),
                        vec![PushOpItem {
                            op_data: KitsuneOpData::new(vec![0, 4, 10]),
                            region: None,
                        }],
                    )],
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_push_op_data_handles_shutdown_on_resolve() {
        let (mut ep_evt_send, _, internal_sender, _, _, _, fetch_pool, meta_net_task_finished) =
            setup().await;

        fetch_pool.push(test_req_op(0, None, test_source(2)));
        assert_eq!(1, fetch_pool.len());

        internal_sender
            .ghost_actor_shutdown_immediate()
            .await
            .unwrap();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PushOpData(wire::PushOpData {
                    op_data_list: vec![(
                        test_space(1),
                        vec![PushOpItem {
                            op_data: KitsuneOpData::new(vec![0, 4, 10]),
                            region: None,
                        }],
                    )],
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_peer_unsolicited() {
        let (mut ep_evt_send, _, _, mut host_receiver_stub, _, _, _, _) = setup().await;

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PeerUnsolicited(wire::PeerUnsolicited {
                    peer_list: vec![mk_agent_info(1).await, mk_agent_info(2).await],
                }),
            })
            .await
            .unwrap();

        // Wait for the agent infos to be received
        let peers = host_receiver_stub
            .next_event(Duration::from_secs(1))
            .await
            .peer_data;

        assert_eq!(mk_agent_info(1).await, peers[0]);
        assert_eq!(mk_agent_info(2).await, peers[1]);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn send_notify_peer_unsolicited_handles_shutdown() {
        let (mut ep_evt_send, _, _, host_receiver_stub, _, _, _, meta_net_task_finished) =
            setup().await;

        host_receiver_stub.abort();

        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::PeerUnsolicited(wire::PeerUnsolicited {
                    peer_list: vec![mk_agent_info(1).await, mk_agent_info(2).await],
                }),
            })
            .await
            .unwrap();

        wait_and_assert_shutdown(meta_net_task_finished).await;
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn ignores_unexpected_notify_payload() {
        let (mut ep_evt_send, _, _, mut host_receiver_stub, _, _, _, _) = setup().await;

        // Send a notification with a payload that is not expected.
        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con_with_id(1),
                data: wire::Wire::PeerQuery(wire::PeerQuery {
                    space: test_space(1),
                    basis_loc: DhtLocation::new(1),
                }),
            })
            .await
            .unwrap();

        // Now check that we can still use the task to send notify messages.
        ep_evt_send
            .send(MetaNetEvt::Notify {
                remote_url: "".to_string(),
                con: mk_test_con(),
                data: wire::Wire::Broadcast(wire::Broadcast {
                    space: test_space(1),
                    to_agent: test_agent(2),
                    data: BroadcastData::AgentInfo(mk_agent_info(6).await),
                }),
            })
            .await
            .unwrap();

        let agent_info_evt = host_receiver_stub
            .next_event(Duration::from_millis(1000))
            .await;
        assert_eq!(1, agent_info_evt.peer_data.len());
        assert_eq!(
            mk_agent_info(6).await,
            agent_info_evt.peer_data.first().unwrap().clone()
        );
    }

    async fn setup() -> (
        Sender<MetaNetEvt>,
        InternalStub,
        GhostSender<Internal>,
        HostReceiverStub,
        Arc<HostStub>,
        FetchResponseQueue<FetchResponseConfig>,
        FetchPool,
        Arc<AtomicBool>,
    ) {
        let task = InternalStub::new();

        let builder = GhostActorBuilder::new();

        let internal_sender = builder
            .channel_factory()
            .create_channel::<Internal>()
            .await
            .unwrap();

        let (host_sender, host_receiver) = channel(10);
        let host_receiver_stub = HostReceiverStub::start(host_receiver);

        tokio::spawn(builder.spawn(task.clone()));

        let host_stub = HostStub::new();

        let fetch_pool = FetchPool::new_bitwise_or();

        let fetch_response_queue =
            FetchResponseQueue::new(FetchResponseConfig::new(Default::default()));

        let (ep_evt_send, ep_evt_rcv) = channel(10);

        let meta_net_task = MetaNetTask::new(
            host_stub.clone().legacy(host_sender),
            KitsuneP2pConfig::mem(),
            fetch_pool.clone(),
            fetch_response_queue.clone(),
            ep_evt_rcv,
            internal_sender.clone(),
        );
        let meta_net_task_finished = meta_net_task.is_finished.clone();

        meta_net_task.spawn();

        (
            ep_evt_send,
            task,
            internal_sender,
            host_receiver_stub,
            host_stub,
            fetch_response_queue,
            fetch_pool,
            meta_net_task_finished,
        )
    }

    async fn do_request(mut ep_evt_send: Sender<MetaNetEvt>, data: wire::Wire) -> wire::Wire {
        let (send_res, read_res) = futures::channel::oneshot::channel();

        ep_evt_send
            .send(MetaNetEvt::Request {
                remote_url: "".to_string(),
                con: mk_test_con_with_id(1),
                data,
                respond: Box::new(|r| {
                    async move {
                        send_res.send(r).unwrap();
                    }
                    .boxed()
                }),
            })
            .await
            .unwrap();

        tokio::time::timeout(Duration::from_secs(1), read_res)
            .await
            .expect("Timed out while waiting for a response")
            .unwrap()
    }

    async fn verify_task_live(
        ep_evt_send: Sender<MetaNetEvt>,
        meta_net_task_finished: Arc<AtomicBool>,
    ) {
        assert!(!meta_net_task_finished.load(Ordering::Acquire));

        let response = do_request(
            ep_evt_send,
            wire::Wire::PeerQuery(wire::PeerQuery {
                space: test_space(1),
                basis_loc: DhtLocation::new(1),
            }),
        )
        .await;

        let peer_list = match response {
            wire::Wire::PeerQueryResp(r) => r.peer_list,
            r => panic!("Unexpected response - {:?}", r),
        };

        assert_eq!(8, peer_list.len());
        assert!(!meta_net_task_finished.load(Ordering::Acquire));
    }

    async fn wait_and_assert_shutdown(meta_net_task_finished: Arc<AtomicBool>) {
        wait_for_condition(|| meta_net_task_finished.load(Ordering::Acquire))
            .await
            .expect("Timed out waiting for shutdown");

        assert!(meta_net_task_finished.load(Ordering::Acquire));
    }

    async fn wait_for_condition(
        cond: impl Fn() -> bool,
    ) -> Result<(), tokio::time::error::Elapsed> {
        tokio::time::timeout(Duration::from_millis(1000), async {
            while !cond() {
                tokio::time::sleep(Duration::from_millis(1)).await;
            }
        })
        .await
    }

    fn mk_test_con() -> MetaNetCon {
        MetaNetCon::Test {
            state: Default::default(),
        }
    }

    fn mk_test_con_with_id(id: u8) -> MetaNetCon {
        MetaNetCon::Test {
            state: Arc::new(parking_lot::RwLock::new(MetaNetConTest::new_with_id(id))),
        }
    }

    fn test_node_id(i: u8) -> NodeId {
        NodeCert::from(Arc::new(vec![i; 32].try_into().unwrap()))
    }

    fn test_agent(i: u8) -> Arc<KitsuneAgent> {
        Arc::new(KitsuneAgent::new(vec![i; 32]))
    }

    fn get_con_state(con: &MetaNetCon) -> Arc<parking_lot::RwLock<MetaNetConTest>> {
        match con {
            MetaNetCon::Test { state } => state.clone(),
            _ => panic!("Not a test con"),
        }
    }
}



================================================
File: crates/kitsune_p2p/kitsune_p2p/src/spawn/actor/space.rs
================================================
use super::*;
use crate::metrics::*;
use crate::types::gossip::GossipModule;
use ghost_actor::dependencies::tracing;
use kitsune_p2p_bootstrap_client::BootstrapNet;
use kitsune_p2p_fetch::FetchPool;
use kitsune_p2p_types::agent_info::AgentInfoSigned;
use kitsune_p2p_types::config::KitsuneP2pConfig;
use kitsune_p2p_types::dht::arq::ArqSize;
use kitsune_p2p_types::dht::prelude::*;
use kitsune_p2p_types::dht_arc::{DhtArcRange, DhtArcSet};
use kitsune_p2p_types::tx_utils::TxUrl;
use std::collections::{HashMap, HashSet};
use std::sync::atomic::AtomicBool;
use url2::Url2;

/// How often to record historical metrics
/// (currently once per hour)
const HISTORICAL_METRIC_RECORD_FREQ_MS: u64 = 1000 * 60 * 60;

mod metric_exchange;
use metric_exchange::*;

mod agent_info_update;
mod bootstrap_task;
mod rpc_multi_logic;

type KSpace = Arc<KitsuneSpace>;
type KAgent = Arc<KitsuneAgent>;
type KBasis = Arc<KitsuneBasis>;
type VecMXM = Vec<MetricExchangeMsg>;
pub(crate) type WireConHnd = MetaNetCon;
type Payload = Box<[u8]>;
type OpHashList = Vec<OpHashSized>;
type MaybeDelegate = Option<(KBasis, u32, u32)>;

ghost_actor::ghost_chan! {
    #[allow(clippy::too_many_arguments)]
    pub(crate) chan SpaceInternal<crate::KitsuneP2pError> {
        /// Notification that we have a new address to be identified at
        fn new_address(local_url: String) -> ();

        /// List online agents that claim to be covering a basis hash
        fn list_online_agents_for_basis_hash(space: KSpace, from_agent: KAgent, basis: KBasis) -> HashSet<KAgent>;

        // TODO document these properly so the difference between update single and update is clear

        /// Update / publish our agent info
        fn update_agent_info() -> ();

        /// Update / publish a single agent info
        fn update_single_agent_info(agent: KAgent) -> ();

        // TODO what about this, this is different again and not properly documented
        /// Update / publish a single agent info
        fn publish_agent_info_signed(input: PutAgentInfoSignedEvt) -> ();

        fn get_all_local_joined_agent_infos() -> Vec<AgentInfoSigned>;

        /// see if an agent is locally joined
        fn is_agent_local(agent: KAgent) -> bool;

        /// Update the arc of a local agent.
        fn update_agent_arc(agent: KAgent, arq: Arq) -> ();

        /// Incoming Delegate Broadcast
        /// We are being requested to delegate a broadcast to our neighborhood
        /// on behalf of an author. `mod_idx` / `mod_cnt` inform us which
        /// neighbors we are responsible for.
        /// (See comments in actual method impl for more detail.)
        fn incoming_delegate_broadcast(
            space: KSpace,
            basis: KBasis,
            to_agent: KAgent,
            mod_idx: u32,
            mod_cnt: u32,
            data: BroadcastData,
        ) -> ();

        /// This should be invoked instead of incoming_delegate_broadcast
        /// in the case of a publish data variant. It will, in turn, call
        /// into incoming_delegate_broadcast once we have the data to act
        /// as a fetch responder for the op data.
        fn incoming_publish(
            space: KSpace,
            to_agent: KAgent,
            source: KAgent,
            transfer_method: kitsune_p2p_fetch::TransferMethod,
            op_hash_list: OpHashList,
            context: kitsune_p2p_fetch::FetchContext,
            maybe_delegate: MaybeDelegate,
        ) -> ();

        /// Send a raw notify.
        fn notify(
            to_agent: KAgent,
            data: wire::Wire,
        ) -> ();

        /// We just received data for an op_hash. Check if we had a pending
        /// delegation action we need to continue now that we have the data.
        fn resolve_publish_pending_delegates(space: KSpace, op_hash: KOpHash) -> ();

        /// Incoming Gossip
        fn incoming_gossip(space: KSpace, con: MetaNetCon, remote_url: String, data: Payload, module_type: crate::types::gossip::GossipModuleType) -> ();

        /// Incoming Metric Exchange
        fn incoming_metric_exchange(space: KSpace, msgs: VecMXM) -> ();

        /// New Con
        fn new_con(url: String, con: WireConHnd) -> ();

        /// Del Con
        fn del_con(url: String) -> ();
    }
}

#[allow(clippy::too_many_arguments)]
pub(crate) async fn spawn_space(
    space: Arc<KitsuneSpace>,
    ep_hnd: MetaNet,
    host: HostApi,
    config: Arc<KitsuneP2pConfig>,
    bootstrap_net: BootstrapNet,
    bandwidth_throttles: BandwidthThrottles,
    parallel_notify_permit: Arc<tokio::sync::Semaphore>,
    fetch_pool: FetchPool,
    local_url: Arc<std::sync::Mutex<Option<String>>>,
) -> KitsuneP2pResult<(
    ghost_actor::GhostSender<KitsuneP2p>,
    ghost_actor::GhostSender<SpaceInternal>,
    KitsuneP2pEventReceiver,
)> {
    let (evt_send, evt_recv) = futures::channel::mpsc::channel(10);

    let builder = ghost_actor::actor_builder::GhostActorBuilder::new();

    let i_s = builder
        .channel_factory()
        .create_channel::<SpaceInternal>()
        .await?;

    let sender = builder
        .channel_factory()
        .create_channel::<KitsuneP2p>()
        .await?;

    let host = HostApiLegacy::new(host, evt_send);

    tokio::task::spawn(builder.spawn(Space::new(
        space,
        i_s.clone(),
        host,
        ep_hnd,
        config,
        bootstrap_net,
        bandwidth_throttles,
        parallel_notify_permit,
        fetch_pool,
        local_url,
    )));

    Ok((sender, i_s, evt_recv))
}

impl ghost_actor::GhostHandler<SpaceInternal> for Space {}

impl SpaceInternalHandler for Space {
    fn handle_new_address(&mut self, local_url: String) -> SpaceInternalHandlerResult<()> {
        // shut down open gossip rounds, since we will
        // now be identified differently
        for agent in self.local_joined_agents.keys() {
            for module in self.gossip_mod.values() {
                module.local_agent_leave(agent.clone());
                module.local_agent_join(agent.clone());
            }
        }
        *self.ro_inner.local_url.lock().unwrap() = Some(local_url);
        self.handle_update_agent_info()
    }

    fn handle_list_online_agents_for_basis_hash(
        &mut self,
        _space: Arc<KitsuneSpace>,
        _from_agent: Arc<KitsuneAgent>,
        // during short-circuit / full-sync mode,
        // we're ignoring the basis_hash and just returning everyone.
        _basis: Arc<KitsuneBasis>,
    ) -> SpaceInternalHandlerResult<HashSet<Arc<KitsuneAgent>>> {
        let mut res: HashSet<Arc<KitsuneAgent>> =
            self.local_joined_agents.keys().cloned().collect();
        let all_peers_fut = self
            .host_api
            .legacy
            .query_agents(QueryAgentsEvt::new(self.space.clone()));
        Ok(async move {
            for peer in all_peers_fut.await? {
                res.insert(peer.agent.clone());
            }
            Ok(res)
        }
        .boxed()
        .into())
    }

    fn handle_update_agent_info(&mut self) -> SpaceInternalHandlerResult<()> {
        let local_url = match self.ro_inner.get_local_url() {
            None => return Ok(async move { Ok(()) }.boxed().into()),
            Some(local_url) => local_url,
        };
        let space = self.space.clone();
        let mut mdns_handles = self.mdns_handles.clone();
        let mut agent_list = Vec::with_capacity(self.local_joined_agents.len());
        for agent in self.local_joined_agents.keys().cloned() {
            let arq = self.get_agent_arq(&agent);
            agent_list.push((agent, arq));
        }
        let bootstrap_net = self.ro_inner.bootstrap_net;
        let evt_sender = self.host_api.legacy.clone();
        let bootstrap_service = self.config.bootstrap_service.clone();
        let expires_after = self.config.tuning_params.agent_info_expires_after_ms as u64;
        #[cfg(feature = "unstable-sharding")]
